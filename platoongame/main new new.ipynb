{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from atari_network import DQN\n",
    "from atari_wrapper import make_atari_env\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tianshou.data import Collector, VectorReplayBuffer\n",
    "from tianshou.policy import DiscreteSACPolicy, ICMPolicy\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from tianshou.utils import TensorboardLogger, WandbLogger\n",
    "from tianshou.utils.net.discrete import Actor, Critic, IntrinsicCuriosityModule\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--task\", type=str, default=\"PongNoFrameskip-v4\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=4213)\n",
    "    parser.add_argument(\"--scale-obs\", type=int, default=0)\n",
    "    parser.add_argument(\"--buffer-size\", type=int, default=100000)\n",
    "    parser.add_argument(\"--actor-lr\", type=float, default=1e-5)\n",
    "    parser.add_argument(\"--critic-lr\", type=float, default=1e-5)\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.99)\n",
    "    parser.add_argument(\"--n-step\", type=int, default=3)\n",
    "    parser.add_argument(\"--tau\", type=float, default=0.005)\n",
    "    parser.add_argument(\"--alpha\", type=float, default=0.05)\n",
    "    parser.add_argument(\"--auto-alpha\", action=\"store_true\", default=False)\n",
    "    parser.add_argument(\"--alpha-lr\", type=float, default=3e-4)\n",
    "    parser.add_argument(\"--epoch\", type=int, default=100)\n",
    "    parser.add_argument(\"--step-per-epoch\", type=int, default=100000)\n",
    "    parser.add_argument(\"--step-per-collect\", type=int, default=10)\n",
    "    parser.add_argument(\"--update-per-step\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=64)\n",
    "    parser.add_argument(\"--hidden-size\", type=int, default=512)\n",
    "    parser.add_argument(\"--training-num\", type=int, default=10)\n",
    "    parser.add_argument(\"--test-num\", type=int, default=10)\n",
    "    parser.add_argument(\"--rew-norm\", type=int, default=False)\n",
    "    parser.add_argument(\"--logdir\", type=str, default=\"log\")\n",
    "    parser.add_argument(\"--render\", type=float, default=0.)\n",
    "    parser.add_argument(\n",
    "        \"--device\", type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "    parser.add_argument(\"--frames-stack\", type=int, default=4)\n",
    "    parser.add_argument(\"--resume-path\", type=str, default=None)\n",
    "    parser.add_argument(\"--resume-id\", type=str, default=None)\n",
    "    parser.add_argument(\n",
    "        \"--logger\",\n",
    "        type=str,\n",
    "        default=\"tensorboard\",\n",
    "        choices=[\"tensorboard\", \"wandb\"],\n",
    "    )\n",
    "    parser.add_argument(\"--wandb-project\", type=str, default=\"atari.benchmark\")\n",
    "    parser.add_argument(\n",
    "        \"--watch\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"watch the play of pre-trained policy only\"\n",
    "    )\n",
    "    parser.add_argument(\"--save-buffer-name\", type=str, default=None)\n",
    "    parser.add_argument(\n",
    "        \"--icm-lr-scale\",\n",
    "        type=float,\n",
    "        default=0.,\n",
    "        help=\"use intrinsic curiosity module with this lr scale\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--icm-reward-scale\",\n",
    "        type=float,\n",
    "        default=0.01,\n",
    "        help=\"scaling factor for intrinsic curiosity reward\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--icm-forward-loss-weight\",\n",
    "        type=float,\n",
    "        default=0.2,\n",
    "        help=\"weight for the forward model loss in ICM\"\n",
    "    )\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def test_discrete_sac(args=get_args()):\n",
    "    env, train_envs, test_envs = make_atari_env(\n",
    "        args.task,\n",
    "        args.seed,\n",
    "        args.training_num,\n",
    "        args.test_num,\n",
    "        scale=args.scale_obs,\n",
    "        frame_stack=args.frames_stack,\n",
    "    )\n",
    "    args.state_shape = env.observation_space.shape or env.observation_space.n\n",
    "    args.action_shape = env.action_space.shape or env.action_space.n\n",
    "    # should be N_FRAMES x H x W\n",
    "    print(\"Observations shape:\", args.state_shape)\n",
    "    print(\"Actions shape:\", args.action_shape)\n",
    "    # seed\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    # define model\n",
    "    net = DQN(\n",
    "        *args.state_shape,\n",
    "        args.action_shape,\n",
    "        device=args.device,\n",
    "        features_only=True,\n",
    "        output_dim=args.hidden_size\n",
    "    )\n",
    "    actor = Actor(net, args.action_shape, device=args.device, softmax_output=False)\n",
    "    actor_optim = torch.optim.Adam(actor.parameters(), lr=args.actor_lr)\n",
    "    critic1 = Critic(net, last_size=args.action_shape, device=args.device)\n",
    "    critic1_optim = torch.optim.Adam(critic1.parameters(), lr=args.critic_lr)\n",
    "    critic2 = Critic(net, last_size=args.action_shape, device=args.device)\n",
    "    critic2_optim = torch.optim.Adam(critic2.parameters(), lr=args.critic_lr)\n",
    "\n",
    "    # define policy\n",
    "    if args.auto_alpha:\n",
    "        target_entropy = 0.98 * np.log(np.prod(args.action_shape))\n",
    "        log_alpha = torch.zeros(1, requires_grad=True, device=args.device)\n",
    "        alpha_optim = torch.optim.Adam([log_alpha], lr=args.alpha_lr)\n",
    "        args.alpha = (target_entropy, log_alpha, alpha_optim)\n",
    "\n",
    "    policy = DiscreteSACPolicy(\n",
    "        actor,\n",
    "        actor_optim,\n",
    "        critic1,\n",
    "        critic1_optim,\n",
    "        critic2,\n",
    "        critic2_optim,\n",
    "        args.tau,\n",
    "        args.gamma,\n",
    "        args.alpha,\n",
    "        estimation_step=args.n_step,\n",
    "        reward_normalization=args.rew_norm,\n",
    "    ).to(args.device)\n",
    "    if args.icm_lr_scale > 0:\n",
    "        feature_net = DQN(\n",
    "            *args.state_shape, args.action_shape, args.device, features_only=True\n",
    "        )\n",
    "        action_dim = np.prod(args.action_shape)\n",
    "        feature_dim = feature_net.output_dim\n",
    "        icm_net = IntrinsicCuriosityModule(\n",
    "            feature_net.net,\n",
    "            feature_dim,\n",
    "            action_dim,\n",
    "            hidden_sizes=[args.hidden_size],\n",
    "            device=args.device,\n",
    "        )\n",
    "        icm_optim = torch.optim.Adam(icm_net.parameters(), lr=args.actor_lr)\n",
    "        policy = ICMPolicy(\n",
    "            policy, icm_net, icm_optim, args.icm_lr_scale, args.icm_reward_scale,\n",
    "            args.icm_forward_loss_weight\n",
    "        ).to(args.device)\n",
    "    # load a previous policy\n",
    "    if args.resume_path:\n",
    "        policy.load_state_dict(torch.load(args.resume_path, map_location=args.device))\n",
    "        print(\"Loaded agent from: \", args.resume_path)\n",
    "    # replay buffer: `save_last_obs` and `stack_num` can be removed together\n",
    "    # when you have enough RAM\n",
    "    buffer = VectorReplayBuffer(\n",
    "        args.buffer_size,\n",
    "        buffer_num=len(train_envs),\n",
    "        ignore_obs_next=True,\n",
    "        save_only_last_obs=True,\n",
    "        stack_num=args.frames_stack,\n",
    "    )\n",
    "    # collector\n",
    "    train_collector = Collector(policy, train_envs, buffer, exploration_noise=True)\n",
    "    test_collector = Collector(policy, test_envs, exploration_noise=True)\n",
    "\n",
    "    # log\n",
    "    now = datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "    args.algo_name = \"discrete_sac_icm\" if args.icm_lr_scale > 0 else \"discrete_sac\"\n",
    "    log_name = os.path.join(args.task, args.algo_name, str(args.seed), now)\n",
    "    log_path = os.path.join(args.logdir, log_name)\n",
    "\n",
    "    # logger\n",
    "    if args.logger == \"wandb\":\n",
    "        logger = WandbLogger(\n",
    "            save_interval=1,\n",
    "            name=log_name.replace(os.path.sep, \"__\"),\n",
    "            run_id=args.resume_id,\n",
    "            config=args,\n",
    "            project=args.wandb_project,\n",
    "        )\n",
    "    writer = SummaryWriter(log_path)\n",
    "    writer.add_text(\"args\", str(args))\n",
    "    if args.logger == \"tensorboard\":\n",
    "        logger = TensorboardLogger(writer)\n",
    "    else:  # wandb\n",
    "        logger.load(writer)\n",
    "\n",
    "    def save_best_fn(policy):\n",
    "        torch.save(policy.state_dict(), os.path.join(log_path, \"policy.pth\"))\n",
    "\n",
    "    def stop_fn(mean_rewards):\n",
    "        if env.spec.reward_threshold:\n",
    "            return mean_rewards >= env.spec.reward_threshold\n",
    "        elif \"Pong\" in args.task:\n",
    "            return mean_rewards >= 20\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def save_checkpoint_fn(epoch, env_step, gradient_step):\n",
    "        # see also: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "        ckpt_path = os.path.join(log_path, \"checkpoint.pth\")\n",
    "        torch.save({\"model\": policy.state_dict()}, ckpt_path)\n",
    "        return ckpt_path\n",
    "\n",
    "    # watch agent's performance\n",
    "    def watch():\n",
    "        print(\"Setup test envs ...\")\n",
    "        policy.eval()\n",
    "        test_envs.seed(args.seed)\n",
    "        if args.save_buffer_name:\n",
    "            print(f\"Generate buffer with size {args.buffer_size}\")\n",
    "            buffer = VectorReplayBuffer(\n",
    "                args.buffer_size,\n",
    "                buffer_num=len(test_envs),\n",
    "                ignore_obs_next=True,\n",
    "                save_only_last_obs=True,\n",
    "                stack_num=args.frames_stack,\n",
    "            )\n",
    "            collector = Collector(policy, test_envs, buffer, exploration_noise=True)\n",
    "            result = collector.collect(n_step=args.buffer_size)\n",
    "            print(f\"Save buffer into {args.save_buffer_name}\")\n",
    "            # Unfortunately, pickle will cause oom with 1M buffer size\n",
    "            buffer.save_hdf5(args.save_buffer_name)\n",
    "        else:\n",
    "            print(\"Testing agent ...\")\n",
    "            test_collector.reset()\n",
    "            result = test_collector.collect(\n",
    "                n_episode=args.test_num, render=args.render\n",
    "            )\n",
    "        rew = result[\"rews\"].mean()\n",
    "        print(f\"Mean reward (over {result['n/ep']} episodes): {rew}\")\n",
    "\n",
    "    if args.watch:\n",
    "        watch()\n",
    "        exit(0)\n",
    "\n",
    "    # test train_collector and start filling replay buffer\n",
    "    train_collector.collect(n_step=args.batch_size * args.training_num)\n",
    "    # trainer\n",
    "    result = offpolicy_trainer(\n",
    "        policy,\n",
    "        train_collector,\n",
    "        test_collector,\n",
    "        args.epoch,\n",
    "        args.step_per_epoch,\n",
    "        args.step_per_collect,\n",
    "        args.test_num,\n",
    "        args.batch_size,\n",
    "        stop_fn=stop_fn,\n",
    "        save_best_fn=save_best_fn,\n",
    "        logger=logger,\n",
    "        update_per_step=args.update_per_step,\n",
    "        test_in_train=False,\n",
    "        resume_from_log=args.resume_id is not None,\n",
    "        save_checkpoint_fn=save_checkpoint_fn,\n",
    "    )\n",
    "\n",
    "    pprint.pprint(result)\n",
    "    watch()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_discrete_sac(get_args())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('subgame')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09b31e4957d63d79e1e76c5537c345194b9f565583fac53cbbb105281d72baf4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
