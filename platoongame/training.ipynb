{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reloading modules sometimes causes errors. It's easier to just restart the notebook kernel and run from the start every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on colab\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir=\"checkpoints\"\n",
    "if 'google.colab' in str(get_ipython()): #type: ignore\n",
    "    COLAB = True\n",
    "    print('Running on colab')\n",
    "    %cd /content/\n",
    "    !git clone https://github.com/TeamDman/Vehicular-Game-Theory\n",
    "    %cd /content/Vehicular-Game-Theory/platoongame\n",
    "    !git pull\n",
    "\n",
    "    from utils import get_device\n",
    "    import torch\n",
    "    assert get_device() == torch.device(\"cuda\")\n",
    "\n",
    "    use_gdrive_checkpoints = True #@param {type:\"boolean\"}\n",
    "    if use_gdrive_checkpoints:\n",
    "        from google.colab import drive #type: ignore\n",
    "        drive.mount('/content/drive')\n",
    "\n",
    "        !mkdir -p /content/drive/MyDrive/checkpoints\n",
    "        checkpoints_dir = \"/content/drive/MyDrive/checkpoints\"\n",
    "else:\n",
    "    COLAB = False\n",
    "    print('Not running on colab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seed RNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x24900d6fdb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random_seed = 42 #@param {type:\"integer\"}\n",
    "random.seed(random_seed)\n",
    "import torch\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vehicle provider config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vehicles import JsonVehicleProvider, RandomVehicleProvider, RubbishVehicleProvider\n",
    "\n",
    "\n",
    "provider = \"random\" #@param [\"random\", \"json\", \"rubbish\"]\n",
    "if provider == \"json\":\n",
    "  vehicle_provider = JsonVehicleProvider(\"../subgame/python/solutions.json\")\n",
    "elif provider == \"random\":\n",
    "  vehicle_provider=RandomVehicleProvider(\n",
    "    # this affects model shape\n",
    "    num_max_vulns=7 #@param {type:\"integer\"}\n",
    "    ,\n",
    "    prob_mu=0.5 #@param {type:\"number\"}\n",
    "    ,\n",
    "    prob_sigma=0.25 #@param {type:\"number\"}\n",
    "    ,\n",
    "    sev_mu=2 #@param {type:\"number\"}\n",
    "    ,\n",
    "    sev_sigma=1 #@param {type:\"number\"}\n",
    "    ,\n",
    "  )\n",
    "elif provider == \"rubbish\":\n",
    "  vehicle_provider=RubbishVehicleProvider()\n",
    "else:\n",
    "  raise ValueError(\"unknown provider chosen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'in_platoon': False,\n",
      " 'risk': 2.3366904258728027,\n",
      " 'vulnerabilities': ({'prob': 0.5841726064682007,\n",
      "                      'severity': 2,\n",
      "                      'state': <CompromiseState.NOT_COMPROMISED: 1>},)}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import dataclasses\n",
    "pprint(dataclasses.asdict(vehicle_provider.next()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import GameConfig\n",
    "\n",
    "game_config=GameConfig(\n",
    "    max_vehicles=10 #@param {type:\"integer\"}\n",
    "    ,\n",
    "    cycle_enabled=False #@param {type:\"boolean\"}\n",
    "    ,\n",
    "    cycle_every=5 #@param {type:\"integer\"}\n",
    "    ,\n",
    "    cycle_allow_platoon=False #@param {type:\"boolean\"}\n",
    "    ,\n",
    "    cycle_num=3 #@param {type:\"integer\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attacker agent config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import AttackerAgent, PassiveAgent, BasicAttackerAgent\n",
    "from game import State\n",
    "from vehicles import CompromiseState\n",
    "\n",
    "attacker_ = \"humanish\" #@param [\"passive\", \"humanish\"]\n",
    "if attacker_ == \"passive\":\n",
    "  attacker = PassiveAgent()\n",
    "elif attacker_ == \"humanish\":\n",
    "  def get_attacker_utility(self:AttackerAgent, state: State) -> float:\n",
    "    members = [vehicle for vehicle in state.vehicles if vehicle.in_platoon]\n",
    "    comp = [1 for v in members if any([True if vuln.state != CompromiseState.NOT_COMPROMISED else False for vuln in v.vulnerabilities])]\n",
    "    return len(comp)\n",
    "\n",
    "  attacker = BasicAttackerAgent(\n",
    "    attack_limit=1 #@param {type:\"integer\"}\n",
    "    ,\n",
    "    attack_interval=5 #@param {type:\"integer\"},\n",
    "    ,\n",
    "    utility_func=get_attacker_utility\n",
    "  )\n",
    "else:\n",
    "  raise ValueError(\"unknown attacker chosen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defender agent config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TeamD\\.conda\\envs\\subgame\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "from agents import WolpertingerDefenderAgent\n",
    "from models import StateShapeData\n",
    "from vehicles import Vehicle, Vulnerability\n",
    "\n",
    "def get_defender_utility(self:WolpertingerDefenderAgent, state: State) -> float:\n",
    "    members = [vehicle for vehicle in state.vehicles if vehicle.in_platoon]\n",
    "    comp = [1 for v in members for vuln in v.vulnerabilities if vuln.state != CompromiseState.NOT_COMPROMISED]\n",
    "    if len(comp) > 0: return 0\n",
    "    return len(members)\n",
    "\n",
    "load_checkpoint = False #@param {type:\"boolean\"}\n",
    "defender=WolpertingerDefenderAgent(\n",
    "    state_shape_data=StateShapeData(\n",
    "        num_vehicles=game_config.max_vehicles,\n",
    "        num_vehicle_features=Vehicle.get_shape()[0],\n",
    "        num_vulns=vehicle_provider.max_vulns,\n",
    "        num_vuln_features=Vulnerability.get_shape()[0]\n",
    "    ),\n",
    "    learning_rate=0.001 #@param {type:\"number\"}\n",
    "    ,\n",
    "    num_proposals=100 #@param {type: \"integer\"}\n",
    "    ,\n",
    "    utility_func=get_defender_utility\n",
    "    ,\n",
    "    ou_theta = 0.0 #@param {type: \"number\"}\n",
    "    ,\n",
    "    ou_mu = 0.0 #@param {type: \"number\"}\n",
    "    ,\n",
    "    ou_sigma = 3 #@param {type: \"number\"}\n",
    "    ,\n",
    "    epsilon_decay_time=50000 #@param {type: \"integer\"}\n",
    ")\n",
    "\n",
    "if load_checkpoint:\n",
    "  defender.load(\n",
    "    dir=checkpoints_dir,\n",
    "    prefix=\"2022-10-13 2258-27 054659\" #@param {type: \"string\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "untrained defender\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x1440 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from agents import RandomDefenderAgent\n",
    "from evaluation import get_episode_metrics\n",
    "\n",
    "print(\"untrained defender\")\n",
    "get_episode_metrics(\n",
    "    defender_agent=defender,\n",
    "    attacker_agent=attacker,\n",
    "    game_config=game_config,\n",
    "    vehicle_provider=vehicle_provider,\n",
    "    num_turns=100 #@param {type:\"integer\"}\n",
    ").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random defender\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x1440 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"random defender\")\n",
    "get_episode_metrics(\n",
    "    defender_agent=RandomDefenderAgent(),\n",
    "    attacker_agent=attacker,\n",
    "    game_config=game_config,\n",
    "    vehicle_provider=vehicle_provider,\n",
    "    num_turns=100 #@param {type:\"integer\"}\n",
    ").plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay memory config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory import DequeReplayMemory\n",
    "\n",
    "memory = DequeReplayMemory(\n",
    "    capacity=100000 #@param {type:\"integer\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training metrics tracker config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import TrainingMetricsTracker\n",
    "tracker = TrainingMetricsTracker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import WolpertingerDefenderAgentTrainer, WolpertingerDefenderAgentTrainerConfig\n",
    "trainer = WolpertingerDefenderAgentTrainer(WolpertingerDefenderAgentTrainerConfig(\n",
    "    game_config=game_config,\n",
    "    vehicle_provider=vehicle_provider,\n",
    "    attacker_agent=attacker,\n",
    "    defender_agent=defender,\n",
    "    checkpoint_interval=-1 #@param {type:\"integer\"}\n",
    "    ,\n",
    "    memory=memory,\n",
    "    metrics_tracker=tracker,\n",
    "    batch_size=50 #@param {type:\"integer\"}\n",
    "    ,\n",
    "    train_steps=25000 #@param {type:\"integer\"}\n",
    "    ,\n",
    "    exploration_per_step=5 #@param {type:\"integer\"}\n",
    "    ,\n",
    "    warmup_replay=2000 #@param {type:\"integer\"}\n",
    "    ,\n",
    "    max_steps_per_episode=5 #@param {type:\"integer\"}\n",
    "    ,\n",
    "    attacker_headstart=3 #@param {type:\"integer\"}\n",
    "    ,\n",
    "    update_policy_interval=1 #@param {type:\"integer\"}\n",
    "    ,\n",
    "    policy_update_type=\"soft\" #@param [\"soft\", \"hard\"]\n",
    "    ,\n",
    "    reward_gamma = 0.99 #@param {type:\"number\"}\n",
    "    ,\n",
    "    soft_update_tau = 0.001 #@param {type:\"number\"}\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8f2de7cd7e44e9a357e5d47d2eb521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup complete~!\n"
     ]
    }
   ],
   "source": [
    "trainer.warmup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arbitrary number, but we need to be sure there are enough transitions that have rewards\n",
    "# lost a lot of time debugging because of this\n",
    "reward_expectation=100 #@param {type:\"integer\"}\n",
    "reward_expectation_steps = 1000 #@param {type:\"integer\"}\n",
    "assert sum([e.reward for e in memory.sample(min(reward_expectation_steps, len(memory)))]) > reward_expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([617.,  87.,  82.,  44.,  45.,  35.,  20.,  32.,  18.,  20.]),\n",
       " array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "rewards = [float(x.reward) for x in memory.sample(1000)]\n",
    "plt.hist(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that there are many more non-zero rewards than zero rewards\n",
    "for _ in range(5):\n",
    "    for x in memory.sample(len(memory)):\n",
    "        if x.reward > 0:\n",
    "            memory.push(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 823., 1919., 1644., 1190., 1110.,  921.,  768.,  775.,  402.,\n",
       "         448.]),\n",
       " array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewards = [float(x.reward) for x in memory.sample(10000)]\n",
    "plt.hist(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([198.,  69., 174., 239., 160.,  90.,  45.,  12.,  12.,   1.]),\n",
       " array([0. , 0.9, 1.8, 2.7, 3.6, 4.5, 5.4, 6.3, 7.2, 8.1, 9. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_vulns = 1000\n",
    "comps = [\n",
    "    x.state.vulnerabilities[:,:,:,3].sum().item()\n",
    "    for x in memory.sample(check_vulns)\n",
    "]\n",
    "plt.hist(comps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ee39d8f28848568e8a55d540a06cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup complete~!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eed61a4564142028cf754cfe71cf2fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train step 00000 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.8699 diff={max=08.5913, min=00.0084, mean=00.9613} policy_loss=-3.4054 policy updated! \n",
      "train step 00001 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=02.5384 diff={max=05.0730, min=00.0069, mean=01.1664} policy_loss=-3.0309 policy updated! \n",
      "train step 00002 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9350 diff={max=04.8879, min=00.0035, mean=01.2698} policy_loss=-2.6268 policy updated! \n",
      "train step 00003 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=02.4284 diff={max=03.8181, min=00.0129, mean=01.2506} policy_loss=-2.8286 policy updated! \n",
      "train step 00004 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9027 diff={max=05.5460, min=00.0304, mean=01.0349} policy_loss=-3.0869 policy updated! \n",
      "train step 00005 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.4596 diff={max=04.5492, min=00.0028, mean=01.2193} policy_loss=-2.7539 policy updated! \n",
      "train step 00006 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=02.6308 diff={max=03.7519, min=00.0250, mean=01.2826} policy_loss=-3.3013 policy updated! \n",
      "train step 00007 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9222 diff={max=04.9584, min=00.0023, mean=01.3280} policy_loss=-3.3851 policy updated! \n",
      "train step 00008 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=02.1801 diff={max=05.2063, min=00.0162, mean=01.0624} policy_loss=-3.7649 policy updated! \n",
      "train step 00009 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=02.6470 diff={max=07.2568, min=00.0062, mean=01.1668} policy_loss=-4.0247 policy updated! \n",
      "train step 00010 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.3635 diff={max=03.5723, min=00.0100, mean=01.2617} policy_loss=-3.4292 policy updated! \n",
      "train step 00011 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2346 diff={max=07.6733, min=00.0087, mean=01.1558} policy_loss=-3.5974 policy updated! \n",
      "train step 00012 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8704 diff={max=04.7359, min=00.0055, mean=01.2298} policy_loss=-3.1280 policy updated! \n",
      "train step 00013 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6226 diff={max=04.5314, min=00.0168, mean=01.2393} policy_loss=-3.1865 policy updated! \n",
      "train step 00014 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=03.3869 diff={max=04.1891, min=00.0384, mean=01.4232} policy_loss=-2.9519 policy updated! \n",
      "train step 00015 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.6186 diff={max=03.8507, min=00.0142, mean=01.3262} policy_loss=-2.8541 policy updated! \n",
      "train step 00016 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2791 diff={max=07.0358, min=00.0056, mean=01.6429} policy_loss=-2.6804 policy updated! \n",
      "train step 00017 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0222 diff={max=04.2190, min=00.0367, mean=01.0865} policy_loss=-2.8455 policy updated! \n",
      "train step 00018 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6485 diff={max=03.8264, min=00.0034, mean=01.3182} policy_loss=-3.2356 policy updated! \n",
      "train step 00019 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6312 diff={max=07.2738, min=00.0327, mean=01.3443} policy_loss=-2.9558 policy updated! \n",
      "train step 00020 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.8329 diff={max=03.1649, min=00.0497, mean=01.0521} policy_loss=-3.2457 policy updated! \n",
      "train step 00021 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=02.7753 diff={max=04.9007, min=00.0431, mean=01.2532} policy_loss=-3.7980 policy updated! \n",
      "train step 00022 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5061 diff={max=05.4479, min=00.0834, mean=01.1933} policy_loss=-3.5743 policy updated! \n",
      "train step 00023 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9184 diff={max=04.4934, min=00.0689, mean=01.0515} policy_loss=-3.5465 policy updated! \n",
      "train step 00024 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4186 diff={max=06.3790, min=00.0045, mean=01.2449} policy_loss=-3.7347 policy updated! \n",
      "train step 00025 reward={max=05.0000, min=05.0000, mean=05.0000} optimizing loss=02.4773 diff={max=04.5454, min=00.0271, mean=01.1909} policy_loss=-3.2810 policy updated! \n",
      "train step 00026 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2961 diff={max=04.3610, min=00.0144, mean=01.1090} policy_loss=-3.0921 policy updated! \n",
      "train step 00027 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8811 diff={max=05.4448, min=00.0343, mean=00.8314} policy_loss=-2.5945 policy updated! \n",
      "train step 00028 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=02.6353 diff={max=05.4101, min=00.0614, mean=01.1259} policy_loss=-2.5791 policy updated! \n",
      "train step 00029 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=03.3175 diff={max=05.6343, min=00.0325, mean=01.2489} policy_loss=-2.9401 policy updated! \n",
      "train step 00030 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.7839 diff={max=06.1201, min=00.0083, mean=01.1076} policy_loss=-2.8430 policy updated! \n",
      "train step 00031 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6238 diff={max=07.6738, min=00.0472, mean=01.5888} policy_loss=-3.1827 policy updated! \n",
      "train step 00032 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4361 diff={max=05.7564, min=00.0085, mean=01.3825} policy_loss=-3.4967 policy updated! \n",
      "train step 00033 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0641 diff={max=05.0524, min=00.1159, mean=01.3583} policy_loss=-3.1590 policy updated! \n",
      "train step 00034 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=02.1087 diff={max=03.7569, min=00.0288, mean=01.0995} policy_loss=-3.2503 policy updated! \n",
      "train step 00035 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.2164 diff={max=04.9540, min=00.0056, mean=01.1300} policy_loss=-3.1918 policy updated! \n",
      "train step 00036 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1613 diff={max=03.5276, min=00.0743, mean=01.1197} policy_loss=-3.2990 policy updated! \n",
      "train step 00037 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8204 diff={max=04.3408, min=00.0237, mean=01.0499} policy_loss=-3.5751 policy updated! \n",
      "train step 00038 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=04.4119 diff={max=06.3813, min=00.0523, mean=01.4960} policy_loss=-3.4138 policy updated! \n",
      "train step 00039 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=02.5983 diff={max=05.9825, min=00.1453, mean=01.1973} policy_loss=-3.1651 policy updated! \n",
      "train step 00040 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.7000 diff={max=06.2022, min=00.0119, mean=01.1360} policy_loss=-3.1447 policy updated! \n",
      "train step 00041 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2017 diff={max=07.0459, min=00.0279, mean=01.3911} policy_loss=-2.8906 policy updated! \n",
      "train step 00042 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5638 diff={max=04.1729, min=00.0142, mean=00.9817} policy_loss=-3.3545 policy updated! \n",
      "train step 00043 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=02.3848 diff={max=04.1268, min=00.0351, mean=01.1686} policy_loss=-3.6210 policy updated! \n",
      "train step 00044 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6779 diff={max=06.9476, min=00.0449, mean=01.2827} policy_loss=-3.4663 policy updated! \n",
      "train step 00045 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.7977 diff={max=03.5836, min=00.0361, mean=01.0664} policy_loss=-3.0781 policy updated! \n",
      "train step 00046 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7254 diff={max=04.4966, min=00.0126, mean=00.9936} policy_loss=-2.9244 policy updated! \n",
      "train step 00047 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1598 diff={max=04.5150, min=00.0630, mean=01.1338} policy_loss=-3.3167 policy updated! \n",
      "train step 00048 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6683 diff={max=03.6191, min=00.0101, mean=00.9500} policy_loss=-3.5478 policy updated! \n",
      "train step 00049 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.2960 diff={max=02.9835, min=00.0030, mean=00.8796} policy_loss=-3.2568 policy updated! \n",
      "train step 00050 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=03.0057 diff={max=05.5062, min=00.0013, mean=01.2543} policy_loss=-3.5301 policy updated! \n",
      "train step 00051 reward={max=05.0000, min=05.0000, mean=05.0000} optimizing loss=02.1513 diff={max=03.8880, min=00.0254, mean=01.0678} policy_loss=-3.4531 policy updated! \n",
      "train step 00052 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8383 diff={max=04.3052, min=00.0620, mean=01.0437} policy_loss=-3.0698 policy updated! \n",
      "train step 00053 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=02.0667 diff={max=03.4892, min=00.1229, mean=01.1438} policy_loss=-2.9943 policy updated! \n",
      "train step 00054 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.6940 diff={max=03.5156, min=00.0064, mean=00.9423} policy_loss=-3.1756 policy updated! \n",
      "train step 00055 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.9292 diff={max=04.8943, min=00.0026, mean=01.3084} policy_loss=-3.1078 policy updated! \n",
      "train step 00056 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=02.3287 diff={max=06.4406, min=00.0175, mean=01.0912} policy_loss=-2.9984 policy updated! \n",
      "train step 00057 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5949 diff={max=05.7589, min=00.0187, mean=01.1376} policy_loss=-3.3148 policy updated! \n",
      "train step 00058 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8183 diff={max=05.2349, min=00.0188, mean=01.1497} policy_loss=-3.5299 policy updated! \n",
      "train step 00059 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=05.2311 diff={max=08.3305, min=00.0054, mean=01.4623} policy_loss=-3.1922 policy updated! \n",
      "train step 00060 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=03.5804 diff={max=06.3713, min=00.0270, mean=01.2908} policy_loss=-3.3429 policy updated! \n",
      "train step 00061 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=02.7171 diff={max=05.6633, min=00.0066, mean=01.1548} policy_loss=-2.8764 policy updated! \n",
      "train step 00062 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9157 diff={max=02.8730, min=00.0056, mean=01.1495} policy_loss=-3.2303 policy updated! \n",
      "train step 00063 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=02.8422 diff={max=05.8493, min=00.0035, mean=01.1368} policy_loss=-3.2713 policy updated! \n",
      "train step 00064 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=02.1185 diff={max=04.4743, min=00.0049, mean=01.0457} policy_loss=-3.4387 policy updated! \n",
      "train step 00065 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.7535 diff={max=03.7395, min=00.0009, mean=01.0368} policy_loss=-3.7073 policy updated! \n",
      "train step 00066 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6446 diff={max=08.4680, min=00.0485, mean=01.2737} policy_loss=-3.4392 policy updated! \n",
      "train step 00067 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5072 diff={max=06.6665, min=00.0200, mean=01.0138} policy_loss=-3.2212 policy updated! \n",
      "train step 00068 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=02.2375 diff={max=04.4375, min=00.0142, mean=01.1151} policy_loss=-3.2320 policy updated! \n",
      "train step 00069 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6688 diff={max=05.0176, min=00.0078, mean=00.9717} policy_loss=-3.3233 policy updated! \n",
      "train step 00070 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.6171 diff={max=04.3132, min=00.0017, mean=01.2615} policy_loss=-2.7181 policy updated! \n",
      "train step 00071 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=02.9758 diff={max=07.0878, min=00.0010, mean=01.0671} policy_loss=-2.8712 policy updated! \n",
      "train step 00072 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4453 diff={max=02.6162, min=00.0096, mean=01.0080} policy_loss=-3.5217 policy updated! \n",
      "train step 00073 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7401 diff={max=04.7162, min=00.0238, mean=00.9423} policy_loss=-3.3994 policy updated! \n",
      "train step 00074 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=02.8341 diff={max=05.0434, min=00.0106, mean=01.2315} policy_loss=-3.3981 policy updated! \n",
      "train step 00075 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.4741 diff={max=08.6319, min=00.0169, mean=01.2427} policy_loss=-3.3652 policy updated! \n",
      "train step 00076 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4367 diff={max=05.4086, min=00.0251, mean=00.8005} policy_loss=-3.4107 policy updated! \n",
      "train step 00077 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2555 diff={max=06.0864, min=00.0305, mean=01.2339} policy_loss=-3.3418 policy updated! \n",
      "train step 00078 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7372 diff={max=08.3312, min=00.0229, mean=01.2009} policy_loss=-3.2802 policy updated! \n",
      "train step 00079 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=02.4651 diff={max=04.2995, min=00.0432, mean=01.1968} policy_loss=-3.3257 policy updated! \n",
      "train step 00080 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.9618 diff={max=04.2745, min=00.0093, mean=00.9851} policy_loss=-2.7375 policy updated! \n",
      "train step 00081 reward={max=06.0000, min=00.0000, mean=03.4000} optimizing loss=03.0350 diff={max=05.8335, min=00.0365, mean=01.3413} policy_loss=-2.9251 policy updated! \n",
      "train step 00082 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4477 diff={max=03.3197, min=00.1673, mean=01.3068} policy_loss=-2.2938 policy updated! \n",
      "train step 00083 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=02.8703 diff={max=05.5094, min=00.0171, mean=01.2785} policy_loss=-2.6783 policy updated! \n",
      "train step 00084 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=03.2637 diff={max=07.3524, min=00.0086, mean=01.2429} policy_loss=-2.8577 policy updated! \n",
      "train step 00085 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.1951 diff={max=04.4720, min=00.0314, mean=01.1006} policy_loss=-3.3167 policy updated! \n",
      "train step 00086 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4133 diff={max=05.7848, min=00.1045, mean=01.3772} policy_loss=-3.2088 policy updated! \n",
      "train step 00087 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3419 diff={max=03.5551, min=00.0329, mean=00.9134} policy_loss=-3.5320 policy updated! \n",
      "train step 00088 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=02.4877 diff={max=04.1003, min=00.0087, mean=01.1710} policy_loss=-3.3530 policy updated! \n",
      "train step 00089 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6110 diff={max=08.0848, min=00.0538, mean=01.3603} policy_loss=-3.6153 policy updated! \n",
      "train step 00090 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.2610 diff={max=04.5042, min=00.0195, mean=01.1418} policy_loss=-3.4461 policy updated! \n",
      "train step 00091 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=02.7011 diff={max=04.6748, min=00.0084, mean=01.1327} policy_loss=-2.8206 policy updated! \n",
      "train step 00092 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4865 diff={max=04.2540, min=00.0255, mean=00.9028} policy_loss=-2.7994 policy updated! \n",
      "train step 00093 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=02.4491 diff={max=04.3780, min=00.0861, mean=01.2168} policy_loss=-2.8284 policy updated! \n",
      "train step 00094 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9254 diff={max=03.7338, min=00.0396, mean=01.0533} policy_loss=-3.1488 policy updated! \n",
      "train step 00095 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.2725 diff={max=04.5158, min=00.0209, mean=01.1982} policy_loss=-3.3773 policy updated! \n",
      "train step 00096 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8438 diff={max=07.9348, min=00.0446, mean=01.2004} policy_loss=-3.6387 policy updated! \n",
      "train step 00097 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6567 diff={max=07.9570, min=00.0046, mean=01.0214} policy_loss=-3.2462 policy updated! \n",
      "train step 00098 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=02.0298 diff={max=04.5481, min=00.0340, mean=01.0063} policy_loss=-3.4329 policy updated! \n",
      "train step 00099 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4492 diff={max=03.6961, min=00.0553, mean=00.9165} policy_loss=-3.3839 policy updated! \n",
      "train step 00100 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.2841 diff={max=05.6280, min=00.0813, mean=01.1369} policy_loss=-3.3564 policy updated! \n",
      "train step 00101 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8058 diff={max=05.5771, min=00.0255, mean=01.2270} policy_loss=-3.1454 policy updated! \n",
      "train step 00102 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7707 diff={max=03.3222, min=00.0054, mean=01.0497} policy_loss=-3.0488 policy updated! \n",
      "train step 00103 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=01.7871 diff={max=04.4198, min=00.1027, mean=01.0545} policy_loss=-2.9944 policy updated! \n",
      "train step 00104 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6794 diff={max=07.8985, min=00.0252, mean=01.0899} policy_loss=-2.9805 policy updated! \n",
      "train step 00105 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.1886 diff={max=04.8103, min=00.0567, mean=01.1343} policy_loss=-3.0858 policy updated! \n",
      "train step 00106 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=02.2167 diff={max=03.5092, min=00.0055, mean=01.1141} policy_loss=-3.4141 policy updated! \n",
      "train step 00107 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4034 diff={max=05.0817, min=00.0414, mean=01.1456} policy_loss=-3.7246 policy updated! \n",
      "train step 00108 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4645 diff={max=03.0025, min=00.0256, mean=00.8902} policy_loss=-3.3659 policy updated! \n",
      "train step 00109 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7501 diff={max=07.7273, min=00.0132, mean=01.1542} policy_loss=-3.0814 policy updated! \n",
      "train step 00110 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.4548 diff={max=02.9322, min=00.0106, mean=00.9631} policy_loss=-3.3175 policy updated! \n",
      "train step 00111 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=03.2032 diff={max=05.1796, min=00.0431, mean=01.3646} policy_loss=-2.9180 policy updated! \n",
      "train step 00112 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8330 diff={max=04.8707, min=00.0099, mean=00.9410} policy_loss=-3.4246 policy updated! \n",
      "train step 00113 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=01.9275 diff={max=04.6178, min=00.0044, mean=01.0209} policy_loss=-3.3654 policy updated! \n",
      "train step 00114 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7882 diff={max=09.3483, min=00.0291, mean=01.4139} policy_loss=-3.5309 policy updated! \n",
      "train step 00115 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.8081 diff={max=05.5328, min=00.0106, mean=01.1832} policy_loss=-3.4699 policy updated! \n",
      "train step 00116 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=02.1740 diff={max=04.3881, min=00.0155, mean=01.0532} policy_loss=-3.4226 policy updated! \n",
      "train step 00117 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6864 diff={max=03.4889, min=00.0589, mean=01.0160} policy_loss=-3.2614 policy updated! \n",
      "train step 00118 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0329 diff={max=02.4738, min=00.1542, mean=00.8317} policy_loss=-3.2593 policy updated! \n",
      "train step 00119 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0906 diff={max=06.2743, min=00.0220, mean=01.2891} policy_loss=-3.7083 policy updated! \n",
      "train step 00120 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.6852 diff={max=03.3734, min=00.0112, mean=00.9674} policy_loss=-3.4793 policy updated! \n",
      "train step 00121 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9048 diff={max=05.7328, min=00.0369, mean=01.1669} policy_loss=-3.5577 policy updated! \n",
      "train step 00122 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1486 diff={max=08.6584, min=00.0788, mean=01.3370} policy_loss=-3.4082 policy updated! \n",
      "train step 00123 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0630 diff={max=04.7327, min=00.0191, mean=01.3058} policy_loss=-3.0353 policy updated! \n",
      "train step 00124 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=02.2176 diff={max=04.7927, min=00.0028, mean=01.0443} policy_loss=-3.1993 policy updated! \n",
      "train step 00125 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.5429 diff={max=06.0297, min=00.0081, mean=01.2150} policy_loss=-3.4356 policy updated! \n",
      "train step 00126 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=02.1640 diff={max=05.0962, min=00.0410, mean=01.0538} policy_loss=-3.3536 policy updated! \n",
      "train step 00127 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8814 diff={max=03.3703, min=00.0256, mean=00.9795} policy_loss=-3.1915 policy updated! \n",
      "train step 00128 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6938 diff={max=03.9403, min=00.0199, mean=00.9841} policy_loss=-3.5444 policy updated! \n",
      "train step 00129 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=01.5740 diff={max=04.2566, min=00.0696, mean=00.9489} policy_loss=-3.3363 policy updated! \n",
      "train step 00130 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.4018 diff={max=05.1601, min=00.0223, mean=01.1227} policy_loss=-3.2632 policy updated! \n",
      "train step 00131 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=01.6606 diff={max=03.6729, min=00.0092, mean=01.0049} policy_loss=-3.0959 policy updated! \n",
      "train step 00132 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4234 diff={max=03.8946, min=00.0331, mean=00.8802} policy_loss=-3.0340 policy updated! \n",
      "train step 00133 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=02.1034 diff={max=04.6388, min=00.0010, mean=01.0034} policy_loss=-3.1413 policy updated! \n",
      "train step 00134 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8193 diff={max=05.3927, min=00.0821, mean=01.1543} policy_loss=-2.9357 policy updated! \n",
      "train step 00135 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.3435 diff={max=05.3498, min=00.0544, mean=01.1283} policy_loss=-3.3134 policy updated! \n",
      "train step 00136 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9973 diff={max=04.5135, min=00.0996, mean=01.0352} policy_loss=-3.2847 policy updated! \n",
      "train step 00137 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6116 diff={max=03.5975, min=00.0190, mean=00.9283} policy_loss=-3.8502 policy updated! \n",
      "train step 00138 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1756 diff={max=07.3962, min=00.0461, mean=00.9317} policy_loss=-3.6654 policy updated! \n",
      "train step 00139 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=02.2536 diff={max=04.5144, min=00.0517, mean=01.2243} policy_loss=-3.4687 policy updated! \n",
      "train step 00140 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.6997 diff={max=04.6685, min=00.0451, mean=01.3011} policy_loss=-3.1932 policy updated! \n",
      "train step 00141 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=01.0496 diff={max=03.2198, min=00.0560, mean=00.7726} policy_loss=-3.3423 policy updated! \n",
      "train step 00142 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1868 diff={max=02.9539, min=00.0081, mean=00.8362} policy_loss=-2.7600 policy updated! \n",
      "train step 00143 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7733 diff={max=05.0997, min=00.0116, mean=01.2246} policy_loss=-2.4870 policy updated! \n",
      "train step 00144 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4357 diff={max=05.3209, min=00.0149, mean=01.2194} policy_loss=-2.7714 policy updated! \n",
      "train step 00145 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.7940 diff={max=05.1721, min=00.0862, mean=01.2217} policy_loss=-2.7528 policy updated! \n",
      "train step 00146 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=02.4651 diff={max=06.5947, min=00.0095, mean=01.0577} policy_loss=-2.7799 policy updated! \n",
      "train step 00147 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5579 diff={max=04.1432, min=00.0026, mean=01.1730} policy_loss=-3.1804 policy updated! \n",
      "train step 00148 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0160 diff={max=04.1443, min=00.0384, mean=01.1054} policy_loss=-3.4601 policy updated! \n",
      "train step 00149 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2677 diff={max=06.0803, min=00.0147, mean=01.0370} policy_loss=-3.7605 policy updated! \n",
      "train step 00150 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.9455 diff={max=06.4752, min=00.0536, mean=01.1243} policy_loss=-3.6896 policy updated! \n",
      "train step 00151 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9581 diff={max=05.4849, min=00.0039, mean=01.1987} policy_loss=-2.8589 policy updated! \n",
      "train step 00152 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5068 diff={max=05.2832, min=00.0237, mean=00.8865} policy_loss=-2.8451 policy updated! \n",
      "train step 00153 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6302 diff={max=02.9712, min=00.0142, mean=01.0431} policy_loss=-2.5071 policy updated! \n",
      "train step 00154 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9187 diff={max=05.0299, min=00.0054, mean=01.0835} policy_loss=-2.7245 policy updated! \n",
      "train step 00155 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.7643 diff={max=03.1965, min=00.0012, mean=01.0850} policy_loss=-3.0946 policy updated! \n",
      "train step 00156 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2390 diff={max=05.3063, min=00.0332, mean=01.0241} policy_loss=-2.8401 policy updated! \n",
      "train step 00157 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1080 diff={max=04.1299, min=00.0125, mean=01.1578} policy_loss=-3.3493 policy updated! \n",
      "train step 00158 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=01.3268 diff={max=03.4419, min=00.0358, mean=00.8716} policy_loss=-3.8926 policy updated! \n",
      "train step 00159 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=02.7760 diff={max=06.8174, min=00.0082, mean=01.0609} policy_loss=-3.0996 policy updated! \n",
      "train step 00160 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.7361 diff={max=04.9193, min=00.0033, mean=01.2244} policy_loss=-3.4171 policy updated! \n",
      "train step 00161 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6911 diff={max=07.4818, min=00.0333, mean=01.0567} policy_loss=-3.0664 policy updated! \n",
      "train step 00162 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3075 diff={max=04.4264, min=00.0061, mean=00.8415} policy_loss=-2.5777 policy updated! \n",
      "train step 00163 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0408 diff={max=05.4229, min=00.0033, mean=01.0341} policy_loss=-2.9795 policy updated! \n",
      "train step 00164 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=03.6887 diff={max=05.4287, min=00.0682, mean=01.3962} policy_loss=-2.7188 policy updated! \n",
      "train step 00165 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.0671 diff={max=03.9672, min=00.0134, mean=01.1101} policy_loss=-3.5460 policy updated! \n",
      "train step 00166 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3944 diff={max=03.0260, min=00.0011, mean=00.8709} policy_loss=-3.6207 policy updated! \n",
      "train step 00167 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3016 diff={max=06.3512, min=00.0124, mean=00.9784} policy_loss=-3.7302 policy updated! \n",
      "train step 00168 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=02.1688 diff={max=04.8256, min=00.0595, mean=01.0279} policy_loss=-4.2406 policy updated! \n",
      "train step 00169 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5365 diff={max=03.3103, min=00.0400, mean=01.0280} policy_loss=-3.4734 policy updated! \n",
      "train step 00170 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.5120 diff={max=06.5417, min=00.0213, mean=01.0458} policy_loss=-3.9670 policy updated! \n",
      "train step 00171 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1431 diff={max=04.1486, min=00.0144, mean=01.1323} policy_loss=-2.6443 policy updated! \n",
      "train step 00172 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1347 diff={max=05.1276, min=00.0138, mean=01.0485} policy_loss=-2.5822 policy updated! \n",
      "train step 00173 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6870 diff={max=04.8441, min=00.0501, mean=00.9230} policy_loss=-2.8153 policy updated! \n",
      "train step 00174 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=01.5536 diff={max=03.5174, min=00.0380, mean=00.9474} policy_loss=-2.9841 policy updated! \n",
      "train step 00175 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.5478 diff={max=04.5257, min=00.0103, mean=00.8153} policy_loss=-3.1356 policy updated! \n",
      "train step 00176 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3082 diff={max=03.5632, min=00.0103, mean=00.8511} policy_loss=-3.2297 policy updated! \n",
      "train step 00177 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8056 diff={max=04.2704, min=00.0549, mean=00.9824} policy_loss=-3.9236 policy updated! \n",
      "train step 00178 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8073 diff={max=05.1101, min=00.0730, mean=00.9577} policy_loss=-3.4651 policy updated! \n",
      "train step 00179 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=01.9916 diff={max=04.2480, min=00.0122, mean=01.0068} policy_loss=-3.4429 policy updated! \n",
      "train step 00180 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.7561 diff={max=07.5922, min=00.0052, mean=01.3572} policy_loss=-3.8490 policy updated! \n",
      "train step 00181 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4236 diff={max=05.5944, min=00.0061, mean=01.0633} policy_loss=-2.9716 policy updated! \n",
      "train step 00182 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3836 diff={max=08.7623, min=00.0596, mean=01.1691} policy_loss=-2.9428 policy updated! \n",
      "train step 00183 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=02.1486 diff={max=04.6185, min=00.0124, mean=01.0680} policy_loss=-2.7765 policy updated! \n",
      "train step 00184 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=02.3252 diff={max=05.8031, min=00.0526, mean=01.1603} policy_loss=-2.5383 policy updated! \n",
      "train step 00185 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.4911 diff={max=05.8291, min=00.0207, mean=01.3536} policy_loss=-2.6164 policy updated! \n",
      "train step 00186 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3647 diff={max=04.7894, min=00.0228, mean=01.0575} policy_loss=-3.0073 policy updated! \n",
      "train step 00187 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4947 diff={max=05.2269, min=00.0449, mean=01.1947} policy_loss=-3.0306 policy updated! \n",
      "train step 00188 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6923 diff={max=03.5592, min=00.0344, mean=00.9917} policy_loss=-3.6131 policy updated! \n",
      "train step 00189 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7015 diff={max=03.0820, min=00.0405, mean=01.0427} policy_loss=-3.8544 policy updated! \n",
      "train step 00190 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.8857 diff={max=04.0058, min=00.0110, mean=00.9943} policy_loss=-3.7914 policy updated! \n",
      "train step 00191 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4059 diff={max=04.0341, min=00.0425, mean=00.8348} policy_loss=-3.9842 policy updated! \n",
      "train step 00192 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6688 diff={max=07.2418, min=00.0038, mean=01.1011} policy_loss=-3.8563 policy updated! \n",
      "train step 00193 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=01.7638 diff={max=04.8953, min=00.0268, mean=00.9998} policy_loss=-3.1775 policy updated! \n",
      "train step 00194 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=01.1329 diff={max=02.7115, min=00.0372, mean=00.8094} policy_loss=-3.5426 policy updated! \n",
      "train step 00195 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.9946 diff={max=06.0156, min=00.0433, mean=00.9450} policy_loss=-2.9878 policy updated! \n",
      "train step 00196 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2037 diff={max=05.4781, min=00.0101, mean=01.0146} policy_loss=-2.8887 policy updated! \n",
      "train step 00197 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6739 diff={max=03.5644, min=00.0034, mean=00.9354} policy_loss=-3.2041 policy updated! \n",
      "train step 00198 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=02.0890 diff={max=05.6055, min=00.0422, mean=01.0458} policy_loss=-3.2616 policy updated! \n",
      "train step 00199 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=02.0887 diff={max=05.5382, min=00.0306, mean=00.9486} policy_loss=-3.5119 policy updated! \n",
      "train step 00200 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=00.7744 diff={max=02.2058, min=00.0063, mean=00.7067} policy_loss=-3.6121 policy updated! \n",
      "train step 00201 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=02.9045 diff={max=06.0408, min=00.0187, mean=01.1760} policy_loss=-3.6377 policy updated! \n",
      "train step 00202 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7039 diff={max=04.7500, min=00.0271, mean=00.8436} policy_loss=-3.4832 policy updated! \n",
      "train step 00203 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5152 diff={max=03.8204, min=00.0067, mean=00.9191} policy_loss=-3.5202 policy updated! \n",
      "train step 00204 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6645 diff={max=05.6521, min=00.0013, mean=00.8759} policy_loss=-3.2668 policy updated! \n",
      "train step 00205 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.8820 diff={max=04.3359, min=00.0234, mean=00.9737} policy_loss=-3.0092 policy updated! \n",
      "train step 00206 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3421 diff={max=03.0264, min=00.0082, mean=00.9525} policy_loss=-2.9958 policy updated! \n",
      "train step 00207 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8511 diff={max=06.8968, min=00.0054, mean=01.1075} policy_loss=-3.2372 policy updated! \n",
      "train step 00208 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8848 diff={max=04.9673, min=00.1276, mean=01.0636} policy_loss=-3.4756 policy updated! \n",
      "train step 00209 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0675 diff={max=02.9091, min=00.0065, mean=00.7557} policy_loss=-3.5274 policy updated! \n",
      "train step 00210 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.2795 diff={max=05.2173, min=00.0019, mean=00.9695} policy_loss=-3.5804 policy updated! \n",
      "train step 00211 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=03.0981 diff={max=05.6224, min=00.0076, mean=01.1841} policy_loss=-3.3720 policy updated! \n",
      "train step 00212 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3660 diff={max=03.7313, min=00.0635, mean=00.7978} policy_loss=-3.2609 policy updated! \n",
      "train step 00213 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9642 diff={max=05.4403, min=00.0068, mean=00.9561} policy_loss=-3.7002 policy updated! \n",
      "train step 00214 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1220 diff={max=03.5413, min=00.0297, mean=00.6894} policy_loss=-3.4527 policy updated! \n",
      "train step 00215 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.3502 diff={max=03.6237, min=00.0047, mean=00.8346} policy_loss=-3.6168 policy updated! \n",
      "train step 00216 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=01.6793 diff={max=04.0304, min=00.0096, mean=01.0404} policy_loss=-3.0582 policy updated! \n",
      "train step 00217 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4099 diff={max=03.4151, min=00.0488, mean=00.8725} policy_loss=-3.0230 policy updated! \n",
      "train step 00218 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7787 diff={max=05.4174, min=00.0028, mean=00.9586} policy_loss=-3.2021 policy updated! \n",
      "train step 00219 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1574 diff={max=05.1763, min=00.0150, mean=01.0691} policy_loss=-3.1706 policy updated! \n",
      "train step 00220 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.5157 diff={max=03.9324, min=00.0101, mean=00.9430} policy_loss=-3.4830 policy updated! \n",
      "train step 00221 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=01.5923 diff={max=03.9568, min=00.0278, mean=00.8788} policy_loss=-3.7493 policy updated! \n",
      "train step 00222 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1321 diff={max=03.9035, min=00.0127, mean=00.9621} policy_loss=-3.6362 policy updated! \n",
      "train step 00223 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=01.6269 diff={max=04.1364, min=00.0376, mean=00.9736} policy_loss=-3.5909 policy updated! \n",
      "train step 00224 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=00.9744 diff={max=02.5574, min=00.0122, mean=00.7812} policy_loss=-3.4070 policy updated! \n",
      "train step 00225 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.1646 diff={max=04.2395, min=00.0034, mean=01.0511} policy_loss=-3.2818 policy updated! \n",
      "train step 00226 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8956 diff={max=03.0210, min=00.0256, mean=00.6667} policy_loss=-3.1553 policy updated! \n",
      "train step 00227 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9210 diff={max=05.1940, min=00.0347, mean=00.9991} policy_loss=-3.0646 policy updated! \n",
      "train step 00228 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=02.3234 diff={max=04.2068, min=00.0141, mean=01.1259} policy_loss=-3.0025 policy updated! \n",
      "train step 00229 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=01.7462 diff={max=05.8161, min=00.0323, mean=00.9517} policy_loss=-3.3683 policy updated! \n",
      "train step 00230 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.0455 diff={max=07.0377, min=00.0140, mean=00.9078} policy_loss=-3.3475 policy updated! \n",
      "train step 00231 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=01.4656 diff={max=03.1370, min=00.0582, mean=00.9236} policy_loss=-3.6565 policy updated! \n",
      "train step 00232 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3332 diff={max=03.3748, min=00.0289, mean=00.8347} policy_loss=-3.4434 policy updated! \n",
      "train step 00233 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6641 diff={max=06.2027, min=00.0013, mean=01.2029} policy_loss=-3.3922 policy updated! \n",
      "train step 00234 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1820 diff={max=02.4804, min=00.0218, mean=00.8321} policy_loss=-3.1962 policy updated! \n",
      "train step 00235 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.7745 diff={max=05.9367, min=00.0141, mean=00.8952} policy_loss=-3.0752 policy updated! \n",
      "train step 00236 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8609 diff={max=03.9504, min=00.0360, mean=00.9445} policy_loss=-3.1040 policy updated! \n",
      "train step 00237 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2340 diff={max=04.6751, min=00.0117, mean=00.7085} policy_loss=-3.1030 policy updated! \n",
      "train step 00238 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=01.3085 diff={max=03.9103, min=00.0159, mean=00.8852} policy_loss=-2.9878 policy updated! \n",
      "train step 00239 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4760 diff={max=04.0510, min=00.0052, mean=00.9260} policy_loss=-3.3428 policy updated! \n",
      "train step 00240 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.3345 diff={max=03.3786, min=00.0177, mean=00.8464} policy_loss=-3.5148 policy updated! \n",
      "train step 00241 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=01.4789 diff={max=04.7219, min=00.0149, mean=00.8905} policy_loss=-3.4508 policy updated! \n",
      "train step 00242 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0831 diff={max=04.1629, min=00.0083, mean=01.0436} policy_loss=-3.7716 policy updated! \n",
      "train step 00243 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3956 diff={max=03.2209, min=00.0100, mean=00.8554} policy_loss=-3.6742 policy updated! \n",
      "train step 00244 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=02.1732 diff={max=05.9902, min=00.0054, mean=00.9886} policy_loss=-3.4401 policy updated! \n",
      "train step 00245 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.0647 diff={max=03.3498, min=00.0128, mean=00.7954} policy_loss=-3.4893 policy updated! \n",
      "train step 00246 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=02.0452 diff={max=05.9004, min=00.0110, mean=00.9989} policy_loss=-3.5491 policy updated! \n",
      "train step 00247 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0727 diff={max=03.6234, min=00.0052, mean=00.7508} policy_loss=-3.2145 policy updated! \n",
      "train step 00248 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=03.5029 diff={max=08.2299, min=00.0042, mean=00.9837} policy_loss=-3.2421 policy updated! \n",
      "train step 00249 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8904 diff={max=06.8007, min=00.0282, mean=00.8795} policy_loss=-2.9118 policy updated! \n",
      "train step 00250 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.0877 diff={max=04.8239, min=00.0092, mean=01.0953} policy_loss=-3.0263 policy updated! \n",
      "train step 00251 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2370 diff={max=04.5831, min=00.0046, mean=01.1433} policy_loss=-3.4374 policy updated! \n",
      "train step 00252 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7104 diff={max=07.1538, min=00.0583, mean=01.1001} policy_loss=-3.2787 policy updated! \n",
      "train step 00253 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=01.8833 diff={max=05.3319, min=00.0040, mean=00.9414} policy_loss=-3.2965 policy updated! \n",
      "train step 00254 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3574 diff={max=04.0508, min=00.0134, mean=00.8138} policy_loss=-3.5112 policy updated! \n",
      "train step 00255 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.4321 diff={max=04.6758, min=00.0211, mean=00.8084} policy_loss=-3.3705 policy updated! \n",
      "train step 00256 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=01.0670 diff={max=05.1928, min=00.0007, mean=00.6586} policy_loss=-3.1892 policy updated! \n",
      "train step 00257 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9562 diff={max=03.6620, min=00.0314, mean=01.0602} policy_loss=-3.4721 policy updated! \n",
      "train step 00258 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9922 diff={max=03.4121, min=00.0028, mean=00.7279} policy_loss=-3.3672 policy updated! \n",
      "train step 00259 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3466 diff={max=04.7174, min=00.0452, mean=00.8447} policy_loss=-3.3182 policy updated! \n",
      "train step 00260 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.5794 diff={max=04.6596, min=00.0014, mean=00.8902} policy_loss=-3.1921 policy updated! \n",
      "train step 00261 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0017 diff={max=05.0072, min=00.0452, mean=00.9808} policy_loss=-3.4047 policy updated! \n",
      "train step 00262 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1346 diff={max=06.4411, min=00.0202, mean=00.8987} policy_loss=-3.2724 policy updated! \n",
      "train step 00263 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3189 diff={max=07.1769, min=00.0376, mean=00.9069} policy_loss=-3.2007 policy updated! \n",
      "train step 00264 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2073 diff={max=05.4616, min=00.0000, mean=00.6576} policy_loss=-3.5200 policy updated! \n",
      "train step 00265 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.6828 diff={max=04.8585, min=00.0022, mean=00.8097} policy_loss=-3.7047 policy updated! \n",
      "train step 00266 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1162 diff={max=05.1916, min=00.0004, mean=00.8871} policy_loss=-3.3997 policy updated! \n",
      "train step 00267 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9457 diff={max=03.8944, min=00.0031, mean=00.6963} policy_loss=-3.4696 policy updated! \n",
      "train step 00268 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=02.4104 diff={max=07.7259, min=00.0199, mean=00.8647} policy_loss=-3.4549 policy updated! \n",
      "train step 00269 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5744 diff={max=04.5515, min=00.0071, mean=00.9291} policy_loss=-3.4747 policy updated! \n",
      "train step 00270 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=00.8218 diff={max=02.6516, min=00.0070, mean=00.7031} policy_loss=-3.5118 policy updated! \n",
      "train step 00271 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1526 diff={max=06.3099, min=00.0279, mean=01.0161} policy_loss=-3.5749 policy updated! \n",
      "train step 00272 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0548 diff={max=03.4910, min=00.0257, mean=00.7906} policy_loss=-3.2345 policy updated! \n",
      "train step 00273 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0113 diff={max=06.1596, min=00.0554, mean=00.9770} policy_loss=-2.9866 policy updated! \n",
      "train step 00274 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8390 diff={max=02.8834, min=00.0361, mean=00.7090} policy_loss=-3.5415 policy updated! \n",
      "train step 00275 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.6505 diff={max=04.0765, min=00.0182, mean=00.8665} policy_loss=-3.1213 policy updated! \n",
      "train step 00276 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=01.5484 diff={max=04.6962, min=00.0423, mean=00.8599} policy_loss=-3.2775 policy updated! \n",
      "train step 00277 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3564 diff={max=03.9317, min=00.0090, mean=00.7943} policy_loss=-3.1206 policy updated! \n",
      "train step 00278 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=02.8617 diff={max=06.2444, min=00.0242, mean=01.1039} policy_loss=-2.9279 policy updated! \n",
      "train step 00279 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3721 diff={max=03.3875, min=00.0337, mean=00.8550} policy_loss=-3.4628 policy updated! \n",
      "train step 00280 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.2268 diff={max=03.8308, min=00.0357, mean=01.1013} policy_loss=-3.2399 policy updated! \n",
      "train step 00281 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6849 diff={max=05.4589, min=00.0381, mean=00.9143} policy_loss=-3.2905 policy updated! \n",
      "train step 00282 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2080 diff={max=05.4807, min=00.0113, mean=00.9624} policy_loss=-3.3901 policy updated! \n",
      "train step 00283 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8560 diff={max=02.5328, min=00.0125, mean=00.7089} policy_loss=-3.2440 policy updated! \n",
      "train step 00284 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=03.0593 diff={max=04.9324, min=00.0053, mean=01.1862} policy_loss=-3.1982 policy updated! \n",
      "train step 00285 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.8136 diff={max=03.3301, min=00.0009, mean=00.9824} policy_loss=-3.1521 policy updated! \n",
      "train step 00286 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1882 diff={max=05.3232, min=00.0070, mean=00.9286} policy_loss=-3.2880 policy updated! \n",
      "train step 00287 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8888 diff={max=05.8234, min=00.0370, mean=01.2633} policy_loss=-3.7634 policy updated! \n",
      "train step 00288 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1172 diff={max=04.2305, min=00.0116, mean=01.0345} policy_loss=-3.8668 policy updated! \n",
      "train step 00289 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=02.3023 diff={max=04.8540, min=00.0271, mean=01.1296} policy_loss=-3.3862 policy updated! \n",
      "train step 00290 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.6385 diff={max=04.2638, min=00.0433, mean=00.9575} policy_loss=-3.7013 policy updated! \n",
      "train step 00291 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=01.4205 diff={max=03.0271, min=00.0568, mean=00.9165} policy_loss=-3.6714 policy updated! \n",
      "train step 00292 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0396 diff={max=07.9926, min=00.0068, mean=00.8236} policy_loss=-3.1249 policy updated! \n",
      "train step 00293 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4960 diff={max=05.5746, min=00.0206, mean=01.0712} policy_loss=-3.2782 policy updated! \n",
      "train step 00294 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3019 diff={max=03.0769, min=00.0567, mean=00.9054} policy_loss=-2.7154 policy updated! \n",
      "train step 00295 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.3490 diff={max=03.8641, min=00.0132, mean=00.9041} policy_loss=-2.8660 policy updated! \n",
      "train step 00296 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4075 diff={max=06.7002, min=00.0905, mean=01.0379} policy_loss=-3.0718 policy updated! \n",
      "train step 00297 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2204 diff={max=04.5749, min=00.0147, mean=00.7870} policy_loss=-3.1899 policy updated! \n",
      "train step 00298 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2046 diff={max=05.5307, min=00.0446, mean=01.1781} policy_loss=-3.3800 policy updated! \n",
      "train step 00299 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0404 diff={max=07.7584, min=00.0470, mean=01.1557} policy_loss=-3.4251 policy updated! \n",
      "train step 00300 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.4213 diff={max=03.0002, min=00.0242, mean=00.8860} policy_loss=-3.8490 policy updated! \n",
      "train step 00301 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=02.5235 diff={max=04.7700, min=00.0158, mean=01.1501} policy_loss=-3.8683 policy updated! \n",
      "train step 00302 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5009 diff={max=03.4457, min=00.0265, mean=00.9441} policy_loss=-3.8653 policy updated! \n",
      "train step 00303 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5419 diff={max=03.7332, min=00.0802, mean=00.9169} policy_loss=-3.3479 policy updated! \n",
      "train step 00304 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6199 diff={max=06.1089, min=00.0265, mean=00.8747} policy_loss=-3.4560 policy updated! \n",
      "train step 00305 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.6222 diff={max=02.1849, min=00.0025, mean=00.5835} policy_loss=-3.1792 policy updated! \n",
      "train step 00306 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8954 diff={max=05.5532, min=00.0655, mean=01.1574} policy_loss=-2.9656 policy updated! \n",
      "train step 00307 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0436 diff={max=05.8774, min=00.0432, mean=00.9481} policy_loss=-3.3109 policy updated! \n",
      "train step 00308 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.9157 diff={max=05.3886, min=00.0274, mean=01.0223} policy_loss=-3.3956 policy updated! \n",
      "train step 00309 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8326 diff={max=05.4857, min=00.0536, mean=01.2030} policy_loss=-3.7143 policy updated! \n",
      "train step 00310 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.3188 diff={max=03.5917, min=00.0296, mean=00.9393} policy_loss=-3.8142 policy updated! \n",
      "train step 00311 reward={max=06.0000, min=00.0000, mean=03.2000} optimizing loss=02.8538 diff={max=06.5772, min=00.0325, mean=01.1768} policy_loss=-3.8899 policy updated! \n",
      "train step 00312 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2937 diff={max=03.2275, min=00.0435, mean=00.8612} policy_loss=-3.6605 policy updated! \n",
      "train step 00313 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6867 diff={max=05.2107, min=00.0194, mean=01.1467} policy_loss=-3.3759 policy updated! \n",
      "train step 00314 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=03.7241 diff={max=06.6033, min=00.0318, mean=01.2817} policy_loss=-3.3765 policy updated! \n",
      "train step 00315 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.9350 diff={max=07.6718, min=00.0174, mean=01.1289} policy_loss=-3.2844 policy updated! \n",
      "train step 00316 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.8520 diff={max=03.9564, min=00.0266, mean=01.0008} policy_loss=-3.1251 policy updated! \n",
      "train step 00317 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5276 diff={max=05.6402, min=00.0814, mean=00.7789} policy_loss=-3.1349 policy updated! \n",
      "train step 00318 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.4994 diff={max=03.8406, min=00.0339, mean=00.9013} policy_loss=-3.3496 policy updated! \n",
      "train step 00319 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=02.5964 diff={max=04.4438, min=00.0172, mean=01.1487} policy_loss=-3.7988 policy updated! \n",
      "train step 00320 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.0824 diff={max=03.9757, min=00.0132, mean=00.7320} policy_loss=-4.2263 policy updated! \n",
      "train step 00321 reward={max=06.0000, min=00.0000, mean=03.2000} optimizing loss=02.1427 diff={max=06.1036, min=00.0135, mean=00.9279} policy_loss=-4.1525 policy updated! \n",
      "train step 00322 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8865 diff={max=02.4186, min=00.0043, mean=00.6976} policy_loss=-4.1882 policy updated! \n",
      "train step 00323 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1592 diff={max=04.8443, min=00.0052, mean=01.0252} policy_loss=-3.7524 policy updated! \n",
      "train step 00324 reward={max=06.0000, min=00.0000, mean=04.4000} optimizing loss=02.8569 diff={max=05.1850, min=00.0502, mean=01.1014} policy_loss=-3.1959 policy updated! \n",
      "train step 00325 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.6849 diff={max=06.7819, min=00.0218, mean=01.0451} policy_loss=-3.1940 policy updated! \n",
      "train step 00326 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4672 diff={max=02.9842, min=00.0684, mean=00.9594} policy_loss=-2.9915 policy updated! \n",
      "train step 00327 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2604 diff={max=02.3604, min=00.0167, mean=00.9174} policy_loss=-3.0806 policy updated! \n",
      "train step 00328 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.8276 diff={max=04.5279, min=00.0438, mean=01.0151} policy_loss=-3.1993 policy updated! \n",
      "train step 00329 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.2813 diff={max=04.7425, min=00.0191, mean=00.7937} policy_loss=-3.3276 policy updated! \n",
      "train step 00330 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.0841 diff={max=05.1551, min=00.0016, mean=01.0256} policy_loss=-3.8012 policy updated! \n",
      "train step 00331 reward={max=06.0000, min=00.0000, mean=03.4000} optimizing loss=01.3248 diff={max=04.4514, min=00.0143, mean=00.8089} policy_loss=-4.0606 policy updated! \n",
      "train step 00332 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=01.6661 diff={max=04.7575, min=00.0002, mean=00.8773} policy_loss=-3.9682 policy updated! \n",
      "train step 00333 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2149 diff={max=05.3994, min=00.0531, mean=01.0134} policy_loss=-3.6423 policy updated! \n",
      "train step 00334 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3973 diff={max=03.6461, min=00.0009, mean=00.7905} policy_loss=-3.4182 policy updated! \n",
      "train step 00335 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.9910 diff={max=03.3003, min=00.0028, mean=00.6817} policy_loss=-3.1271 policy updated! \n",
      "train step 00336 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.7896 diff={max=04.8080, min=00.0468, mean=00.9497} policy_loss=-3.0980 policy updated! \n",
      "train step 00337 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3782 diff={max=05.2619, min=00.0143, mean=01.1490} policy_loss=-2.9507 policy updated! \n",
      "train step 00338 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=03.7250 diff={max=06.5142, min=00.0657, mean=01.3754} policy_loss=-2.9501 policy updated! \n",
      "train step 00339 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5326 diff={max=05.3753, min=00.0027, mean=00.8438} policy_loss=-3.4154 policy updated! \n",
      "train step 00340 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.9145 diff={max=02.6288, min=00.0204, mean=00.7205} policy_loss=-3.3934 policy updated! \n",
      "train step 00341 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9652 diff={max=03.9333, min=00.0258, mean=00.9936} policy_loss=-3.5051 policy updated! \n",
      "train step 00342 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5725 diff={max=05.5264, min=00.0027, mean=00.8941} policy_loss=-3.8361 policy updated! \n",
      "train step 00343 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=00.5862 diff={max=01.9737, min=00.0198, mean=00.5902} policy_loss=-3.7611 policy updated! \n",
      "train step 00344 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5976 diff={max=04.8941, min=00.0304, mean=00.8841} policy_loss=-3.7611 policy updated! \n",
      "train step 00345 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.5163 diff={max=08.4377, min=00.0011, mean=00.9452} policy_loss=-3.4718 policy updated! \n",
      "train step 00346 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=01.7178 diff={max=05.9087, min=00.0009, mean=00.7747} policy_loss=-3.0600 policy updated! \n",
      "train step 00347 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3321 diff={max=05.3355, min=00.0242, mean=01.0285} policy_loss=-3.2084 policy updated! \n",
      "train step 00348 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6094 diff={max=05.1793, min=00.0161, mean=00.8148} policy_loss=-3.1972 policy updated! \n",
      "train step 00349 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=01.8548 diff={max=04.1029, min=00.0322, mean=01.0412} policy_loss=-3.4144 policy updated! \n",
      "train step 00350 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=01.6263 diff={max=03.9284, min=00.0360, mean=00.9485} policy_loss=-3.3084 policy updated! \n",
      "train step 00351 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=01.2523 diff={max=04.6162, min=00.0063, mean=00.7212} policy_loss=-4.1044 policy updated! \n",
      "train step 00352 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0694 diff={max=03.3439, min=00.0092, mean=00.7954} policy_loss=-3.9033 policy updated! \n",
      "train step 00353 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0605 diff={max=03.9686, min=00.0195, mean=00.7816} policy_loss=-4.0697 policy updated! \n",
      "train step 00354 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=01.4436 diff={max=03.8448, min=00.0602, mean=00.9237} policy_loss=-4.2334 policy updated! \n",
      "train step 00355 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.0818 diff={max=03.9396, min=00.0023, mean=01.0674} policy_loss=-3.4237 policy updated! \n",
      "train step 00356 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3798 diff={max=04.7553, min=00.0223, mean=00.7553} policy_loss=-3.6558 policy updated! \n",
      "train step 00357 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7269 diff={max=05.4942, min=00.0064, mean=00.8391} policy_loss=-3.2768 policy updated! \n",
      "train step 00358 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1622 diff={max=03.2167, min=00.0042, mean=00.8118} policy_loss=-3.2446 policy updated! \n",
      "train step 00359 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=01.6860 diff={max=03.8479, min=00.0300, mean=00.9097} policy_loss=-3.0153 policy updated! \n",
      "train step 00360 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.1726 diff={max=03.7479, min=00.0063, mean=00.8189} policy_loss=-3.2545 policy updated! \n",
      "train step 00361 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=00.8547 diff={max=03.4744, min=00.0070, mean=00.6879} policy_loss=-3.0595 policy updated! \n",
      "train step 00362 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6698 diff={max=04.7007, min=00.0174, mean=00.8788} policy_loss=-3.2109 policy updated! \n",
      "train step 00363 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1666 diff={max=03.3712, min=00.0175, mean=00.8009} policy_loss=-3.5790 policy updated! \n",
      "train step 00364 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1721 diff={max=03.3565, min=00.0118, mean=00.7510} policy_loss=-3.9117 policy updated! \n",
      "train step 00365 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=00.9542 diff={max=02.8579, min=00.0295, mean=00.7074} policy_loss=-3.5809 policy updated! \n",
      "train step 00366 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=00.8312 diff={max=02.5347, min=00.0055, mean=00.7176} policy_loss=-3.3388 policy updated! \n",
      "train step 00367 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0606 diff={max=06.0609, min=00.0036, mean=00.9136} policy_loss=-3.3557 policy updated! \n",
      "train step 00368 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2218 diff={max=09.7463, min=00.0287, mean=00.9302} policy_loss=-3.3232 policy updated! \n",
      "train step 00369 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=02.0609 diff={max=04.5115, min=00.0125, mean=01.0255} policy_loss=-3.0021 policy updated! \n",
      "train step 00370 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.1749 diff={max=03.2799, min=00.0075, mean=00.8340} policy_loss=-2.9501 policy updated! \n",
      "train step 00371 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9258 diff={max=02.6441, min=00.0094, mean=00.7249} policy_loss=-3.1571 policy updated! \n",
      "train step 00372 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0419 diff={max=04.3819, min=00.0144, mean=01.0759} policy_loss=-3.2469 policy updated! \n",
      "train step 00373 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=01.2335 diff={max=03.6631, min=00.0028, mean=00.8171} policy_loss=-3.8657 policy updated! \n",
      "train step 00374 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9500 diff={max=04.7433, min=00.0188, mean=00.6117} policy_loss=-3.8064 policy updated! \n",
      "train step 00375 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.8452 diff={max=07.9229, min=00.0076, mean=00.7684} policy_loss=-3.6827 policy updated! \n",
      "train step 00376 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=01.5089 diff={max=04.7508, min=00.0156, mean=00.8505} policy_loss=-3.7858 policy updated! \n",
      "train step 00377 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7365 diff={max=05.6291, min=00.0120, mean=01.2330} policy_loss=-3.6569 policy updated! \n",
      "train step 00378 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4284 diff={max=03.8417, min=00.0210, mean=00.7930} policy_loss=-3.0308 policy updated! \n",
      "train step 00379 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=00.9273 diff={max=03.7848, min=00.0059, mean=00.6770} policy_loss=-3.6922 policy updated! \n",
      "train step 00380 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.0030 diff={max=07.8619, min=00.0172, mean=00.8427} policy_loss=-3.2872 policy updated! \n",
      "train step 00381 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5399 diff={max=05.1094, min=00.0009, mean=01.0241} policy_loss=-3.3261 policy updated! \n",
      "train step 00382 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2998 diff={max=04.0231, min=00.0015, mean=00.7838} policy_loss=-3.7882 policy updated! \n",
      "train step 00383 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5457 diff={max=05.1709, min=00.0059, mean=00.7778} policy_loss=-3.8906 policy updated! \n",
      "train step 00384 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3332 diff={max=04.8333, min=00.0042, mean=00.7724} policy_loss=-3.8339 policy updated! \n",
      "train step 00385 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.7181 diff={max=05.0838, min=00.0233, mean=00.7380} policy_loss=-3.8060 policy updated! \n",
      "train step 00386 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7069 diff={max=05.2525, min=00.0427, mean=00.8473} policy_loss=-3.1414 policy updated! \n",
      "train step 00387 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1344 diff={max=03.1666, min=00.0203, mean=00.8106} policy_loss=-3.1307 policy updated! \n",
      "train step 00388 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8216 diff={max=03.6107, min=00.0539, mean=00.6372} policy_loss=-3.1778 policy updated! \n",
      "train step 00389 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2764 diff={max=05.1013, min=00.0299, mean=01.0767} policy_loss=-2.7372 policy updated! \n",
      "train step 00390 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.1651 diff={max=04.6421, min=00.0274, mean=00.7915} policy_loss=-3.2797 policy updated! \n",
      "train step 00391 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5175 diff={max=05.6355, min=00.0116, mean=01.0367} policy_loss=-3.6945 policy updated! \n",
      "train step 00392 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9536 diff={max=06.6238, min=00.0179, mean=00.8502} policy_loss=-3.4754 policy updated! \n",
      "train step 00393 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.9262 diff={max=04.1252, min=00.0146, mean=00.9474} policy_loss=-3.3868 policy updated! \n",
      "train step 00394 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.6955 diff={max=05.6535, min=00.0197, mean=00.8367} policy_loss=-4.0033 policy updated! \n",
      "train step 00395 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.2932 diff={max=04.9075, min=00.0213, mean=00.7454} policy_loss=-3.6434 policy updated! \n",
      "train step 00396 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0010 diff={max=05.6421, min=00.0142, mean=00.9653} policy_loss=-3.2521 policy updated! \n",
      "train step 00397 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7321 diff={max=05.6000, min=00.0276, mean=00.7953} policy_loss=-3.2003 policy updated! \n",
      "train step 00398 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6760 diff={max=06.1894, min=00.0175, mean=01.2489} policy_loss=-3.3090 policy updated! \n",
      "train step 00399 reward={max=06.0000, min=00.0000, mean=04.2000} optimizing loss=00.7786 diff={max=02.1704, min=00.0006, mean=00.6831} policy_loss=-3.0593 policy updated! \n",
      "train step 00400 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.7493 diff={max=04.9543, min=00.0211, mean=00.8968} policy_loss=-3.0032 policy updated! \n",
      "train step 00401 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=01.9931 diff={max=03.8435, min=00.0212, mean=00.9948} policy_loss=-3.4260 policy updated! \n",
      "train step 00402 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=03.1625 diff={max=05.6552, min=00.0243, mean=01.1326} policy_loss=-3.6994 policy updated! \n",
      "train step 00403 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2093 diff={max=03.4057, min=00.0835, mean=00.8345} policy_loss=-3.2951 policy updated! \n",
      "train step 00404 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=00.6678 diff={max=03.0496, min=00.0214, mean=00.6361} policy_loss=-3.8353 policy updated! \n",
      "train step 00405 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.4434 diff={max=05.3372, min=00.0177, mean=00.8261} policy_loss=-3.3148 policy updated! \n",
      "train step 00406 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1901 diff={max=03.7533, min=00.0189, mean=00.7552} policy_loss=-3.3588 policy updated! \n",
      "train step 00407 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4661 diff={max=04.9837, min=00.0189, mean=00.7856} policy_loss=-3.3287 policy updated! \n",
      "train step 00408 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9043 diff={max=03.2163, min=00.0020, mean=00.6425} policy_loss=-3.0303 policy updated! \n",
      "train step 00409 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=01.6394 diff={max=04.8864, min=00.0104, mean=00.8257} policy_loss=-3.3514 policy updated! \n",
      "train step 00410 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.6629 diff={max=03.8809, min=00.0167, mean=00.9536} policy_loss=-3.2686 policy updated! \n",
      "train step 00411 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=01.4272 diff={max=03.5633, min=00.0264, mean=00.8337} policy_loss=-3.3524 policy updated! \n",
      "train step 00412 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.5172 diff={max=01.7847, min=00.0195, mean=00.5558} policy_loss=-3.5211 policy updated! \n",
      "train step 00413 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4967 diff={max=06.6898, min=00.0176, mean=01.0400} policy_loss=-3.4439 policy updated! \n",
      "train step 00414 reward={max=06.0000, min=00.0000, mean=04.4000} optimizing loss=01.8907 diff={max=05.7862, min=00.0145, mean=00.8637} policy_loss=-4.0470 policy updated! \n",
      "train step 00415 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.3764 diff={max=05.8165, min=00.0091, mean=00.7173} policy_loss=-3.6955 policy updated! \n",
      "train step 00416 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9850 diff={max=05.0908, min=00.0116, mean=01.1232} policy_loss=-3.3065 policy updated! \n",
      "train step 00417 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1546 diff={max=03.3634, min=00.0808, mean=00.8179} policy_loss=-3.4891 policy updated! \n",
      "train step 00418 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=02.1977 diff={max=05.2978, min=00.0139, mean=00.9904} policy_loss=-3.4385 policy updated! \n",
      "train step 00419 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=01.3794 diff={max=04.0133, min=00.0273, mean=00.7643} policy_loss=-3.4183 policy updated! \n",
      "train step 00420 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.9339 diff={max=06.1823, min=00.0240, mean=00.8047} policy_loss=-3.3935 policy updated! \n",
      "train step 00421 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4126 diff={max=04.9863, min=00.0132, mean=00.8143} policy_loss=-3.5081 policy updated! \n",
      "train step 00422 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0528 diff={max=05.5468, min=00.0082, mean=00.8836} policy_loss=-3.8372 policy updated! \n",
      "train step 00423 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5036 diff={max=05.6032, min=00.0170, mean=00.7007} policy_loss=-3.9122 policy updated! \n",
      "train step 00424 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.6834 diff={max=02.1700, min=00.0583, mean=00.6493} policy_loss=-4.0608 policy updated! \n",
      "train step 00425 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.5547 diff={max=07.1288, min=00.0390, mean=00.7135} policy_loss=-3.4606 policy updated! \n",
      "train step 00426 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=01.1112 diff={max=04.5553, min=00.0197, mean=00.6816} policy_loss=-3.3502 policy updated! \n",
      "train step 00427 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7674 diff={max=05.6072, min=00.0549, mean=00.9048} policy_loss=-3.5192 policy updated! \n",
      "train step 00428 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9707 diff={max=02.5191, min=00.0272, mean=00.7564} policy_loss=-3.5189 policy updated! \n",
      "train step 00429 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1203 diff={max=03.4606, min=00.0204, mean=00.7349} policy_loss=-3.6834 policy updated! \n",
      "train step 00430 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.1878 diff={max=06.8345, min=00.0415, mean=01.1434} policy_loss=-3.7439 policy updated! \n",
      "train step 00431 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.3739 diff={max=03.8053, min=00.0091, mean=00.7857} policy_loss=-3.6977 policy updated! \n",
      "train step 00432 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4304 diff={max=04.3988, min=00.0002, mean=01.0063} policy_loss=-3.3874 policy updated! \n",
      "train step 00433 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7594 diff={max=06.9478, min=00.0102, mean=00.9672} policy_loss=-3.4969 policy updated! \n",
      "train step 00434 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1626 diff={max=06.3018, min=00.0155, mean=01.1594} policy_loss=-3.3952 policy updated! \n",
      "train step 00435 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.4320 diff={max=03.8655, min=00.0236, mean=00.7360} policy_loss=-3.1255 policy updated! \n",
      "train step 00436 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.4749 diff={max=06.6791, min=00.0509, mean=00.7310} policy_loss=-3.5398 policy updated! \n",
      "train step 00437 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0418 diff={max=07.4352, min=00.0012, mean=00.8246} policy_loss=-3.5649 policy updated! \n",
      "train step 00438 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=00.9548 diff={max=02.3675, min=00.0493, mean=00.7697} policy_loss=-3.7040 policy updated! \n",
      "train step 00439 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2649 diff={max=03.0760, min=00.0297, mean=00.8161} policy_loss=-3.5996 policy updated! \n",
      "train step 00440 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.8036 diff={max=04.8601, min=00.0699, mean=00.8922} policy_loss=-3.6727 policy updated! \n",
      "train step 00441 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=01.1351 diff={max=03.5502, min=00.0439, mean=00.8150} policy_loss=-3.6271 policy updated! \n",
      "train step 00442 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8397 diff={max=03.1322, min=00.0063, mean=00.6608} policy_loss=-3.7636 policy updated! \n",
      "train step 00443 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.6164 diff={max=02.3071, min=00.0450, mean=00.5645} policy_loss=-3.6394 policy updated! \n",
      "train step 00444 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3462 diff={max=04.6666, min=00.0097, mean=00.7052} policy_loss=-3.4150 policy updated! \n",
      "train step 00445 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.2630 diff={max=03.6952, min=00.0149, mean=00.7649} policy_loss=-3.0774 policy updated! \n",
      "train step 00446 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9119 diff={max=03.7313, min=00.0047, mean=00.6365} policy_loss=-3.1905 policy updated! \n",
      "train step 00447 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7364 diff={max=05.4527, min=00.0366, mean=00.8480} policy_loss=-3.4662 policy updated! \n",
      "train step 00448 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3821 diff={max=04.8419, min=00.0054, mean=01.0865} policy_loss=-3.7478 policy updated! \n",
      "train step 00449 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3601 diff={max=05.1408, min=00.0011, mean=00.7751} policy_loss=-3.6599 policy updated! \n",
      "train step 00450 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.6281 diff={max=04.4323, min=00.0381, mean=00.7440} policy_loss=-3.6639 policy updated! \n",
      "train step 00451 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6133 diff={max=06.4911, min=00.0172, mean=00.7345} policy_loss=-4.5342 policy updated! \n",
      "train step 00452 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9314 diff={max=05.7891, min=00.0129, mean=00.7939} policy_loss=-3.6130 policy updated! \n",
      "train step 00453 reward={max=07.0000, min=00.0000, mean=02.6000} optimizing loss=01.3086 diff={max=04.0950, min=00.0095, mean=00.8458} policy_loss=-3.2130 policy updated! \n",
      "train step 00454 reward={max=06.0000, min=00.0000, mean=04.2000} optimizing loss=01.2815 diff={max=04.3070, min=00.0201, mean=00.7539} policy_loss=-3.4699 policy updated! \n",
      "train step 00455 reward={max=06.0000, min=05.0000, mean=05.4000} optimizing loss=01.7136 diff={max=04.9365, min=00.0184, mean=00.9500} policy_loss=-3.6322 policy updated! \n",
      "train step 00456 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=00.9095 diff={max=02.7993, min=00.0421, mean=00.7232} policy_loss=-3.4842 policy updated! \n",
      "train step 00457 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2673 diff={max=04.2555, min=00.0049, mean=01.0133} policy_loss=-3.7885 policy updated! \n",
      "train step 00458 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8182 diff={max=04.4961, min=00.0027, mean=00.9873} policy_loss=-4.0676 policy updated! \n",
      "train step 00459 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7558 diff={max=03.9218, min=00.0079, mean=01.0287} policy_loss=-3.7257 policy updated! \n",
      "train step 00460 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.7566 diff={max=06.6590, min=00.0149, mean=00.7901} policy_loss=-3.5673 policy updated! \n",
      "train step 00461 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=02.2025 diff={max=06.0287, min=00.0008, mean=00.8659} policy_loss=-3.1522 policy updated! \n",
      "train step 00462 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8787 diff={max=02.6298, min=00.0018, mean=00.7079} policy_loss=-3.3196 policy updated! \n",
      "train step 00463 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8659 diff={max=04.8546, min=00.0173, mean=01.1303} policy_loss=-3.7221 policy updated! \n",
      "train step 00464 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9717 diff={max=02.2459, min=00.0889, mean=00.7803} policy_loss=-3.3502 policy updated! \n",
      "train step 00465 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.7339 diff={max=04.6307, min=00.0259, mean=00.8300} policy_loss=-3.9694 policy updated! \n",
      "train step 00466 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0797 diff={max=04.4663, min=00.0033, mean=00.6348} policy_loss=-4.0061 policy updated! \n",
      "train step 00467 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4910 diff={max=05.4075, min=00.0104, mean=00.8014} policy_loss=-4.0599 policy updated! \n",
      "train step 00468 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5343 diff={max=06.7422, min=00.0878, mean=01.1356} policy_loss=-3.6312 policy updated! \n",
      "train step 00469 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5767 diff={max=06.3491, min=00.0571, mean=00.8235} policy_loss=-3.3634 policy updated! \n",
      "train step 00470 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.9742 diff={max=03.7533, min=00.0143, mean=00.6877} policy_loss=-2.7448 policy updated! \n",
      "train step 00471 reward={max=05.0000, min=05.0000, mean=05.0000} optimizing loss=02.4910 diff={max=04.9561, min=00.0541, mean=01.0629} policy_loss=-2.6435 policy updated! \n",
      "train step 00472 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4003 diff={max=03.1602, min=00.0146, mean=00.9390} policy_loss=-3.1792 policy updated! \n",
      "train step 00473 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=02.9032 diff={max=04.8756, min=00.0101, mean=01.1653} policy_loss=-3.2963 policy updated! \n",
      "train step 00474 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.2797 diff={max=03.5441, min=00.0280, mean=00.7978} policy_loss=-3.5143 policy updated! \n",
      "train step 00475 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.5781 diff={max=02.9537, min=00.0074, mean=00.9599} policy_loss=-4.3361 policy updated! \n",
      "train step 00476 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.6361 diff={max=04.1635, min=00.0430, mean=00.8995} policy_loss=-4.3774 policy updated! \n",
      "train step 00477 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5415 diff={max=03.9965, min=00.0324, mean=01.2232} policy_loss=-3.8513 policy updated! \n",
      "train step 00478 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0686 diff={max=06.5374, min=00.0053, mean=00.8398} policy_loss=-3.8173 policy updated! \n",
      "train step 00479 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.3483 diff={max=03.0596, min=00.0632, mean=00.8822} policy_loss=-3.4311 policy updated! \n",
      "train step 00480 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.4689 diff={max=08.9531, min=00.0003, mean=00.8333} policy_loss=-3.4067 policy updated! \n",
      "train step 00481 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6485 diff={max=05.0426, min=00.0062, mean=00.8497} policy_loss=-3.0152 policy updated! \n",
      "train step 00482 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3131 diff={max=06.8670, min=00.0184, mean=01.2881} policy_loss=-3.2192 policy updated! \n",
      "train step 00483 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=03.5711 diff={max=05.1392, min=00.0693, mean=01.4323} policy_loss=-3.2596 policy updated! \n",
      "train step 00484 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=02.3592 diff={max=03.9990, min=00.0031, mean=01.1404} policy_loss=-3.7457 policy updated! \n",
      "train step 00485 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.7633 diff={max=03.4776, min=00.1218, mean=01.0471} policy_loss=-3.8198 policy updated! \n",
      "train step 00486 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9645 diff={max=04.8754, min=00.0292, mean=00.9983} policy_loss=-3.4793 policy updated! \n",
      "train step 00487 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7408 diff={max=05.8270, min=00.0039, mean=00.8314} policy_loss=-4.1618 policy updated! \n",
      "train step 00488 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=01.1008 diff={max=03.0951, min=00.0160, mean=00.7800} policy_loss=-3.5895 policy updated! \n",
      "train step 00489 reward={max=07.0000, min=00.0000, mean=05.0000} optimizing loss=00.9070 diff={max=02.6915, min=00.0493, mean=00.6765} policy_loss=-3.6953 policy updated! \n",
      "train step 00490 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.1287 diff={max=05.1145, min=00.0008, mean=00.8977} policy_loss=-3.7996 policy updated! \n",
      "train step 00491 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5221 diff={max=10.2066, min=00.0062, mean=01.1857} policy_loss=-3.6017 policy updated! \n",
      "train step 00492 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0785 diff={max=03.3834, min=00.0160, mean=00.7502} policy_loss=-3.4893 policy updated! \n",
      "train step 00493 reward={max=07.0000, min=00.0000, mean=02.6000} optimizing loss=01.9668 diff={max=04.8714, min=00.0821, mean=01.0354} policy_loss=-3.1048 policy updated! \n",
      "train step 00494 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8832 diff={max=04.7753, min=00.0001, mean=00.9552} policy_loss=-3.0307 policy updated! \n",
      "train step 00495 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.8687 diff={max=04.5770, min=00.0288, mean=00.9102} policy_loss=-3.2074 policy updated! \n",
      "train step 00496 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=01.1920 diff={max=02.8812, min=00.0065, mean=00.8306} policy_loss=-3.3385 policy updated! \n",
      "train step 00497 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7347 diff={max=06.1141, min=00.0283, mean=01.1124} policy_loss=-3.5161 policy updated! \n",
      "train step 00498 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=02.5043 diff={max=07.6998, min=00.0434, mean=00.9191} policy_loss=-4.2073 policy updated! \n",
      "train step 00499 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=01.3362 diff={max=05.6814, min=00.0879, mean=00.7279} policy_loss=-3.9450 policy updated! \n",
      "train step 00500 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.2859 diff={max=05.4813, min=00.0010, mean=00.9077} policy_loss=-3.8325 policy updated! \n",
      "train step 00501 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3492 diff={max=03.2536, min=00.0127, mean=00.7764} policy_loss=-4.1260 policy updated! \n",
      "train step 00502 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=02.9586 diff={max=07.5547, min=00.0235, mean=01.1058} policy_loss=-3.7636 policy updated! \n",
      "train step 00503 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5428 diff={max=03.2356, min=00.0143, mean=00.9226} policy_loss=-3.5499 policy updated! \n",
      "train step 00504 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6449 diff={max=03.5026, min=00.0395, mean=00.9704} policy_loss=-3.3999 policy updated! \n",
      "train step 00505 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.2594 diff={max=02.7361, min=00.0204, mean=00.8986} policy_loss=-3.7792 policy updated! \n",
      "train step 00506 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9265 diff={max=03.0387, min=00.0033, mean=00.7778} policy_loss=-3.4624 policy updated! \n",
      "train step 00507 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0852 diff={max=04.8553, min=00.0247, mean=01.0164} policy_loss=-3.6680 policy updated! \n",
      "train step 00508 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=01.4324 diff={max=05.9141, min=00.0163, mean=00.7488} policy_loss=-3.4515 policy updated! \n",
      "train step 00509 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3701 diff={max=03.4542, min=00.0112, mean=00.8538} policy_loss=-3.7269 policy updated! \n",
      "train step 00510 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.6832 diff={max=04.3394, min=00.0079, mean=00.8964} policy_loss=-3.4842 policy updated! \n",
      "train step 00511 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=01.7036 diff={max=03.9083, min=00.0221, mean=00.8523} policy_loss=-3.6188 policy updated! \n",
      "train step 00512 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3526 diff={max=03.6528, min=00.0002, mean=00.8640} policy_loss=-3.7692 policy updated! \n",
      "train step 00513 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=01.4653 diff={max=05.5138, min=00.0503, mean=00.7799} policy_loss=-3.6417 policy updated! \n",
      "train step 00514 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5809 diff={max=07.3157, min=00.0326, mean=01.0271} policy_loss=-3.7185 policy updated! \n",
      "train step 00515 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.2916 diff={max=04.9075, min=00.0006, mean=00.9111} policy_loss=-3.4375 policy updated! \n",
      "train step 00516 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4860 diff={max=06.5650, min=00.0022, mean=01.0468} policy_loss=-3.8563 policy updated! \n",
      "train step 00517 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5671 diff={max=04.7017, min=00.0843, mean=00.9413} policy_loss=-3.5531 policy updated! \n",
      "train step 00518 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1374 diff={max=02.9046, min=00.0010, mean=00.8223} policy_loss=-3.6635 policy updated! \n",
      "train step 00519 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.5708 diff={max=01.6155, min=00.0083, mean=00.6408} policy_loss=-4.3085 policy updated! \n",
      "train step 00520 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=00.9183 diff={max=03.4665, min=00.0001, mean=00.6403} policy_loss=-3.9031 policy updated! \n",
      "train step 00521 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7019 diff={max=02.8808, min=00.0029, mean=00.5721} policy_loss=-3.9746 policy updated! \n",
      "train step 00522 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6664 diff={max=06.6803, min=00.0024, mean=00.9554} policy_loss=-3.9770 policy updated! \n",
      "train step 00523 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7879 diff={max=04.4829, min=00.0068, mean=00.8639} policy_loss=-3.4436 policy updated! \n",
      "train step 00524 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3610 diff={max=07.9723, min=00.0173, mean=01.1531} policy_loss=-3.5726 policy updated! \n",
      "train step 00525 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.2616 diff={max=04.7473, min=00.0096, mean=00.7330} policy_loss=-3.2247 policy updated! \n",
      "train step 00526 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=00.6355 diff={max=02.5292, min=00.0056, mean=00.5735} policy_loss=-2.8921 policy updated! \n",
      "train step 00527 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1985 diff={max=05.0256, min=00.0481, mean=01.0350} policy_loss=-3.2454 policy updated! \n",
      "train step 00528 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1292 diff={max=02.5199, min=00.0406, mean=00.9045} policy_loss=-3.6994 policy updated! \n",
      "train step 00529 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=02.8966 diff={max=07.8025, min=00.0303, mean=01.0652} policy_loss=-3.4429 policy updated! \n",
      "train step 00530 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.0658 diff={max=03.0654, min=00.0032, mean=00.7366} policy_loss=-3.7627 policy updated! \n",
      "train step 00531 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=02.0747 diff={max=04.5193, min=00.0424, mean=01.0177} policy_loss=-3.9030 policy updated! \n",
      "train step 00532 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2838 diff={max=05.2263, min=00.0482, mean=01.0615} policy_loss=-4.3574 policy updated! \n",
      "train step 00533 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=02.6367 diff={max=08.0368, min=00.0112, mean=00.9592} policy_loss=-3.8018 policy updated! \n",
      "train step 00534 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.5396 diff={max=02.5385, min=00.0116, mean=00.5288} policy_loss=-4.2170 policy updated! \n",
      "train step 00535 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=01.2812 diff={max=03.6774, min=00.0282, mean=00.7961} policy_loss=-4.0392 policy updated! \n",
      "train step 00536 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5371 diff={max=04.7523, min=00.0558, mean=01.1064} policy_loss=-3.8228 policy updated! \n",
      "train step 00537 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6888 diff={max=05.4839, min=00.0049, mean=00.7483} policy_loss=-3.9087 policy updated! \n",
      "train step 00538 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1050 diff={max=04.6088, min=00.0100, mean=00.7278} policy_loss=-3.8344 policy updated! \n",
      "train step 00539 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5859 diff={max=05.1337, min=00.0386, mean=00.7819} policy_loss=-3.5166 policy updated! \n",
      "train step 00540 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.6070 diff={max=04.9523, min=00.0370, mean=01.0936} policy_loss=-3.3491 policy updated! \n",
      "train step 00541 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=00.9817 diff={max=03.1986, min=00.0318, mean=00.6852} policy_loss=-3.7865 policy updated! \n",
      "train step 00542 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2085 diff={max=05.9798, min=00.0218, mean=00.6768} policy_loss=-3.7733 policy updated! \n",
      "train step 00543 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4948 diff={max=04.9733, min=00.0049, mean=00.8566} policy_loss=-3.8701 policy updated! \n",
      "train step 00544 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0875 diff={max=04.2287, min=00.0237, mean=00.7392} policy_loss=-3.8795 policy updated! \n",
      "train step 00545 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.3165 diff={max=03.9450, min=00.0103, mean=00.7730} policy_loss=-3.7290 policy updated! \n",
      "train step 00546 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=00.7382 diff={max=02.6017, min=00.0112, mean=00.5840} policy_loss=-3.5307 policy updated! \n",
      "train step 00547 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4890 diff={max=04.9669, min=00.0169, mean=01.0026} policy_loss=-3.2193 policy updated! \n",
      "train step 00548 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.7011 diff={max=03.5027, min=00.0349, mean=00.9626} policy_loss=-3.5232 policy updated! \n",
      "train step 00549 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=00.7723 diff={max=02.4643, min=00.0743, mean=00.6964} policy_loss=-3.6751 policy updated! \n",
      "train step 00550 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.9136 diff={max=05.3105, min=00.0118, mean=00.7969} policy_loss=-3.3310 policy updated! \n",
      "train step 00551 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5239 diff={max=05.4878, min=00.0195, mean=00.7483} policy_loss=-3.6967 policy updated! \n",
      "train step 00552 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4000 diff={max=04.1167, min=00.0382, mean=00.7802} policy_loss=-3.9142 policy updated! \n",
      "train step 00553 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=01.1755 diff={max=03.6442, min=00.0081, mean=00.7196} policy_loss=-3.8054 policy updated! \n",
      "train step 00554 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.4764 diff={max=04.8974, min=00.0029, mean=00.7864} policy_loss=-3.9937 policy updated! \n",
      "train step 00555 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.9008 diff={max=04.8134, min=00.0291, mean=00.8878} policy_loss=-3.4642 policy updated! \n",
      "train step 00556 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9698 diff={max=03.8578, min=00.0101, mean=00.6814} policy_loss=-3.7662 policy updated! \n",
      "train step 00557 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8079 diff={max=05.1217, min=00.0371, mean=00.9446} policy_loss=-3.7758 policy updated! \n",
      "train step 00558 reward={max=06.0000, min=00.0000, mean=02.2000} optimizing loss=02.7522 diff={max=06.2823, min=00.0203, mean=00.9958} policy_loss=-4.1934 policy updated! \n",
      "train step 00559 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.6389 diff={max=01.8451, min=00.0032, mean=00.6366} policy_loss=-4.1144 policy updated! \n",
      "train step 00560 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.2495 diff={max=03.7391, min=00.0557, mean=00.7480} policy_loss=-3.7177 policy updated! \n",
      "train step 00561 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7309 diff={max=04.7299, min=00.0287, mean=00.9417} policy_loss=-3.4639 policy updated! \n",
      "train step 00562 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9626 diff={max=06.3442, min=00.0164, mean=00.8783} policy_loss=-4.1067 policy updated! \n",
      "train step 00563 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3388 diff={max=05.2249, min=00.0047, mean=00.7729} policy_loss=-4.1058 policy updated! \n",
      "train step 00564 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=02.5373 diff={max=06.1902, min=00.0064, mean=01.0824} policy_loss=-3.6756 policy updated! \n",
      "train step 00565 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.7868 diff={max=02.4412, min=00.0148, mean=00.6364} policy_loss=-3.7934 policy updated! \n",
      "train step 00566 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=00.6188 diff={max=02.5608, min=00.0157, mean=00.5862} policy_loss=-4.1479 policy updated! \n",
      "train step 00567 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2653 diff={max=04.9946, min=00.0082, mean=00.6333} policy_loss=-3.6968 policy updated! \n",
      "train step 00568 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.3652 diff={max=03.9378, min=00.0025, mean=00.7209} policy_loss=-3.9837 policy updated! \n",
      "train step 00569 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5786 diff={max=03.8736, min=00.0016, mean=00.8353} policy_loss=-3.7915 policy updated! \n",
      "train step 00570 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.9761 diff={max=04.2386, min=00.0137, mean=00.6798} policy_loss=-4.1183 policy updated! \n",
      "train step 00571 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=02.0909 diff={max=06.0438, min=00.0127, mean=00.9628} policy_loss=-3.3752 policy updated! \n",
      "train step 00572 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9075 diff={max=06.1480, min=00.0068, mean=00.8592} policy_loss=-3.4005 policy updated! \n",
      "train step 00573 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9000 diff={max=04.8580, min=00.0046, mean=00.9170} policy_loss=-3.2438 policy updated! \n",
      "train step 00574 reward={max=06.0000, min=00.0000, mean=04.4000} optimizing loss=01.2231 diff={max=05.4653, min=00.0072, mean=00.6346} policy_loss=-3.4858 policy updated! \n",
      "train step 00575 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.9199 diff={max=06.5173, min=00.0187, mean=00.8928} policy_loss=-3.8506 policy updated! \n",
      "train step 00576 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=00.4456 diff={max=02.2782, min=00.0047, mean=00.4975} policy_loss=-3.7151 policy updated! \n",
      "train step 00577 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4288 diff={max=03.9227, min=00.0003, mean=00.8037} policy_loss=-3.9897 policy updated! \n",
      "train step 00578 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4772 diff={max=05.2814, min=00.0043, mean=01.1232} policy_loss=-3.6427 policy updated! \n",
      "train step 00579 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=01.3410 diff={max=04.5678, min=00.0359, mean=00.7345} policy_loss=-3.8256 policy updated! \n",
      "train step 00580 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=00.6115 diff={max=02.3314, min=00.0035, mean=00.6052} policy_loss=-3.4319 policy updated! \n",
      "train step 00581 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4509 diff={max=05.0258, min=00.0021, mean=00.7917} policy_loss=-3.2326 policy updated! \n",
      "train step 00582 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3258 diff={max=06.2895, min=00.0006, mean=00.9940} policy_loss=-3.6785 policy updated! \n",
      "train step 00583 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=01.2773 diff={max=04.1924, min=00.0177, mean=00.8081} policy_loss=-4.0861 policy updated! \n",
      "train step 00584 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3873 diff={max=05.1898, min=00.0006, mean=00.7538} policy_loss=-4.1443 policy updated! \n",
      "train step 00585 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.8543 diff={max=08.5439, min=00.0704, mean=01.0026} policy_loss=-4.3971 policy updated! \n",
      "train step 00586 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8495 diff={max=06.9272, min=00.0003, mean=01.0781} policy_loss=-3.7127 policy updated! \n",
      "train step 00587 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8998 diff={max=05.3183, min=00.0142, mean=00.9475} policy_loss=-3.5388 policy updated! \n",
      "train step 00588 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=01.8256 diff={max=04.6246, min=00.0044, mean=00.9357} policy_loss=-3.2409 policy updated! \n",
      "train step 00589 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1352 diff={max=02.8803, min=00.0127, mean=00.8125} policy_loss=-3.5770 policy updated! \n",
      "train step 00590 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.4743 diff={max=05.0958, min=00.0032, mean=00.8423} policy_loss=-3.4593 policy updated! \n",
      "train step 00591 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0901 diff={max=08.0602, min=00.0091, mean=01.0061} policy_loss=-3.8707 policy updated! \n",
      "train step 00592 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4412 diff={max=04.0183, min=00.0327, mean=00.8393} policy_loss=-4.4080 policy updated! \n",
      "train step 00593 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8920 diff={max=04.9339, min=00.0011, mean=00.8750} policy_loss=-4.0435 policy updated! \n",
      "train step 00594 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=01.4800 diff={max=04.9506, min=00.0227, mean=00.8325} policy_loss=-4.4122 policy updated! \n",
      "train step 00595 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=00.8403 diff={max=04.4569, min=00.0037, mean=00.6217} policy_loss=-3.9068 policy updated! \n",
      "train step 00596 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1823 diff={max=04.3527, min=00.0111, mean=00.6710} policy_loss=-3.4736 policy updated! \n",
      "train step 00597 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3113 diff={max=03.7928, min=00.0356, mean=00.8082} policy_loss=-4.1969 policy updated! \n",
      "train step 00598 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6443 diff={max=05.6204, min=00.0383, mean=00.8675} policy_loss=-3.1431 policy updated! \n",
      "train step 00599 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=02.1838 diff={max=05.5031, min=00.0722, mean=01.0709} policy_loss=-3.1489 policy updated! \n",
      "train step 00600 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.1978 diff={max=03.5335, min=00.0289, mean=00.7801} policy_loss=-3.5348 policy updated! \n",
      "train step 00601 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9176 diff={max=04.2301, min=00.0083, mean=00.5627} policy_loss=-3.9196 policy updated! \n",
      "train step 00602 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=01.9014 diff={max=05.5526, min=00.0030, mean=00.8974} policy_loss=-3.9016 policy updated! \n",
      "train step 00603 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8907 diff={max=03.2518, min=00.0067, mean=00.6177} policy_loss=-3.7084 policy updated! \n",
      "train step 00604 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3467 diff={max=06.1210, min=00.0028, mean=00.6600} policy_loss=-3.9569 policy updated! \n",
      "train step 00605 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.2154 diff={max=07.3141, min=00.0034, mean=01.1289} policy_loss=-3.8712 policy updated! \n",
      "train step 00606 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=01.2689 diff={max=04.9682, min=00.0208, mean=00.7002} policy_loss=-3.8550 policy updated! \n",
      "train step 00607 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8496 diff={max=05.1279, min=00.0098, mean=00.8655} policy_loss=-3.7062 policy updated! \n",
      "train step 00608 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6115 diff={max=04.7301, min=00.0109, mean=00.8419} policy_loss=-3.3124 policy updated! \n",
      "train step 00609 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=01.0729 diff={max=03.3744, min=00.0041, mean=00.7031} policy_loss=-3.5735 policy updated! \n",
      "train step 00610 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.4843 diff={max=04.1054, min=00.0013, mean=00.8261} policy_loss=-3.2770 policy updated! \n",
      "train step 00611 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4144 diff={max=05.9397, min=00.0027, mean=01.0471} policy_loss=-3.9515 policy updated! \n",
      "train step 00612 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0149 diff={max=04.5476, min=00.0147, mean=00.6218} policy_loss=-4.7337 policy updated! \n",
      "train step 00613 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6869 diff={max=05.2643, min=00.0289, mean=01.1114} policy_loss=-4.1620 policy updated! \n",
      "train step 00614 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1355 diff={max=04.5109, min=00.0053, mean=00.9711} policy_loss=-4.0961 policy updated! \n",
      "train step 00615 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=05.6119 diff={max=10.7239, min=00.0208, mean=01.3918} policy_loss=-4.3498 policy updated! \n",
      "train step 00616 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=02.7395 diff={max=06.5999, min=00.0088, mean=01.1112} policy_loss=-3.4859 policy updated! \n",
      "train step 00617 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4733 diff={max=07.1994, min=00.0200, mean=00.9801} policy_loss=-3.0301 policy updated! \n",
      "train step 00618 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=01.9498 diff={max=03.7289, min=00.0007, mean=01.0661} policy_loss=-3.1150 policy updated! \n",
      "train step 00619 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4854 diff={max=05.2351, min=00.0932, mean=01.2280} policy_loss=-3.1260 policy updated! \n",
      "train step 00620 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.3595 diff={max=05.4936, min=00.0110, mean=01.5247} policy_loss=-3.5525 policy updated! \n",
      "train step 00621 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=01.7871 diff={max=03.6340, min=00.0117, mean=00.9151} policy_loss=-4.2907 policy updated! \n",
      "train step 00622 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9920 diff={max=06.2048, min=00.0182, mean=01.1809} policy_loss=-4.6285 policy updated! \n",
      "train step 00623 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=01.2530 diff={max=04.0212, min=00.0293, mean=00.8538} policy_loss=-5.1480 policy updated! \n",
      "train step 00624 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1540 diff={max=09.2684, min=00.0797, mean=01.4664} policy_loss=-4.7100 policy updated! \n",
      "train step 00625 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.9733 diff={max=03.3586, min=00.0162, mean=01.0766} policy_loss=-3.8758 policy updated! \n",
      "train step 00626 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=01.3183 diff={max=04.2257, min=00.0027, mean=00.7823} policy_loss=-3.9750 policy updated! \n",
      "train step 00627 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1968 diff={max=02.7933, min=00.0020, mean=00.8231} policy_loss=-3.5641 policy updated! \n",
      "train step 00628 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2479 diff={max=04.6145, min=00.1171, mean=01.1710} policy_loss=-3.1771 policy updated! \n",
      "train step 00629 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=01.7887 diff={max=04.1592, min=00.0072, mean=01.0192} policy_loss=-3.1699 policy updated! \n",
      "train step 00630 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.2590 diff={max=04.4445, min=00.0108, mean=01.1285} policy_loss=-3.7253 policy updated! \n",
      "train step 00631 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=03.0124 diff={max=07.0819, min=00.0379, mean=01.2212} policy_loss=-3.8416 policy updated! \n",
      "train step 00632 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7827 diff={max=04.2140, min=00.0285, mean=00.9673} policy_loss=-4.5436 policy updated! \n",
      "train step 00633 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0115 diff={max=05.8533, min=00.0315, mean=00.9032} policy_loss=-4.3330 policy updated! \n",
      "train step 00634 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=01.3798 diff={max=04.3686, min=00.0374, mean=00.8436} policy_loss=-4.5658 policy updated! \n",
      "train step 00635 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.4329 diff={max=04.9512, min=00.0170, mean=01.2480} policy_loss=-3.9141 policy updated! \n",
      "train step 00636 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=01.9445 diff={max=04.6732, min=00.0152, mean=00.9527} policy_loss=-4.1904 policy updated! \n",
      "train step 00637 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8405 diff={max=06.1776, min=00.0083, mean=00.8651} policy_loss=-4.0647 policy updated! \n",
      "train step 00638 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=01.2851 diff={max=04.0451, min=00.0056, mean=00.7299} policy_loss=-4.0288 policy updated! \n",
      "train step 00639 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=00.9296 diff={max=03.0407, min=00.0253, mean=00.7112} policy_loss=-3.5663 policy updated! \n",
      "train step 00640 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.4000 diff={max=05.9482, min=00.0355, mean=00.7246} policy_loss=-3.7579 policy updated! \n",
      "train step 00641 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0480 diff={max=04.0483, min=00.0042, mean=00.9702} policy_loss=-3.3000 policy updated! \n",
      "train step 00642 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8189 diff={max=02.2475, min=00.0005, mean=00.6739} policy_loss=-3.6882 policy updated! \n",
      "train step 00643 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=02.2384 diff={max=04.8736, min=00.0123, mean=01.0894} policy_loss=-3.5333 policy updated! \n",
      "train step 00644 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9720 diff={max=04.6239, min=00.0168, mean=00.9770} policy_loss=-3.7512 policy updated! \n",
      "train step 00645 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.7478 diff={max=03.3853, min=00.0015, mean=00.6298} policy_loss=-4.2880 policy updated! \n",
      "train step 00646 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.4874 diff={max=01.5201, min=00.0099, mean=00.5743} policy_loss=-4.2230 policy updated! \n",
      "train step 00647 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9241 diff={max=07.0075, min=00.0062, mean=01.1474} policy_loss=-4.2946 policy updated! \n",
      "train step 00648 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=02.2334 diff={max=04.8063, min=00.0033, mean=00.9660} policy_loss=-4.3636 policy updated! \n",
      "train step 00649 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=01.5040 diff={max=04.4112, min=00.0157, mean=00.8697} policy_loss=-4.0910 policy updated! \n",
      "train step 00650 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.1473 diff={max=04.1260, min=00.0044, mean=00.7455} policy_loss=-4.0740 policy updated! \n",
      "train step 00651 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.6548 diff={max=02.9519, min=00.0213, mean=00.6079} policy_loss=-3.9167 policy updated! \n",
      "train step 00652 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7578 diff={max=06.4689, min=00.0105, mean=00.8551} policy_loss=-3.1097 policy updated! \n",
      "train step 00653 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=01.6189 diff={max=03.7344, min=00.0019, mean=00.8793} policy_loss=-2.9774 policy updated! \n",
      "train step 00654 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4572 diff={max=04.1652, min=00.0055, mean=00.8707} policy_loss=-3.9476 policy updated! \n",
      "train step 00655 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.2072 diff={max=05.2460, min=00.0259, mean=00.7252} policy_loss=-3.6630 policy updated! \n",
      "train step 00656 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6886 diff={max=07.1237, min=00.0158, mean=01.0832} policy_loss=-3.8101 policy updated! \n",
      "train step 00657 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3619 diff={max=03.1131, min=00.0341, mean=00.8403} policy_loss=-4.2954 policy updated! \n",
      "train step 00658 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7206 diff={max=04.4172, min=00.0058, mean=00.8015} policy_loss=-3.8461 policy updated! \n",
      "train step 00659 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9825 diff={max=05.5337, min=00.0665, mean=01.1826} policy_loss=-4.2157 policy updated! \n",
      "train step 00660 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.8989 diff={max=04.5096, min=00.0058, mean=00.9218} policy_loss=-4.4095 policy updated! \n",
      "train step 00661 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4010 diff={max=04.3803, min=00.0129, mean=00.7236} policy_loss=-3.8149 policy updated! \n",
      "train step 00662 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8630 diff={max=03.1591, min=00.0359, mean=00.6555} policy_loss=-3.7444 policy updated! \n",
      "train step 00663 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8086 diff={max=02.3749, min=00.0402, mean=00.7053} policy_loss=-3.5291 policy updated! \n",
      "train step 00664 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=01.7630 diff={max=04.0253, min=00.0121, mean=00.9201} policy_loss=-4.1815 policy updated! \n",
      "train step 00665 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=02.3708 diff={max=05.2721, min=00.0020, mean=00.9634} policy_loss=-3.8845 policy updated! \n",
      "train step 00666 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.4441 diff={max=02.3036, min=00.0010, mean=00.4845} policy_loss=-4.2221 policy updated! \n",
      "train step 00667 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5288 diff={max=04.4757, min=00.0191, mean=00.8699} policy_loss=-3.5714 policy updated! \n",
      "train step 00668 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7247 diff={max=02.5679, min=00.0099, mean=00.6242} policy_loss=-4.1951 policy updated! \n",
      "train step 00669 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=01.1758 diff={max=03.8509, min=00.0180, mean=00.7941} policy_loss=-3.9542 policy updated! \n",
      "train step 00670 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.4411 diff={max=04.5227, min=00.0307, mean=00.8087} policy_loss=-4.0361 policy updated! \n",
      "train step 00671 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=01.3374 diff={max=04.2808, min=00.0070, mean=00.7618} policy_loss=-3.9756 policy updated! \n",
      "train step 00672 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7032 diff={max=07.3374, min=00.0012, mean=00.9196} policy_loss=-3.9489 policy updated! \n",
      "train step 00673 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9393 diff={max=02.9550, min=00.0299, mean=00.6840} policy_loss=-3.2499 policy updated! \n",
      "train step 00674 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4072 diff={max=05.1365, min=00.0308, mean=01.0534} policy_loss=-3.5531 policy updated! \n",
      "train step 00675 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=00.8228 diff={max=04.2019, min=00.0462, mean=00.6001} policy_loss=-3.7464 policy updated! \n",
      "train step 00676 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=01.7791 diff={max=06.1092, min=00.0023, mean=00.7791} policy_loss=-3.9705 policy updated! \n",
      "train step 00677 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6581 diff={max=05.8236, min=00.0052, mean=00.7832} policy_loss=-3.7368 policy updated! \n",
      "train step 00678 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=01.4121 diff={max=03.3911, min=00.0067, mean=00.8335} policy_loss=-3.8302 policy updated! \n",
      "train step 00679 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6080 diff={max=05.2923, min=00.0176, mean=01.0733} policy_loss=-3.7407 policy updated! \n",
      "train step 00680 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.3703 diff={max=04.2251, min=00.0250, mean=00.7832} policy_loss=-3.8641 policy updated! \n",
      "train step 00681 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8712 diff={max=06.0538, min=00.0163, mean=00.8968} policy_loss=-4.0204 policy updated! \n",
      "train step 00682 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9787 diff={max=02.6033, min=00.0138, mean=00.6960} policy_loss=-4.1351 policy updated! \n",
      "train step 00683 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=01.3077 diff={max=04.0423, min=00.0280, mean=00.8199} policy_loss=-3.9419 policy updated! \n",
      "train step 00684 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3310 diff={max=03.5751, min=00.0059, mean=00.7549} policy_loss=-3.8052 policy updated! \n",
      "train step 00685 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=00.9257 diff={max=03.9862, min=00.0040, mean=00.6201} policy_loss=-3.7441 policy updated! \n",
      "train step 00686 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4826 diff={max=03.9147, min=00.0279, mean=00.8352} policy_loss=-4.1747 policy updated! \n",
      "train step 00687 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8447 diff={max=05.4745, min=00.0814, mean=00.8818} policy_loss=-4.1701 policy updated! \n",
      "train step 00688 reward={max=06.0000, min=00.0000, mean=02.2000} optimizing loss=01.4069 diff={max=04.2551, min=00.0128, mean=00.7647} policy_loss=-4.6619 policy updated! \n",
      "train step 00689 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1717 diff={max=05.3077, min=00.0057, mean=00.6504} policy_loss=-3.8843 policy updated! \n",
      "train step 00690 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.4696 diff={max=05.7964, min=00.0187, mean=00.7370} policy_loss=-4.3848 policy updated! \n",
      "train step 00691 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.6190 diff={max=02.7925, min=00.0138, mean=00.5222} policy_loss=-3.7920 policy updated! \n",
      "train step 00692 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3917 diff={max=08.7366, min=00.0009, mean=00.7034} policy_loss=-3.7807 policy updated! \n",
      "train step 00693 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2305 diff={max=04.8135, min=00.0147, mean=00.7746} policy_loss=-3.6925 policy updated! \n",
      "train step 00694 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.3748 diff={max=05.5558, min=00.0007, mean=00.6625} policy_loss=-3.4146 policy updated! \n",
      "train step 00695 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.6519 diff={max=05.5916, min=00.0391, mean=00.8557} policy_loss=-3.6670 policy updated! \n",
      "train step 00696 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9963 diff={max=03.9213, min=00.0026, mean=00.7313} policy_loss=-3.4379 policy updated! \n",
      "train step 00697 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7609 diff={max=05.6937, min=00.0062, mean=00.8524} policy_loss=-3.7700 policy updated! \n",
      "train step 00698 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.5790 diff={max=04.8477, min=00.0001, mean=00.8022} policy_loss=-4.1145 policy updated! \n",
      "train step 00699 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=02.2532 diff={max=07.6900, min=00.0176, mean=00.8265} policy_loss=-4.1864 policy updated! \n",
      "train step 00700 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.2394 diff={max=03.8967, min=00.0381, mean=00.7709} policy_loss=-3.8778 policy updated! \n",
      "train step 00701 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5091 diff={max=05.2512, min=00.0091, mean=00.7533} policy_loss=-4.2363 policy updated! \n",
      "train step 00702 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8724 diff={max=02.1615, min=00.0249, mean=00.7337} policy_loss=-4.0169 policy updated! \n",
      "train step 00703 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=00.7731 diff={max=02.5778, min=00.0486, mean=00.6599} policy_loss=-4.0380 policy updated! \n",
      "train step 00704 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.3713 diff={max=01.8187, min=00.0145, mean=00.4961} policy_loss=-4.3288 policy updated! \n",
      "train step 00705 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.2514 diff={max=06.6300, min=00.0013, mean=00.9702} policy_loss=-4.3894 policy updated! \n",
      "train step 00706 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1302 diff={max=05.3747, min=00.0343, mean=00.6712} policy_loss=-3.2082 policy updated! \n",
      "train step 00707 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3034 diff={max=05.3602, min=00.0025, mean=00.8849} policy_loss=-4.0330 policy updated! \n",
      "train step 00708 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.0583 diff={max=04.4028, min=00.0209, mean=00.6885} policy_loss=-3.9893 policy updated! \n",
      "train step 00709 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.5801 diff={max=06.4785, min=00.0041, mean=00.7736} policy_loss=-4.2124 policy updated! \n",
      "train step 00710 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.4371 diff={max=05.0107, min=00.0161, mean=01.0432} policy_loss=-3.6744 policy updated! \n",
      "train step 00711 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=00.7305 diff={max=02.7880, min=00.0469, mean=00.6334} policy_loss=-4.0623 policy updated! \n",
      "train step 00712 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0913 diff={max=03.7723, min=00.0186, mean=00.7081} policy_loss=-4.2188 policy updated! \n",
      "train step 00713 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.2611 diff={max=04.2142, min=00.0110, mean=00.7338} policy_loss=-3.9935 policy updated! \n",
      "train step 00714 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0106 diff={max=05.1334, min=00.0352, mean=00.6111} policy_loss=-4.2188 policy updated! \n",
      "train step 00715 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.5605 diff={max=04.7023, min=00.0147, mean=01.0316} policy_loss=-3.3992 policy updated! \n",
      "train step 00716 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.2479 diff={max=04.5151, min=00.0022, mean=00.7027} policy_loss=-3.5304 policy updated! \n",
      "train step 00717 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5665 diff={max=04.9708, min=00.0177, mean=01.0419} policy_loss=-3.6846 policy updated! \n",
      "train step 00718 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.2679 diff={max=04.2522, min=00.0004, mean=00.6871} policy_loss=-3.5362 policy updated! \n",
      "train step 00719 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2190 diff={max=04.7710, min=00.0436, mean=00.7852} policy_loss=-3.8984 policy updated! \n",
      "train step 00720 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.5525 diff={max=05.8900, min=00.0019, mean=00.7866} policy_loss=-3.9379 policy updated! \n",
      "train step 00721 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.1037 diff={max=03.9478, min=00.0405, mean=00.6965} policy_loss=-4.0816 policy updated! \n",
      "train step 00722 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2348 diff={max=04.3910, min=00.0147, mean=00.7060} policy_loss=-4.3111 policy updated! \n",
      "train step 00723 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9876 diff={max=04.7048, min=00.0810, mean=00.6716} policy_loss=-3.9148 policy updated! \n",
      "train step 00724 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=00.9231 diff={max=03.9206, min=00.0305, mean=00.6380} policy_loss=-4.1177 policy updated! \n",
      "train step 00725 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.6462 diff={max=03.6729, min=00.0468, mean=00.9582} policy_loss=-3.9124 policy updated! \n",
      "train step 00726 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1522 diff={max=06.9457, min=00.0049, mean=00.7697} policy_loss=-3.9269 policy updated! \n",
      "train step 00727 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1054 diff={max=07.3982, min=00.0566, mean=01.0236} policy_loss=-3.8533 policy updated! \n",
      "train step 00728 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4978 diff={max=04.1435, min=00.0069, mean=00.7771} policy_loss=-3.9565 policy updated! \n",
      "train step 00729 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.7798 diff={max=04.6286, min=00.0255, mean=00.8676} policy_loss=-4.3278 policy updated! \n",
      "train step 00730 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.0305 diff={max=03.4250, min=00.0150, mean=00.7006} policy_loss=-3.9891 policy updated! \n",
      "train step 00731 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5847 diff={max=05.5990, min=00.0075, mean=00.9480} policy_loss=-4.1794 policy updated! \n",
      "train step 00732 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8163 diff={max=03.2129, min=00.0107, mean=00.6591} policy_loss=-4.5206 policy updated! \n",
      "train step 00733 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8630 diff={max=06.1543, min=00.0078, mean=00.8383} policy_loss=-4.4337 policy updated! \n",
      "train step 00734 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.6928 diff={max=05.3764, min=00.0046, mean=00.9155} policy_loss=-4.0423 policy updated! \n",
      "train step 00735 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.9951 diff={max=03.4498, min=00.0134, mean=00.6294} policy_loss=-3.5554 policy updated! \n",
      "train step 00736 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0487 diff={max=03.3154, min=00.0043, mean=00.7379} policy_loss=-4.6013 policy updated! \n",
      "train step 00737 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9750 diff={max=04.4590, min=00.0259, mean=00.9736} policy_loss=-3.8031 policy updated! \n",
      "train step 00738 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0295 diff={max=07.5742, min=00.0459, mean=00.8351} policy_loss=-4.1419 policy updated! \n",
      "train step 00739 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.6960 diff={max=02.9155, min=00.0007, mean=00.5839} policy_loss=-3.3500 policy updated! \n",
      "train step 00740 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.9474 diff={max=03.3058, min=00.0067, mean=00.6816} policy_loss=-4.2640 policy updated! \n",
      "train step 00741 reward={max=05.0000, min=05.0000, mean=05.0000} optimizing loss=00.7401 diff={max=03.1840, min=00.0084, mean=00.5587} policy_loss=-3.8794 policy updated! \n",
      "train step 00742 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8725 diff={max=03.0074, min=00.0043, mean=00.6592} policy_loss=-4.1657 policy updated! \n",
      "train step 00743 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1321 diff={max=03.2161, min=00.0072, mean=00.6893} policy_loss=-4.6299 policy updated! \n",
      "train step 00744 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=00.9702 diff={max=03.7438, min=00.0000, mean=00.6423} policy_loss=-4.5155 policy updated! \n",
      "train step 00745 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.5697 diff={max=07.0147, min=00.0107, mean=01.0040} policy_loss=-4.2460 policy updated! \n",
      "train step 00746 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9030 diff={max=03.8805, min=00.0046, mean=00.9774} policy_loss=-3.6518 policy updated! \n",
      "train step 00747 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4918 diff={max=03.7308, min=00.0019, mean=00.7873} policy_loss=-3.6984 policy updated! \n",
      "train step 00748 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.3412 diff={max=05.6921, min=00.0088, mean=00.7361} policy_loss=-4.6384 policy updated! \n",
      "train step 00749 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.5786 diff={max=05.6886, min=00.0147, mean=00.7586} policy_loss=-3.5922 policy updated! \n",
      "train step 00750 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.6189 diff={max=06.6234, min=00.0162, mean=00.9835} policy_loss=-3.8744 policy updated! \n",
      "train step 00751 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.1586 diff={max=03.2587, min=00.0006, mean=00.7378} policy_loss=-4.1653 policy updated! \n",
      "train step 00752 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2446 diff={max=04.3008, min=00.0128, mean=00.7622} policy_loss=-3.8726 policy updated! \n",
      "train step 00753 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5799 diff={max=04.5523, min=00.0177, mean=00.7353} policy_loss=-4.2444 policy updated! \n",
      "train step 00754 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7424 diff={max=03.3870, min=00.0028, mean=00.6035} policy_loss=-3.7254 policy updated! \n",
      "train step 00755 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.2820 diff={max=04.6958, min=00.0585, mean=00.9953} policy_loss=-3.9728 policy updated! \n",
      "train step 00756 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1198 diff={max=03.2421, min=00.0005, mean=00.6820} policy_loss=-4.5846 policy updated! \n",
      "train step 00757 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0062 diff={max=05.3580, min=00.0133, mean=00.9003} policy_loss=-4.4271 policy updated! \n",
      "train step 00758 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5342 diff={max=04.9990, min=00.0138, mean=00.7453} policy_loss=-4.5664 policy updated! \n",
      "train step 00759 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=02.3693 diff={max=04.8496, min=00.0398, mean=01.0347} policy_loss=-4.4318 policy updated! \n",
      "train step 00760 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.8992 diff={max=05.8956, min=00.0386, mean=00.9644} policy_loss=-3.8888 policy updated! \n",
      "train step 00761 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1858 diff={max=04.0757, min=00.0739, mean=00.7343} policy_loss=-3.9582 policy updated! \n",
      "train step 00762 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8638 diff={max=04.3196, min=00.0017, mean=00.6327} policy_loss=-3.5895 policy updated! \n",
      "train step 00763 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5714 diff={max=03.8300, min=00.0124, mean=00.8754} policy_loss=-3.6863 policy updated! \n",
      "train step 00764 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4009 diff={max=04.2429, min=00.0089, mean=00.7677} policy_loss=-3.6780 policy updated! \n",
      "train step 00765 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.0079 diff={max=03.7579, min=00.0340, mean=00.7230} policy_loss=-3.6741 policy updated! \n",
      "train step 00766 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.4361 diff={max=06.8339, min=00.0270, mean=00.6771} policy_loss=-4.1899 policy updated! \n",
      "train step 00767 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3659 diff={max=03.4618, min=00.0189, mean=00.7768} policy_loss=-4.2659 policy updated! \n",
      "train step 00768 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6293 diff={max=06.0974, min=00.0040, mean=00.7478} policy_loss=-4.0094 policy updated! \n",
      "train step 00769 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=02.8227 diff={max=06.2070, min=00.0290, mean=01.2122} policy_loss=-4.2353 policy updated! \n",
      "train step 00770 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.4448 diff={max=05.7662, min=00.0238, mean=00.9703} policy_loss=-4.0547 policy updated! \n",
      "train step 00771 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1170 diff={max=06.5972, min=00.0108, mean=00.5229} policy_loss=-3.8948 policy updated! \n",
      "train step 00772 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8119 diff={max=03.6512, min=00.0145, mean=00.5591} policy_loss=-3.2857 policy updated! \n",
      "train step 00773 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=00.8564 diff={max=03.0991, min=00.0195, mean=00.7031} policy_loss=-3.3161 policy updated! \n",
      "train step 00774 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.7294 diff={max=04.7868, min=00.0024, mean=00.9796} policy_loss=-3.4790 policy updated! \n",
      "train step 00775 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.4055 diff={max=03.7220, min=00.0525, mean=00.9045} policy_loss=-3.8461 policy updated! \n",
      "train step 00776 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8623 diff={max=07.0545, min=00.0230, mean=01.0253} policy_loss=-3.9534 policy updated! \n",
      "train step 00777 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2381 diff={max=04.5867, min=00.0030, mean=00.7003} policy_loss=-3.9606 policy updated! \n",
      "train step 00778 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=02.5904 diff={max=06.4104, min=00.0011, mean=00.9197} policy_loss=-4.4184 policy updated! \n",
      "train step 00779 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6272 diff={max=07.3543, min=00.0004, mean=01.0601} policy_loss=-4.4592 policy updated! \n",
      "train step 00780 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.6838 diff={max=06.5743, min=00.0577, mean=00.8437} policy_loss=-4.6755 policy updated! \n",
      "train step 00781 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=00.9149 diff={max=03.3434, min=00.0001, mean=00.6568} policy_loss=-3.7892 policy updated! \n",
      "train step 00782 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2274 diff={max=03.8297, min=00.0111, mean=00.7711} policy_loss=-4.1640 policy updated! \n",
      "train step 00783 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.6885 diff={max=02.9229, min=00.0061, mean=00.5816} policy_loss=-3.9806 policy updated! \n",
      "train step 00784 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.6573 diff={max=02.3579, min=00.0407, mean=00.5754} policy_loss=-4.3482 policy updated! \n",
      "train step 00785 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.1898 diff={max=03.5388, min=00.0074, mean=00.7368} policy_loss=-4.0757 policy updated! \n",
      "train step 00786 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0584 diff={max=05.6524, min=00.0081, mean=00.8994} policy_loss=-4.2801 policy updated! \n",
      "train step 00787 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0904 diff={max=06.9476, min=00.0005, mean=00.7952} policy_loss=-4.2939 policy updated! \n",
      "train step 00788 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7041 diff={max=02.0562, min=00.0166, mean=00.6719} policy_loss=-3.8321 policy updated! \n",
      "train step 00789 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0785 diff={max=07.5189, min=00.0113, mean=00.7743} policy_loss=-3.8136 policy updated! \n",
      "train step 00790 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.2176 diff={max=03.4706, min=00.0029, mean=00.7388} policy_loss=-3.8944 policy updated! \n",
      "train step 00791 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=00.6669 diff={max=02.6881, min=00.0033, mean=00.6023} policy_loss=-4.3501 policy updated! \n",
      "train step 00792 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7616 diff={max=03.7277, min=00.0106, mean=00.6306} policy_loss=-4.0218 policy updated! \n",
      "train step 00793 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=02.0549 diff={max=05.5785, min=00.0137, mean=00.9655} policy_loss=-3.9183 policy updated! \n",
      "train step 00794 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.0252 diff={max=03.1176, min=00.0133, mean=00.7270} policy_loss=-4.1006 policy updated! \n",
      "train step 00795 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.2070 diff={max=05.2330, min=00.0059, mean=00.9379} policy_loss=-4.0305 policy updated! \n",
      "train step 00796 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5389 diff={max=06.1237, min=00.0002, mean=01.0175} policy_loss=-3.8946 policy updated! \n",
      "train step 00797 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5339 diff={max=05.6830, min=00.0128, mean=00.7646} policy_loss=-3.9780 policy updated! \n",
      "train step 00798 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.6528 diff={max=03.9703, min=00.0276, mean=00.8744} policy_loss=-3.6173 policy updated! \n",
      "train step 00799 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.7982 diff={max=06.4081, min=00.0002, mean=00.8332} policy_loss=-3.6493 policy updated! \n",
      "train step 00800 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.4402 diff={max=04.9652, min=00.0124, mean=00.8176} policy_loss=-3.9534 policy updated! \n",
      "train step 00801 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9343 diff={max=06.6198, min=00.0016, mean=01.0043} policy_loss=-3.8638 policy updated! \n",
      "train step 00802 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5316 diff={max=04.9752, min=00.0064, mean=00.8092} policy_loss=-4.1213 policy updated! \n",
      "train step 00803 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.4110 diff={max=01.8515, min=00.0080, mean=00.4955} policy_loss=-3.9439 policy updated! \n",
      "train step 00804 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6775 diff={max=04.8565, min=00.0112, mean=00.8509} policy_loss=-4.0837 policy updated! \n",
      "train step 00805 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.3625 diff={max=06.5167, min=00.0338, mean=00.8831} policy_loss=-4.9350 policy updated! \n",
      "train step 00806 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=00.8225 diff={max=02.7807, min=00.0042, mean=00.6658} policy_loss=-4.3486 policy updated! \n",
      "train step 00807 reward={max=05.0000, min=05.0000, mean=05.0000} optimizing loss=01.7106 diff={max=05.4379, min=00.0070, mean=00.8596} policy_loss=-4.2701 policy updated! \n",
      "train step 00808 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.6448 diff={max=06.1596, min=00.0158, mean=00.7947} policy_loss=-3.9020 policy updated! \n",
      "train step 00809 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5420 diff={max=05.7243, min=00.0000, mean=00.9206} policy_loss=-3.9896 policy updated! \n",
      "train step 00810 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.7372 diff={max=05.7641, min=00.0440, mean=00.8602} policy_loss=-3.8009 policy updated! \n",
      "train step 00811 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.3959 diff={max=03.4557, min=00.0049, mean=00.8900} policy_loss=-3.1519 policy updated! \n",
      "train step 00812 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0893 diff={max=02.3907, min=00.0386, mean=00.8745} policy_loss=-3.8360 policy updated! \n",
      "train step 00813 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=02.3297 diff={max=05.6304, min=00.1210, mean=01.1099} policy_loss=-3.2353 policy updated! \n",
      "train step 00814 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4021 diff={max=06.0282, min=00.0135, mean=01.0269} policy_loss=-3.5548 policy updated! \n",
      "train step 00815 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.5094 diff={max=04.7107, min=00.0143, mean=00.9854} policy_loss=-4.0338 policy updated! \n",
      "train step 00816 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1927 diff={max=05.2017, min=00.0033, mean=00.9364} policy_loss=-4.3518 policy updated! \n",
      "train step 00817 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1407 diff={max=05.0519, min=00.0070, mean=00.9783} policy_loss=-4.5206 policy updated! \n",
      "train step 00818 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1823 diff={max=03.5948, min=00.0121, mean=00.7666} policy_loss=-4.4901 policy updated! \n",
      "train step 00819 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6496 diff={max=04.6343, min=00.0085, mean=00.8897} policy_loss=-4.2314 policy updated! \n",
      "train step 00820 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.6709 diff={max=02.0529, min=00.0366, mean=00.6390} policy_loss=-4.3252 policy updated! \n",
      "train step 00821 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4874 diff={max=03.5450, min=00.0128, mean=00.8379} policy_loss=-4.1852 policy updated! \n",
      "train step 00822 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0848 diff={max=03.9593, min=00.0098, mean=00.6844} policy_loss=-4.2484 policy updated! \n",
      "train step 00823 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3619 diff={max=04.2759, min=00.0067, mean=00.7060} policy_loss=-3.9786 policy updated! \n",
      "train step 00824 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8641 diff={max=06.8540, min=00.0268, mean=00.7419} policy_loss=-3.8104 policy updated! \n",
      "train step 00825 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.5495 diff={max=03.9847, min=00.0123, mean=00.8215} policy_loss=-4.8518 policy updated! \n",
      "train step 00826 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1336 diff={max=05.7470, min=00.0033, mean=00.6284} policy_loss=-4.5735 policy updated! \n",
      "train step 00827 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9962 diff={max=04.5802, min=00.0276, mean=00.6090} policy_loss=-4.4241 policy updated! \n",
      "train step 00828 reward={max=05.0000, min=05.0000, mean=05.0000} optimizing loss=00.9259 diff={max=03.7905, min=00.0067, mean=00.6458} policy_loss=-4.8482 policy updated! \n",
      "train step 00829 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.4164 diff={max=01.6571, min=00.0125, mean=00.5127} policy_loss=-4.5690 policy updated! \n",
      "train step 00830 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.3981 diff={max=04.5981, min=00.0037, mean=00.7473} policy_loss=-4.5703 policy updated! \n",
      "train step 00831 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7677 diff={max=02.5843, min=00.0311, mean=00.5815} policy_loss=-3.8907 policy updated! \n",
      "train step 00832 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.6460 diff={max=02.5040, min=00.0153, mean=00.5823} policy_loss=-4.1067 policy updated! \n",
      "train step 00833 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.3312 diff={max=03.4899, min=00.0193, mean=00.8048} policy_loss=-3.9614 policy updated! \n",
      "train step 00834 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=02.2080 diff={max=05.7441, min=00.0012, mean=00.8933} policy_loss=-4.0163 policy updated! \n",
      "train step 00835 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.2756 diff={max=04.7154, min=00.0127, mean=00.7574} policy_loss=-3.6759 policy updated! \n",
      "train step 00836 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.5363 diff={max=05.3761, min=00.0065, mean=00.7903} policy_loss=-3.9721 policy updated! \n",
      "train step 00837 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8820 diff={max=04.0159, min=00.0038, mean=00.6305} policy_loss=-4.3867 policy updated! \n",
      "train step 00838 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.8179 diff={max=06.6807, min=00.0325, mean=00.7797} policy_loss=-4.4928 policy updated! \n",
      "train step 00839 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=02.0655 diff={max=05.5283, min=00.0028, mean=00.9345} policy_loss=-4.4145 policy updated! \n",
      "train step 00840 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.8181 diff={max=06.0439, min=00.0040, mean=00.7639} policy_loss=-4.4043 policy updated! \n",
      "train step 00841 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=00.9879 diff={max=03.4974, min=00.0002, mean=00.7117} policy_loss=-4.4990 policy updated! \n",
      "train step 00842 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1486 diff={max=07.8028, min=00.0043, mean=00.8283} policy_loss=-4.6217 policy updated! \n",
      "train step 00843 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=00.4505 diff={max=01.8833, min=00.0417, mean=00.5173} policy_loss=-4.4783 policy updated! \n",
      "train step 00844 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.0774 diff={max=03.3314, min=00.0237, mean=00.7176} policy_loss=-4.1549 policy updated! \n",
      "train step 00845 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.1700 diff={max=03.6577, min=00.0007, mean=00.7191} policy_loss=-4.0288 policy updated! \n",
      "train step 00846 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4259 diff={max=04.1986, min=00.0194, mean=00.8442} policy_loss=-4.1494 policy updated! \n",
      "train step 00847 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3497 diff={max=04.5024, min=00.0129, mean=00.7161} policy_loss=-4.3750 policy updated! \n",
      "train step 00848 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9385 diff={max=03.9276, min=00.0174, mean=00.6323} policy_loss=-4.0126 policy updated! \n",
      "train step 00849 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.7577 diff={max=05.5856, min=00.0036, mean=00.7742} policy_loss=-4.4728 policy updated! \n",
      "train step 00850 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.5088 diff={max=05.1917, min=00.0003, mean=00.8966} policy_loss=-4.3848 policy updated! \n",
      "train step 00851 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3137 diff={max=04.2043, min=00.0009, mean=00.7354} policy_loss=-4.5207 policy updated! \n",
      "train step 00852 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1989 diff={max=04.3812, min=00.0659, mean=00.6678} policy_loss=-4.7976 policy updated! \n",
      "train step 00853 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=00.8771 diff={max=02.7563, min=00.0096, mean=00.6032} policy_loss=-4.2429 policy updated! \n",
      "train step 00854 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1719 diff={max=04.1392, min=00.0000, mean=00.6430} policy_loss=-4.3758 policy updated! \n",
      "train step 00855 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.8343 diff={max=06.9784, min=00.0030, mean=00.9841} policy_loss=-3.9807 policy updated! \n",
      "train step 00856 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4962 diff={max=05.0768, min=00.0031, mean=00.7375} policy_loss=-4.0426 policy updated! \n",
      "train step 00857 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.6043 diff={max=02.2086, min=00.0014, mean=00.5213} policy_loss=-4.0532 policy updated! \n",
      "train step 00858 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8209 diff={max=03.4885, min=00.0111, mean=00.6211} policy_loss=-4.1595 policy updated! \n",
      "train step 00859 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7602 diff={max=04.1444, min=00.0050, mean=00.8442} policy_loss=-4.0468 policy updated! \n",
      "train step 00860 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.8878 diff={max=04.6956, min=00.0153, mean=00.5925} policy_loss=-4.2413 policy updated! \n",
      "train step 00861 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.8322 diff={max=04.9727, min=00.0032, mean=00.8934} policy_loss=-4.3863 policy updated! \n",
      "train step 00862 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3095 diff={max=05.8397, min=00.0255, mean=00.9787} policy_loss=-3.7997 policy updated! \n",
      "train step 00863 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0125 diff={max=03.2059, min=00.0379, mean=00.6805} policy_loss=-4.3637 policy updated! \n",
      "train step 00864 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8669 diff={max=04.2167, min=00.0150, mean=00.6143} policy_loss=-4.1105 policy updated! \n",
      "train step 00865 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=03.1340 diff={max=07.5534, min=00.0018, mean=01.1082} policy_loss=-4.6367 policy updated! \n",
      "train step 00866 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1432 diff={max=03.6503, min=00.0115, mean=00.6710} policy_loss=-4.4354 policy updated! \n",
      "train step 00867 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8983 diff={max=02.6880, min=00.0147, mean=00.6833} policy_loss=-3.9262 policy updated! \n",
      "train step 00868 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.3780 diff={max=01.8236, min=00.0084, mean=00.4529} policy_loss=-4.0354 policy updated! \n",
      "train step 00869 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.3005 diff={max=03.5381, min=00.0421, mean=00.7925} policy_loss=-4.0977 policy updated! \n",
      "train step 00870 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.7983 diff={max=04.5205, min=00.0007, mean=00.9110} policy_loss=-4.2740 policy updated! \n",
      "train step 00871 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0318 diff={max=07.4774, min=00.0126, mean=00.7602} policy_loss=-4.3038 policy updated! \n",
      "train step 00872 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1389 diff={max=03.7979, min=00.0129, mean=00.6864} policy_loss=-4.2041 policy updated! \n",
      "train step 00873 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.2915 diff={max=04.1283, min=00.0095, mean=00.7300} policy_loss=-4.2642 policy updated! \n",
      "train step 00874 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=02.0294 diff={max=04.8578, min=00.0092, mean=00.9468} policy_loss=-4.0816 policy updated! \n",
      "train step 00875 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=03.4796 diff={max=09.7664, min=00.0035, mean=00.9730} policy_loss=-4.1342 policy updated! \n",
      "train step 00876 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9399 diff={max=05.9397, min=00.0326, mean=00.7892} policy_loss=-3.8380 policy updated! \n",
      "train step 00877 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7256 diff={max=05.6601, min=00.0590, mean=01.0812} policy_loss=-3.5453 policy updated! \n",
      "train step 00878 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.3772 diff={max=04.0322, min=00.0354, mean=00.8630} policy_loss=-3.7182 policy updated! \n",
      "train step 00879 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4711 diff={max=03.8091, min=00.0039, mean=00.8167} policy_loss=-4.0475 policy updated! \n",
      "train step 00880 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.1608 diff={max=06.5297, min=00.0140, mean=00.9117} policy_loss=-4.5734 policy updated! \n",
      "train step 00881 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=00.9935 diff={max=03.8883, min=00.0332, mean=00.6670} policy_loss=-4.5808 policy updated! \n",
      "train step 00882 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4024 diff={max=04.8989, min=00.0278, mean=00.7818} policy_loss=-4.2447 policy updated! \n",
      "train step 00883 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.7003 diff={max=07.5976, min=00.0258, mean=00.6746} policy_loss=-3.9119 policy updated! \n",
      "train step 00884 reward={max=05.0000, min=00.0000, mean=03.8000} optimizing loss=00.5153 diff={max=03.1685, min=00.0033, mean=00.5028} policy_loss=-3.9987 policy updated! \n",
      "train step 00885 reward={max=04.0000, min=00.0000, mean=00.8000} optimizing loss=03.1282 diff={max=06.2811, min=00.0234, mean=01.0898} policy_loss=-3.9104 policy updated! \n",
      "train step 00886 reward={max=04.0000, min=04.0000, mean=04.0000} optimizing loss=01.9207 diff={max=06.3188, min=00.0155, mean=00.8176} policy_loss=-3.9585 policy updated! \n",
      "train step 00887 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4321 diff={max=04.6934, min=00.0010, mean=00.6871} policy_loss=-4.1690 policy updated! \n",
      "train step 00888 reward={max=04.0000, min=00.0000, mean=00.8000} optimizing loss=02.3494 diff={max=05.8511, min=00.0011, mean=00.8908} policy_loss=-4.1570 policy updated! \n",
      "train step 00889 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.4940 diff={max=05.3610, min=00.0027, mean=00.6084} policy_loss=-4.4121 policy updated! \n",
      "train step 00890 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.6291 diff={max=04.7329, min=00.0021, mean=00.8417} policy_loss=-4.4574 policy updated! \n",
      "train step 00891 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=00.4456 diff={max=01.8463, min=00.0061, mean=00.4915} policy_loss=-4.5296 policy updated! \n",
      "train step 00892 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4347 diff={max=04.8509, min=00.0055, mean=00.8125} policy_loss=-4.3129 policy updated! \n",
      "train step 00893 reward={max=04.0000, min=00.0000, mean=01.6000} optimizing loss=01.1946 diff={max=03.9349, min=00.0123, mean=00.7460} policy_loss=-4.1916 policy updated! \n",
      "train step 00894 reward={max=04.0000, min=00.0000, mean=03.2000} optimizing loss=01.4887 diff={max=03.6600, min=00.0012, mean=00.8657} policy_loss=-4.1881 policy updated! \n",
      "train step 00895 reward={max=04.0000, min=00.0000, mean=00.8000} optimizing loss=00.9258 diff={max=04.0056, min=00.0008, mean=00.6624} policy_loss=-3.9592 policy updated! \n",
      "train step 00896 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.6372 diff={max=02.5875, min=00.0035, mean=00.5553} policy_loss=-4.2963 policy updated! \n",
      "train step 00897 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6416 diff={max=04.1250, min=00.0174, mean=00.9423} policy_loss=-4.0386 policy updated! \n",
      "train step 00898 reward={max=04.0000, min=00.0000, mean=01.6000} optimizing loss=01.3969 diff={max=04.1277, min=00.0315, mean=00.7862} policy_loss=-4.5231 policy updated! \n",
      "train step 00899 reward={max=04.0000, min=00.0000, mean=03.2000} optimizing loss=01.3862 diff={max=03.8039, min=00.0147, mean=00.8087} policy_loss=-4.1720 policy updated! \n",
      "train step 00900 reward={max=04.0000, min=00.0000, mean=00.8000} optimizing loss=01.0541 diff={max=03.6242, min=00.0002, mean=00.6904} policy_loss=-4.4180 policy updated! \n",
      "train step 00901 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2250 diff={max=04.5829, min=00.0077, mean=00.9786} policy_loss=-3.5943 policy updated! \n",
      "train step 00902 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1305 diff={max=03.5929, min=00.0071, mean=00.6618} policy_loss=-3.9360 policy updated! \n",
      "train step 00903 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0188 diff={max=04.7402, min=00.0316, mean=00.6265} policy_loss=-4.6969 policy updated! \n",
      "train step 00904 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.2611 diff={max=04.1490, min=00.0002, mean=00.7339} policy_loss=-4.4136 policy updated! \n",
      "train step 00905 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.5060 diff={max=04.6413, min=00.0028, mean=00.7433} policy_loss=-4.4965 policy updated! \n",
      "train step 00906 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.0160 diff={max=03.2184, min=00.0190, mean=00.6875} policy_loss=-4.0584 policy updated! \n",
      "train step 00907 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2170 diff={max=05.2283, min=00.0104, mean=00.6945} policy_loss=-4.1353 policy updated! \n",
      "train step 00908 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=02.0808 diff={max=04.4268, min=00.0435, mean=01.0122} policy_loss=-4.3607 policy updated! \n",
      "train step 00909 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.5534 diff={max=04.3516, min=00.0050, mean=00.8118} policy_loss=-4.1823 policy updated! \n",
      "train step 00910 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.5937 diff={max=03.0497, min=00.0201, mean=00.5541} policy_loss=-4.7062 policy updated! \n",
      "train step 00911 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=00.9299 diff={max=04.3769, min=00.0144, mean=00.5750} policy_loss=-3.9203 policy updated! \n",
      "train step 00912 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4259 diff={max=04.3598, min=00.0051, mean=00.8249} policy_loss=-3.9987 policy updated! \n",
      "train step 00913 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.1655 diff={max=04.4081, min=00.0104, mean=00.6765} policy_loss=-4.2769 policy updated! \n",
      "train step 00914 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=00.8179 diff={max=03.5986, min=00.0146, mean=00.6117} policy_loss=-4.4674 policy updated! \n",
      "train step 00915 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.1181 diff={max=07.3160, min=00.0023, mean=00.8080} policy_loss=-4.3138 policy updated! \n",
      "train step 00916 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.9455 diff={max=06.1522, min=00.0257, mean=00.8357} policy_loss=-4.5798 policy updated! \n",
      "train step 00917 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6296 diff={max=05.1859, min=00.0079, mean=00.7265} policy_loss=-4.5648 policy updated! \n",
      "train step 00918 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.1867 diff={max=05.4408, min=00.0168, mean=00.6742} policy_loss=-4.1125 policy updated! \n",
      "train step 00919 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.5484 diff={max=03.5421, min=00.0104, mean=00.4548} policy_loss=-3.9499 policy updated! \n",
      "train step 00920 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.0089 diff={max=03.7169, min=00.0134, mean=00.7343} policy_loss=-4.2267 policy updated! \n",
      "train step 00921 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.2878 diff={max=05.1348, min=00.0055, mean=00.6801} policy_loss=-4.4905 policy updated! \n",
      "train step 00922 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7020 diff={max=05.5484, min=00.0138, mean=00.7860} policy_loss=-4.2205 policy updated! \n",
      "train step 00923 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2345 diff={max=05.4644, min=00.0028, mean=00.9029} policy_loss=-4.3410 policy updated! \n",
      "train step 00924 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=02.2798 diff={max=05.2981, min=00.0076, mean=00.8748} policy_loss=-4.5579 policy updated! \n",
      "train step 00925 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.3507 diff={max=04.4967, min=00.0084, mean=00.6842} policy_loss=-4.5851 policy updated! \n",
      "train step 00926 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.4816 diff={max=02.2760, min=00.0038, mean=00.4952} policy_loss=-4.3601 policy updated! \n",
      "train step 00927 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1866 diff={max=04.6753, min=00.0118, mean=00.9014} policy_loss=-4.6214 policy updated! \n",
      "train step 00928 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3907 diff={max=04.6818, min=00.0189, mean=00.7216} policy_loss=-4.1418 policy updated! \n",
      "train step 00929 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4339 diff={max=06.3106, min=00.0044, mean=00.7098} policy_loss=-4.3501 policy updated! \n",
      "train step 00930 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.1613 diff={max=05.4174, min=00.0065, mean=00.6331} policy_loss=-4.3839 policy updated! \n",
      "train step 00931 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2875 diff={max=04.7341, min=00.0044, mean=00.6964} policy_loss=-3.7615 policy updated! \n",
      "train step 00932 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9082 diff={max=03.1595, min=00.0302, mean=00.6446} policy_loss=-4.1023 policy updated! \n",
      "train step 00933 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3128 diff={max=04.0524, min=00.0455, mean=00.7166} policy_loss=-3.9956 policy updated! \n",
      "train step 00934 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.0014 diff={max=03.5151, min=00.0041, mean=00.6306} policy_loss=-3.7394 policy updated! \n",
      "train step 00935 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.2536 diff={max=04.4745, min=00.0089, mean=00.7074} policy_loss=-4.2014 policy updated! \n",
      "train step 00936 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=00.5180 diff={max=02.4622, min=00.0168, mean=00.5235} policy_loss=-4.1312 policy updated! \n",
      "train step 00937 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.5474 diff={max=02.4721, min=00.0034, mean=00.5655} policy_loss=-4.1374 policy updated! \n",
      "train step 00938 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5703 diff={max=04.4527, min=00.0320, mean=00.8156} policy_loss=-4.4584 policy updated! \n",
      "train step 00939 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.2076 diff={max=04.9596, min=00.0030, mean=00.6749} policy_loss=-4.6822 policy updated! \n",
      "train step 00940 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.0941 diff={max=04.0566, min=00.0083, mean=00.7597} policy_loss=-4.2801 policy updated! \n",
      "train step 00941 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=00.9342 diff={max=05.1484, min=00.0109, mean=00.5681} policy_loss=-4.0648 policy updated! \n",
      "train step 00942 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2237 diff={max=03.7905, min=00.0132, mean=00.7860} policy_loss=-4.2192 policy updated! \n",
      "train step 00943 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.9608 diff={max=04.6347, min=00.0179, mean=00.9204} policy_loss=-4.5220 policy updated! \n",
      "train step 00944 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.4115 diff={max=05.4202, min=00.0007, mean=00.6790} policy_loss=-3.7346 policy updated! \n",
      "train step 00945 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.4669 diff={max=04.8352, min=00.0066, mean=00.8036} policy_loss=-4.7082 policy updated! \n",
      "train step 00946 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.6531 diff={max=02.8640, min=00.0062, mean=00.5266} policy_loss=-4.5340 policy updated! \n",
      "train step 00947 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9699 diff={max=02.5611, min=00.0020, mean=00.6385} policy_loss=-4.1459 policy updated! \n",
      "train step 00948 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=02.6861 diff={max=06.9182, min=00.0061, mean=00.8873} policy_loss=-4.7437 policy updated! \n",
      "train step 00949 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.8109 diff={max=07.3568, min=00.0054, mean=00.7325} policy_loss=-4.3482 policy updated! \n",
      "train step 00950 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.3597 diff={max=04.6921, min=00.0069, mean=00.7068} policy_loss=-4.2994 policy updated! \n",
      "train step 00951 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4373 diff={max=06.9298, min=00.0038, mean=00.8332} policy_loss=-3.9243 policy updated! \n",
      "train step 00952 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5508 diff={max=08.2860, min=00.0329, mean=00.8664} policy_loss=-4.0118 policy updated! \n",
      "train step 00953 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1251 diff={max=03.2950, min=00.0207, mean=00.8061} policy_loss=-3.9840 policy updated! \n",
      "train step 00954 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=00.8897 diff={max=04.5491, min=00.0027, mean=00.6003} policy_loss=-4.4002 policy updated! \n",
      "train step 00955 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.2551 diff={max=05.1298, min=00.0282, mean=01.0163} policy_loss=-4.4798 policy updated! \n",
      "train step 00956 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8473 diff={max=06.6543, min=00.0025, mean=00.8355} policy_loss=-4.5566 policy updated! \n",
      "train step 00957 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7945 diff={max=03.0750, min=00.0154, mean=00.6403} policy_loss=-5.2744 policy updated! \n",
      "train step 00958 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7999 diff={max=03.9397, min=00.0110, mean=00.5476} policy_loss=-4.7754 policy updated! \n",
      "train step 00959 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.1183 diff={max=04.0616, min=00.0401, mean=00.7186} policy_loss=-4.7387 policy updated! \n",
      "train step 00960 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.7838 diff={max=06.7208, min=00.0167, mean=00.7487} policy_loss=-4.4998 policy updated! \n",
      "train step 00961 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=00.6607 diff={max=03.1444, min=00.0090, mean=00.5712} policy_loss=-4.2798 policy updated! \n",
      "train step 00962 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9666 diff={max=04.7190, min=00.0036, mean=00.8007} policy_loss=-4.4266 policy updated! \n",
      "train step 00963 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.7744 diff={max=05.5431, min=00.0028, mean=00.7256} policy_loss=-4.3724 policy updated! \n",
      "train step 00964 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3208 diff={max=04.0821, min=00.0016, mean=00.7570} policy_loss=-4.2109 policy updated! \n",
      "train step 00965 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.3522 diff={max=03.5559, min=00.0354, mean=00.8155} policy_loss=-4.2767 policy updated! \n",
      "train step 00966 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5596 diff={max=04.7570, min=00.0089, mean=00.7916} policy_loss=-4.9043 policy updated! \n",
      "train step 00967 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9261 diff={max=04.5226, min=00.0136, mean=00.8698} policy_loss=-4.6491 policy updated! \n",
      "train step 00968 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7809 diff={max=05.8099, min=00.0273, mean=00.6978} policy_loss=-4.8928 policy updated! \n",
      "train step 00969 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3724 diff={max=06.1387, min=00.0092, mean=00.8852} policy_loss=-4.2945 policy updated! \n",
      "train step 00970 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.2507 diff={max=04.1427, min=00.0103, mean=00.9868} policy_loss=-4.3766 policy updated! \n",
      "train step 00971 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=00.8451 diff={max=04.6994, min=00.0820, mean=00.5831} policy_loss=-4.4547 policy updated! \n",
      "train step 00972 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2527 diff={max=05.1452, min=00.0137, mean=00.6379} policy_loss=-4.1591 policy updated! \n",
      "train step 00973 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1885 diff={max=04.6750, min=00.0042, mean=00.7081} policy_loss=-4.1562 policy updated! \n",
      "train step 00974 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.0657 diff={max=04.3542, min=00.0059, mean=00.7133} policy_loss=-4.7744 policy updated! \n",
      "train step 00975 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.2115 diff={max=03.3834, min=00.0016, mean=00.7874} policy_loss=-4.7547 policy updated! \n",
      "train step 00976 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6972 diff={max=06.4963, min=00.0327, mean=00.8555} policy_loss=-4.3645 policy updated! \n",
      "train step 00977 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5526 diff={max=07.0111, min=00.0193, mean=00.9543} policy_loss=-4.2934 policy updated! \n",
      "train step 00978 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0896 diff={max=02.9271, min=00.0156, mean=00.7704} policy_loss=-4.6022 policy updated! \n",
      "train step 00979 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6290 diff={max=06.4596, min=00.0237, mean=01.0620} policy_loss=-4.5831 policy updated! \n",
      "train step 00980 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.0803 diff={max=03.1847, min=00.0323, mean=00.7676} policy_loss=-4.6594 policy updated! \n",
      "train step 00981 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=02.1509 diff={max=08.4248, min=00.0191, mean=00.7441} policy_loss=-4.4262 policy updated! \n",
      "train step 00982 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2082 diff={max=03.3358, min=00.0275, mean=00.8356} policy_loss=-4.8053 policy updated! \n",
      "train step 00983 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.4950 diff={max=04.0964, min=00.0120, mean=00.8140} policy_loss=-5.0039 policy updated! \n",
      "train step 00984 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0997 diff={max=04.7805, min=00.0012, mean=00.6467} policy_loss=-5.0734 policy updated! \n",
      "train step 00985 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.1426 diff={max=04.7710, min=00.0416, mean=00.6624} policy_loss=-4.7888 policy updated! \n",
      "train step 00986 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8033 diff={max=02.8879, min=00.0053, mean=00.6562} policy_loss=-4.0451 policy updated! \n",
      "train step 00987 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1976 diff={max=09.5239, min=00.0165, mean=00.8774} policy_loss=-3.7450 policy updated! \n",
      "train step 00988 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=02.0773 diff={max=06.2243, min=00.0017, mean=00.8856} policy_loss=-3.8837 policy updated! \n",
      "train step 00989 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7710 diff={max=02.4669, min=00.0079, mean=00.6204} policy_loss=-4.3127 policy updated! \n",
      "train step 00990 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.9959 diff={max=02.6563, min=00.0608, mean=00.7960} policy_loss=-3.9656 policy updated! \n",
      "train step 00991 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6813 diff={max=03.6001, min=00.1006, mean=00.9388} policy_loss=-5.1116 policy updated! \n",
      "train step 00992 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2621 diff={max=05.3970, min=00.0060, mean=00.9625} policy_loss=-4.4966 policy updated! \n",
      "train step 00993 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9358 diff={max=03.3266, min=00.0023, mean=00.6583} policy_loss=-4.7389 policy updated! \n",
      "train step 00994 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.4594 diff={max=03.9000, min=00.0102, mean=00.8438} policy_loss=-4.6460 policy updated! \n",
      "train step 00995 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.8671 diff={max=03.2736, min=00.0139, mean=00.6988} policy_loss=-4.3110 policy updated! \n",
      "train step 00996 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7146 diff={max=02.7258, min=00.0700, mean=00.6219} policy_loss=-4.1510 policy updated! \n",
      "train step 00997 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.6336 diff={max=02.6808, min=00.0386, mean=00.5885} policy_loss=-3.9815 policy updated! \n",
      "train step 00998 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2231 diff={max=04.2387, min=00.0214, mean=00.7346} policy_loss=-4.1491 policy updated! \n",
      "train step 00999 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0918 diff={max=04.7297, min=00.0166, mean=00.9770} policy_loss=-3.8524 policy updated! \n",
      "train step 01000 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.4633 diff={max=04.1618, min=00.0139, mean=00.7749} policy_loss=-4.3873 policy updated! \n",
      "train step 01001 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.4120 diff={max=04.5874, min=00.0009, mean=00.7167} policy_loss=-4.4894 policy updated! \n",
      "train step 01002 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2443 diff={max=04.1512, min=00.0130, mean=00.7190} policy_loss=-5.3190 policy updated! \n",
      "train step 01003 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.2759 diff={max=04.6748, min=00.0102, mean=00.7233} policy_loss=-4.8088 policy updated! \n",
      "train step 01004 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=00.9946 diff={max=03.7248, min=00.0059, mean=00.6390} policy_loss=-4.9239 policy updated! \n",
      "train step 01005 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.3814 diff={max=03.9308, min=00.0095, mean=00.7812} policy_loss=-4.2351 policy updated! \n",
      "train step 01006 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7914 diff={max=04.0308, min=00.0017, mean=00.5473} policy_loss=-4.0612 policy updated! \n",
      "train step 01007 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3037 diff={max=05.0186, min=00.0026, mean=00.6703} policy_loss=-4.0556 policy updated! \n",
      "train step 01008 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.1213 diff={max=04.1762, min=00.0018, mean=00.5828} policy_loss=-4.1725 policy updated! \n",
      "train step 01009 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=02.3444 diff={max=05.9358, min=00.0145, mean=00.8789} policy_loss=-4.1050 policy updated! \n",
      "train step 01010 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.1191 diff={max=04.7874, min=00.0075, mean=00.7297} policy_loss=-4.6300 policy updated! \n",
      "train step 01011 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4081 diff={max=06.5129, min=00.0288, mean=00.9118} policy_loss=-4.7397 policy updated! \n",
      "train step 01012 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.6039 diff={max=03.7390, min=00.0026, mean=00.5310} policy_loss=-4.8335 policy updated! \n",
      "train step 01013 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.0622 diff={max=03.8927, min=00.0073, mean=00.6991} policy_loss=-4.9748 policy updated! \n",
      "train step 01014 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1341 diff={max=07.1585, min=00.0155, mean=01.1155} policy_loss=-4.4961 policy updated! \n",
      "train step 01015 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.3487 diff={max=04.4049, min=00.0543, mean=00.7683} policy_loss=-4.4137 policy updated! \n",
      "train step 01016 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.6309 diff={max=01.7611, min=00.0212, mean=00.5981} policy_loss=-3.7121 policy updated! \n",
      "train step 01017 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5624 diff={max=04.0049, min=00.0023, mean=00.8530} policy_loss=-4.8431 policy updated! \n",
      "train step 01018 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7022 diff={max=02.2036, min=00.0024, mean=00.6347} policy_loss=-3.8742 policy updated! \n",
      "train step 01019 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6904 diff={max=05.3116, min=00.0514, mean=00.8171} policy_loss=-3.9909 policy updated! \n",
      "train step 01020 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.3737 diff={max=03.8690, min=00.0248, mean=00.7778} policy_loss=-4.6988 policy updated! \n",
      "train step 01021 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2292 diff={max=03.7914, min=00.0086, mean=00.7467} policy_loss=-4.9915 policy updated! \n",
      "train step 01022 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9151 diff={max=03.6487, min=00.0002, mean=00.6575} policy_loss=-4.0697 policy updated! \n",
      "train step 01023 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9654 diff={max=03.1247, min=00.0257, mean=00.7059} policy_loss=-4.6332 policy updated! \n",
      "train step 01024 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.4258 diff={max=05.1929, min=00.0176, mean=00.7914} policy_loss=-4.3678 policy updated! \n",
      "train step 01025 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.0870 diff={max=05.2355, min=00.0171, mean=00.6220} policy_loss=-3.9690 policy updated! \n",
      "train step 01026 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7254 diff={max=04.8347, min=00.0024, mean=00.8399} policy_loss=-4.0092 policy updated! \n",
      "train step 01027 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0197 diff={max=04.3439, min=00.0205, mean=00.6287} policy_loss=-3.8259 policy updated! \n",
      "train step 01028 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.9599 diff={max=06.6429, min=00.0043, mean=00.8360} policy_loss=-5.0693 policy updated! \n",
      "train step 01029 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=00.6667 diff={max=03.4333, min=00.0243, mean=00.5353} policy_loss=-4.7951 policy updated! \n",
      "train step 01030 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.5417 diff={max=04.7339, min=00.0032, mean=00.7556} policy_loss=-4.1087 policy updated! \n",
      "train step 01031 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6374 diff={max=05.3729, min=00.0052, mean=00.7671} policy_loss=-5.1460 policy updated! \n",
      "train step 01032 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4140 diff={max=03.3881, min=00.0093, mean=00.8423} policy_loss=-4.8468 policy updated! \n",
      "train step 01033 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7398 diff={max=03.7819, min=00.0086, mean=00.5550} policy_loss=-4.8665 policy updated! \n",
      "train step 01034 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=00.9934 diff={max=02.8286, min=00.0248, mean=00.7284} policy_loss=-4.7866 policy updated! \n",
      "train step 01035 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.7830 diff={max=02.7601, min=00.0394, mean=00.6564} policy_loss=-3.8728 policy updated! \n",
      "train step 01036 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4781 diff={max=05.3020, min=00.0003, mean=00.7343} policy_loss=-4.6762 policy updated! \n",
      "train step 01037 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3655 diff={max=04.7674, min=00.0041, mean=00.9996} policy_loss=-4.5123 policy updated! \n",
      "train step 01038 reward={max=04.0000, min=00.0000, mean=01.6000} optimizing loss=01.1280 diff={max=04.7160, min=00.0223, mean=00.7007} policy_loss=-3.8424 policy updated! \n",
      "train step 01039 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8263 diff={max=02.8688, min=00.0643, mean=00.6976} policy_loss=-4.7609 policy updated! \n",
      "train step 01040 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.7160 diff={max=02.7230, min=00.0176, mean=00.6300} policy_loss=-4.7070 policy updated! \n",
      "train step 01041 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=00.4410 diff={max=02.1103, min=00.0780, mean=00.5058} policy_loss=-5.4435 policy updated! \n",
      "train step 01042 reward={max=05.0000, min=05.0000, mean=05.0000} optimizing loss=01.7978 diff={max=06.1016, min=00.0050, mean=00.8711} policy_loss=-4.9970 policy updated! \n",
      "train step 01043 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=02.8263 diff={max=04.8745, min=00.0128, mean=01.1682} policy_loss=-4.7773 policy updated! \n",
      "train step 01044 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6502 diff={max=05.6533, min=00.0542, mean=01.0799} policy_loss=-4.0180 policy updated! \n",
      "train step 01045 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.9395 diff={max=03.7327, min=00.0034, mean=00.5747} policy_loss=-4.5063 policy updated! \n",
      "train step 01046 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=00.8105 diff={max=03.0308, min=00.0230, mean=00.6282} policy_loss=-4.3159 policy updated! \n",
      "train step 01047 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2550 diff={max=03.9144, min=00.0458, mean=00.7760} policy_loss=-4.1647 policy updated! \n",
      "train step 01048 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=02.4884 diff={max=05.6450, min=00.0228, mean=01.0131} policy_loss=-3.6208 policy updated! \n",
      "train step 01049 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3377 diff={max=04.0619, min=00.0030, mean=00.8504} policy_loss=-4.6252 policy updated! \n",
      "train step 01050 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.4061 diff={max=05.1422, min=00.0352, mean=01.0077} policy_loss=-4.4839 policy updated! \n",
      "train step 01051 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.8738 diff={max=05.7946, min=00.0061, mean=00.7713} policy_loss=-5.1420 policy updated! \n",
      "train step 01052 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8237 diff={max=03.0333, min=00.0285, mean=00.6979} policy_loss=-4.9383 policy updated! \n",
      "train step 01053 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.5229 diff={max=04.2248, min=00.0316, mean=00.8830} policy_loss=-4.9171 policy updated! \n",
      "train step 01054 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=00.3780 diff={max=01.4467, min=00.0016, mean=00.5181} policy_loss=-4.4179 policy updated! \n",
      "train step 01055 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.6915 diff={max=03.5138, min=00.0452, mean=00.5941} policy_loss=-4.2905 policy updated! \n",
      "train step 01056 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5776 diff={max=03.3743, min=00.0192, mean=00.8908} policy_loss=-4.6356 policy updated! \n",
      "train step 01057 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1096 diff={max=03.1510, min=00.0122, mean=00.7404} policy_loss=-4.0931 policy updated! \n",
      "train step 01058 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0553 diff={max=05.5016, min=00.0586, mean=00.9736} policy_loss=-3.9449 policy updated! \n",
      "train step 01059 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.6154 diff={max=04.7989, min=00.0086, mean=00.8236} policy_loss=-3.6439 policy updated! \n",
      "train step 01060 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.6464 diff={max=04.4304, min=00.0174, mean=00.8418} policy_loss=-4.2541 policy updated! \n",
      "train step 01061 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5018 diff={max=05.5280, min=00.0305, mean=00.7340} policy_loss=-4.2819 policy updated! \n",
      "train step 01062 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0240 diff={max=04.3190, min=00.0050, mean=00.7272} policy_loss=-4.4749 policy updated! \n",
      "train step 01063 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.7493 diff={max=05.0850, min=00.0008, mean=00.7577} policy_loss=-5.1150 policy updated! \n",
      "train step 01064 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.4497 diff={max=04.0376, min=00.0325, mean=00.8435} policy_loss=-4.9178 policy updated! \n",
      "train step 01065 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.0746 diff={max=03.3146, min=00.0107, mean=00.7103} policy_loss=-4.9686 policy updated! \n",
      "train step 01066 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5287 diff={max=04.4597, min=00.0015, mean=00.8584} policy_loss=-4.6841 policy updated! \n",
      "train step 01067 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6557 diff={max=04.8445, min=00.0025, mean=01.0610} policy_loss=-4.0623 policy updated! \n",
      "train step 01068 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.0588 diff={max=02.6881, min=00.0167, mean=00.7416} policy_loss=-4.1066 policy updated! \n",
      "train step 01069 reward={max=05.0000, min=05.0000, mean=05.0000} optimizing loss=01.8989 diff={max=03.4944, min=00.0325, mean=01.0439} policy_loss=-4.3121 policy updated! \n",
      "train step 01070 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.2655 diff={max=03.5754, min=00.0147, mean=00.7883} policy_loss=-4.3080 policy updated! \n",
      "train step 01071 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7872 diff={max=05.2684, min=00.0082, mean=01.2025} policy_loss=-4.0731 policy updated! \n",
      "train step 01072 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7772 diff={max=04.4536, min=00.0242, mean=00.9215} policy_loss=-4.6251 policy updated! \n",
      "train step 01073 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8379 diff={max=04.2566, min=00.0076, mean=00.8650} policy_loss=-4.9745 policy updated! \n",
      "train step 01074 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=02.4888 diff={max=04.6412, min=00.1000, mean=01.1101} policy_loss=-4.8850 policy updated! \n",
      "train step 01075 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.1851 diff={max=03.7122, min=00.0200, mean=00.7606} policy_loss=-4.6741 policy updated! \n",
      "train step 01076 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=02.1914 diff={max=04.6008, min=00.0331, mean=01.0407} policy_loss=-5.0357 policy updated! \n",
      "train step 01077 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4644 diff={max=03.7612, min=00.0060, mean=00.8624} policy_loss=-4.5108 policy updated! \n",
      "train step 01078 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8109 diff={max=02.8951, min=00.0068, mean=00.6448} policy_loss=-4.3183 policy updated! \n",
      "train step 01079 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=00.6483 diff={max=03.0947, min=00.0200, mean=00.6103} policy_loss=-4.7380 policy updated! \n",
      "train step 01080 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.7887 diff={max=04.5839, min=00.0103, mean=00.8384} policy_loss=-4.2718 policy updated! \n",
      "train step 01081 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0738 diff={max=03.4891, min=00.0058, mean=00.7171} policy_loss=-4.8702 policy updated! \n",
      "train step 01082 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0323 diff={max=03.8121, min=00.0332, mean=00.6957} policy_loss=-4.2386 policy updated! \n",
      "train step 01083 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2745 diff={max=04.4345, min=00.0236, mean=00.8122} policy_loss=-4.2863 policy updated! \n",
      "train step 01084 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=00.8268 diff={max=03.0132, min=00.0282, mean=00.7018} policy_loss=-4.7529 policy updated! \n",
      "train step 01085 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.2635 diff={max=03.8316, min=00.0250, mean=00.7202} policy_loss=-4.0674 policy updated! \n",
      "train step 01086 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=00.8417 diff={max=03.4336, min=00.0813, mean=00.6604} policy_loss=-4.6749 policy updated! \n",
      "train step 01087 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1198 diff={max=03.1626, min=00.0048, mean=00.7383} policy_loss=-4.2981 policy updated! \n",
      "train step 01088 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9456 diff={max=05.0239, min=00.0092, mean=00.8934} policy_loss=-4.4693 policy updated! \n",
      "train step 01089 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1846 diff={max=03.6925, min=00.0073, mean=00.7659} policy_loss=-4.3389 policy updated! \n",
      "train step 01090 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=00.9888 diff={max=02.7617, min=00.0520, mean=00.7444} policy_loss=-4.8113 policy updated! \n",
      "train step 01091 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=01.8850 diff={max=06.9811, min=00.0229, mean=00.7263} policy_loss=-4.6207 policy updated! \n",
      "train step 01092 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.0255 diff={max=03.0330, min=00.0126, mean=00.7007} policy_loss=-4.7772 policy updated! \n",
      "train step 01093 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.0260 diff={max=04.6949, min=00.0214, mean=00.7024} policy_loss=-4.0692 policy updated! \n",
      "train step 01094 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=00.8986 diff={max=02.4271, min=00.0116, mean=00.7322} policy_loss=-3.7999 policy updated! \n",
      "train step 01095 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.5413 diff={max=03.0997, min=00.0019, mean=00.9013} policy_loss=-4.4304 policy updated! \n",
      "train step 01096 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=00.9580 diff={max=03.9153, min=00.0046, mean=00.6023} policy_loss=-4.3311 policy updated! \n",
      "train step 01097 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2390 diff={max=04.6418, min=00.0010, mean=00.7816} policy_loss=-4.3305 policy updated! \n",
      "train step 01098 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8902 diff={max=02.6492, min=00.0170, mean=00.6999} policy_loss=-4.9311 policy updated! \n",
      "train step 01099 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=00.8679 diff={max=03.5254, min=00.0028, mean=00.6241} policy_loss=-5.2137 policy updated! \n",
      "train step 01100 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.7314 diff={max=03.1374, min=00.0154, mean=00.5912} policy_loss=-4.5821 policy updated! \n",
      "train step 01101 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=00.9985 diff={max=04.3054, min=00.0000, mean=00.6661} policy_loss=-4.4061 policy updated! \n",
      "train step 01102 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7082 diff={max=05.8869, min=00.0012, mean=00.8071} policy_loss=-4.3687 policy updated! \n",
      "train step 01103 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2494 diff={max=04.0226, min=00.0156, mean=00.6659} policy_loss=-4.5243 policy updated! \n",
      "train step 01104 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.5602 diff={max=05.2861, min=00.0037, mean=00.8050} policy_loss=-4.6012 policy updated! \n",
      "train step 01105 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.2661 diff={max=04.6469, min=00.0351, mean=01.0559} policy_loss=-4.7718 policy updated! \n",
      "train step 01106 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8935 diff={max=03.9370, min=00.0016, mean=00.5943} policy_loss=-4.5308 policy updated! \n",
      "train step 01107 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2658 diff={max=04.5343, min=00.0007, mean=00.6835} policy_loss=-5.2305 policy updated! \n",
      "train step 01108 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4874 diff={max=05.1044, min=00.0021, mean=01.0172} policy_loss=-5.2983 policy updated! \n",
      "train step 01109 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=02.1308 diff={max=05.7140, min=00.0031, mean=00.9014} policy_loss=-4.9104 policy updated! \n",
      "train step 01110 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.1620 diff={max=03.5759, min=00.0116, mean=00.7191} policy_loss=-4.1471 policy updated! \n",
      "train step 01111 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=02.7811 diff={max=04.3063, min=00.0153, mean=01.1287} policy_loss=-4.2978 policy updated! \n",
      "train step 01112 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4932 diff={max=04.0151, min=00.0383, mean=00.9269} policy_loss=-3.9662 policy updated! \n",
      "train step 01113 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6414 diff={max=04.7550, min=00.0037, mean=00.9109} policy_loss=-3.8455 policy updated! \n",
      "train step 01114 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4871 diff={max=08.0229, min=00.0530, mean=01.1442} policy_loss=-4.5255 policy updated! \n",
      "train step 01115 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.1840 diff={max=04.4665, min=00.0186, mean=01.0030} policy_loss=-4.8830 policy updated! \n",
      "train step 01116 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5564 diff={max=04.6802, min=00.0208, mean=00.8113} policy_loss=-4.7703 policy updated! \n",
      "train step 01117 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1528 diff={max=04.8164, min=00.0068, mean=00.6744} policy_loss=-4.9654 policy updated! \n",
      "train step 01118 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=00.5936 diff={max=02.7940, min=00.0262, mean=00.5759} policy_loss=-4.6988 policy updated! \n",
      "train step 01119 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5519 diff={max=05.6241, min=00.0087, mean=01.0233} policy_loss=-4.7153 policy updated! \n",
      "train step 01120 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=00.9504 diff={max=02.8157, min=00.0882, mean=00.7799} policy_loss=-5.4773 policy updated! \n",
      "train step 01121 reward={max=05.0000, min=00.0000, mean=03.8000} optimizing loss=02.1826 diff={max=07.2705, min=00.0196, mean=00.9507} policy_loss=-5.1950 policy updated! \n",
      "train step 01122 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2943 diff={max=03.7495, min=00.0104, mean=00.8277} policy_loss=-4.4760 policy updated! \n",
      "train step 01123 reward={max=04.0000, min=00.0000, mean=01.6000} optimizing loss=03.7748 diff={max=09.5849, min=00.0359, mean=00.9793} policy_loss=-4.4247 policy updated! \n",
      "train step 01124 reward={max=04.0000, min=00.0000, mean=03.2000} optimizing loss=01.1767 diff={max=04.4553, min=00.0010, mean=00.7532} policy_loss=-4.7420 policy updated! \n",
      "train step 01125 reward={max=04.0000, min=00.0000, mean=00.8000} optimizing loss=01.1329 diff={max=02.9533, min=00.0213, mean=00.8273} policy_loss=-3.8706 policy updated! \n",
      "train step 01126 reward={max=04.0000, min=00.0000, mean=02.4000} optimizing loss=01.9420 diff={max=03.5579, min=00.0268, mean=01.0376} policy_loss=-3.9808 policy updated! \n",
      "train step 01127 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2121 diff={max=04.3645, min=00.0454, mean=01.0999} policy_loss=-4.3274 policy updated! \n",
      "train step 01128 reward={max=04.0000, min=00.0000, mean=01.6000} optimizing loss=01.4528 diff={max=03.8510, min=00.0099, mean=00.9269} policy_loss=-4.2057 policy updated! \n",
      "train step 01129 reward={max=04.0000, min=00.0000, mean=03.2000} optimizing loss=01.6903 diff={max=04.9527, min=00.0009, mean=00.9466} policy_loss=-4.6932 policy updated! \n",
      "train step 01130 reward={max=04.0000, min=00.0000, mean=00.8000} optimizing loss=01.8418 diff={max=05.2536, min=00.0185, mean=00.9043} policy_loss=-5.5493 policy updated! \n",
      "train step 01131 reward={max=04.0000, min=00.0000, mean=02.4000} optimizing loss=04.0147 diff={max=07.6735, min=00.0134, mean=01.3199} policy_loss=-5.5740 policy updated! \n",
      "train step 01132 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6046 diff={max=03.5073, min=00.0005, mean=00.9916} policy_loss=-4.7273 policy updated! \n",
      "train step 01133 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0649 diff={max=03.3823, min=00.1276, mean=00.7547} policy_loss=-4.6098 policy updated! \n",
      "train step 01134 reward={max=05.0000, min=00.0000, mean=03.4000} optimizing loss=02.3399 diff={max=05.3736, min=00.0117, mean=01.0753} policy_loss=-4.8462 policy updated! \n",
      "train step 01135 reward={max=04.0000, min=00.0000, mean=00.8000} optimizing loss=01.9336 diff={max=03.5163, min=00.0109, mean=01.0357} policy_loss=-4.4664 policy updated! \n",
      "train step 01136 reward={max=05.0000, min=00.0000, mean=02.8000} optimizing loss=02.0424 diff={max=06.3993, min=00.0199, mean=00.8638} policy_loss=-4.9359 policy updated! \n",
      "train step 01137 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0364 diff={max=05.0555, min=00.0119, mean=00.9828} policy_loss=-4.7965 policy updated! \n",
      "train step 01138 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=00.6307 diff={max=01.6410, min=00.0199, mean=00.6360} policy_loss=-4.5211 policy updated! \n",
      "train step 01139 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=02.5675 diff={max=07.3843, min=00.0094, mean=00.9571} policy_loss=-4.2669 policy updated! \n",
      "train step 01140 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.4613 diff={max=02.9313, min=00.0017, mean=00.8779} policy_loss=-4.6633 policy updated! \n",
      "train step 01141 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3615 diff={max=03.6580, min=00.0388, mean=00.8424} policy_loss=-4.6817 policy updated! \n",
      "train step 01142 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.6790 diff={max=02.8848, min=00.0022, mean=00.6493} policy_loss=-5.1653 policy updated! \n",
      "train step 01143 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0148 diff={max=03.6628, min=00.0248, mean=00.7085} policy_loss=-4.4502 policy updated! \n",
      "train step 01144 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7075 diff={max=05.3001, min=00.0390, mean=00.8485} policy_loss=-4.8364 policy updated! \n",
      "train step 01145 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=04.0523 diff={max=09.8565, min=00.0331, mean=01.1927} policy_loss=-4.5565 policy updated! \n",
      "train step 01146 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=00.7411 diff={max=03.2856, min=00.0172, mean=00.6127} policy_loss=-3.6911 policy updated! \n",
      "train step 01147 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7843 diff={max=03.4251, min=00.0323, mean=00.6077} policy_loss=-4.4749 policy updated! \n",
      "train step 01148 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=00.9180 diff={max=04.5613, min=00.0102, mean=00.5901} policy_loss=-3.6546 policy updated! \n",
      "train step 01149 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7480 diff={max=03.9753, min=00.0238, mean=00.5809} policy_loss=-4.6465 policy updated! \n",
      "train step 01150 reward={max=04.0000, min=00.0000, mean=00.8000} optimizing loss=00.8989 diff={max=03.5970, min=00.0216, mean=00.6646} policy_loss=-4.7799 policy updated! \n",
      "train step 01151 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9719 diff={max=03.3249, min=00.0499, mean=00.6548} policy_loss=-5.0418 policy updated! \n",
      "train step 01152 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7686 diff={max=03.0187, min=00.0329, mean=00.6521} policy_loss=-4.9095 policy updated! \n",
      "train step 01153 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=00.8065 diff={max=04.0255, min=00.0110, mean=00.6279} policy_loss=-5.4447 policy updated! \n",
      "train step 01154 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8886 diff={max=04.1953, min=00.0208, mean=01.0210} policy_loss=-4.5802 policy updated! \n",
      "train step 01155 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.7490 diff={max=05.1084, min=00.0056, mean=00.7472} policy_loss=-3.8186 policy updated! \n",
      "train step 01156 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.1829 diff={max=05.6841, min=00.0111, mean=00.6253} policy_loss=-4.4494 policy updated! \n",
      "train step 01157 reward={max=05.0000, min=05.0000, mean=05.0000} optimizing loss=01.4025 diff={max=04.0724, min=00.0096, mean=00.8064} policy_loss=-4.7156 policy updated! \n",
      "train step 01158 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=00.8519 diff={max=02.8898, min=00.0099, mean=00.6822} policy_loss=-4.2655 policy updated! \n",
      "train step 01159 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8267 diff={max=06.1951, min=00.0091, mean=01.0501} policy_loss=-4.8928 policy updated! \n",
      "train step 01160 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=04.5359 diff={max=06.0166, min=00.0022, mean=01.4142} policy_loss=-5.0561 policy updated! \n",
      "train step 01161 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3677 diff={max=05.3770, min=00.0323, mean=01.1119} policy_loss=-4.5805 policy updated! \n",
      "train step 01162 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9063 diff={max=02.9978, min=00.0355, mean=00.7186} policy_loss=-4.4154 policy updated! \n",
      "train step 01163 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=00.8258 diff={max=03.9662, min=00.0053, mean=00.6054} policy_loss=-4.4149 policy updated! \n",
      "train step 01164 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.8858 diff={max=05.4978, min=00.0130, mean=00.9140} policy_loss=-4.3122 policy updated! \n",
      "train step 01165 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.5838 diff={max=02.8490, min=00.0010, mean=00.5123} policy_loss=-5.0177 policy updated! \n",
      "train step 01166 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=02.7723 diff={max=05.9653, min=00.0066, mean=00.9554} policy_loss=-4.8022 policy updated! \n",
      "train step 01167 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9404 diff={max=04.4873, min=00.0273, mean=00.6110} policy_loss=-4.5664 policy updated! \n",
      "train step 01168 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.0291 diff={max=03.6641, min=00.0009, mean=00.6545} policy_loss=-4.8065 policy updated! \n",
      "train step 01169 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1545 diff={max=03.4607, min=00.0348, mean=00.7670} policy_loss=-4.8597 policy updated! \n",
      "train step 01170 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.2753 diff={max=03.7015, min=00.0149, mean=00.7626} policy_loss=-5.0395 policy updated! \n",
      "train step 01171 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=02.3996 diff={max=05.4542, min=00.0319, mean=01.0005} policy_loss=-4.5352 policy updated! \n",
      "train step 01172 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.4846 diff={max=01.7168, min=00.0013, mean=00.5165} policy_loss=-4.6708 policy updated! \n",
      "train step 01173 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4490 diff={max=04.9830, min=00.0082, mean=00.6677} policy_loss=-4.6493 policy updated! \n",
      "train step 01174 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=02.5155 diff={max=05.5206, min=00.0196, mean=00.9711} policy_loss=-4.8073 policy updated! \n",
      "train step 01175 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.4440 diff={max=04.8426, min=00.0173, mean=00.7606} policy_loss=-4.4192 policy updated! \n",
      "train step 01176 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=02.3289 diff={max=06.2936, min=00.0027, mean=00.8820} policy_loss=-5.0695 policy updated! \n",
      "train step 01177 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8184 diff={max=02.3913, min=00.0025, mean=00.6775} policy_loss=-4.5311 policy updated! \n",
      "train step 01178 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=02.0152 diff={max=03.4401, min=00.0328, mean=01.0970} policy_loss=-4.5288 policy updated! \n",
      "train step 01179 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.3862 diff={max=03.7039, min=00.0122, mean=00.8339} policy_loss=-4.7625 policy updated! \n",
      "train step 01180 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.8587 diff={max=04.7193, min=00.0180, mean=00.8063} policy_loss=-4.8662 policy updated! \n",
      "train step 01181 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7489 diff={max=05.7153, min=00.0093, mean=00.7743} policy_loss=-4.7828 policy updated! \n",
      "train step 01182 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0994 diff={max=06.9915, min=00.0187, mean=00.7485} policy_loss=-4.6903 policy updated! \n",
      "train step 01183 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0738 diff={max=02.9607, min=00.0033, mean=00.6829} policy_loss=-4.6976 policy updated! \n",
      "train step 01184 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.7845 diff={max=06.0022, min=00.0107, mean=00.8029} policy_loss=-4.8052 policy updated! \n",
      "train step 01185 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.3434 diff={max=04.6589, min=00.0166, mean=00.6975} policy_loss=-4.9828 policy updated! \n",
      "train step 01186 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.2509 diff={max=04.0083, min=00.0033, mean=00.7693} policy_loss=-4.5192 policy updated! \n",
      "train step 01187 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6247 diff={max=04.7009, min=00.0143, mean=00.8911} policy_loss=-4.5138 policy updated! \n",
      "train step 01188 reward={max=05.0000, min=05.0000, mean=05.0000} optimizing loss=00.9103 diff={max=03.5258, min=00.0098, mean=00.6910} policy_loss=-4.1150 policy updated! \n",
      "train step 01189 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=00.7491 diff={max=03.2842, min=00.0168, mean=00.5636} policy_loss=-4.6676 policy updated! \n",
      "train step 01190 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.3540 diff={max=03.7532, min=00.0279, mean=00.8069} policy_loss=-4.4236 policy updated! \n",
      "train step 01191 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8135 diff={max=02.9558, min=00.0122, mean=00.6638} policy_loss=-4.6069 policy updated! \n",
      "train step 01192 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.4541 diff={max=02.2591, min=00.0104, mean=00.5077} policy_loss=-4.6036 policy updated! \n",
      "train step 01193 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6523 diff={max=04.5859, min=00.0109, mean=00.8309} policy_loss=-4.7237 policy updated! \n",
      "train step 01194 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.6367 diff={max=03.1209, min=00.0307, mean=00.5495} policy_loss=-5.5077 policy updated! \n",
      "train step 01195 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.7998 diff={max=03.1405, min=00.0117, mean=00.6097} policy_loss=-5.5000 policy updated! \n",
      "train step 01196 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=02.7820 diff={max=05.0104, min=00.0124, mean=01.0993} policy_loss=-5.0867 policy updated! \n",
      "train step 01197 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2852 diff={max=04.9151, min=00.0160, mean=00.6795} policy_loss=-4.7800 policy updated! \n",
      "train step 01198 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.0663 diff={max=03.3309, min=00.0305, mean=00.6925} policy_loss=-4.8922 policy updated! \n",
      "train step 01199 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=00.6849 diff={max=03.3122, min=00.0096, mean=00.5041} policy_loss=-5.2746 policy updated! \n",
      "train step 01200 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.9276 diff={max=02.8067, min=00.0074, mean=00.6775} policy_loss=-4.1035 policy updated! \n",
      "train step 01201 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.8296 diff={max=04.6752, min=00.0115, mean=00.8135} policy_loss=-4.1666 policy updated! \n",
      "train step 01202 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1901 diff={max=04.6998, min=00.0013, mean=00.6409} policy_loss=-4.1781 policy updated! \n",
      "train step 01203 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7783 diff={max=03.1631, min=00.0058, mean=00.5877} policy_loss=-5.0829 policy updated! \n",
      "train step 01204 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=00.6852 diff={max=03.8266, min=00.0122, mean=00.5258} policy_loss=-5.0242 policy updated! \n",
      "train step 01205 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.7164 diff={max=02.4440, min=00.0034, mean=00.6225} policy_loss=-5.4109 policy updated! \n",
      "train step 01206 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8124 diff={max=02.4166, min=00.0350, mean=00.6540} policy_loss=-5.3872 policy updated! \n",
      "train step 01207 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2260 diff={max=05.6161, min=00.0251, mean=00.8987} policy_loss=-4.7565 policy updated! \n",
      "train step 01208 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.6385 diff={max=04.7512, min=00.0320, mean=00.7906} policy_loss=-4.8600 policy updated! \n",
      "train step 01209 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7888 diff={max=05.5479, min=00.0123, mean=00.8444} policy_loss=-4.1439 policy updated! \n",
      "train step 01210 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.0786 diff={max=03.9967, min=00.0116, mean=01.0368} policy_loss=-4.8292 policy updated! \n",
      "train step 01211 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.3295 diff={max=03.8563, min=00.0475, mean=00.7895} policy_loss=-4.5170 policy updated! \n",
      "train step 01212 reward={max=05.0000, min=05.0000, mean=05.0000} optimizing loss=00.9454 diff={max=02.5227, min=00.0227, mean=00.7374} policy_loss=-4.7181 policy updated! \n",
      "train step 01213 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.9488 diff={max=04.4631, min=00.0094, mean=00.8720} policy_loss=-4.5351 policy updated! \n",
      "train step 01214 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.2668 diff={max=05.5482, min=00.0165, mean=00.6632} policy_loss=-5.2689 policy updated! \n",
      "train step 01215 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.6193 diff={max=02.4838, min=00.0155, mean=00.5622} policy_loss=-5.1702 policy updated! \n",
      "train step 01216 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=02.5756 diff={max=06.5115, min=00.0310, mean=01.0087} policy_loss=-5.2738 policy updated! \n",
      "train step 01217 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2507 diff={max=04.0873, min=00.0418, mean=00.8363} policy_loss=-4.8721 policy updated! \n",
      "train step 01218 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5126 diff={max=05.5933, min=00.0280, mean=00.7413} policy_loss=-4.5472 policy updated! \n",
      "train step 01219 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=02.1869 diff={max=04.2937, min=00.0313, mean=01.0319} policy_loss=-4.3906 policy updated! \n",
      "train step 01220 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.7588 diff={max=02.1848, min=00.0065, mean=00.6728} policy_loss=-4.1022 policy updated! \n",
      "train step 01221 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.9299 diff={max=04.5786, min=00.0276, mean=00.9894} policy_loss=-4.3548 policy updated! \n",
      "train step 01222 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7046 diff={max=03.5255, min=00.0498, mean=00.8965} policy_loss=-4.8907 policy updated! \n",
      "train step 01223 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8064 diff={max=03.7823, min=00.0126, mean=00.5852} policy_loss=-5.3590 policy updated! \n",
      "train step 01224 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.7260 diff={max=04.1361, min=00.0162, mean=00.8591} policy_loss=-5.5810 policy updated! \n",
      "train step 01225 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.7940 diff={max=02.5912, min=00.0068, mean=00.6455} policy_loss=-4.4561 policy updated! \n",
      "train step 01226 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4040 diff={max=04.8850, min=00.0033, mean=00.7310} policy_loss=-5.0989 policy updated! \n",
      "train step 01227 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6605 diff={max=06.7631, min=00.0042, mean=00.9265} policy_loss=-4.5837 policy updated! \n",
      "train step 01228 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=00.8457 diff={max=02.9765, min=00.0000, mean=00.6238} policy_loss=-4.2178 policy updated! \n",
      "train step 01229 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.9174 diff={max=04.7079, min=00.0046, mean=00.9287} policy_loss=-4.5574 policy updated! \n",
      "train step 01230 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.9217 diff={max=03.2924, min=00.0211, mean=00.6672} policy_loss=-4.7030 policy updated! \n",
      "train step 01231 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8365 diff={max=03.3535, min=00.0087, mean=00.6300} policy_loss=-4.7451 policy updated! \n",
      "train step 01232 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8198 diff={max=05.7466, min=00.0030, mean=00.7719} policy_loss=-4.5835 policy updated! \n",
      "train step 01233 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3823 diff={max=04.1998, min=00.0194, mean=00.7761} policy_loss=-5.2191 policy updated! \n",
      "train step 01234 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=04.2438 diff={max=08.5697, min=00.0084, mean=01.1654} policy_loss=-4.6945 policy updated! \n",
      "train step 01235 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.0272 diff={max=03.5541, min=00.0469, mean=00.7345} policy_loss=-5.0338 policy updated! \n",
      "train step 01236 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2212 diff={max=06.7075, min=00.0365, mean=00.8644} policy_loss=-4.3019 policy updated! \n",
      "train step 01237 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9122 diff={max=05.3902, min=00.0233, mean=00.9930} policy_loss=-4.9575 policy updated! \n",
      "train step 01238 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.0082 diff={max=02.5108, min=00.0194, mean=00.7901} policy_loss=-4.6317 policy updated! \n",
      "train step 01239 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7946 diff={max=05.2871, min=00.0027, mean=00.8679} policy_loss=-4.5178 policy updated! \n",
      "train step 01240 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.6860 diff={max=05.9703, min=00.0051, mean=00.8528} policy_loss=-5.1070 policy updated! \n",
      "train step 01241 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2050 diff={max=03.2972, min=00.0122, mean=00.7434} policy_loss=-4.8423 policy updated! \n",
      "train step 01242 reward={max=05.0000, min=05.0000, mean=05.0000} optimizing loss=01.5223 diff={max=04.9753, min=00.0021, mean=00.7876} policy_loss=-4.8797 policy updated! \n",
      "train step 01243 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1825 diff={max=10.9235, min=00.0100, mean=00.8243} policy_loss=-4.8209 policy updated! \n",
      "train step 01244 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.2820 diff={max=03.1125, min=00.0021, mean=00.7696} policy_loss=-4.5497 policy updated! \n",
      "train step 01245 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.9802 diff={max=03.6044, min=00.0327, mean=00.7021} policy_loss=-5.2677 policy updated! \n",
      "train step 01246 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.2269 diff={max=04.0031, min=00.0171, mean=00.6140} policy_loss=-4.7293 policy updated! \n",
      "train step 01247 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0437 diff={max=05.3845, min=00.0014, mean=00.7481} policy_loss=-4.4109 policy updated! \n",
      "train step 01248 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6036 diff={max=03.5693, min=00.0054, mean=00.8525} policy_loss=-4.6990 policy updated! \n",
      "train step 01249 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9023 diff={max=04.7591, min=00.0096, mean=00.8898} policy_loss=-5.1711 policy updated! \n",
      "train step 01250 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.7976 diff={max=06.7851, min=00.0166, mean=01.0234} policy_loss=-4.5161 policy updated! \n",
      "train step 01251 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2762 diff={max=03.9974, min=00.0041, mean=00.7683} policy_loss=-4.6634 policy updated! \n",
      "train step 01252 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.6646 diff={max=02.8829, min=00.0146, mean=00.5707} policy_loss=-4.6679 policy updated! \n",
      "train step 01253 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6537 diff={max=04.4395, min=00.0221, mean=00.8302} policy_loss=-4.7174 policy updated! \n",
      "train step 01254 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.0269 diff={max=03.0165, min=00.0455, mean=00.7399} policy_loss=-5.1156 policy updated! \n",
      "train step 01255 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.8192 diff={max=04.0493, min=00.0929, mean=00.6343} policy_loss=-4.8471 policy updated! \n",
      "train step 01256 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.4867 diff={max=05.5962, min=00.0003, mean=00.7588} policy_loss=-5.1381 policy updated! \n",
      "train step 01257 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5750 diff={max=04.2415, min=00.0141, mean=00.8366} policy_loss=-4.6735 policy updated! \n",
      "train step 01258 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7164 diff={max=07.0838, min=00.0097, mean=00.7468} policy_loss=-4.2712 policy updated! \n",
      "train step 01259 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=02.1138 diff={max=04.3422, min=00.0028, mean=00.9703} policy_loss=-4.5272 policy updated! \n",
      "train step 01260 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.6903 diff={max=06.4576, min=00.0149, mean=00.7506} policy_loss=-4.4998 policy updated! \n",
      "train step 01261 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.0499 diff={max=04.1871, min=00.0137, mean=00.6691} policy_loss=-4.9778 policy updated! \n",
      "train step 01262 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6610 diff={max=06.0630, min=00.0050, mean=00.7116} policy_loss=-4.8826 policy updated! \n",
      "train step 01263 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.8405 diff={max=04.7744, min=00.0095, mean=00.9516} policy_loss=-4.7266 policy updated! \n",
      "train step 01264 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7355 diff={max=06.3333, min=00.0001, mean=00.9708} policy_loss=-4.8385 policy updated! \n",
      "train step 01265 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.4100 diff={max=03.3391, min=00.0068, mean=00.7072} policy_loss=-4.4025 policy updated! \n",
      "train step 01266 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8624 diff={max=06.3268, min=00.0059, mean=00.7296} policy_loss=-4.6092 policy updated! \n",
      "train step 01267 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0603 diff={max=04.9701, min=00.0126, mean=00.9757} policy_loss=-4.2707 policy updated! \n",
      "train step 01268 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2586 diff={max=04.3756, min=00.0112, mean=00.9942} policy_loss=-4.1516 policy updated! \n",
      "train step 01269 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7547 diff={max=04.6806, min=00.0030, mean=00.8539} policy_loss=-4.2256 policy updated! \n",
      "train step 01270 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.8472 diff={max=06.6409, min=00.0040, mean=00.7926} policy_loss=-4.5731 policy updated! \n",
      "train step 01271 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.3810 diff={max=03.7755, min=00.0189, mean=00.7471} policy_loss=-5.2659 policy updated! \n",
      "train step 01272 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9023 diff={max=05.4093, min=00.0153, mean=00.7904} policy_loss=-5.2546 policy updated! \n",
      "train step 01273 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=02.5590 diff={max=04.4130, min=00.0131, mean=01.0935} policy_loss=-5.3789 policy updated! \n",
      "train step 01274 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8361 diff={max=04.1757, min=00.0216, mean=00.6246} policy_loss=-5.2160 policy updated! \n",
      "train step 01275 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.9624 diff={max=03.3224, min=00.0241, mean=00.6620} policy_loss=-5.2109 policy updated! \n",
      "train step 01276 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.4888 diff={max=05.1279, min=00.0155, mean=00.7157} policy_loss=-5.5305 policy updated! \n",
      "train step 01277 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4195 diff={max=08.1899, min=00.0288, mean=01.0930} policy_loss=-5.4902 policy updated! \n",
      "train step 01278 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3168 diff={max=04.3356, min=00.0471, mean=00.7361} policy_loss=-4.9259 policy updated! \n",
      "train step 01279 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2033 diff={max=04.4553, min=00.0067, mean=00.7876} policy_loss=-3.9218 policy updated! \n",
      "train step 01280 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.8062 diff={max=04.6466, min=00.0527, mean=01.0024} policy_loss=-4.7192 policy updated! \n",
      "train step 01281 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.8254 diff={max=06.0188, min=00.0578, mean=00.8853} policy_loss=-4.8935 policy updated! \n",
      "train step 01282 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8328 diff={max=05.4266, min=00.0054, mean=01.1124} policy_loss=-5.5600 policy updated! \n",
      "train step 01283 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6426 diff={max=04.2803, min=00.0163, mean=00.9324} policy_loss=-5.6286 policy updated! \n",
      "train step 01284 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.5442 diff={max=04.7344, min=00.0012, mean=00.8078} policy_loss=-4.8677 policy updated! \n",
      "train step 01285 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.0826 diff={max=06.7837, min=00.0038, mean=00.8821} policy_loss=-4.9373 policy updated! \n",
      "train step 01286 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2172 diff={max=03.3522, min=00.0065, mean=00.7712} policy_loss=-4.4849 policy updated! \n",
      "train step 01287 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8926 diff={max=03.5531, min=00.0188, mean=00.6398} policy_loss=-5.1533 policy updated! \n",
      "train step 01288 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1250 diff={max=03.4544, min=00.0048, mean=00.6929} policy_loss=-5.0930 policy updated! \n",
      "train step 01289 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2498 diff={max=09.0245, min=00.0088, mean=00.8963} policy_loss=-4.8073 policy updated! \n",
      "train step 01290 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.3081 diff={max=03.3445, min=00.0329, mean=00.8012} policy_loss=-4.4735 policy updated! \n",
      "train step 01291 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.3469 diff={max=04.0367, min=00.0265, mean=00.7830} policy_loss=-4.6621 policy updated! \n",
      "train step 01292 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3859 diff={max=03.2436, min=00.0139, mean=00.8931} policy_loss=-5.2215 policy updated! \n",
      "train step 01293 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.0294 diff={max=04.1387, min=00.0241, mean=00.6074} policy_loss=-5.1668 policy updated! \n",
      "train step 01294 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2020 diff={max=05.2454, min=00.0135, mean=00.6498} policy_loss=-4.9545 policy updated! \n",
      "train step 01295 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.7013 diff={max=04.0553, min=00.0175, mean=00.8801} policy_loss=-5.4919 policy updated! \n",
      "train step 01296 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.1640 diff={max=04.3973, min=00.0004, mean=00.7505} policy_loss=-5.6995 policy updated! \n",
      "train step 01297 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8099 diff={max=05.4075, min=00.0073, mean=00.7911} policy_loss=-5.1667 policy updated! \n",
      "train step 01298 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.0472 diff={max=04.8572, min=00.0056, mean=00.6497} policy_loss=-4.7230 policy updated! \n",
      "train step 01299 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=03.2557 diff={max=07.3339, min=00.0329, mean=01.0196} policy_loss=-4.2198 policy updated! \n",
      "train step 01300 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.8385 diff={max=03.5175, min=00.0085, mean=00.9252} policy_loss=-4.6809 policy updated! \n",
      "train step 01301 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=00.9218 diff={max=03.5439, min=00.0033, mean=00.7107} policy_loss=-5.1328 policy updated! \n",
      "train step 01302 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0470 diff={max=03.1513, min=00.0440, mean=00.7517} policy_loss=-5.0580 policy updated! \n",
      "train step 01303 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.0494 diff={max=03.0963, min=00.0069, mean=00.7019} policy_loss=-5.3458 policy updated! \n",
      "train step 01304 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6977 diff={max=04.8042, min=00.0287, mean=00.8101} policy_loss=-4.8217 policy updated! \n",
      "train step 01305 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.2381 diff={max=03.7059, min=00.0317, mean=00.7795} policy_loss=-4.6227 policy updated! \n",
      "train step 01306 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=02.7726 diff={max=09.5077, min=00.0101, mean=00.8311} policy_loss=-4.7427 policy updated! \n",
      "train step 01307 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9653 diff={max=02.7417, min=00.0032, mean=00.6945} policy_loss=-4.6904 policy updated! \n",
      "train step 01308 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1549 diff={max=04.5221, min=00.0026, mean=00.6743} policy_loss=-4.6206 policy updated! \n",
      "train step 01309 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=02.8009 diff={max=07.7168, min=00.0065, mean=00.9481} policy_loss=-5.0233 policy updated! \n",
      "train step 01310 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.9852 diff={max=04.8934, min=00.0022, mean=00.8597} policy_loss=-4.9976 policy updated! \n",
      "train step 01311 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=00.8985 diff={max=03.2641, min=00.0092, mean=00.6778} policy_loss=-5.0153 policy updated! \n",
      "train step 01312 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8722 diff={max=03.8884, min=00.0094, mean=00.9116} policy_loss=-5.2237 policy updated! \n",
      "train step 01313 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.6750 diff={max=06.0685, min=00.0081, mean=00.7855} policy_loss=-5.1202 policy updated! \n",
      "train step 01314 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2656 diff={max=06.4259, min=00.0197, mean=00.9770} policy_loss=-5.0878 policy updated! \n",
      "train step 01315 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.6440 diff={max=02.8676, min=00.0066, mean=00.5121} policy_loss=-5.1420 policy updated! \n",
      "train step 01316 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=00.9978 diff={max=04.4578, min=00.0052, mean=00.6644} policy_loss=-5.2228 policy updated! \n",
      "train step 01317 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5645 diff={max=03.8803, min=00.0128, mean=00.8403} policy_loss=-4.8100 policy updated! \n",
      "train step 01318 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.0207 diff={max=05.0946, min=00.0325, mean=00.6512} policy_loss=-4.8194 policy updated! \n",
      "train step 01319 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=02.0610 diff={max=04.5944, min=00.0051, mean=00.8763} policy_loss=-4.7863 policy updated! \n",
      "train step 01320 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.9468 diff={max=03.8366, min=00.0176, mean=00.6217} policy_loss=-4.8971 policy updated! \n",
      "train step 01321 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3222 diff={max=04.6480, min=00.0448, mean=00.7004} policy_loss=-4.9598 policy updated! \n",
      "train step 01322 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2643 diff={max=04.2794, min=00.0281, mean=00.7420} policy_loss=-4.9643 policy updated! \n",
      "train step 01323 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.6103 diff={max=05.6619, min=00.0014, mean=00.7963} policy_loss=-5.0243 policy updated! \n",
      "train step 01324 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0809 diff={max=03.4084, min=00.0014, mean=00.6789} policy_loss=-4.5615 policy updated! \n",
      "train step 01325 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.4952 diff={max=02.9479, min=00.0013, mean=00.4805} policy_loss=-4.3729 policy updated! \n",
      "train step 01326 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=01.3512 diff={max=04.2847, min=00.0280, mean=00.7337} policy_loss=-5.3305 policy updated! \n",
      "train step 01327 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9474 diff={max=06.8248, min=00.0080, mean=00.8328} policy_loss=-5.1772 policy updated! \n",
      "train step 01328 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1096 diff={max=03.7151, min=00.0165, mean=00.6899} policy_loss=-5.1675 policy updated! \n",
      "train step 01329 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=02.1554 diff={max=05.9438, min=00.0180, mean=00.8657} policy_loss=-5.0723 policy updated! \n",
      "train step 01330 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.0628 diff={max=04.5694, min=00.0051, mean=00.5876} policy_loss=-5.0264 policy updated! \n",
      "train step 01331 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3819 diff={max=04.2066, min=00.0037, mean=00.8002} policy_loss=-5.0082 policy updated! \n",
      "train step 01332 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8918 diff={max=02.7442, min=00.0048, mean=00.7114} policy_loss=-5.6138 policy updated! \n",
      "train step 01333 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7723 diff={max=04.7628, min=00.0531, mean=00.8762} policy_loss=-4.9350 policy updated! \n",
      "train step 01334 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5594 diff={max=05.2522, min=00.0015, mean=01.1091} policy_loss=-4.9213 policy updated! \n",
      "train step 01335 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.0668 diff={max=04.2457, min=00.0029, mean=00.6533} policy_loss=-4.9293 policy updated! \n",
      "train step 01336 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0166 diff={max=05.5740, min=00.0053, mean=00.8128} policy_loss=-4.8782 policy updated! \n",
      "train step 01337 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7123 diff={max=07.9778, min=00.0142, mean=00.6885} policy_loss=-5.0802 policy updated! \n",
      "train step 01338 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3406 diff={max=04.1485, min=00.0078, mean=00.7436} policy_loss=-5.0808 policy updated! \n",
      "train step 01339 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7150 diff={max=02.3877, min=00.0259, mean=00.6669} policy_loss=-4.9809 policy updated! \n",
      "train step 01340 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=01.2142 diff={max=04.8188, min=00.0350, mean=00.7571} policy_loss=-4.3419 policy updated! \n",
      "train step 01341 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=00.9674 diff={max=02.4906, min=00.0170, mean=00.7855} policy_loss=-4.7033 policy updated! \n",
      "train step 01342 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1134 diff={max=04.5528, min=00.0108, mean=00.7018} policy_loss=-5.0329 policy updated! \n",
      "train step 01343 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2225 diff={max=03.9251, min=00.0246, mean=00.7569} policy_loss=-5.1794 policy updated! \n",
      "train step 01344 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3465 diff={max=05.2887, min=00.0601, mean=00.7627} policy_loss=-5.2464 policy updated! \n",
      "train step 01345 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=00.8749 diff={max=03.1980, min=00.0557, mean=00.6781} policy_loss=-4.8312 policy updated! \n",
      "train step 01346 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7980 diff={max=03.1152, min=00.0234, mean=00.6097} policy_loss=-4.8926 policy updated! \n",
      "train step 01347 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8342 diff={max=05.1516, min=00.0172, mean=00.8510} policy_loss=-5.0499 policy updated! \n",
      "train step 01348 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9435 diff={max=07.3999, min=00.0063, mean=00.7586} policy_loss=-4.7035 policy updated! \n",
      "train step 01349 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=01.6453 diff={max=04.7795, min=00.0055, mean=00.7801} policy_loss=-5.0566 policy updated! \n",
      "train step 01350 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=00.7074 diff={max=02.5913, min=00.0361, mean=00.6006} policy_loss=-4.7076 policy updated! \n",
      "train step 01351 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=01.0292 diff={max=03.6677, min=00.0028, mean=00.6987} policy_loss=-5.5308 policy updated! \n",
      "train step 01352 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4389 diff={max=04.3324, min=00.0679, mean=00.8388} policy_loss=-5.1316 policy updated! \n",
      "train step 01353 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=01.8948 diff={max=04.6437, min=00.0302, mean=00.8588} policy_loss=-4.8927 policy updated! \n",
      "train step 01354 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.5690 diff={max=02.8508, min=00.0072, mean=00.5233} policy_loss=-5.1292 policy updated! \n",
      "train step 01355 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.9326 diff={max=04.5573, min=00.0143, mean=00.8195} policy_loss=-5.6525 policy updated! \n",
      "train step 01356 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1424 diff={max=05.2739, min=00.0335, mean=01.1683} policy_loss=-4.9023 policy updated! \n",
      "train step 01357 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7777 diff={max=03.4310, min=00.0127, mean=00.6250} policy_loss=-4.7357 policy updated! \n",
      "train step 01358 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6102 diff={max=03.9343, min=00.0238, mean=00.9241} policy_loss=-4.9583 policy updated! \n",
      "train step 01359 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7932 diff={max=02.5552, min=00.0163, mean=00.6887} policy_loss=-4.4598 policy updated! \n",
      "train step 01360 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.1435 diff={max=04.3941, min=00.0366, mean=00.7905} policy_loss=-4.1499 policy updated! \n",
      "train step 01361 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0630 diff={max=07.1873, min=00.0010, mean=00.9320} policy_loss=-5.6077 policy updated! \n",
      "train step 01362 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1388 diff={max=04.8777, min=00.0023, mean=00.9324} policy_loss=-5.0336 policy updated! \n",
      "train step 01363 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4778 diff={max=05.8621, min=00.0204, mean=00.7432} policy_loss=-5.3419 policy updated! \n",
      "train step 01364 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9972 diff={max=03.0985, min=00.0175, mean=00.6363} policy_loss=-5.3758 policy updated! \n",
      "train step 01365 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=00.8419 diff={max=04.2037, min=00.0289, mean=00.5617} policy_loss=-5.3971 policy updated! \n",
      "train step 01366 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=02.6074 diff={max=07.8443, min=00.0074, mean=00.9722} policy_loss=-4.8392 policy updated! \n",
      "train step 01367 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0324 diff={max=03.5621, min=00.0343, mean=00.7131} policy_loss=-4.4760 policy updated! \n",
      "train step 01368 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=01.7266 diff={max=03.4687, min=00.0013, mean=00.9073} policy_loss=-4.3999 policy updated! \n",
      "train step 01369 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4835 diff={max=04.0835, min=00.0038, mean=00.8078} policy_loss=-4.7812 policy updated! \n",
      "train step 01370 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.2602 diff={max=03.4010, min=00.0057, mean=00.8056} policy_loss=-4.9959 policy updated! \n",
      "train step 01371 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3616 diff={max=03.6227, min=00.0112, mean=00.8910} policy_loss=-5.3159 policy updated! \n",
      "train step 01372 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1253 diff={max=03.1211, min=00.0101, mean=00.7764} policy_loss=-5.3882 policy updated! \n",
      "train step 01373 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3038 diff={max=03.3858, min=00.0449, mean=00.8442} policy_loss=-4.8998 policy updated! \n",
      "train step 01374 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0228 diff={max=04.4609, min=00.0069, mean=00.7278} policy_loss=-5.0037 policy updated! \n",
      "train step 01375 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.9496 diff={max=08.0194, min=00.0208, mean=01.0966} policy_loss=-5.5255 policy updated! \n",
      "train step 01376 reward={max=06.0000, min=00.0000, mean=03.4000} optimizing loss=00.5761 diff={max=02.6608, min=00.0108, mean=00.5404} policy_loss=-4.7213 policy updated! \n",
      "train step 01377 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3927 diff={max=04.0710, min=00.0040, mean=00.8438} policy_loss=-4.4504 policy updated! \n",
      "train step 01378 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=01.9048 diff={max=06.3538, min=00.0125, mean=00.8576} policy_loss=-4.2947 policy updated! \n",
      "train step 01379 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9280 diff={max=05.1119, min=00.0531, mean=00.9631} policy_loss=-4.6674 policy updated! \n",
      "train step 01380 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.3444 diff={max=04.8446, min=00.0385, mean=01.1023} policy_loss=-4.6519 policy updated! \n",
      "train step 01381 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=04.4034 diff={max=07.0305, min=00.0478, mean=01.4021} policy_loss=-5.2496 policy updated! \n",
      "train step 01382 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7432 diff={max=04.3834, min=00.0019, mean=00.9160} policy_loss=-5.3059 policy updated! \n",
      "train step 01383 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4718 diff={max=06.8534, min=00.0076, mean=00.6366} policy_loss=-4.8813 policy updated! \n",
      "train step 01384 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0865 diff={max=03.8198, min=00.0249, mean=00.7032} policy_loss=-5.3381 policy updated! \n",
      "train step 01385 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.9507 diff={max=05.4573, min=00.0021, mean=00.9505} policy_loss=-4.9938 policy updated! \n",
      "train step 01386 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7660 diff={max=02.7868, min=00.0172, mean=00.5978} policy_loss=-5.0117 policy updated! \n",
      "train step 01387 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3231 diff={max=06.4459, min=00.0390, mean=00.8922} policy_loss=-4.9043 policy updated! \n",
      "train step 01388 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9314 diff={max=03.7496, min=00.0205, mean=00.6528} policy_loss=-4.3409 policy updated! \n",
      "train step 01389 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=00.7839 diff={max=02.5665, min=00.0349, mean=00.6814} policy_loss=-4.2862 policy updated! \n",
      "train step 01390 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.6753 diff={max=04.0967, min=00.0568, mean=00.9848} policy_loss=-4.8169 policy updated! \n",
      "train step 01391 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7292 diff={max=04.9647, min=00.0330, mean=01.0911} policy_loss=-5.1609 policy updated! \n",
      "train step 01392 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9420 diff={max=04.8008, min=00.0043, mean=01.0061} policy_loss=-5.7455 policy updated! \n",
      "train step 01393 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7535 diff={max=04.7735, min=00.0076, mean=00.7907} policy_loss=-5.6578 policy updated! \n",
      "train step 01394 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1107 diff={max=04.5888, min=00.0175, mean=00.7365} policy_loss=-5.8570 policy updated! \n",
      "train step 01395 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=05.2800 diff={max=08.9500, min=00.0544, mean=01.3445} policy_loss=-6.1129 policy updated! \n",
      "train step 01396 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.3985 diff={max=05.2655, min=00.0251, mean=00.7189} policy_loss=-4.8664 policy updated! \n",
      "train step 01397 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7508 diff={max=05.8612, min=00.0075, mean=00.8650} policy_loss=-4.2927 policy updated! \n",
      "train step 01398 reward={max=05.0000, min=05.0000, mean=05.0000} optimizing loss=01.3423 diff={max=04.4118, min=00.1032, mean=00.8814} policy_loss=-4.8764 policy updated! \n",
      "train step 01399 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.7967 diff={max=03.9356, min=00.0361, mean=00.9899} policy_loss=-4.8507 policy updated! \n",
      "train step 01400 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.3729 diff={max=05.2657, min=00.0224, mean=00.9227} policy_loss=-4.6965 policy updated! \n",
      "train step 01401 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7946 diff={max=02.7284, min=00.0105, mean=00.7005} policy_loss=-5.3629 policy updated! \n",
      "train step 01402 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2394 diff={max=04.4776, min=00.0246, mean=01.0431} policy_loss=-4.9639 policy updated! \n",
      "train step 01403 reward={max=06.0000, min=05.0000, mean=05.8000} optimizing loss=01.9291 diff={max=04.6902, min=00.0298, mean=00.9504} policy_loss=-5.4802 policy updated! \n",
      "train step 01404 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9410 diff={max=03.7074, min=00.0254, mean=00.6530} policy_loss=-5.3905 policy updated! \n",
      "train step 01405 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.1516 diff={max=03.5693, min=00.0513, mean=00.7361} policy_loss=-5.0135 policy updated! \n",
      "train step 01406 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3619 diff={max=04.2145, min=00.0294, mean=01.1103} policy_loss=-4.8892 policy updated! \n",
      "train step 01407 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6657 diff={max=04.5849, min=00.0148, mean=00.8744} policy_loss=-5.0267 policy updated! \n",
      "train step 01408 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.4227 diff={max=03.9463, min=00.0049, mean=00.7765} policy_loss=-5.0195 policy updated! \n",
      "train step 01409 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8331 diff={max=03.2287, min=00.0083, mean=00.6455} policy_loss=-5.0561 policy updated! \n",
      "train step 01410 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.1936 diff={max=03.8154, min=00.0259, mean=00.7542} policy_loss=-4.7555 policy updated! \n",
      "train step 01411 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0125 diff={max=05.0440, min=00.0184, mean=00.9032} policy_loss=-5.3415 policy updated! \n",
      "train step 01412 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1840 diff={max=04.1022, min=00.0140, mean=00.7260} policy_loss=-4.9810 policy updated! \n",
      "train step 01413 reward={max=05.0000, min=05.0000, mean=05.0000} optimizing loss=01.1615 diff={max=03.2878, min=00.0761, mean=00.7773} policy_loss=-4.9486 policy updated! \n",
      "train step 01414 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=03.8632 diff={max=07.2770, min=00.0309, mean=01.2389} policy_loss=-5.4462 policy updated! \n",
      "train step 01415 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.8578 diff={max=03.5585, min=00.0063, mean=00.5859} policy_loss=-5.5468 policy updated! \n",
      "train step 01416 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8376 diff={max=05.4882, min=00.0342, mean=00.8369} policy_loss=-4.5784 policy updated! \n",
      "train step 01417 reward={max=05.0000, min=05.0000, mean=05.0000} optimizing loss=02.0740 diff={max=06.2260, min=00.0074, mean=00.8095} policy_loss=-4.6052 policy updated! \n",
      "train step 01418 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.4826 diff={max=04.1559, min=00.0029, mean=00.7781} policy_loss=-5.3180 policy updated! \n",
      "train step 01419 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=00.9281 diff={max=03.8216, min=00.0388, mean=00.7047} policy_loss=-5.1471 policy updated! \n",
      "train step 01420 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.7425 diff={max=02.4976, min=00.0082, mean=00.6447} policy_loss=-5.4262 policy updated! \n",
      "train step 01421 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.7870 diff={max=06.0183, min=00.0086, mean=00.8762} policy_loss=-4.9318 policy updated! \n",
      "train step 01422 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2073 diff={max=05.6728, min=00.0086, mean=00.6398} policy_loss=-5.9575 policy updated! \n",
      "train step 01423 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7754 diff={max=05.9107, min=00.0067, mean=00.7837} policy_loss=-5.7828 policy updated! \n",
      "train step 01424 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2948 diff={max=03.8559, min=00.0262, mean=00.7580} policy_loss=-5.8765 policy updated! \n",
      "train step 01425 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.8221 diff={max=03.3335, min=00.0227, mean=00.6418} policy_loss=-4.9490 policy updated! \n",
      "train step 01426 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=02.1203 diff={max=05.8828, min=00.0341, mean=00.9834} policy_loss=-5.4445 policy updated! \n",
      "train step 01427 reward={max=05.0000, min=05.0000, mean=05.0000} optimizing loss=01.9366 diff={max=04.2607, min=00.0075, mean=00.9600} policy_loss=-5.4369 policy updated! \n",
      "train step 01428 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1136 diff={max=05.2325, min=00.0025, mean=00.7977} policy_loss=-5.0161 policy updated! \n",
      "train step 01429 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2242 diff={max=04.1715, min=00.0059, mean=00.7015} policy_loss=-5.3431 policy updated! \n",
      "train step 01430 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=00.6907 diff={max=02.6782, min=00.0007, mean=00.5862} policy_loss=-5.1324 policy updated! \n",
      "train step 01431 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=02.2816 diff={max=05.2078, min=00.0452, mean=01.0056} policy_loss=-4.6653 policy updated! \n",
      "train step 01432 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4612 diff={max=04.5303, min=00.0268, mean=00.7802} policy_loss=-4.7588 policy updated! \n",
      "train step 01433 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5729 diff={max=04.4142, min=00.0088, mean=00.8206} policy_loss=-5.0603 policy updated! \n",
      "train step 01434 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.4039 diff={max=04.0630, min=00.0149, mean=00.8077} policy_loss=-5.5171 policy updated! \n",
      "train step 01435 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.6902 diff={max=06.0895, min=00.0197, mean=01.0561} policy_loss=-5.0375 policy updated! \n",
      "train step 01436 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7932 diff={max=05.5304, min=00.0085, mean=00.8010} policy_loss=-5.3567 policy updated! \n",
      "train step 01437 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1007 diff={max=04.9531, min=00.0118, mean=00.8781} policy_loss=-5.0003 policy updated! \n",
      "train step 01438 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0826 diff={max=04.3133, min=00.0023, mean=01.0116} policy_loss=-5.2504 policy updated! \n",
      "train step 01439 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=00.8892 diff={max=02.4995, min=00.0293, mean=00.7313} policy_loss=-4.7026 policy updated! \n",
      "train step 01440 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.6491 diff={max=03.6384, min=00.0137, mean=00.9186} policy_loss=-5.2582 policy updated! \n",
      "train step 01441 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7206 diff={max=04.6836, min=00.0138, mean=00.8014} policy_loss=-5.2846 policy updated! \n",
      "train step 01442 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1582 diff={max=05.0512, min=00.0128, mean=00.9220} policy_loss=-5.0521 policy updated! \n",
      "train step 01443 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3697 diff={max=04.4186, min=00.0259, mean=00.7450} policy_loss=-5.0787 policy updated! \n",
      "train step 01444 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.1293 diff={max=03.6729, min=00.0111, mean=00.6984} policy_loss=-4.9940 policy updated! \n",
      "train step 01445 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.0971 diff={max=07.4459, min=00.0178, mean=00.8029} policy_loss=-4.7999 policy updated! \n",
      "train step 01446 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5056 diff={max=06.6219, min=00.0073, mean=00.9936} policy_loss=-4.8493 policy updated! \n",
      "train step 01447 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5243 diff={max=06.0391, min=00.0377, mean=00.8244} policy_loss=-5.4611 policy updated! \n",
      "train step 01448 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.5512 diff={max=04.9778, min=00.0131, mean=00.7408} policy_loss=-4.7619 policy updated! \n",
      "train step 01449 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.2280 diff={max=04.5547, min=00.0199, mean=00.7250} policy_loss=-4.9435 policy updated! \n",
      "train step 01450 reward={max=05.0000, min=05.0000, mean=05.0000} optimizing loss=01.5739 diff={max=04.6775, min=00.0304, mean=00.8474} policy_loss=-5.1341 policy updated! \n",
      "train step 01451 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=02.4296 diff={max=05.5719, min=00.0100, mean=01.0400} policy_loss=-5.2604 policy updated! \n",
      "train step 01452 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9017 diff={max=03.0706, min=00.0054, mean=00.6458} policy_loss=-5.4868 policy updated! \n",
      "train step 01453 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6240 diff={max=04.6620, min=00.0290, mean=00.7261} policy_loss=-4.9753 policy updated! \n",
      "train step 01454 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0975 diff={max=05.7883, min=00.0038, mean=00.9229} policy_loss=-5.5773 policy updated! \n",
      "train step 01455 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.3240 diff={max=03.3408, min=00.0112, mean=00.8410} policy_loss=-5.2634 policy updated! \n",
      "train step 01456 reward={max=05.0000, min=05.0000, mean=05.0000} optimizing loss=01.0660 diff={max=03.7616, min=00.0096, mean=00.7732} policy_loss=-6.1536 policy updated! \n",
      "train step 01457 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7106 diff={max=05.0022, min=00.0551, mean=00.8733} policy_loss=-5.0851 policy updated! \n",
      "train step 01458 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.4402 diff={max=05.1024, min=00.0062, mean=00.7996} policy_loss=-5.4898 policy updated! \n",
      "train step 01459 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4786 diff={max=03.5553, min=00.0181, mean=00.8865} policy_loss=-4.5054 policy updated! \n",
      "train step 01460 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.7554 diff={max=04.3325, min=00.0052, mean=00.9021} policy_loss=-4.8922 policy updated! \n",
      "train step 01461 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9718 diff={max=04.6267, min=00.0456, mean=00.9362} policy_loss=-4.2698 policy updated! \n",
      "train step 01462 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2115 diff={max=04.8088, min=00.0132, mean=00.7547} policy_loss=-5.2551 policy updated! \n",
      "train step 01463 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3216 diff={max=04.0041, min=00.0059, mean=00.7890} policy_loss=-5.8003 policy updated! \n",
      "train step 01464 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=02.5352 diff={max=04.7934, min=00.0500, mean=01.0316} policy_loss=-5.1115 policy updated! \n",
      "train step 01465 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.6866 diff={max=05.3598, min=00.0178, mean=00.8310} policy_loss=-5.5389 policy updated! \n",
      "train step 01466 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.1621 diff={max=05.2173, min=00.0057, mean=00.6670} policy_loss=-5.3180 policy updated! \n",
      "train step 01467 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1844 diff={max=04.9551, min=00.0067, mean=00.6651} policy_loss=-4.4345 policy updated! \n",
      "train step 01468 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9479 diff={max=05.6287, min=00.0115, mean=01.2232} policy_loss=-5.4202 policy updated! \n",
      "train step 01469 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5367 diff={max=03.5684, min=00.0284, mean=00.9224} policy_loss=-5.5959 policy updated! \n",
      "train step 01470 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=03.9448 diff={max=07.7662, min=00.0017, mean=01.2109} policy_loss=-5.5773 policy updated! \n",
      "train step 01471 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=00.4879 diff={max=01.5219, min=00.0031, mean=00.5643} policy_loss=-5.3669 policy updated! \n",
      "train step 01472 reward={max=05.0000, min=05.0000, mean=05.0000} optimizing loss=02.1741 diff={max=05.9080, min=00.0081, mean=00.8563} policy_loss=-5.2003 policy updated! \n",
      "train step 01473 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=02.1665 diff={max=03.5347, min=00.0043, mean=01.0815} policy_loss=-5.5957 policy updated! \n",
      "train step 01474 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5809 diff={max=05.7837, min=00.0157, mean=00.9583} policy_loss=-5.0455 policy updated! \n",
      "train step 01475 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.3207 diff={max=04.8944, min=00.0068, mean=00.9901} policy_loss=-5.6634 policy updated! \n",
      "train step 01476 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3987 diff={max=04.8024, min=00.0234, mean=00.9441} policy_loss=-4.6373 policy updated! \n",
      "train step 01477 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8170 diff={max=03.1371, min=00.0014, mean=00.6665} policy_loss=-5.1398 policy updated! \n",
      "train step 01478 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.4625 diff={max=04.5753, min=00.0508, mean=00.8245} policy_loss=-5.1693 policy updated! \n",
      "train step 01479 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=01.4904 diff={max=04.1837, min=00.0092, mean=00.8282} policy_loss=-4.8642 policy updated! \n",
      "train step 01480 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.2451 diff={max=06.1779, min=00.0149, mean=00.9915} policy_loss=-5.0131 policy updated! \n",
      "train step 01481 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=00.7155 diff={max=02.7580, min=00.0284, mean=00.6327} policy_loss=-5.4478 policy updated! \n",
      "train step 01482 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6763 diff={max=03.9356, min=00.0067, mean=00.9843} policy_loss=-5.8016 policy updated! \n",
      "train step 01483 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=02.7397 diff={max=03.9580, min=00.0732, mean=01.2838} policy_loss=-5.7934 policy updated! \n",
      "train step 01484 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4095 diff={max=04.2565, min=00.0387, mean=00.7710} policy_loss=-5.6444 policy updated! \n",
      "train step 01485 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.5120 diff={max=04.6838, min=00.0105, mean=00.8411} policy_loss=-5.2805 policy updated! \n",
      "train step 01486 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.8121 diff={max=05.0408, min=00.0233, mean=00.9026} policy_loss=-6.0921 policy updated! \n",
      "train step 01487 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.6897 diff={max=02.4179, min=00.0242, mean=00.6495} policy_loss=-5.5838 policy updated! \n",
      "train step 01488 reward={max=05.0000, min=05.0000, mean=05.0000} optimizing loss=01.6720 diff={max=03.9499, min=00.0163, mean=00.8256} policy_loss=-5.6220 policy updated! \n",
      "train step 01489 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0950 diff={max=05.0590, min=00.0109, mean=00.6666} policy_loss=-5.5224 policy updated! \n",
      "train step 01490 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.1106 diff={max=03.8450, min=00.0294, mean=00.7526} policy_loss=-5.4704 policy updated! \n",
      "train step 01491 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7782 diff={max=04.4604, min=00.0041, mean=00.8387} policy_loss=-4.9755 policy updated! \n",
      "train step 01492 reward={max=05.0000, min=05.0000, mean=05.0000} optimizing loss=02.0462 diff={max=04.7804, min=00.0107, mean=00.9669} policy_loss=-5.4016 policy updated! \n",
      "train step 01493 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=01.6071 diff={max=05.5370, min=00.0033, mean=00.7701} policy_loss=-5.9462 policy updated! \n",
      "train step 01494 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8397 diff={max=02.9057, min=00.0176, mean=00.6537} policy_loss=-5.7459 policy updated! \n",
      "train step 01495 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.7348 diff={max=06.2405, min=00.0274, mean=00.8993} policy_loss=-5.2881 policy updated! \n",
      "train step 01496 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8525 diff={max=05.2417, min=00.0112, mean=00.8046} policy_loss=-5.8901 policy updated! \n",
      "train step 01497 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.6528 diff={max=02.3804, min=00.0198, mean=00.6135} policy_loss=-4.6720 policy updated! \n",
      "train step 01498 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8846 diff={max=02.7832, min=00.0151, mean=00.6566} policy_loss=-5.1497 policy updated! \n",
      "train step 01499 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=00.9378 diff={max=03.3722, min=00.0051, mean=00.6572} policy_loss=-5.2656 policy updated! \n",
      "train step 01500 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.3638 diff={max=05.1540, min=00.0033, mean=00.7839} policy_loss=-4.9878 policy updated! \n",
      "train step 01501 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6068 diff={max=05.6621, min=00.0171, mean=00.7068} policy_loss=-4.9014 policy updated! \n",
      "train step 01502 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7316 diff={max=02.0275, min=00.0089, mean=00.6413} policy_loss=-5.8289 policy updated! \n",
      "train step 01503 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=02.2123 diff={max=05.0142, min=00.0010, mean=00.8850} policy_loss=-5.1833 policy updated! \n",
      "train step 01504 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2889 diff={max=04.2691, min=00.0003, mean=00.6952} policy_loss=-4.6162 policy updated! \n",
      "train step 01505 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.0712 diff={max=04.1948, min=00.0216, mean=00.6389} policy_loss=-5.2464 policy updated! \n",
      "train step 01506 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=01.0777 diff={max=04.0190, min=00.0080, mean=00.7430} policy_loss=-5.3794 policy updated! \n",
      "train step 01507 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0684 diff={max=03.8693, min=00.0170, mean=00.7627} policy_loss=-4.8245 policy updated! \n",
      "train step 01508 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5107 diff={max=04.1162, min=00.0004, mean=00.8514} policy_loss=-5.4301 policy updated! \n",
      "train step 01509 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4247 diff={max=04.5408, min=00.0333, mean=00.8737} policy_loss=-5.3746 policy updated! \n",
      "train step 01510 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.6233 diff={max=04.1721, min=00.0082, mean=00.8506} policy_loss=-5.4016 policy updated! \n",
      "train step 01511 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9934 diff={max=03.1222, min=00.0109, mean=00.7142} policy_loss=-5.5326 policy updated! \n",
      "train step 01512 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9700 diff={max=05.0098, min=00.0069, mean=00.8177} policy_loss=-5.2757 policy updated! \n",
      "train step 01513 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0368 diff={max=04.0517, min=00.0009, mean=00.7035} policy_loss=-5.5642 policy updated! \n",
      "train step 01514 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6859 diff={max=05.3386, min=00.0086, mean=00.9832} policy_loss=-5.0009 policy updated! \n",
      "train step 01515 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.5696 diff={max=05.3023, min=00.0211, mean=00.7528} policy_loss=-5.2573 policy updated! \n",
      "train step 01516 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9866 diff={max=06.0098, min=00.0073, mean=00.8549} policy_loss=-5.7820 policy updated! \n",
      "train step 01517 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8881 diff={max=05.1451, min=00.0397, mean=00.8683} policy_loss=-5.4560 policy updated! \n",
      "train step 01518 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=01.3868 diff={max=04.0437, min=00.0354, mean=00.8121} policy_loss=-5.1927 policy updated! \n",
      "train step 01519 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=01.6018 diff={max=04.7330, min=00.0153, mean=00.7971} policy_loss=-5.1635 policy updated! \n",
      "train step 01520 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.1838 diff={max=03.7465, min=00.0012, mean=00.6822} policy_loss=-5.2595 policy updated! \n",
      "train step 01521 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4516 diff={max=05.0056, min=00.0132, mean=00.8050} policy_loss=-5.2463 policy updated! \n",
      "train step 01522 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5604 diff={max=05.2391, min=00.0001, mean=01.0412} policy_loss=-5.5343 policy updated! \n",
      "train step 01523 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6086 diff={max=06.5457, min=00.0105, mean=00.7994} policy_loss=-4.6445 policy updated! \n",
      "train step 01524 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7007 diff={max=02.4145, min=00.0194, mean=00.6656} policy_loss=-4.7876 policy updated! \n",
      "train step 01525 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=00.9040 diff={max=03.1533, min=00.0009, mean=00.6373} policy_loss=-5.4097 policy updated! \n",
      "train step 01526 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6143 diff={max=03.8001, min=00.0245, mean=00.7601} policy_loss=-5.3905 policy updated! \n",
      "train step 01527 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2530 diff={max=03.9104, min=00.0034, mean=00.7761} policy_loss=-5.3211 policy updated! \n",
      "train step 01528 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7553 diff={max=05.9698, min=00.0212, mean=00.7961} policy_loss=-5.7312 policy updated! \n",
      "train step 01529 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=01.0823 diff={max=03.8951, min=00.0086, mean=00.6856} policy_loss=-5.4586 policy updated! \n",
      "train step 01530 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.7122 diff={max=04.3332, min=00.0101, mean=00.8336} policy_loss=-5.0172 policy updated! \n",
      "train step 01531 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.6009 diff={max=02.7559, min=00.0049, mean=00.5251} policy_loss=-5.1577 policy updated! \n",
      "train step 01532 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.4856 diff={max=01.7737, min=00.0110, mean=00.5582} policy_loss=-5.3429 policy updated! \n",
      "train step 01533 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4740 diff={max=05.4382, min=00.1000, mean=00.7533} policy_loss=-5.5803 policy updated! \n",
      "train step 01534 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0689 diff={max=05.0701, min=00.0093, mean=00.6646} policy_loss=-5.8282 policy updated! \n",
      "train step 01535 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.8015 diff={max=05.6112, min=00.0336, mean=00.8500} policy_loss=-4.7875 policy updated! \n",
      "train step 01536 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6406 diff={max=04.5296, min=00.0437, mean=00.8077} policy_loss=-5.3954 policy updated! \n",
      "train step 01537 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.5281 diff={max=02.8464, min=00.0069, mean=00.5002} policy_loss=-5.1583 policy updated! \n",
      "train step 01538 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2054 diff={max=05.6622, min=00.0063, mean=00.9743} policy_loss=-4.2807 policy updated! \n",
      "train step 01539 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8780 diff={max=04.5549, min=00.0174, mean=00.9501} policy_loss=-4.4353 policy updated! \n",
      "train step 01540 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.8856 diff={max=04.1924, min=00.0235, mean=00.9639} policy_loss=-5.3312 policy updated! \n",
      "train step 01541 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0985 diff={max=03.2847, min=00.0064, mean=00.6848} policy_loss=-5.1367 policy updated! \n",
      "train step 01542 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8221 diff={max=06.3888, min=00.0125, mean=00.9487} policy_loss=-5.7545 policy updated! \n",
      "train step 01543 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.2757 diff={max=04.6434, min=00.0019, mean=00.6931} policy_loss=-5.7212 policy updated! \n",
      "train step 01544 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9105 diff={max=03.2214, min=00.0018, mean=00.6842} policy_loss=-5.5016 policy updated! \n",
      "train step 01545 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.3030 diff={max=03.6203, min=00.0162, mean=00.7770} policy_loss=-5.8275 policy updated! \n",
      "train step 01546 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6184 diff={max=05.4172, min=00.0003, mean=00.7701} policy_loss=-4.8356 policy updated! \n",
      "train step 01547 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3362 diff={max=04.8337, min=00.0245, mean=00.6530} policy_loss=-5.0950 policy updated! \n",
      "train step 01548 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3399 diff={max=04.6989, min=00.0014, mean=00.7346} policy_loss=-4.9015 policy updated! \n",
      "train step 01549 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.3694 diff={max=03.5134, min=00.0388, mean=00.8195} policy_loss=-5.0216 policy updated! \n",
      "train step 01550 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.1009 diff={max=04.3368, min=00.0259, mean=00.9682} policy_loss=-5.4510 policy updated! \n",
      "train step 01551 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3640 diff={max=05.9466, min=00.0008, mean=00.9616} policy_loss=-5.1017 policy updated! \n",
      "train step 01552 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3238 diff={max=03.1524, min=00.0032, mean=00.8616} policy_loss=-5.2230 policy updated! \n",
      "train step 01553 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.4604 diff={max=03.4025, min=00.0670, mean=00.8791} policy_loss=-6.4297 policy updated! \n",
      "train step 01554 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2244 diff={max=03.9037, min=00.0064, mean=00.8103} policy_loss=-5.3438 policy updated! \n",
      "train step 01555 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.1020 diff={max=03.4771, min=00.0043, mean=00.7377} policy_loss=-5.1749 policy updated! \n",
      "train step 01556 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6849 diff={max=09.5469, min=00.0029, mean=01.0384} policy_loss=-5.8715 policy updated! \n",
      "train step 01557 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1161 diff={max=05.4872, min=00.0183, mean=00.6562} policy_loss=-5.5584 policy updated! \n",
      "train step 01558 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4632 diff={max=04.6709, min=00.0059, mean=01.0612} policy_loss=-5.2454 policy updated! \n",
      "train step 01559 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8740 diff={max=05.7283, min=00.0311, mean=00.9955} policy_loss=-4.5303 policy updated! \n",
      "train step 01560 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.6669 diff={max=05.3163, min=00.0056, mean=00.9334} policy_loss=-4.7210 policy updated! \n",
      "train step 01561 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.3193 diff={max=04.4431, min=00.0385, mean=01.0616} policy_loss=-5.2208 policy updated! \n",
      "train step 01562 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9213 diff={max=03.2009, min=00.0078, mean=00.7020} policy_loss=-5.6095 policy updated! \n",
      "train step 01563 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.1033 diff={max=05.1706, min=00.0808, mean=01.0284} policy_loss=-5.9824 policy updated! \n",
      "train step 01564 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6822 diff={max=04.7203, min=00.0219, mean=00.9521} policy_loss=-6.4551 policy updated! \n",
      "train step 01565 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.2096 diff={max=07.9614, min=00.0140, mean=01.0469} policy_loss=-5.9324 policy updated! \n",
      "train step 01566 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1447 diff={max=04.5333, min=00.0187, mean=00.6558} policy_loss=-4.6648 policy updated! \n",
      "train step 01567 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5181 diff={max=03.8459, min=00.0186, mean=00.8635} policy_loss=-4.5431 policy updated! \n",
      "train step 01568 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.9577 diff={max=06.0836, min=00.0296, mean=01.2679} policy_loss=-5.7721 policy updated! \n",
      "train step 01569 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.1841 diff={max=04.6800, min=00.0199, mean=00.9854} policy_loss=-5.3199 policy updated! \n",
      "train step 01570 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.5204 diff={max=04.6846, min=00.0038, mean=00.7447} policy_loss=-6.2201 policy updated! \n",
      "train step 01571 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1424 diff={max=03.7368, min=00.0080, mean=00.8531} policy_loss=-6.1564 policy updated! \n",
      "train step 01572 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5766 diff={max=03.8057, min=00.0014, mean=00.9467} policy_loss=-6.1963 policy updated! \n",
      "train step 01573 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9445 diff={max=05.9490, min=00.0292, mean=01.2408} policy_loss=-5.2735 policy updated! \n",
      "train step 01574 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.0770 diff={max=03.4601, min=00.0098, mean=00.7373} policy_loss=-5.2762 policy updated! \n",
      "train step 01575 reward={max=07.0000, min=00.0000, mean=02.6000} optimizing loss=02.1639 diff={max=04.9527, min=00.0124, mean=00.9110} policy_loss=-4.5433 policy updated! \n",
      "train step 01576 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=02.0594 diff={max=04.8021, min=00.1426, mean=01.0220} policy_loss=-4.6666 policy updated! \n",
      "train step 01577 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7331 diff={max=05.3580, min=00.0115, mean=00.9046} policy_loss=-4.6295 policy updated! \n",
      "train step 01578 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1162 diff={max=03.3067, min=00.0392, mean=00.8266} policy_loss=-4.8117 policy updated! \n",
      "train step 01579 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6795 diff={max=04.5284, min=00.0264, mean=00.9364} policy_loss=-5.3145 policy updated! \n",
      "train step 01580 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.8943 diff={max=04.7895, min=00.0275, mean=00.9322} policy_loss=-6.0430 policy updated! \n",
      "train step 01581 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=02.4297 diff={max=03.9452, min=00.0598, mean=01.1887} policy_loss=-5.1804 policy updated! \n",
      "train step 01582 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2719 diff={max=05.7514, min=00.0058, mean=01.0532} policy_loss=-6.1647 policy updated! \n",
      "train step 01583 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5869 diff={max=03.6530, min=00.0090, mean=00.9212} policy_loss=-5.0424 policy updated! \n",
      "train step 01584 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0480 diff={max=05.0531, min=00.0706, mean=00.9448} policy_loss=-5.5010 policy updated! \n",
      "train step 01585 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=00.5537 diff={max=02.5835, min=00.0046, mean=00.5184} policy_loss=-5.2624 policy updated! \n",
      "train step 01586 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4587 diff={max=05.5230, min=00.0059, mean=01.0222} policy_loss=-4.9706 policy updated! \n",
      "train step 01587 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5690 diff={max=04.4087, min=00.0508, mean=00.9337} policy_loss=-5.1983 policy updated! \n",
      "train step 01588 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.8316 diff={max=04.8567, min=00.0680, mean=01.1324} policy_loss=-4.9924 policy updated! \n",
      "train step 01589 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2005 diff={max=03.2331, min=00.0102, mean=00.7738} policy_loss=-5.5248 policy updated! \n",
      "train step 01590 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.5232 diff={max=05.0298, min=00.0133, mean=00.7178} policy_loss=-5.8669 policy updated! \n",
      "train step 01591 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0379 diff={max=04.1583, min=00.0204, mean=00.6966} policy_loss=-5.4467 policy updated! \n",
      "train step 01592 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7733 diff={max=06.8148, min=00.0150, mean=00.8864} policy_loss=-6.3347 policy updated! \n",
      "train step 01593 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.0208 diff={max=05.3017, min=00.0209, mean=01.0652} policy_loss=-5.8532 policy updated! \n",
      "train step 01594 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0505 diff={max=04.2585, min=00.0317, mean=00.7093} policy_loss=-5.4353 policy updated! \n",
      "train step 01595 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.0770 diff={max=03.2598, min=00.0339, mean=00.7240} policy_loss=-5.5542 policy updated! \n",
      "train step 01596 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5892 diff={max=03.2451, min=00.0644, mean=00.9681} policy_loss=-4.7734 policy updated! \n",
      "train step 01597 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0623 diff={max=05.1684, min=00.0108, mean=00.9468} policy_loss=-5.5805 policy updated! \n",
      "train step 01598 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.6753 diff={max=02.5152, min=00.0760, mean=00.6459} policy_loss=-5.6356 policy updated! \n",
      "train step 01599 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5616 diff={max=04.7210, min=00.0029, mean=00.7952} policy_loss=-5.1466 policy updated! \n",
      "train step 01600 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.9453 diff={max=06.0613, min=00.0178, mean=00.8810} policy_loss=-6.3423 policy updated! \n",
      "train step 01601 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.1462 diff={max=04.9310, min=00.0039, mean=01.0433} policy_loss=-6.0852 policy updated! \n",
      "train step 01602 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6219 diff={max=03.6901, min=00.0032, mean=00.9475} policy_loss=-5.6021 policy updated! \n",
      "train step 01603 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3399 diff={max=03.1073, min=00.0004, mean=00.8919} policy_loss=-5.7323 policy updated! \n",
      "train step 01604 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.3983 diff={max=08.2632, min=00.0064, mean=00.8429} policy_loss=-5.2032 policy updated! \n",
      "train step 01605 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.3638 diff={max=03.0590, min=00.0184, mean=00.8878} policy_loss=-5.1435 policy updated! \n",
      "train step 01606 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7262 diff={max=05.5756, min=00.0498, mean=01.1854} policy_loss=-5.4589 policy updated! \n",
      "train step 01607 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9570 diff={max=03.0773, min=00.0151, mean=00.7333} policy_loss=-5.6762 policy updated! \n",
      "train step 01608 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4303 diff={max=05.4536, min=00.0024, mean=01.0596} policy_loss=-5.5869 policy updated! \n",
      "train step 01609 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0440 diff={max=03.4735, min=00.0087, mean=00.7613} policy_loss=-5.7718 policy updated! \n",
      "train step 01610 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.2429 diff={max=03.7686, min=00.0122, mean=00.7646} policy_loss=-6.5339 policy updated! \n",
      "train step 01611 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8304 diff={max=03.5569, min=00.0444, mean=00.9997} policy_loss=-5.7850 policy updated! \n",
      "train step 01612 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3045 diff={max=05.0517, min=00.0044, mean=01.1628} policy_loss=-6.3897 policy updated! \n",
      "train step 01613 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.7753 diff={max=04.9418, min=00.0224, mean=00.9068} policy_loss=-5.0028 policy updated! \n",
      "train step 01614 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9312 diff={max=03.2399, min=00.0138, mean=00.7050} policy_loss=-5.5036 policy updated! \n",
      "train step 01615 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.8069 diff={max=04.0591, min=00.0871, mean=01.0759} policy_loss=-4.7949 policy updated! \n",
      "train step 01616 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.6909 diff={max=04.3339, min=00.0123, mean=00.9579} policy_loss=-5.3135 policy updated! \n",
      "train step 01617 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6241 diff={max=04.6093, min=00.0029, mean=01.2082} policy_loss=-4.9013 policy updated! \n",
      "train step 01618 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4285 diff={max=03.3404, min=00.0191, mean=00.9311} policy_loss=-5.3232 policy updated! \n",
      "train step 01619 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8606 diff={max=07.2558, min=00.0012, mean=00.9023} policy_loss=-5.4414 policy updated! \n",
      "train step 01620 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.1074 diff={max=03.1436, min=00.0188, mean=00.7771} policy_loss=-6.1917 policy updated! \n",
      "train step 01621 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5462 diff={max=03.7800, min=00.0219, mean=00.8817} policy_loss=-6.2583 policy updated! \n",
      "train step 01622 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5875 diff={max=03.3564, min=00.0420, mean=00.9960} policy_loss=-5.5833 policy updated! \n",
      "train step 01623 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6501 diff={max=03.1024, min=00.0016, mean=00.9642} policy_loss=-5.4707 policy updated! \n",
      "train step 01624 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.9154 diff={max=04.2775, min=00.0415, mean=00.9643} policy_loss=-5.4625 policy updated! \n",
      "train step 01625 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.0015 diff={max=02.7626, min=00.0013, mean=00.7085} policy_loss=-5.5939 policy updated! \n",
      "train step 01626 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=00.7550 diff={max=02.3706, min=00.0072, mean=00.6419} policy_loss=-5.7107 policy updated! \n",
      "train step 01627 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4199 diff={max=04.6689, min=00.0042, mean=00.8180} policy_loss=-4.9646 policy updated! \n",
      "train step 01628 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.0509 diff={max=05.3443, min=00.0003, mean=00.9937} policy_loss=-5.5454 policy updated! \n",
      "train step 01629 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6328 diff={max=03.6970, min=00.0011, mean=00.9803} policy_loss=-5.4527 policy updated! \n",
      "train step 01630 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=00.6917 diff={max=03.0570, min=00.0038, mean=00.6025} policy_loss=-4.7628 policy updated! \n",
      "train step 01631 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4046 diff={max=06.7057, min=00.0007, mean=00.9142} policy_loss=-5.2242 policy updated! \n",
      "train step 01632 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6590 diff={max=06.0162, min=00.0061, mean=00.7714} policy_loss=-5.4797 policy updated! \n",
      "train step 01633 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0698 diff={max=03.3094, min=00.0637, mean=00.7005} policy_loss=-5.6087 policy updated! \n",
      "train step 01634 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.2665 diff={max=03.7884, min=00.0482, mean=00.7837} policy_loss=-5.1355 policy updated! \n",
      "train step 01635 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.6331 diff={max=04.8262, min=00.0267, mean=00.9080} policy_loss=-5.5406 policy updated! \n",
      "train step 01636 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0398 diff={max=04.5867, min=00.0008, mean=00.6737} policy_loss=-5.8602 policy updated! \n",
      "train step 01637 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0557 diff={max=05.3358, min=00.0179, mean=00.9096} policy_loss=-5.8028 policy updated! \n",
      "train step 01638 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6662 diff={max=04.2813, min=00.0208, mean=00.9338} policy_loss=-5.9194 policy updated! \n",
      "train step 01639 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8549 diff={max=04.3889, min=00.0136, mean=00.9722} policy_loss=-5.1778 policy updated! \n",
      "train step 01640 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.5843 diff={max=06.3646, min=00.0150, mean=00.7428} policy_loss=-5.1611 policy updated! \n",
      "train step 01641 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4236 diff={max=07.1683, min=00.0104, mean=00.9851} policy_loss=-5.6930 policy updated! \n",
      "train step 01642 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8905 diff={max=04.1307, min=00.0014, mean=00.9327} policy_loss=-5.0209 policy updated! \n",
      "train step 01643 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9013 diff={max=04.8948, min=00.0092, mean=00.9931} policy_loss=-4.5273 policy updated! \n",
      "train step 01644 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5984 diff={max=04.2027, min=00.0076, mean=00.9019} policy_loss=-5.4035 policy updated! \n",
      "train step 01645 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.1494 diff={max=07.1752, min=00.0195, mean=00.7890} policy_loss=-6.0964 policy updated! \n",
      "train step 01646 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8032 diff={max=04.6183, min=00.0268, mean=00.8432} policy_loss=-6.4021 policy updated! \n",
      "train step 01647 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7203 diff={max=02.5557, min=00.0019, mean=00.6315} policy_loss=-5.9049 policy updated! \n",
      "train step 01648 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.0709 diff={max=05.2195, min=00.0534, mean=01.0094} policy_loss=-5.6870 policy updated! \n",
      "train step 01649 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1982 diff={max=05.7027, min=00.0405, mean=01.0175} policy_loss=-5.4946 policy updated! \n",
      "train step 01650 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.6745 diff={max=03.8117, min=00.0073, mean=00.8715} policy_loss=-5.2937 policy updated! \n",
      "train step 01651 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8627 diff={max=04.4043, min=00.0383, mean=00.6189} policy_loss=-5.2688 policy updated! \n",
      "train step 01652 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5744 diff={max=05.6688, min=00.0136, mean=00.8142} policy_loss=-6.2995 policy updated! \n",
      "train step 01653 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8704 diff={max=03.0783, min=00.0029, mean=00.6608} policy_loss=-5.2347 policy updated! \n",
      "train step 01654 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.2001 diff={max=04.1843, min=00.0085, mean=00.7091} policy_loss=-5.4699 policy updated! \n",
      "train step 01655 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.4344 diff={max=04.4083, min=00.0406, mean=00.8197} policy_loss=-5.3482 policy updated! \n",
      "train step 01656 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0952 diff={max=06.0245, min=00.0204, mean=01.1940} policy_loss=-4.9647 policy updated! \n",
      "train step 01657 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8861 diff={max=04.1587, min=00.0289, mean=00.6615} policy_loss=-5.3162 policy updated! \n",
      "train step 01658 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6191 diff={max=05.1805, min=00.0419, mean=00.8311} policy_loss=-5.7336 policy updated! \n",
      "train step 01659 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1369 diff={max=03.2533, min=00.0001, mean=00.7442} policy_loss=-5.6345 policy updated! \n",
      "train step 01660 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.7297 diff={max=07.9067, min=00.0073, mean=00.9716} policy_loss=-5.7969 policy updated! \n",
      "train step 01661 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4890 diff={max=05.1970, min=00.0236, mean=00.8106} policy_loss=-5.4746 policy updated! \n",
      "train step 01662 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7570 diff={max=02.4342, min=00.0372, mean=00.6606} policy_loss=-6.2925 policy updated! \n",
      "train step 01663 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.4888 diff={max=03.5466, min=00.0169, mean=00.8829} policy_loss=-5.7825 policy updated! \n",
      "train step 01664 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0301 diff={max=08.2381, min=00.0353, mean=01.0535} policy_loss=-5.4239 policy updated! \n",
      "train step 01665 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.8305 diff={max=04.9020, min=00.0214, mean=00.8444} policy_loss=-4.9157 policy updated! \n",
      "train step 01666 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4588 diff={max=04.1837, min=00.0191, mean=00.8241} policy_loss=-4.6940 policy updated! \n",
      "train step 01667 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5127 diff={max=02.7948, min=00.0157, mean=00.9830} policy_loss=-5.6340 policy updated! \n",
      "train step 01668 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8558 diff={max=08.0978, min=00.0114, mean=01.0092} policy_loss=-5.0029 policy updated! \n",
      "train step 01669 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.4537 diff={max=04.0315, min=00.0050, mean=00.8326} policy_loss=-5.7903 policy updated! \n",
      "train step 01670 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=00.8273 diff={max=04.6635, min=00.0056, mean=00.5771} policy_loss=-5.2963 policy updated! \n",
      "train step 01671 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2450 diff={max=04.0744, min=00.0153, mean=00.7728} policy_loss=-6.5513 policy updated! \n",
      "train step 01672 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7114 diff={max=07.0170, min=00.0351, mean=00.9960} policy_loss=-6.0451 policy updated! \n",
      "train step 01673 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4061 diff={max=04.6624, min=00.0230, mean=00.8364} policy_loss=-5.0998 policy updated! \n",
      "train step 01674 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7572 diff={max=03.0557, min=00.0077, mean=00.6122} policy_loss=-6.0934 policy updated! \n",
      "train step 01675 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.5725 diff={max=04.7841, min=00.0223, mean=01.0453} policy_loss=-5.9337 policy updated! \n",
      "train step 01676 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5888 diff={max=03.5368, min=00.0110, mean=00.8289} policy_loss=-5.3283 policy updated! \n",
      "train step 01677 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4147 diff={max=07.0191, min=00.0652, mean=00.9513} policy_loss=-5.6080 policy updated! \n",
      "train step 01678 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.3191 diff={max=03.7282, min=00.0129, mean=00.7656} policy_loss=-5.3871 policy updated! \n",
      "train step 01679 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.3165 diff={max=04.0302, min=00.0962, mean=00.8263} policy_loss=-5.3894 policy updated! \n",
      "train step 01680 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.8385 diff={max=04.5926, min=00.0255, mean=00.8766} policy_loss=-4.8986 policy updated! \n",
      "train step 01681 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0439 diff={max=03.2232, min=00.0185, mean=00.7663} policy_loss=-6.1475 policy updated! \n",
      "train step 01682 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3161 diff={max=03.5839, min=00.0094, mean=00.7663} policy_loss=-5.6678 policy updated! \n",
      "train step 01683 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.0203 diff={max=03.8917, min=00.0057, mean=00.6940} policy_loss=-6.4336 policy updated! \n",
      "train step 01684 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.3643 diff={max=04.6755, min=00.0200, mean=00.8791} policy_loss=-6.1154 policy updated! \n",
      "train step 01685 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.4609 diff={max=04.4078, min=00.0242, mean=00.8481} policy_loss=-5.8709 policy updated! \n",
      "train step 01686 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2120 diff={max=02.9288, min=00.0033, mean=00.8128} policy_loss=-5.8219 policy updated! \n",
      "train step 01687 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8736 diff={max=02.0802, min=00.0900, mean=00.7728} policy_loss=-5.6856 policy updated! \n",
      "train step 01688 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=01.2797 diff={max=04.2803, min=00.0208, mean=00.7046} policy_loss=-5.2337 policy updated! \n",
      "train step 01689 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.2295 diff={max=03.2271, min=00.0014, mean=00.8434} policy_loss=-5.3009 policy updated! \n",
      "train step 01690 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.1206 diff={max=06.1432, min=00.0876, mean=01.3151} policy_loss=-5.1747 policy updated! \n",
      "train step 01691 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.6533 diff={max=05.5330, min=00.0051, mean=00.8797} policy_loss=-5.4805 policy updated! \n",
      "train step 01692 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5715 diff={max=04.6275, min=00.0006, mean=00.8765} policy_loss=-5.3897 policy updated! \n",
      "train step 01693 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.2374 diff={max=05.0895, min=00.0284, mean=00.9559} policy_loss=-5.9269 policy updated! \n",
      "train step 01694 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4827 diff={max=04.4916, min=00.0082, mean=00.8929} policy_loss=-5.6604 policy updated! \n",
      "train step 01695 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.4223 diff={max=04.2592, min=00.0266, mean=00.7940} policy_loss=-6.3341 policy updated! \n",
      "train step 01696 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0698 diff={max=02.9737, min=00.0127, mean=00.8117} policy_loss=-6.3214 policy updated! \n",
      "train step 01697 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9651 diff={max=03.0727, min=00.0281, mean=00.7512} policy_loss=-5.6420 policy updated! \n",
      "train step 01698 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1105 diff={max=06.8860, min=00.0378, mean=00.8374} policy_loss=-5.2178 policy updated! \n",
      "train step 01699 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7080 diff={max=05.3448, min=00.0073, mean=00.8602} policy_loss=-6.0483 policy updated! \n",
      "train step 01700 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.2332 diff={max=05.6614, min=00.0804, mean=01.0306} policy_loss=-4.9582 policy updated! \n",
      "train step 01701 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8235 diff={max=06.7170, min=00.0530, mean=01.3688} policy_loss=-4.9088 policy updated! \n",
      "train step 01702 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0607 diff={max=05.8361, min=00.0147, mean=01.0261} policy_loss=-4.9327 policy updated! \n",
      "train step 01703 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.8423 diff={max=04.8681, min=00.0373, mean=01.0149} policy_loss=-5.3118 policy updated! \n",
      "train step 01704 reward={max=07.0000, min=00.0000, mean=05.2000} optimizing loss=01.1849 diff={max=04.5614, min=00.0116, mean=00.7426} policy_loss=-5.8012 policy updated! \n",
      "train step 01705 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.3402 diff={max=03.0163, min=00.0232, mean=00.8713} policy_loss=-6.1066 policy updated! \n",
      "train step 01706 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.1071 diff={max=06.2243, min=00.0004, mean=00.9307} policy_loss=-6.8461 policy updated! \n",
      "train step 01707 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3107 diff={max=04.0043, min=00.0067, mean=00.7662} policy_loss=-5.7497 policy updated! \n",
      "train step 01708 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.1830 diff={max=04.8115, min=00.0035, mean=00.6997} policy_loss=-5.3996 policy updated! \n",
      "train step 01709 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5079 diff={max=07.4901, min=00.0343, mean=00.9677} policy_loss=-5.3866 policy updated! \n",
      "train step 01710 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.7446 diff={max=05.0888, min=00.0824, mean=00.8908} policy_loss=-4.8534 policy updated! \n",
      "train step 01711 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1998 diff={max=03.3538, min=00.0625, mean=00.8646} policy_loss=-4.7348 policy updated! \n",
      "train step 01712 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8032 diff={max=05.1201, min=00.0002, mean=00.8767} policy_loss=-5.0185 policy updated! \n",
      "train step 01713 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.0276 diff={max=03.5054, min=00.0029, mean=00.7274} policy_loss=-5.5144 policy updated! \n",
      "train step 01714 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5005 diff={max=05.6015, min=00.0023, mean=01.0051} policy_loss=-6.5016 policy updated! \n",
      "train step 01715 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=00.8420 diff={max=03.8285, min=00.0067, mean=00.6028} policy_loss=-5.4041 policy updated! \n",
      "train step 01716 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.6611 diff={max=02.7224, min=00.0113, mean=00.5710} policy_loss=-6.3557 policy updated! \n",
      "train step 01717 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5627 diff={max=05.3676, min=00.0475, mean=01.0573} policy_loss=-6.3953 policy updated! \n",
      "train step 01718 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.8475 diff={max=05.4687, min=00.0023, mean=00.8755} policy_loss=-5.5506 policy updated! \n",
      "train step 01719 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.0492 diff={max=03.3789, min=00.0023, mean=00.7296} policy_loss=-5.3195 policy updated! \n",
      "train step 01720 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.1971 diff={max=04.1505, min=00.0549, mean=00.8095} policy_loss=-5.1506 policy updated! \n",
      "train step 01721 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0500 diff={max=03.9809, min=00.1127, mean=00.7922} policy_loss=-4.9700 policy updated! \n",
      "train step 01722 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3775 diff={max=04.0154, min=00.1037, mean=00.8941} policy_loss=-4.7670 policy updated! \n",
      "train step 01723 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0000 diff={max=05.8034, min=00.0285, mean=00.9906} policy_loss=-5.6185 policy updated! \n",
      "train step 01724 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0510 diff={max=04.0323, min=00.0071, mean=00.6123} policy_loss=-5.2890 policy updated! \n",
      "train step 01725 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.3551 diff={max=05.4403, min=00.0291, mean=01.0019} policy_loss=-6.1627 policy updated! \n",
      "train step 01726 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9976 diff={max=04.0325, min=00.0034, mean=00.6855} policy_loss=-6.0467 policy updated! \n",
      "train step 01727 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9930 diff={max=10.2036, min=00.0071, mean=01.1076} policy_loss=-6.2042 policy updated! \n",
      "train step 01728 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2261 diff={max=03.3625, min=00.0085, mean=00.7526} policy_loss=-5.5603 policy updated! \n",
      "train step 01729 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.4695 diff={max=05.4032, min=00.0159, mean=00.7633} policy_loss=-5.8781 policy updated! \n",
      "train step 01730 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.8210 diff={max=04.2606, min=00.0004, mean=00.7898} policy_loss=-5.0758 policy updated! \n",
      "train step 01731 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.5798 diff={max=04.3634, min=00.0020, mean=00.8135} policy_loss=-5.7235 policy updated! \n",
      "train step 01732 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2335 diff={max=04.0377, min=00.0034, mean=00.7812} policy_loss=-5.7690 policy updated! \n",
      "train step 01733 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.4255 diff={max=06.8022, min=00.0349, mean=00.9821} policy_loss=-5.7660 policy updated! \n",
      "train step 01734 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2224 diff={max=03.3769, min=00.0145, mean=00.8188} policy_loss=-5.8311 policy updated! \n",
      "train step 01735 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.2697 diff={max=04.7211, min=00.0157, mean=00.6654} policy_loss=-5.3222 policy updated! \n",
      "train step 01736 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6024 diff={max=04.4724, min=00.0030, mean=00.9023} policy_loss=-6.7387 policy updated! \n",
      "train step 01737 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9946 diff={max=03.0263, min=00.0020, mean=00.7232} policy_loss=-5.2338 policy updated! \n",
      "train step 01738 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6731 diff={max=05.4408, min=00.0112, mean=00.8217} policy_loss=-5.4355 policy updated! \n",
      "train step 01739 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.0374 diff={max=04.6918, min=00.0111, mean=00.6739} policy_loss=-5.2954 policy updated! \n",
      "train step 01740 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.1369 diff={max=04.2907, min=00.0030, mean=00.7473} policy_loss=-5.5621 policy updated! \n",
      "train step 01741 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.3042 diff={max=04.2392, min=00.0181, mean=01.0819} policy_loss=-5.2442 policy updated! \n",
      "train step 01742 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8086 diff={max=02.8750, min=00.0034, mean=00.6556} policy_loss=-6.3678 policy updated! \n",
      "train step 01743 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4742 diff={max=04.1179, min=00.0471, mean=00.8077} policy_loss=-5.5514 policy updated! \n",
      "train step 01744 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5348 diff={max=05.0319, min=00.0047, mean=00.7437} policy_loss=-6.1875 policy updated! \n",
      "train step 01745 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=00.9868 diff={max=04.0414, min=00.0378, mean=00.7164} policy_loss=-6.1482 policy updated! \n",
      "train step 01746 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8070 diff={max=02.2885, min=00.0654, mean=00.6924} policy_loss=-6.0964 policy updated! \n",
      "train step 01747 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8187 diff={max=02.5383, min=00.0185, mean=00.6939} policy_loss=-6.0072 policy updated! \n",
      "train step 01748 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.9025 diff={max=05.3564, min=00.0359, mean=00.8954} policy_loss=-6.0916 policy updated! \n",
      "train step 01749 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6733 diff={max=06.2454, min=00.0002, mean=00.7402} policy_loss=-6.0632 policy updated! \n",
      "train step 01750 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=00.9508 diff={max=03.7195, min=00.0080, mean=00.6406} policy_loss=-4.9654 policy updated! \n",
      "train step 01751 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9913 diff={max=06.2332, min=00.0181, mean=00.8536} policy_loss=-5.3898 policy updated! \n",
      "train step 01752 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9350 diff={max=04.6081, min=00.0355, mean=00.6336} policy_loss=-5.4973 policy updated! \n",
      "train step 01753 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.4417 diff={max=03.1835, min=00.0148, mean=00.8511} policy_loss=-5.4551 policy updated! \n",
      "train step 01754 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8223 diff={max=07.3158, min=00.0213, mean=00.7979} policy_loss=-5.6305 policy updated! \n",
      "train step 01755 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.5522 diff={max=03.5350, min=00.0112, mean=00.8967} policy_loss=-5.4314 policy updated! \n",
      "train step 01756 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3043 diff={max=03.1913, min=00.0092, mean=00.8116} policy_loss=-5.1471 policy updated! \n",
      "train step 01757 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=04.0912 diff={max=05.9489, min=00.0315, mean=01.3274} policy_loss=-5.7157 policy updated! \n",
      "train step 01758 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5280 diff={max=03.0119, min=00.0812, mean=01.0144} policy_loss=-5.5067 policy updated! \n",
      "train step 01759 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.2936 diff={max=03.9040, min=00.0084, mean=00.7725} policy_loss=-5.9477 policy updated! \n",
      "train step 01760 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.2962 diff={max=04.8234, min=00.0046, mean=01.1484} policy_loss=-6.2743 policy updated! \n",
      "train step 01761 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0766 diff={max=04.0616, min=00.0135, mean=01.0103} policy_loss=-5.0815 policy updated! \n",
      "train step 01762 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8652 diff={max=04.5599, min=00.0012, mean=01.1031} policy_loss=-5.3626 policy updated! \n",
      "train step 01763 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.2736 diff={max=04.0605, min=00.0077, mean=00.7439} policy_loss=-6.1972 policy updated! \n",
      "train step 01764 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.7542 diff={max=05.8812, min=00.0254, mean=00.8412} policy_loss=-5.6274 policy updated! \n",
      "train step 01765 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.0696 diff={max=04.5843, min=00.0034, mean=00.6711} policy_loss=-5.4730 policy updated! \n",
      "train step 01766 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9001 diff={max=04.7310, min=00.0093, mean=00.8982} policy_loss=-5.1182 policy updated! \n",
      "train step 01767 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3421 diff={max=05.1666, min=00.0028, mean=01.0226} policy_loss=-6.0070 policy updated! \n",
      "train step 01768 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=00.9314 diff={max=04.3010, min=00.0034, mean=00.6937} policy_loss=-6.1182 policy updated! \n",
      "train step 01769 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9986 diff={max=04.3125, min=00.0377, mean=01.0137} policy_loss=-5.6623 policy updated! \n",
      "train step 01770 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.2303 diff={max=03.8129, min=00.0045, mean=00.7143} policy_loss=-5.5963 policy updated! \n",
      "train step 01771 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7620 diff={max=04.4206, min=00.0470, mean=00.9245} policy_loss=-5.2788 policy updated! \n",
      "train step 01772 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1806 diff={max=04.3578, min=00.0098, mean=00.7427} policy_loss=-5.8068 policy updated! \n",
      "train step 01773 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2673 diff={max=04.1529, min=00.0005, mean=00.7110} policy_loss=-5.5630 policy updated! \n",
      "train step 01774 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7878 diff={max=04.1559, min=00.0054, mean=00.8280} policy_loss=-5.9085 policy updated! \n",
      "train step 01775 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.0863 diff={max=04.1069, min=00.0152, mean=00.7207} policy_loss=-6.0837 policy updated! \n",
      "train step 01776 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1324 diff={max=03.6951, min=00.0094, mean=00.7571} policy_loss=-5.6251 policy updated! \n",
      "train step 01777 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5404 diff={max=04.4838, min=00.0014, mean=00.8410} policy_loss=-5.8955 policy updated! \n",
      "train step 01778 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3769 diff={max=05.7104, min=00.0084, mean=00.9791} policy_loss=-5.0246 policy updated! \n",
      "train step 01779 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2057 diff={max=04.8683, min=00.0062, mean=00.6538} policy_loss=-4.6394 policy updated! \n",
      "train step 01780 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.2421 diff={max=03.1415, min=00.0010, mean=00.7689} policy_loss=-5.7541 policy updated! \n",
      "train step 01781 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2466 diff={max=06.4880, min=00.0541, mean=00.8659} policy_loss=-6.4857 policy updated! \n",
      "train step 01782 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=01.7585 diff={max=04.1590, min=00.0080, mean=00.9675} policy_loss=-4.9801 policy updated! \n",
      "train step 01783 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8225 diff={max=04.2943, min=00.0206, mean=00.9010} policy_loss=-6.5469 policy updated! \n",
      "train step 01784 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5219 diff={max=05.5665, min=00.0101, mean=00.7932} policy_loss=-5.1712 policy updated! \n",
      "train step 01785 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.5604 diff={max=05.8413, min=00.0278, mean=01.0753} policy_loss=-4.9612 policy updated! \n",
      "train step 01786 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.6124 diff={max=05.1605, min=00.0277, mean=00.8007} policy_loss=-5.3898 policy updated! \n",
      "train step 01787 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3592 diff={max=06.1411, min=00.0154, mean=00.9351} policy_loss=-5.7155 policy updated! \n",
      "train step 01788 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.1118 diff={max=03.4559, min=00.0070, mean=00.6823} policy_loss=-5.6853 policy updated! \n",
      "train step 01789 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8421 diff={max=03.1190, min=00.0537, mean=00.6924} policy_loss=-5.4645 policy updated! \n",
      "train step 01790 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.4193 diff={max=04.5026, min=00.0059, mean=00.7996} policy_loss=-5.5833 policy updated! \n",
      "train step 01791 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4936 diff={max=06.7689, min=00.0311, mean=00.9217} policy_loss=-5.6311 policy updated! \n",
      "train step 01792 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7578 diff={max=04.4650, min=00.0029, mean=00.9207} policy_loss=-5.2482 policy updated! \n",
      "train step 01793 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9229 diff={max=02.4900, min=00.0032, mean=00.7512} policy_loss=-6.0564 policy updated! \n",
      "train step 01794 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1915 diff={max=04.7057, min=00.0146, mean=00.6936} policy_loss=-6.2246 policy updated! \n",
      "train step 01795 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.4883 diff={max=07.0361, min=00.0259, mean=00.9205} policy_loss=-6.1508 policy updated! \n",
      "train step 01796 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2029 diff={max=05.8174, min=00.0061, mean=01.2119} policy_loss=-5.3122 policy updated! \n",
      "train step 01797 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7816 diff={max=04.3778, min=00.0380, mean=00.8962} policy_loss=-5.5885 policy updated! \n",
      "train step 01798 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8641 diff={max=06.6418, min=00.0230, mean=00.8496} policy_loss=-4.6664 policy updated! \n",
      "train step 01799 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.4588 diff={max=04.4045, min=00.0503, mean=00.8868} policy_loss=-6.0068 policy updated! \n",
      "train step 01800 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.0473 diff={max=05.9173, min=00.0004, mean=00.8741} policy_loss=-5.5502 policy updated! \n",
      "train step 01801 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3779 diff={max=07.8057, min=00.0393, mean=01.0774} policy_loss=-5.4655 policy updated! \n",
      "train step 01802 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6794 diff={max=04.1344, min=00.0098, mean=00.9594} policy_loss=-5.6453 policy updated! \n",
      "train step 01803 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1040 diff={max=04.4644, min=00.0135, mean=01.0898} policy_loss=-6.1839 policy updated! \n",
      "train step 01804 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7592 diff={max=03.6698, min=00.0847, mean=01.0009} policy_loss=-5.6464 policy updated! \n",
      "train step 01805 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.2882 diff={max=03.0976, min=00.0616, mean=00.8479} policy_loss=-5.7143 policy updated! \n",
      "train step 01806 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.4827 diff={max=04.2908, min=00.0330, mean=01.0695} policy_loss=-4.9847 policy updated! \n",
      "train step 01807 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1976 diff={max=04.4590, min=00.0147, mean=01.0324} policy_loss=-5.6397 policy updated! \n",
      "train step 01808 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.5761 diff={max=04.5583, min=00.0058, mean=01.1097} policy_loss=-5.0017 policy updated! \n",
      "train step 01809 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5344 diff={max=05.6464, min=00.0696, mean=01.0811} policy_loss=-5.7068 policy updated! \n",
      "train step 01810 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.4404 diff={max=07.4712, min=00.0078, mean=00.9988} policy_loss=-5.9497 policy updated! \n",
      "train step 01811 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.3172 diff={max=08.7041, min=00.0046, mean=00.7941} policy_loss=-6.0217 policy updated! \n",
      "train step 01812 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3157 diff={max=03.9293, min=00.0150, mean=00.7974} policy_loss=-5.4121 policy updated! \n",
      "train step 01813 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.4466 diff={max=04.0600, min=00.0037, mean=00.8222} policy_loss=-5.1739 policy updated! \n",
      "train step 01814 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6921 diff={max=04.1734, min=00.0092, mean=00.8875} policy_loss=-5.6380 policy updated! \n",
      "train step 01815 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.6535 diff={max=03.8342, min=00.0157, mean=00.8698} policy_loss=-5.7589 policy updated! \n",
      "train step 01816 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.7251 diff={max=05.2649, min=00.0081, mean=00.8310} policy_loss=-5.3639 policy updated! \n",
      "train step 01817 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8026 diff={max=03.7543, min=00.0144, mean=00.6474} policy_loss=-6.0581 policy updated! \n",
      "train step 01818 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.6867 diff={max=02.2754, min=00.0040, mean=00.6654} policy_loss=-5.2685 policy updated! \n",
      "train step 01819 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1466 diff={max=03.0499, min=00.0370, mean=00.7877} policy_loss=-5.9470 policy updated! \n",
      "train step 01820 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=00.9114 diff={max=04.3062, min=00.0038, mean=00.6331} policy_loss=-5.3844 policy updated! \n",
      "train step 01821 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5282 diff={max=05.4635, min=00.0092, mean=01.0137} policy_loss=-5.3186 policy updated! \n",
      "train step 01822 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0742 diff={max=03.1435, min=00.0020, mean=00.8018} policy_loss=-5.9188 policy updated! \n",
      "train step 01823 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8714 diff={max=09.9519, min=00.0017, mean=00.8561} policy_loss=-5.6295 policy updated! \n",
      "train step 01824 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.4523 diff={max=04.7232, min=00.0566, mean=00.8406} policy_loss=-5.9440 policy updated! \n",
      "train step 01825 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=00.9215 diff={max=03.3640, min=00.0061, mean=00.6372} policy_loss=-5.9594 policy updated! \n",
      "train step 01826 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.4162 diff={max=05.1149, min=00.0142, mean=00.8270} policy_loss=-5.6266 policy updated! \n",
      "train step 01827 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7635 diff={max=02.7426, min=00.0231, mean=00.6368} policy_loss=-5.9097 policy updated! \n",
      "train step 01828 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1575 diff={max=05.7330, min=00.0235, mean=00.8408} policy_loss=-4.5919 policy updated! \n",
      "train step 01829 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.7096 diff={max=07.0427, min=00.0092, mean=00.9599} policy_loss=-4.9772 policy updated! \n",
      "train step 01830 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.0883 diff={max=04.0989, min=00.0108, mean=00.7190} policy_loss=-5.2414 policy updated! \n",
      "train step 01831 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.1164 diff={max=03.5257, min=00.0050, mean=00.6850} policy_loss=-5.1595 policy updated! \n",
      "train step 01832 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.5834 diff={max=02.4746, min=00.0107, mean=00.5477} policy_loss=-5.2589 policy updated! \n",
      "train step 01833 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1728 diff={max=04.2935, min=00.0138, mean=00.7346} policy_loss=-6.1176 policy updated! \n",
      "train step 01834 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8801 diff={max=05.8300, min=00.0542, mean=00.8487} policy_loss=-5.8436 policy updated! \n",
      "train step 01835 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=00.6782 diff={max=02.5958, min=00.0233, mean=00.6154} policy_loss=-5.4819 policy updated! \n",
      "train step 01836 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3505 diff={max=04.2740, min=00.0254, mean=00.7667} policy_loss=-5.5480 policy updated! \n",
      "train step 01837 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0226 diff={max=02.3889, min=00.0124, mean=00.7771} policy_loss=-4.8179 policy updated! \n",
      "train step 01838 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9374 diff={max=05.1958, min=00.0119, mean=00.9217} policy_loss=-5.8540 policy updated! \n",
      "train step 01839 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.2408 diff={max=04.1302, min=00.0287, mean=01.0623} policy_loss=-6.0205 policy updated! \n",
      "train step 01840 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.0011 diff={max=03.9877, min=00.0200, mean=00.7164} policy_loss=-5.0078 policy updated! \n",
      "train step 01841 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.6085 diff={max=06.1798, min=00.0384, mean=01.0044} policy_loss=-5.7242 policy updated! \n",
      "train step 01842 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1099 diff={max=02.9861, min=00.0579, mean=00.7658} policy_loss=-4.8892 policy updated! \n",
      "train step 01843 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0313 diff={max=05.2788, min=00.0439, mean=00.8131} policy_loss=-5.9693 policy updated! \n",
      "train step 01844 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=02.8812 diff={max=05.1821, min=00.0914, mean=01.2232} policy_loss=-6.0132 policy updated! \n",
      "train step 01845 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=00.7222 diff={max=02.5463, min=00.0027, mean=00.6501} policy_loss=-4.8425 policy updated! \n",
      "train step 01846 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.6026 diff={max=05.3831, min=00.0325, mean=00.7613} policy_loss=-5.6866 policy updated! \n",
      "train step 01847 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6900 diff={max=04.1249, min=00.0020, mean=00.8564} policy_loss=-5.8241 policy updated! \n",
      "train step 01848 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.0172 diff={max=05.5500, min=00.0531, mean=00.9349} policy_loss=-5.5196 policy updated! \n",
      "train step 01849 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.7658 diff={max=05.1239, min=00.0011, mean=01.1695} policy_loss=-5.7358 policy updated! \n",
      "train step 01850 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.1316 diff={max=06.5171, min=00.0038, mean=00.8359} policy_loss=-5.9452 policy updated! \n",
      "train step 01851 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4714 diff={max=04.2014, min=00.0317, mean=00.8118} policy_loss=-6.4629 policy updated! \n",
      "train step 01852 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6925 diff={max=07.5873, min=00.0197, mean=01.2718} policy_loss=-6.9978 policy updated! \n",
      "train step 01853 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9812 diff={max=04.2293, min=00.0659, mean=01.0028} policy_loss=-6.0276 policy updated! \n",
      "train step 01854 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9032 diff={max=03.4319, min=00.0011, mean=01.0449} policy_loss=-5.3400 policy updated! \n",
      "train step 01855 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.3560 diff={max=03.4736, min=00.0830, mean=00.8988} policy_loss=-5.0453 policy updated! \n",
      "train step 01856 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6673 diff={max=04.7780, min=00.0278, mean=00.8990} policy_loss=-5.1703 policy updated! \n",
      "train step 01857 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6050 diff={max=03.9076, min=00.1682, mean=00.9836} policy_loss=-5.4042 policy updated! \n",
      "train step 01858 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8936 diff={max=02.3697, min=00.0255, mean=00.7214} policy_loss=-6.2113 policy updated! \n",
      "train step 01859 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2648 diff={max=06.5086, min=00.0118, mean=01.0196} policy_loss=-6.5422 policy updated! \n",
      "train step 01860 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.5296 diff={max=07.0004, min=00.0490, mean=01.0481} policy_loss=-6.2497 policy updated! \n",
      "train step 01861 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2491 diff={max=03.2125, min=00.0754, mean=00.9047} policy_loss=-6.3540 policy updated! \n",
      "train step 01862 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6162 diff={max=04.2409, min=00.0014, mean=00.9310} policy_loss=-6.3659 policy updated! \n",
      "train step 01863 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5207 diff={max=04.3230, min=00.0576, mean=00.8699} policy_loss=-5.1523 policy updated! \n",
      "train step 01864 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4514 diff={max=05.2744, min=00.0091, mean=00.7384} policy_loss=-5.4409 policy updated! \n",
      "train step 01865 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.0849 diff={max=03.6126, min=00.0271, mean=00.7459} policy_loss=-4.6210 policy updated! \n",
      "train step 01866 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4799 diff={max=04.2838, min=00.0049, mean=00.8886} policy_loss=-5.1988 policy updated! \n",
      "train step 01867 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0832 diff={max=05.5693, min=00.0054, mean=01.0796} policy_loss=-5.8877 policy updated! \n",
      "train step 01868 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1097 diff={max=04.5891, min=00.0153, mean=00.7308} policy_loss=-6.0115 policy updated! \n",
      "train step 01869 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2704 diff={max=05.7643, min=00.0023, mean=00.9195} policy_loss=-6.7525 policy updated! \n",
      "train step 01870 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.1513 diff={max=03.5108, min=00.0101, mean=00.7957} policy_loss=-6.8918 policy updated! \n",
      "train step 01871 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0863 diff={max=05.8341, min=00.0093, mean=01.0523} policy_loss=-6.2735 policy updated! \n",
      "train step 01872 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4356 diff={max=04.4095, min=00.0467, mean=00.7411} policy_loss=-5.0950 policy updated! \n",
      "train step 01873 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.0112 diff={max=03.9074, min=00.0051, mean=00.6766} policy_loss=-5.8706 policy updated! \n",
      "train step 01874 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3708 diff={max=02.7897, min=00.0063, mean=00.8720} policy_loss=-5.3696 policy updated! \n",
      "train step 01875 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.2430 diff={max=03.3821, min=00.0007, mean=00.7986} policy_loss=-5.9772 policy updated! \n",
      "train step 01876 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2346 diff={max=06.0775, min=00.0143, mean=01.2134} policy_loss=-5.5994 policy updated! \n",
      "train step 01877 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8826 diff={max=06.2139, min=00.0784, mean=01.0553} policy_loss=-6.2929 policy updated! \n",
      "train step 01878 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7609 diff={max=04.7790, min=00.0124, mean=00.9388} policy_loss=-6.5415 policy updated! \n",
      "train step 01879 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.0961 diff={max=03.6553, min=00.0071, mean=00.7523} policy_loss=-6.1094 policy updated! \n",
      "train step 01880 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.3419 diff={max=05.8000, min=00.0300, mean=01.1153} policy_loss=-6.2723 policy updated! \n",
      "train step 01881 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=00.9257 diff={max=02.5346, min=00.0047, mean=00.7469} policy_loss=-6.3399 policy updated! \n",
      "train step 01882 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7920 diff={max=04.4650, min=00.0234, mean=00.8760} policy_loss=-5.8654 policy updated! \n",
      "train step 01883 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9017 diff={max=04.2205, min=00.0145, mean=01.0547} policy_loss=-6.1526 policy updated! \n",
      "train step 01884 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3103 diff={max=03.9898, min=00.0038, mean=00.7088} policy_loss=-5.7367 policy updated! \n",
      "train step 01885 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.1494 diff={max=06.2404, min=00.0378, mean=00.9126} policy_loss=-5.6453 policy updated! \n",
      "train step 01886 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.6029 diff={max=04.8165, min=00.0107, mean=00.8186} policy_loss=-5.7665 policy updated! \n",
      "train step 01887 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2730 diff={max=03.4774, min=00.0019, mean=00.7058} policy_loss=-5.0614 policy updated! \n",
      "train step 01888 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6654 diff={max=06.4427, min=00.0084, mean=00.9772} policy_loss=-6.0845 policy updated! \n",
      "train step 01889 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5663 diff={max=04.5292, min=00.0013, mean=00.7679} policy_loss=-5.9865 policy updated! \n",
      "train step 01890 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=00.6627 diff={max=02.3743, min=00.0278, mean=00.6276} policy_loss=-5.5584 policy updated! \n",
      "train step 01891 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7533 diff={max=02.4347, min=00.0115, mean=00.6431} policy_loss=-5.7207 policy updated! \n",
      "train step 01892 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8777 diff={max=05.1886, min=00.0116, mean=00.8454} policy_loss=-6.3000 policy updated! \n",
      "train step 01893 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.6907 diff={max=04.9888, min=00.0081, mean=00.8358} policy_loss=-5.7218 policy updated! \n",
      "train step 01894 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6100 diff={max=03.4383, min=00.0219, mean=00.8881} policy_loss=-5.3135 policy updated! \n",
      "train step 01895 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.3225 diff={max=04.3757, min=00.0007, mean=01.1010} policy_loss=-6.2420 policy updated! \n",
      "train step 01896 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0120 diff={max=02.5498, min=00.0029, mean=00.7496} policy_loss=-6.2247 policy updated! \n",
      "train step 01897 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8865 diff={max=05.7578, min=00.0116, mean=01.1020} policy_loss=-5.1269 policy updated! \n",
      "train step 01898 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7581 diff={max=04.6331, min=00.0144, mean=00.9343} policy_loss=-6.2599 policy updated! \n",
      "train step 01899 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=00.9209 diff={max=03.2966, min=00.0151, mean=00.7098} policy_loss=-5.7447 policy updated! \n",
      "train step 01900 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.2696 diff={max=04.3432, min=00.0887, mean=00.8405} policy_loss=-5.6350 policy updated! \n",
      "train step 01901 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3854 diff={max=06.0600, min=00.0108, mean=00.9757} policy_loss=-5.4462 policy updated! \n",
      "train step 01902 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0397 diff={max=05.1709, min=00.0032, mean=00.8371} policy_loss=-5.7171 policy updated! \n",
      "train step 01903 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7801 diff={max=05.8642, min=00.0521, mean=01.0351} policy_loss=-5.4374 policy updated! \n",
      "train step 01904 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5208 diff={max=03.0048, min=00.0049, mean=00.8584} policy_loss=-6.7371 policy updated! \n",
      "train step 01905 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.7462 diff={max=04.6163, min=00.0453, mean=00.9046} policy_loss=-6.4274 policy updated! \n",
      "train step 01906 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1255 diff={max=07.1742, min=00.0248, mean=00.9003} policy_loss=-5.7212 policy updated! \n",
      "train step 01907 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1180 diff={max=03.7303, min=00.0207, mean=00.7815} policy_loss=-5.6289 policy updated! \n",
      "train step 01908 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7020 diff={max=04.3643, min=00.0073, mean=00.9076} policy_loss=-5.4008 policy updated! \n",
      "train step 01909 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.0390 diff={max=04.4263, min=00.0086, mean=00.8966} policy_loss=-5.4112 policy updated! \n",
      "train step 01910 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.3460 diff={max=02.5075, min=00.0027, mean=00.8760} policy_loss=-5.5888 policy updated! \n",
      "train step 01911 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=00.9664 diff={max=02.7365, min=00.0370, mean=00.7350} policy_loss=-6.1102 policy updated! \n",
      "train step 01912 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4505 diff={max=05.5047, min=00.0813, mean=01.0437} policy_loss=-5.5151 policy updated! \n",
      "train step 01913 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.7387 diff={max=04.8163, min=00.0012, mean=00.7471} policy_loss=-5.9981 policy updated! \n",
      "train step 01914 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5572 diff={max=04.7993, min=00.0842, mean=00.8156} policy_loss=-5.6156 policy updated! \n",
      "train step 01915 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.8207 diff={max=05.8359, min=00.0367, mean=00.9963} policy_loss=-5.0379 policy updated! \n",
      "train step 01916 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5383 diff={max=06.9045, min=00.0069, mean=01.3159} policy_loss=-5.2731 policy updated! \n",
      "train step 01917 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7222 diff={max=04.8886, min=00.0001, mean=00.8711} policy_loss=-5.3086 policy updated! \n",
      "train step 01918 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4100 diff={max=04.5605, min=00.0029, mean=01.0637} policy_loss=-5.7046 policy updated! \n",
      "train step 01919 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5056 diff={max=06.2000, min=00.0189, mean=01.0945} policy_loss=-6.4019 policy updated! \n",
      "train step 01920 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.4279 diff={max=05.5894, min=00.0308, mean=00.9811} policy_loss=-6.0536 policy updated! \n",
      "train step 01921 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9978 diff={max=04.0311, min=00.0704, mean=01.0259} policy_loss=-5.7873 policy updated! \n",
      "train step 01922 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2489 diff={max=03.3159, min=00.0357, mean=00.9022} policy_loss=-6.1741 policy updated! \n",
      "train step 01923 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.2886 diff={max=04.6178, min=00.0133, mean=00.7231} policy_loss=-4.9051 policy updated! \n",
      "train step 01924 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1749 diff={max=06.0244, min=00.0127, mean=00.9967} policy_loss=-5.7155 policy updated! \n",
      "train step 01925 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.8035 diff={max=06.1526, min=00.0278, mean=00.9106} policy_loss=-6.4397 policy updated! \n",
      "train step 01926 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4464 diff={max=06.9999, min=00.0052, mean=00.9637} policy_loss=-5.4044 policy updated! \n",
      "train step 01927 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8636 diff={max=04.7650, min=00.0376, mean=00.9102} policy_loss=-6.6508 policy updated! \n",
      "train step 01928 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=01.0850 diff={max=03.7968, min=00.0264, mean=00.7289} policy_loss=-6.1116 policy updated! \n",
      "train step 01929 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6605 diff={max=04.6596, min=00.0311, mean=00.8768} policy_loss=-6.1868 policy updated! \n",
      "train step 01930 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.8765 diff={max=05.1513, min=00.0443, mean=01.2327} policy_loss=-5.9369 policy updated! \n",
      "train step 01931 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6986 diff={max=04.8198, min=00.0164, mean=01.1209} policy_loss=-6.2947 policy updated! \n",
      "train step 01932 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0470 diff={max=04.3917, min=00.0315, mean=01.0252} policy_loss=-5.5105 policy updated! \n",
      "train step 01933 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1153 diff={max=03.3421, min=00.0012, mean=00.8217} policy_loss=-5.9707 policy updated! \n",
      "train step 01934 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6366 diff={max=04.2355, min=00.0007, mean=01.3317} policy_loss=-6.3918 policy updated! \n",
      "train step 01935 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.6030 diff={max=04.3493, min=00.0285, mean=00.8453} policy_loss=-6.8321 policy updated! \n",
      "train step 01936 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9383 diff={max=04.9726, min=00.0069, mean=00.9185} policy_loss=-5.2815 policy updated! \n",
      "train step 01937 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3740 diff={max=06.3523, min=00.0179, mean=01.2186} policy_loss=-6.0775 policy updated! \n",
      "train step 01938 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1237 diff={max=02.5317, min=00.0069, mean=00.7874} policy_loss=-6.2013 policy updated! \n",
      "train step 01939 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9420 diff={max=02.8858, min=00.0040, mean=00.6761} policy_loss=-5.2225 policy updated! \n",
      "train step 01940 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=00.7301 diff={max=02.9765, min=00.0046, mean=00.6092} policy_loss=-5.5657 policy updated! \n",
      "train step 01941 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0956 diff={max=05.8368, min=00.0100, mean=00.9613} policy_loss=-5.3096 policy updated! \n",
      "train step 01942 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5837 diff={max=05.4305, min=00.0221, mean=00.9161} policy_loss=-5.5162 policy updated! \n",
      "train step 01943 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9709 diff={max=03.3145, min=00.0447, mean=00.7339} policy_loss=-7.0834 policy updated! \n",
      "train step 01944 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1553 diff={max=05.6020, min=00.0130, mean=00.8831} policy_loss=-5.5728 policy updated! \n",
      "train step 01945 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.5430 diff={max=04.3080, min=00.0132, mean=00.8406} policy_loss=-6.4285 policy updated! \n",
      "train step 01946 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.3866 diff={max=03.6360, min=00.0076, mean=00.8842} policy_loss=-6.0049 policy updated! \n",
      "train step 01947 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4014 diff={max=04.1787, min=00.0151, mean=00.8732} policy_loss=-5.8037 policy updated! \n",
      "train step 01948 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3523 diff={max=08.8414, min=00.0116, mean=00.8166} policy_loss=-5.5778 policy updated! \n",
      "train step 01949 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6249 diff={max=06.8906, min=00.0149, mean=00.6926} policy_loss=-5.3613 policy updated! \n",
      "train step 01950 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.0867 diff={max=04.6787, min=00.0341, mean=01.0308} policy_loss=-4.8760 policy updated! \n",
      "train step 01951 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4589 diff={max=06.0771, min=00.0023, mean=01.0473} policy_loss=-5.3896 policy updated! \n",
      "train step 01952 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1932 diff={max=02.5165, min=00.1297, mean=00.9147} policy_loss=-5.4479 policy updated! \n",
      "train step 01953 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.9705 diff={max=03.9242, min=00.0948, mean=00.9830} policy_loss=-6.4662 policy updated! \n",
      "train step 01954 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3943 diff={max=04.9299, min=00.0002, mean=00.9601} policy_loss=-6.2738 policy updated! \n",
      "train step 01955 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.5365 diff={max=03.4787, min=00.0069, mean=00.9458} policy_loss=-6.6119 policy updated! \n",
      "train step 01956 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.3272 diff={max=03.4647, min=00.0949, mean=00.8253} policy_loss=-6.2061 policy updated! \n",
      "train step 01957 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4734 diff={max=07.4268, min=00.0190, mean=01.0929} policy_loss=-6.2779 policy updated! \n",
      "train step 01958 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.0187 diff={max=03.6839, min=00.0120, mean=00.7174} policy_loss=-5.9852 policy updated! \n",
      "train step 01959 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0957 diff={max=07.0653, min=00.0296, mean=00.8785} policy_loss=-6.4132 policy updated! \n",
      "train step 01960 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.4051 diff={max=04.6386, min=00.0211, mean=01.0021} policy_loss=-5.6430 policy updated! \n",
      "train step 01961 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.8669 diff={max=04.4924, min=00.0047, mean=00.8694} policy_loss=-5.6822 policy updated! \n",
      "train step 01962 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9272 diff={max=03.5743, min=00.0174, mean=00.7152} policy_loss=-6.6161 policy updated! \n",
      "train step 01963 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.5680 diff={max=05.4245, min=00.0076, mean=00.9812} policy_loss=-5.2822 policy updated! \n",
      "train step 01964 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7011 diff={max=04.3295, min=00.0170, mean=00.9000} policy_loss=-5.8483 policy updated! \n",
      "train step 01965 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.5075 diff={max=05.3144, min=00.0052, mean=00.6791} policy_loss=-5.2006 policy updated! \n",
      "train step 01966 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3589 diff={max=05.2862, min=00.0001, mean=01.0442} policy_loss=-5.5747 policy updated! \n",
      "train step 01967 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4637 diff={max=03.5950, min=00.0395, mean=00.8582} policy_loss=-5.9264 policy updated! \n",
      "train step 01968 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.4869 diff={max=07.7883, min=00.0352, mean=00.9799} policy_loss=-6.7545 policy updated! \n",
      "train step 01969 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6483 diff={max=05.3855, min=00.0418, mean=01.1334} policy_loss=-6.1726 policy updated! \n",
      "train step 01970 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.8992 diff={max=05.8973, min=00.0117, mean=00.9215} policy_loss=-6.0599 policy updated! \n",
      "train step 01971 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5316 diff={max=04.2574, min=00.0594, mean=00.8288} policy_loss=-5.1162 policy updated! \n",
      "train step 01972 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9770 diff={max=03.4280, min=00.1159, mean=00.7342} policy_loss=-5.6595 policy updated! \n",
      "train step 01973 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.2657 diff={max=06.4454, min=00.0334, mean=00.9553} policy_loss=-6.5898 policy updated! \n",
      "train step 01974 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0815 diff={max=06.2796, min=00.0365, mean=00.9523} policy_loss=-5.7367 policy updated! \n",
      "train step 01975 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.3047 diff={max=07.4423, min=00.0197, mean=00.8772} policy_loss=-5.9545 policy updated! \n",
      "train step 01976 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1146 diff={max=02.5353, min=00.0184, mean=00.8160} policy_loss=-6.7816 policy updated! \n",
      "train step 01977 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1780 diff={max=04.4492, min=00.0312, mean=00.7427} policy_loss=-7.1622 policy updated! \n",
      "train step 01978 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4141 diff={max=04.8380, min=00.0107, mean=00.8148} policy_loss=-5.8793 policy updated! \n",
      "train step 01979 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7042 diff={max=02.9846, min=00.0329, mean=00.6058} policy_loss=-6.0309 policy updated! \n",
      "train step 01980 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.0797 diff={max=04.1144, min=00.0043, mean=01.0467} policy_loss=-6.6803 policy updated! \n",
      "train step 01981 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2351 diff={max=04.0747, min=00.0132, mean=00.6794} policy_loss=-5.5354 policy updated! \n",
      "train step 01982 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1653 diff={max=05.1929, min=00.0076, mean=00.8653} policy_loss=-5.9760 policy updated! \n",
      "train step 01983 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.7782 diff={max=06.4774, min=00.0318, mean=00.7712} policy_loss=-5.6206 policy updated! \n",
      "train step 01984 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.4320 diff={max=02.0838, min=00.0263, mean=00.4429} policy_loss=-5.7571 policy updated! \n",
      "train step 01985 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=00.9993 diff={max=02.7321, min=00.0038, mean=00.7410} policy_loss=-6.4120 policy updated! \n",
      "train step 01986 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.5125 diff={max=04.9166, min=00.0180, mean=00.6971} policy_loss=-6.0360 policy updated! \n",
      "train step 01987 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4385 diff={max=04.6028, min=00.0073, mean=00.7723} policy_loss=-5.6804 policy updated! \n",
      "train step 01988 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9120 diff={max=06.0800, min=00.0000, mean=00.8395} policy_loss=-5.5079 policy updated! \n",
      "train step 01989 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7655 diff={max=06.8586, min=00.0035, mean=00.7456} policy_loss=-6.0334 policy updated! \n",
      "train step 01990 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=00.8953 diff={max=02.6094, min=00.0009, mean=00.7265} policy_loss=-5.7806 policy updated! \n",
      "train step 01991 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1152 diff={max=04.2304, min=00.0131, mean=00.6681} policy_loss=-6.0504 policy updated! \n",
      "train step 01992 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4864 diff={max=05.3813, min=00.0232, mean=00.6742} policy_loss=-6.9864 policy updated! \n",
      "train step 01993 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8322 diff={max=05.7034, min=00.0027, mean=00.8437} policy_loss=-5.9916 policy updated! \n",
      "train step 01994 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7455 diff={max=06.5243, min=00.0206, mean=00.7278} policy_loss=-5.7698 policy updated! \n",
      "train step 01995 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.2612 diff={max=03.7332, min=00.0338, mean=00.8113} policy_loss=-6.4378 policy updated! \n",
      "train step 01996 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.4008 diff={max=03.5197, min=00.0524, mean=00.8361} policy_loss=-5.6497 policy updated! \n",
      "train step 01997 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9407 diff={max=03.6256, min=00.0213, mean=00.6823} policy_loss=-5.9285 policy updated! \n",
      "train step 01998 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.9066 diff={max=05.1274, min=00.0125, mean=00.9218} policy_loss=-5.9593 policy updated! \n",
      "train step 01999 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=00.9493 diff={max=03.3969, min=00.0459, mean=00.6752} policy_loss=-7.2765 policy updated! \n",
      "train step 02000 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.7387 diff={max=04.6918, min=00.0040, mean=01.0175} policy_loss=-6.3231 policy updated! \n",
      "train step 02001 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9032 diff={max=03.0236, min=00.0453, mean=00.7354} policy_loss=-6.5802 policy updated! \n",
      "train step 02002 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2986 diff={max=03.3841, min=00.0660, mean=00.9351} policy_loss=-5.2810 policy updated! \n",
      "train step 02003 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0963 diff={max=06.4181, min=00.0216, mean=01.1173} policy_loss=-6.7762 policy updated! \n",
      "train step 02004 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9574 diff={max=03.4156, min=00.0273, mean=00.7221} policy_loss=-6.6343 policy updated! \n",
      "train step 02005 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=00.6077 diff={max=02.5117, min=00.0078, mean=00.5926} policy_loss=-5.4845 policy updated! \n",
      "train step 02006 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.4047 diff={max=04.3139, min=00.0060, mean=00.7687} policy_loss=-5.8397 policy updated! \n",
      "train step 02007 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9392 diff={max=02.5690, min=00.0131, mean=00.7650} policy_loss=-5.2058 policy updated! \n",
      "train step 02008 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0592 diff={max=05.7913, min=00.0346, mean=01.2210} policy_loss=-5.7016 policy updated! \n",
      "train step 02009 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.1793 diff={max=03.9695, min=00.0250, mean=00.7741} policy_loss=-5.9044 policy updated! \n",
      "train step 02010 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.4786 diff={max=06.5838, min=00.0161, mean=00.9502} policy_loss=-5.6747 policy updated! \n",
      "train step 02011 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.4781 diff={max=04.6410, min=00.0041, mean=00.7893} policy_loss=-5.5084 policy updated! \n",
      "train step 02012 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2048 diff={max=05.8208, min=00.0174, mean=01.1736} policy_loss=-6.6770 policy updated! \n",
      "train step 02013 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2123 diff={max=05.8691, min=00.0013, mean=01.1738} policy_loss=-5.6050 policy updated! \n",
      "train step 02014 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3132 diff={max=04.7970, min=00.0084, mean=00.7488} policy_loss=-6.4870 policy updated! \n",
      "train step 02015 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.7832 diff={max=05.6868, min=00.0117, mean=00.7825} policy_loss=-5.7066 policy updated! \n",
      "train step 02016 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.4174 diff={max=04.5250, min=00.0045, mean=00.8525} policy_loss=-5.9994 policy updated! \n",
      "train step 02017 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6640 diff={max=05.0510, min=00.0057, mean=00.7465} policy_loss=-5.8426 policy updated! \n",
      "train step 02018 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7440 diff={max=04.8812, min=00.0099, mean=00.8613} policy_loss=-6.3565 policy updated! \n",
      "train step 02019 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3523 diff={max=04.5990, min=00.0103, mean=00.7783} policy_loss=-5.3125 policy updated! \n",
      "train step 02020 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.4426 diff={max=05.0212, min=00.0245, mean=00.7483} policy_loss=-5.8131 policy updated! \n",
      "train step 02021 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6484 diff={max=03.8419, min=00.0090, mean=00.9167} policy_loss=-6.2258 policy updated! \n",
      "train step 02022 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2099 diff={max=04.8178, min=00.0064, mean=00.7665} policy_loss=-6.5193 policy updated! \n",
      "train step 02023 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.5792 diff={max=04.2174, min=00.0029, mean=00.8142} policy_loss=-6.0856 policy updated! \n",
      "train step 02024 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8182 diff={max=05.0385, min=00.0073, mean=00.8781} policy_loss=-5.2769 policy updated! \n",
      "train step 02025 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.6924 diff={max=05.6952, min=00.0039, mean=00.9077} policy_loss=-6.3728 policy updated! \n",
      "train step 02026 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5949 diff={max=06.0304, min=00.0184, mean=00.7994} policy_loss=-6.1139 policy updated! \n",
      "train step 02027 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7554 diff={max=03.4024, min=00.0031, mean=00.6145} policy_loss=-6.0852 policy updated! \n",
      "train step 02028 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=00.8965 diff={max=04.0085, min=00.0015, mean=00.5945} policy_loss=-5.6665 policy updated! \n",
      "train step 02029 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9733 diff={max=03.6706, min=00.0069, mean=00.6783} policy_loss=-6.2364 policy updated! \n",
      "train step 02030 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.3367 diff={max=05.0721, min=00.0199, mean=00.8736} policy_loss=-6.4886 policy updated! \n",
      "train step 02031 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0910 diff={max=07.6652, min=00.0346, mean=00.7883} policy_loss=-5.8577 policy updated! \n",
      "train step 02032 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1258 diff={max=02.9559, min=00.0029, mean=00.8159} policy_loss=-6.0979 policy updated! \n",
      "train step 02033 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7145 diff={max=04.5491, min=00.0079, mean=00.8771} policy_loss=-6.1942 policy updated! \n",
      "train step 02034 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7776 diff={max=05.1740, min=00.0036, mean=00.4720} policy_loss=-4.7882 policy updated! \n",
      "train step 02035 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.6594 diff={max=05.7247, min=00.0219, mean=00.7749} policy_loss=-5.7740 policy updated! \n",
      "train step 02036 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.8663 diff={max=04.4616, min=00.0148, mean=00.9302} policy_loss=-5.6901 policy updated! \n",
      "train step 02037 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6978 diff={max=06.5321, min=00.0071, mean=01.0270} policy_loss=-5.7661 policy updated! \n",
      "train step 02038 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.4617 diff={max=06.1909, min=00.0012, mean=00.6658} policy_loss=-5.4740 policy updated! \n",
      "train step 02039 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.1635 diff={max=03.5411, min=00.0372, mean=00.7822} policy_loss=-6.4809 policy updated! \n",
      "train step 02040 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.4571 diff={max=05.8725, min=00.0581, mean=00.7628} policy_loss=-6.0200 policy updated! \n",
      "train step 02041 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2195 diff={max=03.7027, min=00.0037, mean=00.7295} policy_loss=-5.9378 policy updated! \n",
      "train step 02042 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4911 diff={max=03.7188, min=00.0025, mean=00.8706} policy_loss=-5.7810 policy updated! \n",
      "train step 02043 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6877 diff={max=07.6929, min=00.0425, mean=00.9781} policy_loss=-6.0647 policy updated! \n",
      "train step 02044 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.5199 diff={max=06.3361, min=00.0013, mean=00.9213} policy_loss=-5.9517 policy updated! \n",
      "train step 02045 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.0901 diff={max=05.5043, min=00.0127, mean=00.9357} policy_loss=-6.1951 policy updated! \n",
      "train step 02046 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=01.9120 diff={max=07.4619, min=00.0023, mean=00.8353} policy_loss=-6.8500 policy updated! \n",
      "train step 02047 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4869 diff={max=03.4122, min=00.0094, mean=00.8930} policy_loss=-5.8957 policy updated! \n",
      "train step 02048 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.4083 diff={max=04.0630, min=00.0315, mean=00.8733} policy_loss=-5.6178 policy updated! \n",
      "train step 02049 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.2037 diff={max=04.2899, min=00.0019, mean=00.7651} policy_loss=-6.2934 policy updated! \n",
      "train step 02050 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.7253 diff={max=04.8913, min=00.0526, mean=00.9077} policy_loss=-6.2493 policy updated! \n",
      "train step 02051 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3770 diff={max=03.7732, min=00.0125, mean=00.8613} policy_loss=-6.0222 policy updated! \n",
      "train step 02052 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9253 diff={max=06.1859, min=00.0203, mean=00.8679} policy_loss=-6.5621 policy updated! \n",
      "train step 02053 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.2487 diff={max=05.0188, min=00.0336, mean=01.0452} policy_loss=-5.4239 policy updated! \n",
      "train step 02054 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.8737 diff={max=05.4996, min=00.0353, mean=00.9161} policy_loss=-4.6037 policy updated! \n",
      "train step 02055 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.5743 diff={max=05.9764, min=00.0576, mean=01.6180} policy_loss=-5.9630 policy updated! \n",
      "train step 02056 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1630 diff={max=05.8653, min=00.0020, mean=00.9532} policy_loss=-6.8585 policy updated! \n",
      "train step 02057 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6317 diff={max=07.9949, min=00.0248, mean=01.2739} policy_loss=-6.7178 policy updated! \n",
      "train step 02058 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7217 diff={max=04.1563, min=00.0554, mean=01.0433} policy_loss=-6.4133 policy updated! \n",
      "train step 02059 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.0774 diff={max=05.3352, min=00.0032, mean=01.3185} policy_loss=-6.8626 policy updated! \n",
      "train step 02060 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.0618 diff={max=06.8463, min=00.0176, mean=01.5220} policy_loss=-6.1504 policy updated! \n",
      "train step 02061 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.4881 diff={max=04.6717, min=00.0384, mean=00.8545} policy_loss=-5.7737 policy updated! \n",
      "train step 02062 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6456 diff={max=03.7622, min=00.0263, mean=00.9403} policy_loss=-6.0207 policy updated! \n",
      "train step 02063 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.7936 diff={max=03.8851, min=00.0442, mean=00.9955} policy_loss=-5.3113 policy updated! \n",
      "train step 02064 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.9896 diff={max=07.3185, min=00.0410, mean=01.4321} policy_loss=-5.2146 policy updated! \n",
      "train step 02065 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.5070 diff={max=05.3747, min=00.0739, mean=01.2174} policy_loss=-5.5037 policy updated! \n",
      "train step 02066 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.9347 diff={max=03.4513, min=00.0100, mean=01.0660} policy_loss=-6.3385 policy updated! \n",
      "train step 02067 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7701 diff={max=03.9221, min=00.0074, mean=01.0320} policy_loss=-6.3082 policy updated! \n",
      "train step 02068 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2843 diff={max=07.4673, min=00.0176, mean=01.6044} policy_loss=-6.7852 policy updated! \n",
      "train step 02069 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0177 diff={max=05.5585, min=00.0368, mean=01.0647} policy_loss=-6.0593 policy updated! \n",
      "train step 02070 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.1942 diff={max=05.7346, min=00.0116, mean=00.9402} policy_loss=-5.3871 policy updated! \n",
      "train step 02071 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=02.0294 diff={max=05.5676, min=00.0153, mean=00.9959} policy_loss=-5.4915 policy updated! \n",
      "train step 02072 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8719 diff={max=05.7926, min=00.0201, mean=00.8998} policy_loss=-5.7264 policy updated! \n",
      "train step 02073 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3799 diff={max=04.5921, min=00.0293, mean=00.8055} policy_loss=-6.0680 policy updated! \n",
      "train step 02074 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6215 diff={max=06.6994, min=00.0063, mean=01.2738} policy_loss=-6.4712 policy updated! \n",
      "train step 02075 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.3632 diff={max=04.5598, min=00.0022, mean=01.1366} policy_loss=-6.7052 policy updated! \n",
      "train step 02076 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.9709 diff={max=05.4865, min=00.0197, mean=01.2609} policy_loss=-6.7628 policy updated! \n",
      "train step 02077 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2856 diff={max=03.9041, min=00.0415, mean=01.1888} policy_loss=-6.3075 policy updated! \n",
      "train step 02078 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6036 diff={max=04.9076, min=00.0288, mean=01.1706} policy_loss=-5.8936 policy updated! \n",
      "train step 02079 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.1493 diff={max=04.4589, min=00.0038, mean=01.0930} policy_loss=-6.5144 policy updated! \n",
      "train step 02080 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.9660 diff={max=04.7378, min=00.0032, mean=00.9775} policy_loss=-6.0634 policy updated! \n",
      "train step 02081 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.1696 diff={max=06.4123, min=00.0396, mean=00.9332} policy_loss=-6.1316 policy updated! \n",
      "train step 02082 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2356 diff={max=05.7368, min=00.0187, mean=00.9604} policy_loss=-5.4699 policy updated! \n",
      "train step 02083 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2323 diff={max=03.8449, min=00.0185, mean=00.7473} policy_loss=-5.2979 policy updated! \n",
      "train step 02084 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3491 diff={max=03.3738, min=00.0103, mean=00.8785} policy_loss=-6.1741 policy updated! \n",
      "train step 02085 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.8823 diff={max=07.4184, min=00.0244, mean=01.2491} policy_loss=-6.1992 policy updated! \n",
      "train step 02086 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2786 diff={max=06.3404, min=00.0213, mean=01.0249} policy_loss=-5.6979 policy updated! \n",
      "train step 02087 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0389 diff={max=07.9552, min=00.0105, mean=00.7976} policy_loss=-5.3546 policy updated! \n",
      "train step 02088 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8489 diff={max=08.3672, min=00.0061, mean=00.9877} policy_loss=-5.3873 policy updated! \n",
      "train step 02089 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6433 diff={max=06.2809, min=00.0187, mean=00.9919} policy_loss=-5.4109 policy updated! \n",
      "train step 02090 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.1346 diff={max=08.6487, min=00.0007, mean=00.9283} policy_loss=-6.3792 policy updated! \n",
      "train step 02091 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2350 diff={max=04.5348, min=00.0102, mean=00.8220} policy_loss=-6.5884 policy updated! \n",
      "train step 02092 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5593 diff={max=05.4229, min=00.0176, mean=00.7461} policy_loss=-6.2599 policy updated! \n",
      "train step 02093 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8667 diff={max=05.4890, min=00.0098, mean=00.9255} policy_loss=-6.4051 policy updated! \n",
      "train step 02094 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.1501 diff={max=04.6633, min=00.0376, mean=00.7202} policy_loss=-5.7714 policy updated! \n",
      "train step 02095 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.1010 diff={max=03.5842, min=00.0069, mean=00.7378} policy_loss=-5.4536 policy updated! \n",
      "train step 02096 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.2323 diff={max=03.2464, min=00.0446, mean=00.8747} policy_loss=-5.6390 policy updated! \n",
      "train step 02097 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7624 diff={max=06.8231, min=00.0151, mean=01.0703} policy_loss=-6.0816 policy updated! \n",
      "train step 02098 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5422 diff={max=05.6888, min=00.0272, mean=01.0427} policy_loss=-6.5734 policy updated! \n",
      "train step 02099 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1115 diff={max=04.4720, min=00.0028, mean=00.9205} policy_loss=-6.6063 policy updated! \n",
      "train step 02100 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.3896 diff={max=03.6225, min=00.0059, mean=00.8162} policy_loss=-6.4903 policy updated! \n",
      "train step 02101 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3499 diff={max=03.1533, min=00.0077, mean=00.8345} policy_loss=-6.1636 policy updated! \n",
      "train step 02102 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9174 diff={max=08.1881, min=00.0030, mean=01.0406} policy_loss=-6.8700 policy updated! \n",
      "train step 02103 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.7488 diff={max=04.8432, min=00.0019, mean=00.8269} policy_loss=-5.8452 policy updated! \n",
      "train step 02104 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.6254 diff={max=06.1422, min=00.0162, mean=00.7409} policy_loss=-6.0653 policy updated! \n",
      "train step 02105 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.6853 diff={max=05.3809, min=00.0258, mean=01.4041} policy_loss=-5.9917 policy updated! \n",
      "train step 02106 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7571 diff={max=04.2231, min=00.0474, mean=00.9580} policy_loss=-6.4875 policy updated! \n",
      "train step 02107 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6843 diff={max=04.7864, min=00.0001, mean=00.8363} policy_loss=-6.0144 policy updated! \n",
      "train step 02108 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0405 diff={max=06.6413, min=00.0611, mean=01.2074} policy_loss=-6.7147 policy updated! \n",
      "train step 02109 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8178 diff={max=04.5390, min=00.0089, mean=00.9219} policy_loss=-6.2856 policy updated! \n",
      "train step 02110 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.9672 diff={max=04.3817, min=00.0126, mean=00.9362} policy_loss=-6.3473 policy updated! \n",
      "train step 02111 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5656 diff={max=05.4042, min=00.0141, mean=01.0625} policy_loss=-6.6029 policy updated! \n",
      "train step 02112 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5542 diff={max=04.8062, min=00.0222, mean=00.8650} policy_loss=-6.3385 policy updated! \n",
      "train step 02113 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.0075 diff={max=05.1945, min=00.0054, mean=00.9949} policy_loss=-6.0290 policy updated! \n",
      "train step 02114 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6911 diff={max=04.7656, min=00.0025, mean=00.8766} policy_loss=-6.4497 policy updated! \n",
      "train step 02115 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.3809 diff={max=03.6504, min=00.0614, mean=00.8100} policy_loss=-6.2328 policy updated! \n",
      "train step 02116 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7220 diff={max=03.9374, min=00.0301, mean=00.8925} policy_loss=-7.0599 policy updated! \n",
      "train step 02117 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8806 diff={max=06.8008, min=00.0413, mean=00.9738} policy_loss=-6.3436 policy updated! \n",
      "train step 02118 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1745 diff={max=03.7190, min=00.0267, mean=00.7775} policy_loss=-5.3859 policy updated! \n",
      "train step 02119 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0202 diff={max=05.7365, min=00.0195, mean=00.9700} policy_loss=-6.4737 policy updated! \n",
      "train step 02120 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.2519 diff={max=05.1155, min=00.0042, mean=00.9817} policy_loss=-6.3317 policy updated! \n",
      "train step 02121 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.7959 diff={max=04.9663, min=00.0118, mean=00.8495} policy_loss=-6.3716 policy updated! \n",
      "train step 02122 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7339 diff={max=04.7429, min=00.0092, mean=00.8189} policy_loss=-5.4844 policy updated! \n",
      "train step 02123 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.8119 diff={max=04.7737, min=00.0249, mean=01.1566} policy_loss=-6.0793 policy updated! \n",
      "train step 02124 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6024 diff={max=04.3098, min=00.0009, mean=00.8790} policy_loss=-6.4586 policy updated! \n",
      "train step 02125 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.0578 diff={max=04.6184, min=00.0078, mean=00.6432} policy_loss=-5.7936 policy updated! \n",
      "train step 02126 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8356 diff={max=05.8936, min=00.0108, mean=00.8601} policy_loss=-6.1650 policy updated! \n",
      "train step 02127 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9751 diff={max=06.3599, min=00.0352, mean=00.8966} policy_loss=-6.3123 policy updated! \n",
      "train step 02128 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=00.7069 diff={max=03.1354, min=00.0057, mean=00.5564} policy_loss=-6.7642 policy updated! \n",
      "train step 02129 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5857 diff={max=06.2311, min=00.0081, mean=01.0587} policy_loss=-6.2463 policy updated! \n",
      "train step 02130 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.2444 diff={max=04.8790, min=00.0287, mean=00.9664} policy_loss=-6.0499 policy updated! \n",
      "train step 02131 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0264 diff={max=02.4723, min=00.0238, mean=00.8110} policy_loss=-6.1650 policy updated! \n",
      "train step 02132 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9649 diff={max=04.7572, min=00.0206, mean=00.9167} policy_loss=-6.4260 policy updated! \n",
      "train step 02133 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1912 diff={max=03.2504, min=00.0079, mean=00.7972} policy_loss=-6.2715 policy updated! \n",
      "train step 02134 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0381 diff={max=06.9350, min=00.0156, mean=00.9024} policy_loss=-7.0252 policy updated! \n",
      "train step 02135 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=00.8179 diff={max=02.7287, min=00.0018, mean=00.6572} policy_loss=-5.8768 policy updated! \n",
      "train step 02136 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6569 diff={max=06.3049, min=00.0712, mean=01.2419} policy_loss=-5.7006 policy updated! \n",
      "train step 02137 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4472 diff={max=06.9178, min=00.0096, mean=00.9992} policy_loss=-6.8211 policy updated! \n",
      "train step 02138 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1224 diff={max=06.6128, min=00.0072, mean=00.7962} policy_loss=-5.9658 policy updated! \n",
      "train step 02139 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=00.9874 diff={max=03.9018, min=00.0120, mean=00.6996} policy_loss=-6.5974 policy updated! \n",
      "train step 02140 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.3463 diff={max=04.3112, min=00.0539, mean=00.8065} policy_loss=-7.1290 policy updated! \n",
      "train step 02141 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8418 diff={max=04.2333, min=00.0289, mean=00.9872} policy_loss=-6.0978 policy updated! \n",
      "train step 02142 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=03.8203 diff={max=06.1214, min=00.0572, mean=01.3774} policy_loss=-6.7101 policy updated! \n",
      "train step 02143 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7817 diff={max=04.0042, min=00.0090, mean=00.9643} policy_loss=-6.0318 policy updated! \n",
      "train step 02144 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3568 diff={max=04.3578, min=00.0194, mean=00.8150} policy_loss=-6.7222 policy updated! \n",
      "train step 02145 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.3173 diff={max=05.4530, min=00.0170, mean=01.2091} policy_loss=-6.2496 policy updated! \n",
      "train step 02146 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3321 diff={max=02.8481, min=00.0126, mean=00.9009} policy_loss=-6.1378 policy updated! \n",
      "train step 02147 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1121 diff={max=05.9606, min=00.0271, mean=01.0119} policy_loss=-6.5298 policy updated! \n",
      "train step 02148 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3292 diff={max=05.3142, min=00.0188, mean=00.9947} policy_loss=-5.4957 policy updated! \n",
      "train step 02149 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=00.4574 diff={max=01.5907, min=00.0114, mean=00.5485} policy_loss=-6.1643 policy updated! \n",
      "train step 02150 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.6149 diff={max=04.4322, min=00.0096, mean=00.8500} policy_loss=-6.9722 policy updated! \n",
      "train step 02151 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1076 diff={max=02.8404, min=00.0203, mean=00.7976} policy_loss=-7.0421 policy updated! \n",
      "train step 02152 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1196 diff={max=04.0601, min=00.0216, mean=00.9995} policy_loss=-5.8736 policy updated! \n",
      "train step 02153 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9557 diff={max=03.4300, min=00.0378, mean=00.7248} policy_loss=-6.7831 policy updated! \n",
      "train step 02154 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=00.9161 diff={max=03.5745, min=00.0401, mean=00.6837} policy_loss=-5.3347 policy updated! \n",
      "train step 02155 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.0431 diff={max=04.5811, min=00.0197, mean=00.9665} policy_loss=-6.2075 policy updated! \n",
      "train step 02156 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9589 diff={max=03.5641, min=00.0131, mean=00.6744} policy_loss=-6.3114 policy updated! \n",
      "train step 02157 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8552 diff={max=05.7592, min=00.0125, mean=00.8407} policy_loss=-6.5637 policy updated! \n",
      "train step 02158 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.2951 diff={max=04.2927, min=00.0050, mean=00.7822} policy_loss=-6.8427 policy updated! \n",
      "train step 02159 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2328 diff={max=04.5505, min=00.0305, mean=00.7829} policy_loss=-6.6262 policy updated! \n",
      "train step 02160 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.6881 diff={max=04.2419, min=00.0015, mean=00.9780} policy_loss=-6.4754 policy updated! \n",
      "train step 02161 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=00.9959 diff={max=03.2991, min=00.0019, mean=00.6272} policy_loss=-6.5530 policy updated! \n",
      "train step 02162 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2721 diff={max=04.7533, min=00.0189, mean=00.6834} policy_loss=-6.4694 policy updated! \n",
      "train step 02163 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.0486 diff={max=03.2199, min=00.0001, mean=00.7715} policy_loss=-6.5451 policy updated! \n",
      "train step 02164 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1577 diff={max=05.9260, min=00.0304, mean=01.0337} policy_loss=-6.2368 policy updated! \n",
      "train step 02165 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.9418 diff={max=05.3589, min=00.0026, mean=01.2915} policy_loss=-6.3998 policy updated! \n",
      "train step 02166 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1786 diff={max=04.5516, min=00.0006, mean=00.6358} policy_loss=-6.0934 policy updated! \n",
      "train step 02167 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2095 diff={max=04.6354, min=00.0118, mean=00.7610} policy_loss=-7.2761 policy updated! \n",
      "train step 02168 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.0465 diff={max=01.9886, min=00.0567, mean=00.8592} policy_loss=-7.0663 policy updated! \n",
      "train step 02169 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6756 diff={max=04.0302, min=00.0306, mean=00.9215} policy_loss=-6.7007 policy updated! \n",
      "train step 02170 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.9311 diff={max=04.6137, min=00.0142, mean=00.9180} policy_loss=-6.2391 policy updated! \n",
      "train step 02171 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1866 diff={max=07.4177, min=00.0027, mean=00.8886} policy_loss=-6.0844 policy updated! \n",
      "train step 02172 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7009 diff={max=02.8131, min=00.0246, mean=00.6152} policy_loss=-6.3409 policy updated! \n",
      "train step 02173 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9401 diff={max=04.3371, min=00.0099, mean=00.9952} policy_loss=-6.4420 policy updated! \n",
      "train step 02174 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4591 diff={max=05.1628, min=00.0148, mean=00.9417} policy_loss=-6.5477 policy updated! \n",
      "train step 02175 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.4799 diff={max=05.9325, min=00.0297, mean=00.7713} policy_loss=-6.0019 policy updated! \n",
      "train step 02176 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.3848 diff={max=05.7782, min=00.0206, mean=00.5886} policy_loss=-6.5592 policy updated! \n",
      "train step 02177 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2105 diff={max=03.5231, min=00.0163, mean=00.7849} policy_loss=-7.0518 policy updated! \n",
      "train step 02178 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.4180 diff={max=03.4452, min=00.0410, mean=00.8584} policy_loss=-7.0283 policy updated! \n",
      "train step 02179 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1414 diff={max=04.2158, min=00.0020, mean=00.7552} policy_loss=-5.9707 policy updated! \n",
      "train step 02180 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.2009 diff={max=06.8924, min=00.0211, mean=00.8465} policy_loss=-6.8853 policy updated! \n",
      "train step 02181 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1003 diff={max=05.4101, min=00.0029, mean=00.8856} policy_loss=-5.9946 policy updated! \n",
      "train step 02182 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8103 diff={max=06.0618, min=00.0465, mean=01.2755} policy_loss=-5.2485 policy updated! \n",
      "train step 02183 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.8113 diff={max=08.1306, min=00.0015, mean=01.4777} policy_loss=-5.8680 policy updated! \n",
      "train step 02184 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1832 diff={max=03.7155, min=00.0490, mean=00.7207} policy_loss=-5.7420 policy updated! \n",
      "train step 02185 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=00.9179 diff={max=03.4138, min=00.0265, mean=00.7055} policy_loss=-7.0195 policy updated! \n",
      "train step 02186 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3042 diff={max=04.9411, min=00.0078, mean=00.7627} policy_loss=-6.7621 policy updated! \n",
      "train step 02187 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2728 diff={max=04.0910, min=00.0544, mean=00.8184} policy_loss=-6.5291 policy updated! \n",
      "train step 02188 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8312 diff={max=04.2853, min=00.0004, mean=00.9028} policy_loss=-7.4670 policy updated! \n",
      "train step 02189 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.3886 diff={max=03.8146, min=00.0448, mean=00.8706} policy_loss=-6.9377 policy updated! \n",
      "train step 02190 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.0282 diff={max=03.1635, min=00.0088, mean=00.7068} policy_loss=-5.3153 policy updated! \n",
      "train step 02191 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.9667 diff={max=06.3313, min=00.0222, mean=00.8414} policy_loss=-6.3089 policy updated! \n",
      "train step 02192 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0520 diff={max=02.9040, min=00.0177, mean=00.7254} policy_loss=-6.3900 policy updated! \n",
      "train step 02193 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4880 diff={max=04.2192, min=00.0363, mean=00.8459} policy_loss=-6.2041 policy updated! \n",
      "train step 02194 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.5293 diff={max=05.8921, min=00.0139, mean=00.7832} policy_loss=-7.2750 policy updated! \n",
      "train step 02195 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.9848 diff={max=07.1660, min=00.0054, mean=01.0577} policy_loss=-6.0741 policy updated! \n",
      "train step 02196 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2257 diff={max=03.1820, min=00.0437, mean=00.8002} policy_loss=-7.0396 policy updated! \n",
      "train step 02197 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5222 diff={max=04.8857, min=00.0330, mean=00.8039} policy_loss=-6.7416 policy updated! \n",
      "train step 02198 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3990 diff={max=06.3585, min=00.0061, mean=00.9638} policy_loss=-5.9291 policy updated! \n",
      "train step 02199 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3488 diff={max=03.7852, min=00.0103, mean=00.7652} policy_loss=-6.3218 policy updated! \n",
      "train step 02200 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.9553 diff={max=04.9423, min=00.0338, mean=00.9381} policy_loss=-6.3259 policy updated! \n",
      "train step 02201 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1480 diff={max=04.3090, min=00.0125, mean=01.0196} policy_loss=-6.5115 policy updated! \n",
      "train step 02202 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1179 diff={max=03.3133, min=00.0139, mean=00.7279} policy_loss=-6.7099 policy updated! \n",
      "train step 02203 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=00.9574 diff={max=02.6103, min=00.0061, mean=00.7250} policy_loss=-5.9627 policy updated! \n",
      "train step 02204 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5107 diff={max=05.7430, min=00.0003, mean=00.9642} policy_loss=-6.4543 policy updated! \n",
      "train step 02205 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.8813 diff={max=05.6036, min=00.0110, mean=01.1592} policy_loss=-6.0550 policy updated! \n",
      "train step 02206 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.8196 diff={max=05.5932, min=00.0205, mean=00.9412} policy_loss=-6.7354 policy updated! \n",
      "train step 02207 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5970 diff={max=05.0971, min=00.0029, mean=00.9039} policy_loss=-5.9987 policy updated! \n",
      "train step 02208 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9335 diff={max=04.8322, min=00.0203, mean=00.8880} policy_loss=-5.8216 policy updated! \n",
      "train step 02209 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.6722 diff={max=03.7589, min=00.0280, mean=00.9552} policy_loss=-5.8756 policy updated! \n",
      "train step 02210 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.6100 diff={max=03.8368, min=00.0016, mean=01.1218} policy_loss=-6.4802 policy updated! \n",
      "train step 02211 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.3784 diff={max=05.8339, min=00.0475, mean=01.1967} policy_loss=-6.5997 policy updated! \n",
      "train step 02212 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3172 diff={max=04.3016, min=00.0000, mean=00.8398} policy_loss=-7.0806 policy updated! \n",
      "train step 02213 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.6043 diff={max=04.6806, min=00.0043, mean=00.8461} policy_loss=-7.1787 policy updated! \n",
      "train step 02214 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4161 diff={max=03.8585, min=00.0441, mean=00.8253} policy_loss=-6.7996 policy updated! \n",
      "train step 02215 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.6369 diff={max=05.3928, min=00.0006, mean=01.0070} policy_loss=-5.9020 policy updated! \n",
      "train step 02216 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.5671 diff={max=06.2528, min=00.0065, mean=01.1164} policy_loss=-6.3304 policy updated! \n",
      "train step 02217 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9096 diff={max=03.8756, min=00.0109, mean=01.0336} policy_loss=-6.0573 policy updated! \n",
      "train step 02218 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7967 diff={max=08.1833, min=00.0097, mean=01.2380} policy_loss=-7.0818 policy updated! \n",
      "train step 02219 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.4554 diff={max=03.9425, min=00.0058, mean=00.8715} policy_loss=-6.4689 policy updated! \n",
      "train step 02220 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.0696 diff={max=05.6568, min=00.0064, mean=01.1572} policy_loss=-6.9818 policy updated! \n",
      "train step 02221 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.1067 diff={max=03.3263, min=00.0283, mean=01.1918} policy_loss=-6.4077 policy updated! \n",
      "train step 02222 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3784 diff={max=09.2752, min=00.0112, mean=01.0765} policy_loss=-6.4973 policy updated! \n",
      "train step 02223 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3690 diff={max=05.3833, min=00.0008, mean=01.0542} policy_loss=-6.4258 policy updated! \n",
      "train step 02224 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9196 diff={max=03.2469, min=00.0189, mean=00.6666} policy_loss=-5.6130 policy updated! \n",
      "train step 02225 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.7475 diff={max=04.6154, min=00.0660, mean=00.9649} policy_loss=-5.7172 policy updated! \n",
      "train step 02226 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.0450 diff={max=05.7071, min=00.0063, mean=00.9442} policy_loss=-5.6944 policy updated! \n",
      "train step 02227 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2380 diff={max=04.9353, min=00.0359, mean=01.0306} policy_loss=-6.6952 policy updated! \n",
      "train step 02228 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.7823 diff={max=04.8268, min=00.0356, mean=01.1784} policy_loss=-6.4614 policy updated! \n",
      "train step 02229 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6171 diff={max=08.1643, min=00.0556, mean=01.4810} policy_loss=-6.6247 policy updated! \n",
      "train step 02230 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.0165 diff={max=04.9668, min=00.0813, mean=01.0768} policy_loss=-7.3301 policy updated! \n",
      "train step 02231 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9335 diff={max=06.3350, min=00.0005, mean=01.2297} policy_loss=-7.0335 policy updated! \n",
      "train step 02232 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1359 diff={max=03.2339, min=00.0145, mean=00.7226} policy_loss=-6.0026 policy updated! \n",
      "train step 02233 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.1931 diff={max=06.0551, min=00.0247, mean=00.9215} policy_loss=-5.4563 policy updated! \n",
      "train step 02234 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1494 diff={max=07.3283, min=00.0861, mean=01.3314} policy_loss=-6.5046 policy updated! \n",
      "train step 02235 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.5139 diff={max=03.5073, min=00.0175, mean=00.8659} policy_loss=-5.9717 policy updated! \n",
      "train step 02236 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9116 diff={max=05.7885, min=00.0071, mean=01.1594} policy_loss=-6.1695 policy updated! \n",
      "train step 02237 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0488 diff={max=05.5280, min=00.0078, mean=00.9464} policy_loss=-6.5083 policy updated! \n",
      "train step 02238 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9983 diff={max=02.1607, min=00.0095, mean=00.8237} policy_loss=-6.7567 policy updated! \n",
      "train step 02239 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9306 diff={max=02.2200, min=00.0082, mean=00.7609} policy_loss=-6.4214 policy updated! \n",
      "train step 02240 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.3065 diff={max=04.9083, min=00.0174, mean=00.7729} policy_loss=-6.2580 policy updated! \n",
      "train step 02241 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8076 diff={max=02.6804, min=00.0050, mean=00.6435} policy_loss=-6.3571 policy updated! \n",
      "train step 02242 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5802 diff={max=05.5185, min=00.0111, mean=01.1703} policy_loss=-6.3458 policy updated! \n",
      "train step 02243 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.6043 diff={max=06.0398, min=00.0420, mean=00.8169} policy_loss=-5.4134 policy updated! \n",
      "train step 02244 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8105 diff={max=05.1733, min=00.0063, mean=01.1317} policy_loss=-5.5765 policy updated! \n",
      "train step 02245 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.3805 diff={max=03.6129, min=00.0754, mean=00.8785} policy_loss=-6.5474 policy updated! \n",
      "train step 02246 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3020 diff={max=06.3355, min=00.0150, mean=00.9002} policy_loss=-6.4275 policy updated! \n",
      "train step 02247 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3469 diff={max=03.7694, min=00.0032, mean=00.8065} policy_loss=-7.1751 policy updated! \n",
      "train step 02248 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.4250 diff={max=03.8846, min=00.0088, mean=00.8388} policy_loss=-6.4825 policy updated! \n",
      "train step 02249 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.0956 diff={max=06.0979, min=00.0044, mean=01.0510} policy_loss=-6.4614 policy updated! \n",
      "train step 02250 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.3335 diff={max=06.2089, min=00.0021, mean=00.8888} policy_loss=-6.9020 policy updated! \n",
      "train step 02251 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.3970 diff={max=06.2847, min=00.0123, mean=01.1292} policy_loss=-6.7735 policy updated! \n",
      "train step 02252 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5170 diff={max=04.6264, min=00.0052, mean=00.7969} policy_loss=-5.9710 policy updated! \n",
      "train step 02253 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9090 diff={max=06.7130, min=00.0126, mean=00.8765} policy_loss=-6.1969 policy updated! \n",
      "train step 02254 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4227 diff={max=02.9311, min=00.0179, mean=00.8624} policy_loss=-5.4514 policy updated! \n",
      "train step 02255 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.9050 diff={max=05.3437, min=00.0171, mean=01.0477} policy_loss=-5.9549 policy updated! \n",
      "train step 02256 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3098 diff={max=06.7965, min=00.0189, mean=01.0480} policy_loss=-5.9446 policy updated! \n",
      "train step 02257 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3892 diff={max=03.7825, min=00.0137, mean=00.9245} policy_loss=-7.1168 policy updated! \n",
      "train step 02258 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.4638 diff={max=06.9522, min=00.0260, mean=01.0105} policy_loss=-6.5896 policy updated! \n",
      "train step 02259 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.2979 diff={max=03.3987, min=00.0195, mean=00.8398} policy_loss=-6.4304 policy updated! \n",
      "train step 02260 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.0848 diff={max=04.4372, min=00.0057, mean=00.9680} policy_loss=-6.4969 policy updated! \n",
      "train step 02261 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.5458 diff={max=03.6184, min=00.0956, mean=00.9426} policy_loss=-6.2735 policy updated! \n",
      "train step 02262 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6990 diff={max=03.7026, min=00.0318, mean=00.8866} policy_loss=-6.4204 policy updated! \n",
      "train step 02263 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0081 diff={max=06.0784, min=00.1088, mean=00.9994} policy_loss=-5.9448 policy updated! \n",
      "train step 02264 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5389 diff={max=03.0979, min=00.0843, mean=00.9927} policy_loss=-6.3214 policy updated! \n",
      "train step 02265 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.6121 diff={max=06.1987, min=00.0025, mean=00.9797} policy_loss=-7.1177 policy updated! \n",
      "train step 02266 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4094 diff={max=04.7242, min=00.0043, mean=00.7793} policy_loss=-7.4724 policy updated! \n",
      "train step 02267 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2138 diff={max=04.7979, min=00.0577, mean=01.0772} policy_loss=-7.6040 policy updated! \n",
      "train step 02268 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2914 diff={max=04.7842, min=00.0006, mean=01.2259} policy_loss=-5.8438 policy updated! \n",
      "train step 02269 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0857 diff={max=08.5111, min=00.0192, mean=01.1108} policy_loss=-7.7964 policy updated! \n",
      "train step 02270 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.6255 diff={max=07.5155, min=00.0044, mean=01.1838} policy_loss=-6.2684 policy updated! \n",
      "train step 02271 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3893 diff={max=03.7491, min=00.0168, mean=00.8296} policy_loss=-5.1866 policy updated! \n",
      "train step 02272 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4298 diff={max=05.3776, min=00.0024, mean=01.0996} policy_loss=-5.5301 policy updated! \n",
      "train step 02273 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5828 diff={max=05.6272, min=00.0548, mean=01.4685} policy_loss=-5.9937 policy updated! \n",
      "train step 02274 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7608 diff={max=04.6388, min=00.0547, mean=01.0530} policy_loss=-6.5050 policy updated! \n",
      "train step 02275 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.1332 diff={max=04.5971, min=00.0231, mean=00.7369} policy_loss=-6.5993 policy updated! \n",
      "train step 02276 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4371 diff={max=04.8210, min=00.0226, mean=01.2315} policy_loss=-7.5637 policy updated! \n",
      "train step 02277 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3234 diff={max=03.8299, min=00.0250, mean=00.9341} policy_loss=-7.5372 policy updated! \n",
      "train step 02278 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7795 diff={max=05.2822, min=00.0152, mean=00.8896} policy_loss=-6.0756 policy updated! \n",
      "train step 02279 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7404 diff={max=03.4057, min=00.0100, mean=00.9916} policy_loss=-7.0748 policy updated! \n",
      "train step 02280 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.5897 diff={max=03.3944, min=00.0074, mean=00.9168} policy_loss=-5.6839 policy updated! \n",
      "train step 02281 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3922 diff={max=03.6669, min=00.0187, mean=00.7961} policy_loss=-5.9977 policy updated! \n",
      "train step 02282 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4437 diff={max=03.6046, min=00.0062, mean=00.8873} policy_loss=-5.2199 policy updated! \n",
      "train step 02283 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3934 diff={max=03.5817, min=00.0100, mean=01.2216} policy_loss=-6.5644 policy updated! \n",
      "train step 02284 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7726 diff={max=08.2252, min=00.0639, mean=01.4110} policy_loss=-5.5639 policy updated! \n",
      "train step 02285 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.0837 diff={max=04.8796, min=00.0101, mean=00.9870} policy_loss=-6.3489 policy updated! \n",
      "train step 02286 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7096 diff={max=04.7321, min=00.0014, mean=00.7635} policy_loss=-6.8401 policy updated! \n",
      "train step 02287 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8521 diff={max=04.2859, min=00.0190, mean=00.9644} policy_loss=-7.3943 policy updated! \n",
      "train step 02288 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.0501 diff={max=05.6885, min=00.0152, mean=01.1911} policy_loss=-6.6158 policy updated! \n",
      "train step 02289 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3060 diff={max=02.5517, min=00.0235, mean=00.8196} policy_loss=-6.4378 policy updated! \n",
      "train step 02290 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.0453 diff={max=05.7215, min=00.0497, mean=00.9190} policy_loss=-6.0879 policy updated! \n",
      "train step 02291 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.3293 diff={max=03.1919, min=00.0788, mean=00.8290} policy_loss=-6.0580 policy updated! \n",
      "train step 02292 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6363 diff={max=05.3739, min=00.0017, mean=01.0556} policy_loss=-6.2348 policy updated! \n",
      "train step 02293 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6368 diff={max=03.5108, min=00.0018, mean=00.9389} policy_loss=-6.4320 policy updated! \n",
      "train step 02294 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5021 diff={max=05.9039, min=00.0206, mean=00.9928} policy_loss=-6.9673 policy updated! \n",
      "train step 02295 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.5584 diff={max=04.0170, min=00.0049, mean=00.8282} policy_loss=-7.0951 policy updated! \n",
      "train step 02296 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7279 diff={max=06.1572, min=00.0425, mean=00.8579} policy_loss=-7.4068 policy updated! \n",
      "train step 02297 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8159 diff={max=04.5168, min=00.0007, mean=00.9390} policy_loss=-6.0015 policy updated! \n",
      "train step 02298 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8047 diff={max=04.9951, min=00.0099, mean=00.9113} policy_loss=-7.1049 policy updated! \n",
      "train step 02299 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.0736 diff={max=09.3764, min=00.0373, mean=01.3861} policy_loss=-7.0297 policy updated! \n",
      "train step 02300 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.2076 diff={max=03.8865, min=00.0214, mean=00.7570} policy_loss=-6.7129 policy updated! \n",
      "train step 02301 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9477 diff={max=04.5861, min=00.0374, mean=01.0043} policy_loss=-6.7708 policy updated! \n",
      "train step 02302 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4099 diff={max=05.8566, min=00.0317, mean=00.9885} policy_loss=-6.7900 policy updated! \n",
      "train step 02303 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.8742 diff={max=06.5048, min=00.0019, mean=01.0212} policy_loss=-5.9263 policy updated! \n",
      "train step 02304 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.1866 diff={max=06.6799, min=00.0404, mean=00.9365} policy_loss=-6.0208 policy updated! \n",
      "train step 02305 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.7118 diff={max=06.4859, min=00.0338, mean=01.0105} policy_loss=-6.4585 policy updated! \n",
      "train step 02306 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.9333 diff={max=05.3216, min=00.0054, mean=00.9409} policy_loss=-6.2984 policy updated! \n",
      "train step 02307 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0250 diff={max=07.6094, min=00.0498, mean=01.0454} policy_loss=-6.0980 policy updated! \n",
      "train step 02308 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8697 diff={max=06.4011, min=00.0007, mean=01.1376} policy_loss=-7.1936 policy updated! \n",
      "train step 02309 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1387 diff={max=05.0678, min=00.0387, mean=01.0435} policy_loss=-6.4907 policy updated! \n",
      "train step 02310 reward={max=08.0000, min=00.0000, mean=03.0000} optimizing loss=01.8849 diff={max=04.4975, min=00.0104, mean=01.0216} policy_loss=-6.7816 policy updated! \n",
      "train step 02311 reward={max=08.0000, min=00.0000, mean=04.6000} optimizing loss=01.5480 diff={max=05.3591, min=00.0223, mean=00.8416} policy_loss=-7.1754 policy updated! \n",
      "train step 02312 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8086 diff={max=04.7932, min=00.0167, mean=00.8904} policy_loss=-7.1312 policy updated! \n",
      "train step 02313 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1428 diff={max=06.5290, min=00.0153, mean=01.1562} policy_loss=-6.8694 policy updated! \n",
      "train step 02314 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8589 diff={max=09.4098, min=00.0000, mean=01.2123} policy_loss=-5.8085 policy updated! \n",
      "train step 02315 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=02.3446 diff={max=05.3494, min=00.0101, mean=00.8950} policy_loss=-5.9888 policy updated! \n",
      "train step 02316 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0596 diff={max=04.0711, min=00.0040, mean=00.7206} policy_loss=-7.2851 policy updated! \n",
      "train step 02317 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5025 diff={max=04.5159, min=00.0042, mean=00.7520} policy_loss=-6.6770 policy updated! \n",
      "train step 02318 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5659 diff={max=03.8209, min=00.0625, mean=00.9167} policy_loss=-6.3322 policy updated! \n",
      "train step 02319 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1022 diff={max=06.6435, min=00.0466, mean=01.1060} policy_loss=-7.2150 policy updated! \n",
      "train step 02320 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.2167 diff={max=05.7426, min=00.0761, mean=01.1699} policy_loss=-5.8659 policy updated! \n",
      "train step 02321 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=02.3439 diff={max=05.9559, min=00.0131, mean=00.9812} policy_loss=-5.5083 policy updated! \n",
      "train step 02322 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2837 diff={max=03.6998, min=00.0221, mean=00.8045} policy_loss=-6.2072 policy updated! \n",
      "train step 02323 reward={max=08.0000, min=00.0000, mean=03.2000} optimizing loss=02.4671 diff={max=07.3815, min=00.0250, mean=00.9217} policy_loss=-6.7743 policy updated! \n",
      "train step 02324 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7971 diff={max=03.7640, min=00.0046, mean=00.9671} policy_loss=-6.5604 policy updated! \n",
      "train step 02325 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.3390 diff={max=03.3610, min=00.0098, mean=00.8352} policy_loss=-6.0283 policy updated! \n",
      "train step 02326 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1810 diff={max=04.8228, min=00.0076, mean=00.9407} policy_loss=-6.3663 policy updated! \n",
      "train step 02327 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3946 diff={max=07.2525, min=00.0563, mean=01.3726} policy_loss=-7.0287 policy updated! \n",
      "train step 02328 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5442 diff={max=05.0874, min=00.0204, mean=00.8035} policy_loss=-7.0478 policy updated! \n",
      "train step 02329 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3055 diff={max=06.7868, min=00.0202, mean=01.0349} policy_loss=-6.5334 policy updated! \n",
      "train step 02330 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=00.6746 diff={max=02.5577, min=00.0447, mean=00.6063} policy_loss=-5.5079 policy updated! \n",
      "train step 02331 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=01.7396 diff={max=03.6615, min=00.0174, mean=00.9152} policy_loss=-7.1328 policy updated! \n",
      "train step 02332 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3997 diff={max=04.3608, min=00.0532, mean=00.7940} policy_loss=-6.7899 policy updated! \n",
      "train step 02333 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.1047 diff={max=04.1166, min=00.0300, mean=00.6740} policy_loss=-6.5394 policy updated! \n",
      "train step 02334 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9530 diff={max=03.4747, min=00.0082, mean=00.7126} policy_loss=-6.6930 policy updated! \n",
      "train step 02335 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.7493 diff={max=05.6008, min=00.0275, mean=01.0236} policy_loss=-5.7253 policy updated! \n",
      "train step 02336 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.5585 diff={max=05.0454, min=00.0066, mean=01.1004} policy_loss=-6.6554 policy updated! \n",
      "train step 02337 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0610 diff={max=06.8866, min=00.0335, mean=00.8545} policy_loss=-6.4648 policy updated! \n",
      "train step 02338 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5311 diff={max=06.6392, min=00.0058, mean=00.9849} policy_loss=-6.4319 policy updated! \n",
      "train step 02339 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0859 diff={max=05.2660, min=00.0107, mean=00.9105} policy_loss=-6.4177 policy updated! \n",
      "train step 02340 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.4973 diff={max=04.1720, min=00.0227, mean=00.7683} policy_loss=-5.7807 policy updated! \n",
      "train step 02341 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5056 diff={max=04.7504, min=00.0571, mean=00.8154} policy_loss=-6.1610 policy updated! \n",
      "train step 02342 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6204 diff={max=05.5544, min=00.0256, mean=00.8254} policy_loss=-6.8068 policy updated! \n",
      "train step 02343 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.4403 diff={max=05.0125, min=00.0112, mean=00.7543} policy_loss=-6.2225 policy updated! \n",
      "train step 02344 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=00.6604 diff={max=03.3734, min=00.0011, mean=00.5709} policy_loss=-5.8703 policy updated! \n",
      "train step 02345 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=00.8188 diff={max=03.8081, min=00.0116, mean=00.6303} policy_loss=-5.8028 policy updated! \n",
      "train step 02346 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7058 diff={max=07.9320, min=00.0243, mean=00.9283} policy_loss=-7.2131 policy updated! \n",
      "train step 02347 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5925 diff={max=05.9254, min=00.0073, mean=00.7591} policy_loss=-5.4677 policy updated! \n",
      "train step 02348 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.6538 diff={max=02.1150, min=00.0070, mean=00.6163} policy_loss=-6.3944 policy updated! \n",
      "train step 02349 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8507 diff={max=04.7924, min=00.0176, mean=00.8121} policy_loss=-6.4020 policy updated! \n",
      "train step 02350 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.4531 diff={max=04.5965, min=00.0304, mean=00.8021} policy_loss=-6.2770 policy updated! \n",
      "train step 02351 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9507 diff={max=05.1643, min=00.0158, mean=00.8551} policy_loss=-6.9869 policy updated! \n",
      "train step 02352 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5237 diff={max=03.8056, min=00.0082, mean=00.7829} policy_loss=-6.4846 policy updated! \n",
      "train step 02353 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9176 diff={max=05.5138, min=00.0047, mean=00.8198} policy_loss=-6.4817 policy updated! \n",
      "train step 02354 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0302 diff={max=07.2345, min=00.0087, mean=00.8061} policy_loss=-6.9729 policy updated! \n",
      "train step 02355 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=00.7707 diff={max=03.0842, min=00.0012, mean=00.6000} policy_loss=-6.6972 policy updated! \n",
      "train step 02356 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8703 diff={max=06.2686, min=00.0073, mean=00.8317} policy_loss=-7.3238 policy updated! \n",
      "train step 02357 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9585 diff={max=06.0024, min=00.0235, mean=00.7638} policy_loss=-6.0706 policy updated! \n",
      "train step 02358 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.6828 diff={max=06.6963, min=00.0086, mean=00.7354} policy_loss=-6.7801 policy updated! \n",
      "train step 02359 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0537 diff={max=05.1332, min=00.0392, mean=00.9556} policy_loss=-5.8006 policy updated! \n",
      "train step 02360 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.0640 diff={max=04.4389, min=00.0312, mean=00.9902} policy_loss=-6.0584 policy updated! \n",
      "train step 02361 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9162 diff={max=03.3246, min=00.0252, mean=00.6933} policy_loss=-6.6412 policy updated! \n",
      "train step 02362 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2504 diff={max=08.5075, min=00.0011, mean=01.1943} policy_loss=-5.4552 policy updated! \n",
      "train step 02363 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3760 diff={max=06.4415, min=00.0099, mean=01.1017} policy_loss=-6.7422 policy updated! \n",
      "train step 02364 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7990 diff={max=04.2461, min=00.0279, mean=00.8797} policy_loss=-6.5325 policy updated! \n",
      "train step 02365 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.2333 diff={max=04.0183, min=00.0285, mean=00.7256} policy_loss=-6.6297 policy updated! \n",
      "train step 02366 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5597 diff={max=04.1699, min=00.0346, mean=00.9357} policy_loss=-6.5386 policy updated! \n",
      "train step 02367 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5339 diff={max=04.2431, min=00.0188, mean=00.8245} policy_loss=-6.1669 policy updated! \n",
      "train step 02368 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.5062 diff={max=04.1897, min=00.0036, mean=00.8616} policy_loss=-6.1767 policy updated! \n",
      "train step 02369 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.6024 diff={max=03.9322, min=00.0024, mean=00.7796} policy_loss=-6.1023 policy updated! \n",
      "train step 02370 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=00.7932 diff={max=02.9665, min=00.0079, mean=00.6039} policy_loss=-5.9558 policy updated! \n",
      "train step 02371 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2868 diff={max=05.7961, min=00.0061, mean=00.9207} policy_loss=-7.2005 policy updated! \n",
      "train step 02372 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5823 diff={max=04.9816, min=00.0153, mean=00.7980} policy_loss=-5.7058 policy updated! \n",
      "train step 02373 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5008 diff={max=04.3984, min=00.0371, mean=00.8258} policy_loss=-7.3731 policy updated! \n",
      "train step 02374 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9202 diff={max=06.3354, min=00.0064, mean=00.9447} policy_loss=-7.0271 policy updated! \n",
      "train step 02375 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.5146 diff={max=04.8649, min=00.0094, mean=00.8259} policy_loss=-6.8748 policy updated! \n",
      "train step 02376 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8418 diff={max=04.1616, min=00.0012, mean=00.8429} policy_loss=-6.2531 policy updated! \n",
      "train step 02377 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2862 diff={max=05.1790, min=00.0144, mean=00.7238} policy_loss=-6.4292 policy updated! \n",
      "train step 02378 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.7947 diff={max=04.2481, min=00.0259, mean=00.9066} policy_loss=-6.1410 policy updated! \n",
      "train step 02379 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2974 diff={max=03.6018, min=00.0025, mean=00.8213} policy_loss=-6.7569 policy updated! \n",
      "train step 02380 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=00.7351 diff={max=03.1836, min=00.0031, mean=00.5972} policy_loss=-6.4391 policy updated! \n",
      "train step 02381 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.4950 diff={max=06.3332, min=00.0104, mean=01.2300} policy_loss=-6.4136 policy updated! \n",
      "train step 02382 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0451 diff={max=04.3433, min=00.0462, mean=00.7173} policy_loss=-6.9246 policy updated! \n",
      "train step 02383 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.9985 diff={max=04.3504, min=00.0053, mean=00.9185} policy_loss=-5.7176 policy updated! \n",
      "train step 02384 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.1283 diff={max=04.7905, min=00.0269, mean=00.9799} policy_loss=-6.7095 policy updated! \n",
      "train step 02385 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.2991 diff={max=03.9902, min=00.0181, mean=00.8314} policy_loss=-6.2693 policy updated! \n",
      "train step 02386 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5254 diff={max=05.2712, min=00.0075, mean=01.0474} policy_loss=-6.9001 policy updated! \n",
      "train step 02387 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6102 diff={max=04.4984, min=00.0296, mean=00.8352} policy_loss=-6.1906 policy updated! \n",
      "train step 02388 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8137 diff={max=05.0278, min=00.0020, mean=01.0825} policy_loss=-6.7188 policy updated! \n",
      "train step 02389 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5337 diff={max=05.4580, min=00.0124, mean=01.0508} policy_loss=-7.2508 policy updated! \n",
      "train step 02390 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.8195 diff={max=04.7060, min=00.0267, mean=00.9610} policy_loss=-7.5285 policy updated! \n",
      "train step 02391 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6282 diff={max=05.5877, min=00.0057, mean=00.7570} policy_loss=-6.4992 policy updated! \n",
      "train step 02392 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6813 diff={max=06.7010, min=00.0145, mean=01.1282} policy_loss=-6.5711 policy updated! \n",
      "train step 02393 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.6249 diff={max=05.7420, min=00.0524, mean=01.0457} policy_loss=-6.2503 policy updated! \n",
      "train step 02394 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.3478 diff={max=06.6509, min=00.0011, mean=00.7861} policy_loss=-6.1901 policy updated! \n",
      "train step 02395 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.9431 diff={max=05.2850, min=00.0462, mean=00.8626} policy_loss=-6.3498 policy updated! \n",
      "train step 02396 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.5985 diff={max=06.1988, min=00.0126, mean=00.7530} policy_loss=-7.0536 policy updated! \n",
      "train step 02397 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5546 diff={max=06.3425, min=00.0349, mean=00.9231} policy_loss=-6.9094 policy updated! \n",
      "train step 02398 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7170 diff={max=02.6486, min=00.0039, mean=00.5846} policy_loss=-6.5386 policy updated! \n",
      "train step 02399 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0301 diff={max=04.8398, min=00.0167, mean=01.0054} policy_loss=-6.5165 policy updated! \n",
      "train step 02400 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.7677 diff={max=05.8370, min=00.0017, mean=00.7989} policy_loss=-6.6128 policy updated! \n",
      "train step 02401 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.6616 diff={max=05.6162, min=00.0058, mean=00.8156} policy_loss=-6.5723 policy updated! \n",
      "train step 02402 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7057 diff={max=07.1953, min=00.0450, mean=01.1690} policy_loss=-6.6097 policy updated! \n",
      "train step 02403 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.9942 diff={max=06.0120, min=00.0491, mean=01.1171} policy_loss=-6.2796 policy updated! \n",
      "train step 02404 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2043 diff={max=05.8204, min=00.0016, mean=01.0196} policy_loss=-6.1660 policy updated! \n",
      "train step 02405 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.1940 diff={max=05.4447, min=00.0274, mean=01.2464} policy_loss=-6.6860 policy updated! \n",
      "train step 02406 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.2215 diff={max=03.9257, min=00.0079, mean=00.8117} policy_loss=-6.3944 policy updated! \n",
      "train step 02407 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2081 diff={max=03.6005, min=00.0011, mean=00.7701} policy_loss=-6.5196 policy updated! \n",
      "train step 02408 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0459 diff={max=05.2993, min=00.0777, mean=00.9520} policy_loss=-7.0167 policy updated! \n",
      "train step 02409 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2366 diff={max=03.5334, min=00.0004, mean=00.8520} policy_loss=-6.9163 policy updated! \n",
      "train step 02410 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.1460 diff={max=02.8525, min=00.0335, mean=00.8586} policy_loss=-6.8120 policy updated! \n",
      "train step 02411 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.8561 diff={max=06.6407, min=00.0224, mean=01.2231} policy_loss=-6.5347 policy updated! \n",
      "train step 02412 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8396 diff={max=03.0336, min=00.0220, mean=00.6667} policy_loss=-6.4838 policy updated! \n",
      "train step 02413 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0572 diff={max=05.4028, min=00.0020, mean=00.9677} policy_loss=-6.2051 policy updated! \n",
      "train step 02414 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3071 diff={max=03.0255, min=00.0313, mean=00.8810} policy_loss=-5.4987 policy updated! \n",
      "train step 02415 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.3669 diff={max=06.4618, min=00.0031, mean=00.6608} policy_loss=-5.7627 policy updated! \n",
      "train step 02416 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1660 diff={max=06.8267, min=00.0226, mean=00.8763} policy_loss=-6.6229 policy updated! \n",
      "train step 02417 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5857 diff={max=04.5913, min=00.0028, mean=00.8362} policy_loss=-6.9008 policy updated! \n",
      "train step 02418 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6389 diff={max=05.9050, min=00.0490, mean=00.7188} policy_loss=-6.2334 policy updated! \n",
      "train step 02419 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=02.1207 diff={max=03.6306, min=00.0178, mean=01.1087} policy_loss=-7.2697 policy updated! \n",
      "train step 02420 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.8878 diff={max=06.1830, min=00.0097, mean=00.8317} policy_loss=-6.4264 policy updated! \n",
      "train step 02421 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=03.5373 diff={max=05.5826, min=00.0212, mean=01.2763} policy_loss=-6.8051 policy updated! \n",
      "train step 02422 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1181 diff={max=05.2571, min=00.0617, mean=01.3181} policy_loss=-7.8835 policy updated! \n",
      "train step 02423 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.0469 diff={max=03.9347, min=00.0329, mean=01.0577} policy_loss=-6.8648 policy updated! \n",
      "train step 02424 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0636 diff={max=04.9015, min=00.0262, mean=00.9715} policy_loss=-6.8423 policy updated! \n",
      "train step 02425 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.0200 diff={max=06.1378, min=00.0108, mean=01.0476} policy_loss=-6.5406 policy updated! \n",
      "train step 02426 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8079 diff={max=05.5038, min=00.0015, mean=00.8320} policy_loss=-6.0166 policy updated! \n",
      "train step 02427 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6673 diff={max=04.6260, min=00.0217, mean=00.9273} policy_loss=-5.8103 policy updated! \n",
      "train step 02428 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.1950 diff={max=04.7943, min=00.0355, mean=01.1041} policy_loss=-6.2934 policy updated! \n",
      "train step 02429 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0815 diff={max=03.1314, min=00.0186, mean=00.8461} policy_loss=-6.9944 policy updated! \n",
      "train step 02430 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.2502 diff={max=07.0077, min=00.0176, mean=01.0964} policy_loss=-6.9308 policy updated! \n",
      "train step 02431 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9467 diff={max=06.4074, min=00.0665, mean=01.0701} policy_loss=-5.8243 policy updated! \n",
      "train step 02432 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.5549 diff={max=03.3064, min=00.0130, mean=00.9891} policy_loss=-6.6649 policy updated! \n",
      "train step 02433 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0580 diff={max=05.6311, min=00.0084, mean=01.1040} policy_loss=-6.0625 policy updated! \n",
      "train step 02434 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8232 diff={max=03.9711, min=00.0016, mean=00.9484} policy_loss=-7.4232 policy updated! \n",
      "train step 02435 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.6136 diff={max=04.4851, min=00.0051, mean=00.9352} policy_loss=-6.4186 policy updated! \n",
      "train step 02436 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1959 diff={max=03.8281, min=00.0001, mean=00.7858} policy_loss=-5.3890 policy updated! \n",
      "train step 02437 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6557 diff={max=04.9390, min=00.0129, mean=00.9560} policy_loss=-6.4367 policy updated! \n",
      "train step 02438 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=03.2153 diff={max=06.5854, min=00.0120, mean=01.2548} policy_loss=-6.6860 policy updated! \n",
      "train step 02439 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.9904 diff={max=06.2070, min=00.0619, mean=00.9773} policy_loss=-6.7394 policy updated! \n",
      "train step 02440 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.7733 diff={max=06.7793, min=00.0018, mean=00.7515} policy_loss=-6.2630 policy updated! \n",
      "train step 02441 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3253 diff={max=03.4344, min=00.0138, mean=00.7742} policy_loss=-6.2306 policy updated! \n",
      "train step 02442 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2451 diff={max=04.7290, min=00.0474, mean=01.0513} policy_loss=-6.5285 policy updated! \n",
      "train step 02443 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0335 diff={max=06.7590, min=00.0349, mean=01.1109} policy_loss=-6.4342 policy updated! \n",
      "train step 02444 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=01.6940 diff={max=04.4377, min=00.0531, mean=00.8229} policy_loss=-6.0471 policy updated! \n",
      "train step 02445 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.2555 diff={max=03.2394, min=00.0082, mean=00.8468} policy_loss=-5.8619 policy updated! \n",
      "train step 02446 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6282 diff={max=05.4408, min=00.0000, mean=00.7340} policy_loss=-6.6313 policy updated! \n",
      "train step 02447 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9492 diff={max=04.7668, min=00.0469, mean=00.9261} policy_loss=-6.0842 policy updated! \n",
      "train step 02448 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1444 diff={max=04.9921, min=00.0222, mean=00.7188} policy_loss=-6.9754 policy updated! \n",
      "train step 02449 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.4589 diff={max=05.2312, min=00.0062, mean=00.9898} policy_loss=-6.7120 policy updated! \n",
      "train step 02450 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.1621 diff={max=07.2299, min=00.0228, mean=00.7681} policy_loss=-6.5413 policy updated! \n",
      "train step 02451 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9562 diff={max=06.1582, min=00.0299, mean=00.7854} policy_loss=-6.1331 policy updated! \n",
      "train step 02452 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8901 diff={max=07.1426, min=00.0076, mean=00.7954} policy_loss=-6.8386 policy updated! \n",
      "train step 02453 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3235 diff={max=06.0286, min=00.0197, mean=01.0579} policy_loss=-7.1203 policy updated! \n",
      "train step 02454 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7625 diff={max=02.9576, min=00.0201, mean=00.6308} policy_loss=-7.2786 policy updated! \n",
      "train step 02455 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.8421 diff={max=06.3594, min=00.0564, mean=01.0414} policy_loss=-5.8836 policy updated! \n",
      "train step 02456 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4425 diff={max=03.2704, min=00.0061, mean=00.8652} policy_loss=-7.4594 policy updated! \n",
      "train step 02457 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9654 diff={max=03.2279, min=00.0006, mean=00.6952} policy_loss=-5.7018 policy updated! \n",
      "train step 02458 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.3118 diff={max=05.7625, min=00.0240, mean=01.2854} policy_loss=-6.8854 policy updated! \n",
      "train step 02459 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=00.5836 diff={max=02.1307, min=00.0006, mean=00.6057} policy_loss=-6.4967 policy updated! \n",
      "train step 02460 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.3232 diff={max=06.6864, min=00.0089, mean=00.9178} policy_loss=-7.1601 policy updated! \n",
      "train step 02461 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8600 diff={max=03.1059, min=00.0172, mean=00.6786} policy_loss=-5.9794 policy updated! \n",
      "train step 02462 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6109 diff={max=04.7743, min=00.0070, mean=00.8706} policy_loss=-6.8143 policy updated! \n",
      "train step 02463 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2881 diff={max=04.8916, min=00.0298, mean=00.9393} policy_loss=-6.7767 policy updated! \n",
      "train step 02464 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=02.1172 diff={max=05.7519, min=00.0065, mean=00.8773} policy_loss=-6.6075 policy updated! \n",
      "train step 02465 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=00.9665 diff={max=03.5963, min=00.0259, mean=00.6965} policy_loss=-6.2104 policy updated! \n",
      "train step 02466 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0395 diff={max=04.5573, min=00.0055, mean=01.0138} policy_loss=-6.4403 policy updated! \n",
      "train step 02467 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2310 diff={max=06.3554, min=00.0053, mean=01.2968} policy_loss=-6.2113 policy updated! \n",
      "train step 02468 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8995 diff={max=02.5388, min=00.0055, mean=00.6620} policy_loss=-7.8462 policy updated! \n",
      "train step 02469 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2541 diff={max=03.7042, min=00.0007, mean=00.7747} policy_loss=-7.8036 policy updated! \n",
      "train step 02470 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.9067 diff={max=03.1710, min=00.0626, mean=01.0808} policy_loss=-6.1799 policy updated! \n",
      "train step 02471 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8751 diff={max=04.4930, min=00.0340, mean=00.9038} policy_loss=-7.9212 policy updated! \n",
      "train step 02472 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6246 diff={max=04.0751, min=00.0108, mean=00.8569} policy_loss=-6.4113 policy updated! \n",
      "train step 02473 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6695 diff={max=06.7302, min=00.0013, mean=01.0487} policy_loss=-6.6444 policy updated! \n",
      "train step 02474 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0050 diff={max=04.9468, min=00.0117, mean=00.9071} policy_loss=-7.1551 policy updated! \n",
      "train step 02475 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.4340 diff={max=05.8766, min=00.0111, mean=00.7535} policy_loss=-6.3689 policy updated! \n",
      "train step 02476 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8376 diff={max=05.4787, min=00.0056, mean=01.0715} policy_loss=-5.7350 policy updated! \n",
      "train step 02477 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3028 diff={max=03.2476, min=00.0718, mean=00.8497} policy_loss=-6.3983 policy updated! \n",
      "train step 02478 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2726 diff={max=03.8252, min=00.0139, mean=00.7655} policy_loss=-6.5482 policy updated! \n",
      "train step 02479 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4648 diff={max=04.3450, min=00.0235, mean=00.7745} policy_loss=-5.8267 policy updated! \n",
      "train step 02480 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.4094 diff={max=03.9122, min=00.0097, mean=00.7894} policy_loss=-6.4331 policy updated! \n",
      "train step 02481 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.7623 diff={max=06.1113, min=00.0191, mean=00.8831} policy_loss=-6.8939 policy updated! \n",
      "train step 02482 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=00.6915 diff={max=03.1349, min=00.0164, mean=00.6083} policy_loss=-5.8216 policy updated! \n",
      "train step 02483 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6242 diff={max=04.3020, min=00.0161, mean=00.8568} policy_loss=-6.2312 policy updated! \n",
      "train step 02484 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.0897 diff={max=03.3666, min=00.0219, mean=00.7640} policy_loss=-6.7246 policy updated! \n",
      "train step 02485 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.8575 diff={max=05.7859, min=00.0224, mean=00.8538} policy_loss=-6.5854 policy updated! \n",
      "train step 02486 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2127 diff={max=05.6560, min=00.0108, mean=01.1825} policy_loss=-6.5918 policy updated! \n",
      "train step 02487 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6270 diff={max=04.4813, min=00.0014, mean=00.8721} policy_loss=-5.9061 policy updated! \n",
      "train step 02488 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.8812 diff={max=05.6418, min=00.0298, mean=00.8793} policy_loss=-6.9143 policy updated! \n",
      "train step 02489 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3873 diff={max=05.2590, min=00.0164, mean=01.2697} policy_loss=-6.9514 policy updated! \n",
      "train step 02490 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=01.7618 diff={max=04.9608, min=00.0077, mean=00.7577} policy_loss=-7.3534 policy updated! \n",
      "train step 02491 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1080 diff={max=05.7663, min=00.0127, mean=00.8539} policy_loss=-7.6616 policy updated! \n",
      "train step 02492 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3673 diff={max=06.2111, min=00.0115, mean=00.9398} policy_loss=-6.7998 policy updated! \n",
      "train step 02493 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9197 diff={max=08.3653, min=00.0305, mean=01.0562} policy_loss=-6.7533 policy updated! \n",
      "train step 02494 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1847 diff={max=07.1871, min=00.0530, mean=01.0858} policy_loss=-6.9144 policy updated! \n",
      "train step 02495 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=00.7823 diff={max=02.8664, min=00.0098, mean=00.6216} policy_loss=-6.1769 policy updated! \n",
      "train step 02496 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2137 diff={max=07.0835, min=00.0028, mean=01.0099} policy_loss=-4.6061 policy updated! \n",
      "train step 02497 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2817 diff={max=07.5034, min=00.0519, mean=01.2928} policy_loss=-6.0716 policy updated! \n",
      "train step 02498 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5075 diff={max=03.4685, min=00.0204, mean=00.9496} policy_loss=-6.6648 policy updated! \n",
      "train step 02499 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2548 diff={max=07.4018, min=00.0298, mean=01.1696} policy_loss=-6.4853 policy updated! \n",
      "train step 02500 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.1805 diff={max=05.2004, min=00.0091, mean=00.9317} policy_loss=-7.0459 policy updated! \n",
      "train step 02501 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4751 diff={max=04.3331, min=00.0420, mean=00.9701} policy_loss=-7.1097 policy updated! \n",
      "train step 02502 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8514 diff={max=05.7712, min=00.0030, mean=00.9887} policy_loss=-7.3467 policy updated! \n",
      "train step 02503 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.0045 diff={max=06.1132, min=00.0121, mean=00.9419} policy_loss=-6.6043 policy updated! \n",
      "train step 02504 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0278 diff={max=04.1999, min=00.0521, mean=01.0278} policy_loss=-6.4700 policy updated! \n",
      "train step 02505 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.5546 diff={max=05.9503, min=00.0035, mean=01.0180} policy_loss=-6.1977 policy updated! \n",
      "train step 02506 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6028 diff={max=03.7597, min=00.0046, mean=00.9895} policy_loss=-6.5601 policy updated! \n",
      "train step 02507 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7642 diff={max=06.3827, min=00.0443, mean=01.6760} policy_loss=-7.5680 policy updated! \n",
      "train step 02508 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.8394 diff={max=04.7320, min=00.0113, mean=00.9257} policy_loss=-6.5020 policy updated! \n",
      "train step 02509 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6071 diff={max=06.9116, min=00.0210, mean=01.0769} policy_loss=-7.2828 policy updated! \n",
      "train step 02510 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.3031 diff={max=06.9960, min=00.0260, mean=01.2903} policy_loss=-7.1107 policy updated! \n",
      "train step 02511 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9634 diff={max=03.7010, min=00.0669, mean=01.0488} policy_loss=-6.8241 policy updated! \n",
      "train step 02512 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5821 diff={max=03.6236, min=00.0509, mean=01.2860} policy_loss=-8.1010 policy updated! \n",
      "train step 02513 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8625 diff={max=07.6204, min=00.0011, mean=01.0211} policy_loss=-5.9738 policy updated! \n",
      "train step 02514 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5473 diff={max=05.2173, min=00.0088, mean=00.8749} policy_loss=-6.5858 policy updated! \n",
      "train step 02515 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.1649 diff={max=05.0691, min=00.0015, mean=00.9215} policy_loss=-5.9805 policy updated! \n",
      "train step 02516 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5949 diff={max=06.9897, min=00.0321, mean=01.3422} policy_loss=-6.5448 policy updated! \n",
      "train step 02517 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6687 diff={max=06.1359, min=00.0048, mean=01.5348} policy_loss=-6.4681 policy updated! \n",
      "train step 02518 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.2916 diff={max=04.3916, min=00.0361, mean=00.8495} policy_loss=-7.7195 policy updated! \n",
      "train step 02519 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.1940 diff={max=07.1746, min=00.0148, mean=00.9354} policy_loss=-7.9132 policy updated! \n",
      "train step 02520 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.9776 diff={max=06.0895, min=00.0126, mean=01.1570} policy_loss=-7.3751 policy updated! \n",
      "train step 02521 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1657 diff={max=05.0181, min=00.0241, mean=01.5903} policy_loss=-7.6854 policy updated! \n",
      "train step 02522 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8091 diff={max=03.3842, min=00.0964, mean=01.0343} policy_loss=-6.1441 policy updated! \n",
      "train step 02523 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0559 diff={max=03.9257, min=00.0017, mean=01.0719} policy_loss=-5.7299 policy updated! \n",
      "train step 02524 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7039 diff={max=05.3260, min=00.0160, mean=00.8483} policy_loss=-6.8519 policy updated! \n",
      "train step 02525 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.1431 diff={max=03.2600, min=00.0308, mean=00.7469} policy_loss=-6.4165 policy updated! \n",
      "train step 02526 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.3092 diff={max=04.3028, min=00.0174, mean=01.1591} policy_loss=-6.4091 policy updated! \n",
      "train step 02527 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9497 diff={max=06.3396, min=00.0033, mean=01.0462} policy_loss=-7.1206 policy updated! \n",
      "train step 02528 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8922 diff={max=03.7658, min=00.0369, mean=00.6517} policy_loss=-7.1917 policy updated! \n",
      "train step 02529 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3632 diff={max=05.4508, min=00.0026, mean=00.9728} policy_loss=-6.5474 policy updated! \n",
      "train step 02530 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.1277 diff={max=04.7577, min=00.0222, mean=00.6577} policy_loss=-5.9878 policy updated! \n",
      "train step 02531 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4425 diff={max=04.9873, min=00.0209, mean=00.8525} policy_loss=-6.0821 policy updated! \n",
      "train step 02532 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6240 diff={max=03.9581, min=00.0111, mean=00.8860} policy_loss=-6.2703 policy updated! \n",
      "train step 02533 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6782 diff={max=02.9399, min=00.0037, mean=01.0478} policy_loss=-6.4691 policy updated! \n",
      "train step 02534 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=00.7754 diff={max=02.2755, min=00.0158, mean=00.7012} policy_loss=-6.6525 policy updated! \n",
      "train step 02535 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.6878 diff={max=08.7582, min=00.0403, mean=01.1274} policy_loss=-7.2451 policy updated! \n",
      "train step 02536 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=04.4285 diff={max=06.6533, min=00.0115, mean=01.3212} policy_loss=-7.6574 policy updated! \n",
      "train step 02537 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6018 diff={max=06.6007, min=00.0210, mean=01.1218} policy_loss=-6.2489 policy updated! \n",
      "train step 02538 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1525 diff={max=05.5176, min=00.0429, mean=01.1765} policy_loss=-6.4757 policy updated! \n",
      "train step 02539 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=03.2443 diff={max=05.6253, min=00.0338, mean=01.2478} policy_loss=-6.7297 policy updated! \n",
      "train step 02540 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=02.6711 diff={max=08.4213, min=00.0179, mean=00.9707} policy_loss=-6.9652 policy updated! \n",
      "train step 02541 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=02.1711 diff={max=05.3827, min=00.0375, mean=00.9750} policy_loss=-6.9536 policy updated! \n",
      "train step 02542 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0524 diff={max=05.9766, min=00.0128, mean=01.0910} policy_loss=-7.1924 policy updated! \n",
      "train step 02543 reward={max=08.0000, min=00.0000, mean=03.2000} optimizing loss=02.3856 diff={max=05.4712, min=00.0082, mean=00.9305} policy_loss=-6.6040 policy updated! \n",
      "train step 02544 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6763 diff={max=04.0136, min=00.0540, mean=00.9184} policy_loss=-6.8003 policy updated! \n",
      "train step 02545 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.1028 diff={max=05.2953, min=00.0188, mean=01.2328} policy_loss=-6.9660 policy updated! \n",
      "train step 02546 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6723 diff={max=05.2431, min=00.0475, mean=00.9003} policy_loss=-7.3571 policy updated! \n",
      "train step 02547 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1128 diff={max=04.9447, min=00.0010, mean=01.0479} policy_loss=-6.7281 policy updated! \n",
      "train step 02548 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7093 diff={max=05.8680, min=00.0002, mean=00.7698} policy_loss=-6.2996 policy updated! \n",
      "train step 02549 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=01.8616 diff={max=05.3966, min=00.0243, mean=00.9311} policy_loss=-5.1892 policy updated! \n",
      "train step 02550 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.8485 diff={max=04.1484, min=00.0144, mean=01.0185} policy_loss=-6.0978 policy updated! \n",
      "train step 02551 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5601 diff={max=09.5952, min=00.0075, mean=01.4017} policy_loss=-7.1040 policy updated! \n",
      "train step 02552 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9734 diff={max=02.5595, min=00.0120, mean=00.7708} policy_loss=-6.8997 policy updated! \n",
      "train step 02553 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5922 diff={max=03.5300, min=00.0068, mean=00.9619} policy_loss=-6.4004 policy updated! \n",
      "train step 02554 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4400 diff={max=07.4315, min=00.0077, mean=00.8987} policy_loss=-6.0394 policy updated! \n",
      "train step 02555 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.0317 diff={max=05.0069, min=00.0078, mean=00.9433} policy_loss=-6.3737 policy updated! \n",
      "train step 02556 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1736 diff={max=05.3464, min=00.0165, mean=00.9926} policy_loss=-7.5023 policy updated! \n",
      "train step 02557 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3144 diff={max=03.7046, min=00.0043, mean=00.8329} policy_loss=-6.2985 policy updated! \n",
      "train step 02558 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2392 diff={max=04.6700, min=00.0360, mean=00.9531} policy_loss=-7.0391 policy updated! \n",
      "train step 02559 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3803 diff={max=05.2738, min=00.0085, mean=00.9780} policy_loss=-6.6588 policy updated! \n",
      "train step 02560 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.7127 diff={max=09.1959, min=00.0094, mean=01.1620} policy_loss=-6.5886 policy updated! \n",
      "train step 02561 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=05.1431 diff={max=06.1743, min=00.0430, mean=01.6080} policy_loss=-7.6629 policy updated! \n",
      "train step 02562 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0939 diff={max=05.3140, min=00.0197, mean=00.9032} policy_loss=-7.2208 policy updated! \n",
      "train step 02563 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1412 diff={max=04.3921, min=00.0516, mean=01.0021} policy_loss=-5.6090 policy updated! \n",
      "train step 02564 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7643 diff={max=06.3752, min=00.0024, mean=01.0130} policy_loss=-6.2736 policy updated! \n",
      "train step 02565 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=02.3636 diff={max=05.5268, min=00.0123, mean=01.0591} policy_loss=-5.1606 policy updated! \n",
      "train step 02566 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8666 diff={max=04.2186, min=00.0130, mean=00.9634} policy_loss=-5.8007 policy updated! \n",
      "train step 02567 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7842 diff={max=06.2399, min=00.0126, mean=01.1958} policy_loss=-5.8446 policy updated! \n",
      "train step 02568 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8069 diff={max=05.5296, min=00.0496, mean=01.1571} policy_loss=-6.1612 policy updated! \n",
      "train step 02569 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8749 diff={max=05.8489, min=00.0030, mean=01.0816} policy_loss=-6.7603 policy updated! \n",
      "train step 02570 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.8409 diff={max=03.2358, min=00.0138, mean=01.0336} policy_loss=-7.6909 policy updated! \n",
      "train step 02571 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9933 diff={max=05.8650, min=00.0145, mean=00.9742} policy_loss=-7.1787 policy updated! \n",
      "train step 02572 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8137 diff={max=04.8303, min=00.0047, mean=01.0189} policy_loss=-6.6985 policy updated! \n",
      "train step 02573 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3556 diff={max=06.6496, min=00.0008, mean=01.1934} policy_loss=-7.1100 policy updated! \n",
      "train step 02574 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1941 diff={max=05.1471, min=00.0014, mean=01.3250} policy_loss=-6.8077 policy updated! \n",
      "train step 02575 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.2109 diff={max=06.3380, min=00.0597, mean=01.1187} policy_loss=-5.9268 policy updated! \n",
      "train step 02576 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4719 diff={max=05.8917, min=00.0040, mean=01.0668} policy_loss=-5.8173 policy updated! \n",
      "train step 02577 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3915 diff={max=06.4785, min=00.0173, mean=01.1372} policy_loss=-5.4293 policy updated! \n",
      "train step 02578 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7295 diff={max=05.7828, min=00.0678, mean=01.4459} policy_loss=-7.3692 policy updated! \n",
      "train step 02579 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3417 diff={max=06.0925, min=00.0187, mean=01.0518} policy_loss=-7.3496 policy updated! \n",
      "train step 02580 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.8150 diff={max=05.5865, min=00.0148, mean=00.8975} policy_loss=-7.4279 policy updated! \n",
      "train step 02581 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=01.9991 diff={max=06.4646, min=00.0114, mean=00.9969} policy_loss=-8.1371 policy updated! \n",
      "train step 02582 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4269 diff={max=04.2387, min=00.0637, mean=01.1920} policy_loss=-7.3874 policy updated! \n",
      "train step 02583 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4530 diff={max=04.1748, min=00.0211, mean=00.8465} policy_loss=-5.6164 policy updated! \n",
      "train step 02584 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8394 diff={max=03.2268, min=00.0120, mean=00.6720} policy_loss=-6.7133 policy updated! \n",
      "train step 02585 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.1038 diff={max=02.5243, min=00.0087, mean=00.8075} policy_loss=-5.8482 policy updated! \n",
      "train step 02586 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=01.7430 diff={max=03.9994, min=00.0002, mean=00.9968} policy_loss=-6.6104 policy updated! \n",
      "train step 02587 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9652 diff={max=06.4815, min=00.0123, mean=00.9736} policy_loss=-6.1534 policy updated! \n",
      "train step 02588 reward={max=09.0000, min=09.0000, mean=09.0000} optimizing loss=02.2336 diff={max=04.4057, min=00.0312, mean=01.0570} policy_loss=-7.0701 policy updated! \n",
      "train step 02589 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5232 diff={max=06.1695, min=00.0485, mean=01.0279} policy_loss=-7.0848 policy updated! \n",
      "train step 02590 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.4785 diff={max=09.9324, min=00.0263, mean=01.1651} policy_loss=-7.1152 policy updated! \n",
      "train step 02591 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0232 diff={max=05.3687, min=00.0024, mean=00.9279} policy_loss=-5.7930 policy updated! \n",
      "train step 02592 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4595 diff={max=03.6158, min=00.0341, mean=00.8646} policy_loss=-7.4434 policy updated! \n",
      "train step 02593 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=01.6101 diff={max=03.2691, min=00.0458, mean=00.9163} policy_loss=-6.9922 policy updated! \n",
      "train step 02594 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2142 diff={max=05.2848, min=00.0021, mean=00.9562} policy_loss=-6.3511 policy updated! \n",
      "train step 02595 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.0730 diff={max=05.8470, min=00.0180, mean=00.8595} policy_loss=-7.1912 policy updated! \n",
      "train step 02596 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5056 diff={max=06.2745, min=00.0018, mean=01.0186} policy_loss=-5.9766 policy updated! \n",
      "train step 02597 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2686 diff={max=05.3208, min=00.0112, mean=00.9478} policy_loss=-6.3461 policy updated! \n",
      "train step 02598 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9727 diff={max=03.9963, min=00.0042, mean=00.9689} policy_loss=-6.9173 policy updated! \n",
      "train step 02599 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6305 diff={max=05.2699, min=00.0016, mean=00.8699} policy_loss=-6.3110 policy updated! \n",
      "train step 02600 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.4179 diff={max=03.9451, min=00.0082, mean=00.8909} policy_loss=-7.3393 policy updated! \n",
      "train step 02601 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0484 diff={max=03.6041, min=00.0145, mean=00.7075} policy_loss=-7.8762 policy updated! \n",
      "train step 02602 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5471 diff={max=06.6673, min=00.0000, mean=00.9406} policy_loss=-7.1467 policy updated! \n",
      "train step 02603 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3585 diff={max=04.1274, min=00.0769, mean=01.1466} policy_loss=-6.6326 policy updated! \n",
      "train step 02604 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1169 diff={max=03.0263, min=00.0313, mean=00.7915} policy_loss=-7.3284 policy updated! \n",
      "train step 02605 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.6126 diff={max=04.4686, min=00.0175, mean=01.1398} policy_loss=-6.3521 policy updated! \n",
      "train step 02606 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7011 diff={max=05.1220, min=00.0431, mean=00.8455} policy_loss=-6.7131 policy updated! \n",
      "train step 02607 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5354 diff={max=06.4887, min=00.0244, mean=01.0225} policy_loss=-7.6773 policy updated! \n",
      "train step 02608 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4639 diff={max=05.0568, min=00.0026, mean=00.7607} policy_loss=-7.1872 policy updated! \n",
      "train step 02609 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=02.0158 diff={max=03.7050, min=00.0080, mean=01.1008} policy_loss=-7.0780 policy updated! \n",
      "train step 02610 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.9263 diff={max=07.3219, min=00.0016, mean=00.7505} policy_loss=-7.1743 policy updated! \n",
      "train step 02611 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7409 diff={max=04.2951, min=00.0194, mean=00.9087} policy_loss=-7.5235 policy updated! \n",
      "train step 02612 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0913 diff={max=04.2445, min=00.0026, mean=01.0529} policy_loss=-6.5259 policy updated! \n",
      "train step 02613 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9439 diff={max=03.7184, min=00.0016, mean=00.6681} policy_loss=-6.3265 policy updated! \n",
      "train step 02614 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5747 diff={max=06.5950, min=00.0131, mean=01.2358} policy_loss=-5.9944 policy updated! \n",
      "train step 02615 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.5397 diff={max=06.8978, min=00.0322, mean=01.2840} policy_loss=-5.4042 policy updated! \n",
      "train step 02616 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1272 diff={max=07.6110, min=00.0135, mean=01.6561} policy_loss=-7.1253 policy updated! \n",
      "train step 02617 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2891 diff={max=06.6281, min=00.0646, mean=01.1562} policy_loss=-6.5696 policy updated! \n",
      "train step 02618 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0804 diff={max=04.1704, min=00.0439, mean=01.0842} policy_loss=-7.0659 policy updated! \n",
      "train step 02619 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3435 diff={max=06.6576, min=00.0289, mean=01.4076} policy_loss=-7.6919 policy updated! \n",
      "train step 02620 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.0698 diff={max=05.0339, min=00.0480, mean=01.0887} policy_loss=-7.8029 policy updated! \n",
      "train step 02621 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7613 diff={max=09.3181, min=00.0116, mean=01.4029} policy_loss=-7.3208 policy updated! \n",
      "train step 02622 reward={max=08.0000, min=08.0000, mean=08.0000} optimizing loss=02.9352 diff={max=04.9320, min=00.0073, mean=01.1939} policy_loss=-6.4933 policy updated! \n",
      "train step 02623 reward={max=08.0000, min=00.0000, mean=03.2000} optimizing loss=01.7808 diff={max=03.9253, min=00.0198, mean=00.9329} policy_loss=-6.5518 policy updated! \n",
      "train step 02624 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3830 diff={max=04.5300, min=00.0239, mean=01.5158} policy_loss=-6.5412 policy updated! \n",
      "train step 02625 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.5155 diff={max=06.6114, min=00.0317, mean=01.3366} policy_loss=-6.1104 policy updated! \n",
      "train step 02626 reward={max=09.0000, min=00.0000, mean=05.2000} optimizing loss=02.4094 diff={max=04.7598, min=00.0464, mean=01.0927} policy_loss=-5.5424 policy updated! \n",
      "train step 02627 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3033 diff={max=03.8011, min=00.0031, mean=01.1890} policy_loss=-6.9650 policy updated! \n",
      "train step 02628 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0543 diff={max=06.8275, min=00.0069, mean=01.0303} policy_loss=-6.9331 policy updated! \n",
      "train step 02629 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8202 diff={max=03.7800, min=00.0081, mean=00.9697} policy_loss=-7.3599 policy updated! \n",
      "train step 02630 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.3429 diff={max=06.6294, min=00.0251, mean=01.2422} policy_loss=-6.9313 policy updated! \n",
      "train step 02631 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3863 diff={max=03.8383, min=00.0422, mean=01.1484} policy_loss=-7.5311 policy updated! \n",
      "train step 02632 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4430 diff={max=05.6646, min=00.0600, mean=01.0481} policy_loss=-5.9417 policy updated! \n",
      "train step 02633 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8919 diff={max=05.3978, min=00.0080, mean=00.8896} policy_loss=-6.3600 policy updated! \n",
      "train step 02634 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3506 diff={max=04.9597, min=00.0026, mean=01.1179} policy_loss=-6.0069 policy updated! \n",
      "train step 02635 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.9150 diff={max=06.0095, min=00.0153, mean=01.1757} policy_loss=-5.3159 policy updated! \n",
      "train step 02636 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4823 diff={max=06.6291, min=00.0072, mean=01.4566} policy_loss=-7.4709 policy updated! \n",
      "train step 02637 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3894 diff={max=04.7345, min=00.0215, mean=01.0488} policy_loss=-6.6820 policy updated! \n",
      "train step 02638 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6478 diff={max=04.8565, min=00.0369, mean=00.8430} policy_loss=-7.0658 policy updated! \n",
      "train step 02639 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3459 diff={max=06.2666, min=00.0200, mean=00.9814} policy_loss=-6.6686 policy updated! \n",
      "train step 02640 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.6043 diff={max=04.3034, min=00.0446, mean=00.9061} policy_loss=-6.9570 policy updated! \n",
      "train step 02641 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8906 diff={max=04.6339, min=00.0325, mean=00.9783} policy_loss=-7.0529 policy updated! \n",
      "train step 02642 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2947 diff={max=04.9763, min=00.0122, mean=00.8570} policy_loss=-8.2187 policy updated! \n",
      "train step 02643 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=02.0688 diff={max=04.5712, min=00.0009, mean=00.8847} policy_loss=-6.6170 policy updated! \n",
      "train step 02644 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4481 diff={max=04.3377, min=00.0049, mean=00.7192} policy_loss=-6.1725 policy updated! \n",
      "train step 02645 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.8660 diff={max=04.2990, min=00.0329, mean=01.0151} policy_loss=-7.1212 policy updated! \n",
      "train step 02646 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=00.8394 diff={max=04.1866, min=00.0022, mean=00.5905} policy_loss=-5.8354 policy updated! \n",
      "train step 02647 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8401 diff={max=05.6968, min=00.0260, mean=00.8994} policy_loss=-6.9815 policy updated! \n",
      "train step 02648 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5234 diff={max=05.0419, min=00.0667, mean=00.9919} policy_loss=-7.6715 policy updated! \n",
      "train step 02649 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9379 diff={max=03.4715, min=00.0106, mean=01.0273} policy_loss=-7.8924 policy updated! \n",
      "train step 02650 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.8687 diff={max=05.3319, min=00.0438, mean=00.8367} policy_loss=-6.6906 policy updated! \n",
      "train step 02651 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3644 diff={max=03.9472, min=00.0276, mean=00.8575} policy_loss=-7.3956 policy updated! \n",
      "train step 02652 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6329 diff={max=06.6943, min=00.0410, mean=01.2521} policy_loss=-6.4506 policy updated! \n",
      "train step 02653 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1157 diff={max=06.2081, min=00.0097, mean=00.9597} policy_loss=-7.3338 policy updated! \n",
      "train step 02654 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8923 diff={max=06.5885, min=00.0017, mean=01.0612} policy_loss=-7.5917 policy updated! \n",
      "train step 02655 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.5734 diff={max=04.3270, min=00.0103, mean=00.8514} policy_loss=-6.3267 policy updated! \n",
      "train step 02656 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4369 diff={max=06.1961, min=00.0153, mean=01.0123} policy_loss=-6.6410 policy updated! \n",
      "train step 02657 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8825 diff={max=06.3781, min=00.0056, mean=01.0788} policy_loss=-6.9291 policy updated! \n",
      "train step 02658 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7979 diff={max=05.3421, min=00.0018, mean=00.8266} policy_loss=-6.6044 policy updated! \n",
      "train step 02659 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3185 diff={max=05.7610, min=00.0042, mean=00.9202} policy_loss=-6.9235 policy updated! \n",
      "train step 02660 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.6579 diff={max=05.6428, min=00.0122, mean=00.8784} policy_loss=-6.8116 policy updated! \n",
      "train step 02661 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8903 diff={max=04.4788, min=00.0347, mean=00.9813} policy_loss=-7.0624 policy updated! \n",
      "train step 02662 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0108 diff={max=06.1132, min=00.0025, mean=00.8875} policy_loss=-6.0316 policy updated! \n",
      "train step 02663 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3516 diff={max=05.1157, min=00.0106, mean=01.0737} policy_loss=-6.7182 policy updated! \n",
      "train step 02664 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1619 diff={max=04.3583, min=00.0082, mean=00.7224} policy_loss=-5.8914 policy updated! \n",
      "train step 02665 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.8746 diff={max=07.8783, min=00.0030, mean=00.7774} policy_loss=-7.1124 policy updated! \n",
      "train step 02666 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8850 diff={max=06.2199, min=00.0466, mean=01.1832} policy_loss=-6.5600 policy updated! \n",
      "train step 02667 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7834 diff={max=05.4446, min=00.0185, mean=00.9475} policy_loss=-6.1957 policy updated! \n",
      "train step 02668 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8223 diff={max=06.1238, min=00.0742, mean=01.2273} policy_loss=-6.3637 policy updated! \n",
      "train step 02669 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0959 diff={max=04.9399, min=00.0210, mean=01.0740} policy_loss=-7.6389 policy updated! \n",
      "train step 02670 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.1091 diff={max=05.5106, min=00.0292, mean=00.9127} policy_loss=-7.8943 policy updated! \n",
      "train step 02671 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=02.1160 diff={max=06.4072, min=00.0408, mean=00.9481} policy_loss=-6.4289 policy updated! \n",
      "train step 02672 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7745 diff={max=03.7260, min=00.0604, mean=01.0264} policy_loss=-8.2418 policy updated! \n",
      "train step 02673 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2568 diff={max=05.6910, min=00.0213, mean=00.8982} policy_loss=-6.1369 policy updated! \n",
      "train step 02674 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8542 diff={max=05.7413, min=00.0157, mean=00.9090} policy_loss=-6.8009 policy updated! \n",
      "train step 02675 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.2656 diff={max=04.3877, min=00.0019, mean=00.7159} policy_loss=-6.7443 policy updated! \n",
      "train step 02676 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4137 diff={max=07.2247, min=00.0007, mean=01.1159} policy_loss=-7.1691 policy updated! \n",
      "train step 02677 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7160 diff={max=06.5813, min=00.0253, mean=01.3071} policy_loss=-6.1267 policy updated! \n",
      "train step 02678 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6081 diff={max=03.5881, min=00.0055, mean=00.9066} policy_loss=-7.0901 policy updated! \n",
      "train step 02679 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9364 diff={max=05.3989, min=00.0089, mean=00.9310} policy_loss=-7.1391 policy updated! \n",
      "train step 02680 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.3308 diff={max=06.0473, min=00.0400, mean=01.0257} policy_loss=-7.2860 policy updated! \n",
      "train step 02681 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4802 diff={max=04.1891, min=00.0058, mean=00.9183} policy_loss=-7.1007 policy updated! \n",
      "train step 02682 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3926 diff={max=07.8435, min=00.0264, mean=00.9379} policy_loss=-6.3903 policy updated! \n",
      "train step 02683 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=01.4409 diff={max=03.9974, min=00.0099, mean=00.8942} policy_loss=-6.7522 policy updated! \n",
      "train step 02684 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6685 diff={max=05.3032, min=00.0373, mean=00.8384} policy_loss=-7.2854 policy updated! \n",
      "train step 02685 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=00.4199 diff={max=01.7100, min=00.0067, mean=00.5062} policy_loss=-6.1611 policy updated! \n",
      "train step 02686 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2737 diff={max=05.9293, min=00.0093, mean=00.9266} policy_loss=-5.7863 policy updated! \n",
      "train step 02687 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4821 diff={max=05.5078, min=00.0297, mean=01.2548} policy_loss=-6.7770 policy updated! \n",
      "train step 02688 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1183 diff={max=06.5632, min=00.0342, mean=00.8063} policy_loss=-6.5589 policy updated! \n",
      "train step 02689 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2670 diff={max=03.3025, min=00.0232, mean=00.7823} policy_loss=-6.4675 policy updated! \n",
      "train step 02690 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.3025 diff={max=07.6254, min=00.0108, mean=00.8385} policy_loss=-6.4890 policy updated! \n",
      "train step 02691 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=01.5473 diff={max=03.7729, min=00.0119, mean=00.9210} policy_loss=-6.6875 policy updated! \n",
      "train step 02692 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0833 diff={max=07.3217, min=00.0154, mean=00.8050} policy_loss=-7.0219 policy updated! \n",
      "train step 02693 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3332 diff={max=05.0234, min=00.0662, mean=01.0000} policy_loss=-7.7461 policy updated! \n",
      "train step 02694 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7489 diff={max=04.7246, min=00.0007, mean=00.9524} policy_loss=-7.4230 policy updated! \n",
      "train step 02695 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.5799 diff={max=06.4673, min=00.0064, mean=01.1681} policy_loss=-6.5441 policy updated! \n",
      "train step 02696 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9705 diff={max=04.9096, min=00.0167, mean=00.9934} policy_loss=-7.6588 policy updated! \n",
      "train step 02697 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3984 diff={max=07.0641, min=00.0140, mean=00.9472} policy_loss=-6.3636 policy updated! \n",
      "train step 02698 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3929 diff={max=03.4123, min=00.0734, mean=00.8750} policy_loss=-6.0836 policy updated! \n",
      "train step 02699 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=02.1166 diff={max=04.4226, min=00.0283, mean=00.9466} policy_loss=-6.2608 policy updated! \n",
      "train step 02700 reward={max=09.0000, min=09.0000, mean=09.0000} optimizing loss=03.0384 diff={max=05.0371, min=00.0192, mean=01.2062} policy_loss=-7.0511 policy updated! \n",
      "train step 02701 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3466 diff={max=06.3624, min=00.0258, mean=00.9855} policy_loss=-6.7894 policy updated! \n",
      "train step 02702 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0285 diff={max=06.2656, min=00.0269, mean=01.0950} policy_loss=-7.4867 policy updated! \n",
      "train step 02703 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1525 diff={max=05.8191, min=00.0139, mean=00.9553} policy_loss=-6.3460 policy updated! \n",
      "train step 02704 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6760 diff={max=05.7292, min=00.0239, mean=01.0211} policy_loss=-7.1597 policy updated! \n",
      "train step 02705 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.2339 diff={max=03.1958, min=00.0085, mean=00.7638} policy_loss=-6.8823 policy updated! \n",
      "train step 02706 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5792 diff={max=06.9639, min=00.0069, mean=01.1589} policy_loss=-6.4699 policy updated! \n",
      "train step 02707 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8539 diff={max=05.2230, min=00.0010, mean=00.9688} policy_loss=-6.9448 policy updated! \n",
      "train step 02708 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3630 diff={max=04.4457, min=00.0562, mean=00.8167} policy_loss=-6.7447 policy updated! \n",
      "train step 02709 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=00.5915 diff={max=02.6769, min=00.0203, mean=00.5344} policy_loss=-6.6139 policy updated! \n",
      "train step 02710 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=00.9427 diff={max=03.8228, min=00.0143, mean=00.6709} policy_loss=-7.1996 policy updated! \n",
      "train step 02711 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7638 diff={max=06.3303, min=00.0327, mean=00.7817} policy_loss=-6.7498 policy updated! \n",
      "train step 02712 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8379 diff={max=05.6826, min=00.0146, mean=00.8736} policy_loss=-5.2788 policy updated! \n",
      "train step 02713 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6844 diff={max=03.9769, min=00.0002, mean=01.0018} policy_loss=-7.2079 policy updated! \n",
      "train step 02714 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4974 diff={max=06.5170, min=00.0203, mean=01.2907} policy_loss=-6.5508 policy updated! \n",
      "train step 02715 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=00.7937 diff={max=03.3349, min=00.0035, mean=00.5792} policy_loss=-6.1164 policy updated! \n",
      "train step 02716 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4429 diff={max=04.5653, min=00.0519, mean=00.7907} policy_loss=-6.0033 policy updated! \n",
      "train step 02717 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5667 diff={max=04.7918, min=00.0227, mean=01.0922} policy_loss=-7.5316 policy updated! \n",
      "train step 02718 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1354 diff={max=05.0975, min=00.0111, mean=00.9468} policy_loss=-7.0487 policy updated! \n",
      "train step 02719 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7560 diff={max=05.1712, min=00.0044, mean=01.2052} policy_loss=-7.2429 policy updated! \n",
      "train step 02720 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=00.5930 diff={max=02.5183, min=00.0602, mean=00.6034} policy_loss=-7.6969 policy updated! \n",
      "train step 02721 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9200 diff={max=05.6202, min=00.0213, mean=00.8333} policy_loss=-6.9439 policy updated! \n",
      "train step 02722 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1119 diff={max=06.8983, min=00.0233, mean=01.1230} policy_loss=-7.1204 policy updated! \n",
      "train step 02723 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1482 diff={max=04.4526, min=00.0180, mean=01.0290} policy_loss=-6.5449 policy updated! \n",
      "train step 02724 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1285 diff={max=06.4285, min=00.0446, mean=00.8780} policy_loss=-6.9843 policy updated! \n",
      "train step 02725 reward={max=09.0000, min=09.0000, mean=09.0000} optimizing loss=00.8463 diff={max=02.9910, min=00.0062, mean=00.6625} policy_loss=-7.6180 policy updated! \n",
      "train step 02726 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3815 diff={max=06.5965, min=00.0035, mean=00.6490} policy_loss=-6.7915 policy updated! \n",
      "train step 02727 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9310 diff={max=04.2408, min=00.0052, mean=00.6663} policy_loss=-7.5935 policy updated! \n",
      "train step 02728 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1862 diff={max=03.5259, min=00.0216, mean=00.7734} policy_loss=-7.3519 policy updated! \n",
      "train step 02729 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7187 diff={max=04.8060, min=00.0330, mean=01.1670} policy_loss=-6.6748 policy updated! \n",
      "train step 02730 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.5703 diff={max=05.7617, min=00.0131, mean=00.9516} policy_loss=-7.3817 policy updated! \n",
      "train step 02731 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6499 diff={max=04.9943, min=00.0182, mean=00.8025} policy_loss=-7.1800 policy updated! \n",
      "train step 02732 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7577 diff={max=05.5250, min=00.0036, mean=00.8504} policy_loss=-6.3197 policy updated! \n",
      "train step 02733 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0647 diff={max=03.1353, min=00.0320, mean=00.7537} policy_loss=-6.5925 policy updated! \n",
      "train step 02734 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5539 diff={max=04.2694, min=00.0508, mean=00.9369} policy_loss=-6.8764 policy updated! \n",
      "train step 02735 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.0103 diff={max=05.9524, min=00.0459, mean=01.1104} policy_loss=-6.7414 policy updated! \n",
      "train step 02736 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8105 diff={max=04.2535, min=00.0142, mean=00.9626} policy_loss=-7.1376 policy updated! \n",
      "train step 02737 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3001 diff={max=07.3598, min=00.0225, mean=01.1067} policy_loss=-7.8907 policy updated! \n",
      "train step 02738 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5583 diff={max=05.1011, min=00.0430, mean=00.8615} policy_loss=-8.5992 policy updated! \n",
      "train step 02739 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=01.7421 diff={max=03.8906, min=00.0599, mean=00.9513} policy_loss=-7.8009 policy updated! \n",
      "train step 02740 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.7703 diff={max=06.3702, min=00.0439, mean=01.1042} policy_loss=-6.5700 policy updated! \n",
      "train step 02741 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1346 diff={max=04.7144, min=00.0060, mean=00.9913} policy_loss=-6.4985 policy updated! \n",
      "train step 02742 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8607 diff={max=05.4610, min=00.0006, mean=00.9073} policy_loss=-6.4169 policy updated! \n",
      "train step 02743 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1713 diff={max=03.3755, min=00.0142, mean=00.7749} policy_loss=-5.6620 policy updated! \n",
      "train step 02744 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=01.2817 diff={max=04.6046, min=00.0128, mean=00.8306} policy_loss=-5.3487 policy updated! \n",
      "train step 02745 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.0600 diff={max=08.4775, min=00.0459, mean=01.1908} policy_loss=-6.0123 policy updated! \n",
      "train step 02746 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7134 diff={max=07.3782, min=00.0109, mean=01.2516} policy_loss=-7.3362 policy updated! \n",
      "train step 02747 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7747 diff={max=04.6802, min=00.0101, mean=00.8537} policy_loss=-7.0269 policy updated! \n",
      "train step 02748 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0441 diff={max=05.5619, min=00.0529, mean=01.0790} policy_loss=-6.3237 policy updated! \n",
      "train step 02749 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9061 diff={max=05.2539, min=00.0125, mean=00.9698} policy_loss=-7.3487 policy updated! \n",
      "train step 02750 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.4503 diff={max=06.0242, min=00.0071, mean=01.1466} policy_loss=-8.8084 policy updated! \n",
      "train step 02751 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6899 diff={max=05.6787, min=00.0113, mean=01.1948} policy_loss=-6.7640 policy updated! \n",
      "train step 02752 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6900 diff={max=04.4301, min=00.0164, mean=01.1291} policy_loss=-6.7444 policy updated! \n",
      "train step 02753 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1636 diff={max=05.9091, min=00.0272, mean=01.1669} policy_loss=-6.8426 policy updated! \n",
      "train step 02754 reward={max=09.0000, min=09.0000, mean=09.0000} optimizing loss=03.2624 diff={max=09.1566, min=00.0102, mean=01.0449} policy_loss=-6.3511 policy updated! \n",
      "train step 02755 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.5636 diff={max=06.0020, min=00.0486, mean=00.8164} policy_loss=-6.0319 policy updated! \n",
      "train step 02756 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2639 diff={max=07.7292, min=00.0085, mean=01.1334} policy_loss=-6.4440 policy updated! \n",
      "train step 02757 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5474 diff={max=04.1324, min=00.0047, mean=00.8943} policy_loss=-7.0811 policy updated! \n",
      "train step 02758 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=01.2531 diff={max=04.3994, min=00.0215, mean=00.7806} policy_loss=-6.7884 policy updated! \n",
      "train step 02759 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7685 diff={max=05.2966, min=00.0083, mean=00.8370} policy_loss=-7.7690 policy updated! \n",
      "train step 02760 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.6411 diff={max=05.5417, min=00.0465, mean=01.1585} policy_loss=-6.5060 policy updated! \n",
      "train step 02761 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7832 diff={max=06.3061, min=00.0278, mean=01.2074} policy_loss=-7.5305 policy updated! \n",
      "train step 02762 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6282 diff={max=04.1599, min=00.0355, mean=00.9048} policy_loss=-6.8809 policy updated! \n",
      "train step 02763 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4330 diff={max=04.9577, min=00.0127, mean=00.8253} policy_loss=-6.2005 policy updated! \n",
      "train step 02764 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2856 diff={max=05.8353, min=00.0122, mean=00.9496} policy_loss=-6.5114 policy updated! \n",
      "train step 02765 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.0971 diff={max=05.5581, min=00.0293, mean=01.2128} policy_loss=-7.0112 policy updated! \n",
      "train step 02766 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7231 diff={max=04.6813, min=00.0306, mean=00.9166} policy_loss=-6.5490 policy updated! \n",
      "train step 02767 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9902 diff={max=06.1575, min=00.0665, mean=00.9302} policy_loss=-5.5618 policy updated! \n",
      "train step 02768 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3768 diff={max=08.6366, min=00.0092, mean=01.3835} policy_loss=-6.9990 policy updated! \n",
      "train step 02769 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6666 diff={max=04.7825, min=00.0012, mean=00.9002} policy_loss=-6.5851 policy updated! \n",
      "train step 02770 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.7647 diff={max=04.5606, min=00.0034, mean=00.9442} policy_loss=-6.7083 policy updated! \n",
      "train step 02771 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2706 diff={max=06.4568, min=00.0178, mean=01.1079} policy_loss=-5.8740 policy updated! \n",
      "train step 02772 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2243 diff={max=06.0055, min=00.0077, mean=00.8974} policy_loss=-7.0552 policy updated! \n",
      "train step 02773 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9093 diff={max=04.8084, min=00.0006, mean=00.9144} policy_loss=-7.5546 policy updated! \n",
      "train step 02774 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8990 diff={max=05.5194, min=00.0363, mean=00.8501} policy_loss=-7.1863 policy updated! \n",
      "train step 02775 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.8836 diff={max=06.3204, min=00.0204, mean=01.4225} policy_loss=-6.4089 policy updated! \n",
      "train step 02776 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4199 diff={max=06.4844, min=00.0018, mean=00.9636} policy_loss=-6.6817 policy updated! \n",
      "train step 02777 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3276 diff={max=05.9396, min=00.0062, mean=01.2316} policy_loss=-7.3985 policy updated! \n",
      "train step 02778 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4785 diff={max=03.3902, min=00.0253, mean=00.9337} policy_loss=-6.6997 policy updated! \n",
      "train step 02779 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1105 diff={max=03.0176, min=00.0030, mean=00.7885} policy_loss=-7.2771 policy updated! \n",
      "train step 02780 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.9952 diff={max=04.8309, min=00.0060, mean=01.1666} policy_loss=-7.5486 policy updated! \n",
      "train step 02781 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=02.9468 diff={max=06.3225, min=00.0136, mean=01.0591} policy_loss=-6.8751 policy updated! \n",
      "train step 02782 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4846 diff={max=05.0083, min=00.0164, mean=01.0562} policy_loss=-7.4890 policy updated! \n",
      "train step 02783 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3588 diff={max=04.3866, min=00.0359, mean=01.1274} policy_loss=-6.7480 policy updated! \n",
      "train step 02784 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1783 diff={max=04.1014, min=00.0025, mean=00.9976} policy_loss=-6.4416 policy updated! \n",
      "train step 02785 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=02.7543 diff={max=04.7486, min=00.0110, mean=01.0839} policy_loss=-6.2403 policy updated! \n",
      "train step 02786 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3037 diff={max=05.3801, min=00.0981, mean=01.1227} policy_loss=-8.1847 policy updated! \n",
      "train step 02787 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7156 diff={max=07.0457, min=00.0269, mean=01.1921} policy_loss=-7.4861 policy updated! \n",
      "train step 02788 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=03.8243 diff={max=05.7016, min=00.0081, mean=01.1672} policy_loss=-6.5566 policy updated! \n",
      "train step 02789 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0050 diff={max=07.1719, min=00.0133, mean=01.0240} policy_loss=-7.1082 policy updated! \n",
      "train step 02790 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=02.5319 diff={max=06.5966, min=00.0051, mean=01.0087} policy_loss=-7.7000 policy updated! \n",
      "train step 02791 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6400 diff={max=05.5486, min=00.0007, mean=01.0879} policy_loss=-6.8414 policy updated! \n",
      "train step 02792 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4991 diff={max=04.8652, min=00.0078, mean=01.1231} policy_loss=-8.1958 policy updated! \n",
      "train step 02793 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8636 diff={max=04.9085, min=00.0094, mean=00.8381} policy_loss=-6.0867 policy updated! \n",
      "train step 02794 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1432 diff={max=06.9181, min=00.0006, mean=00.8572} policy_loss=-6.0460 policy updated! \n",
      "train step 02795 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.3022 diff={max=06.5299, min=00.0164, mean=01.1910} policy_loss=-6.4491 policy updated! \n",
      "train step 02796 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4811 diff={max=03.4528, min=00.0390, mean=00.9633} policy_loss=-7.4433 policy updated! \n",
      "train step 02797 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2428 diff={max=05.4367, min=00.0358, mean=01.0092} policy_loss=-7.5686 policy updated! \n",
      "train step 02798 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1435 diff={max=03.6652, min=00.0018, mean=01.0808} policy_loss=-7.1798 policy updated! \n",
      "train step 02799 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7932 diff={max=05.6853, min=00.1244, mean=00.9534} policy_loss=-7.8824 policy updated! \n",
      "train step 02800 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=02.1249 diff={max=03.8677, min=00.0051, mean=01.0866} policy_loss=-6.6818 policy updated! \n",
      "train step 02801 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7420 diff={max=03.8125, min=00.0112, mean=00.9426} policy_loss=-6.6619 policy updated! \n",
      "train step 02802 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1413 diff={max=03.7988, min=00.0204, mean=00.7084} policy_loss=-5.8118 policy updated! \n",
      "train step 02803 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4133 diff={max=02.8658, min=00.0680, mean=00.9473} policy_loss=-6.7754 policy updated! \n",
      "train step 02804 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6656 diff={max=04.8750, min=00.0339, mean=01.1548} policy_loss=-6.5477 policy updated! \n",
      "train step 02805 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.9116 diff={max=10.1429, min=00.0152, mean=00.9802} policy_loss=-6.7129 policy updated! \n",
      "train step 02806 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6826 diff={max=04.4070, min=00.0390, mean=00.9501} policy_loss=-7.3982 policy updated! \n",
      "train step 02807 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1225 diff={max=04.7098, min=00.0037, mean=01.0211} policy_loss=-7.3657 policy updated! \n",
      "train step 02808 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3416 diff={max=03.0569, min=00.0274, mean=00.9113} policy_loss=-8.4944 policy updated! \n",
      "train step 02809 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3361 diff={max=05.8794, min=00.0043, mean=01.2460} policy_loss=-6.5318 policy updated! \n",
      "train step 02810 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.4894 diff={max=03.5546, min=00.0328, mean=00.9208} policy_loss=-7.1537 policy updated! \n",
      "train step 02811 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7637 diff={max=05.6105, min=00.0059, mean=00.7784} policy_loss=-7.2620 policy updated! \n",
      "train step 02812 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9745 diff={max=03.2872, min=00.0346, mean=01.0592} policy_loss=-6.6671 policy updated! \n",
      "train step 02813 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0537 diff={max=07.3128, min=00.0087, mean=00.8303} policy_loss=-7.0362 policy updated! \n",
      "train step 02814 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7947 diff={max=04.3681, min=00.0170, mean=01.1135} policy_loss=-6.0377 policy updated! \n",
      "train step 02815 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=04.1043 diff={max=05.5452, min=00.0220, mean=01.4141} policy_loss=-7.7284 policy updated! \n",
      "train step 02816 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2056 diff={max=06.3274, min=00.0097, mean=00.9448} policy_loss=-6.4825 policy updated! \n",
      "train step 02817 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7760 diff={max=02.8619, min=00.0145, mean=00.5989} policy_loss=-7.1902 policy updated! \n",
      "train step 02818 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4756 diff={max=06.3042, min=00.0159, mean=01.1720} policy_loss=-6.9711 policy updated! \n",
      "train step 02819 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3877 diff={max=05.3800, min=00.0044, mean=00.9561} policy_loss=-6.5151 policy updated! \n",
      "train step 02820 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.7522 diff={max=05.7117, min=00.0386, mean=01.1910} policy_loss=-7.4753 policy updated! \n",
      "train step 02821 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0115 diff={max=07.2166, min=00.0025, mean=01.2183} policy_loss=-7.2768 policy updated! \n",
      "train step 02822 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4784 diff={max=03.7464, min=00.0079, mean=01.1798} policy_loss=-7.0687 policy updated! \n",
      "train step 02823 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2498 diff={max=07.9342, min=00.0025, mean=00.8572} policy_loss=-6.6823 policy updated! \n",
      "train step 02824 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=03.0692 diff={max=06.8107, min=00.0148, mean=01.1810} policy_loss=-6.9005 policy updated! \n",
      "train step 02825 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.4545 diff={max=06.8346, min=00.0040, mean=01.1960} policy_loss=-7.4739 policy updated! \n",
      "train step 02826 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3395 diff={max=08.1953, min=00.0373, mean=01.1140} policy_loss=-7.2519 policy updated! \n",
      "train step 02827 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8449 diff={max=08.2635, min=00.0050, mean=00.9990} policy_loss=-6.8344 policy updated! \n",
      "train step 02828 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3293 diff={max=04.4188, min=00.0106, mean=00.7637} policy_loss=-6.0281 policy updated! \n",
      "train step 02829 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=02.2364 diff={max=06.4576, min=00.0141, mean=01.0070} policy_loss=-7.5447 policy updated! \n",
      "train step 02830 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.0886 diff={max=05.8355, min=00.0054, mean=00.9608} policy_loss=-7.3679 policy updated! \n",
      "train step 02831 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2198 diff={max=02.8685, min=00.0089, mean=00.8933} policy_loss=-8.1022 policy updated! \n",
      "train step 02832 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5945 diff={max=06.5108, min=00.0594, mean=01.2293} policy_loss=-6.7783 policy updated! \n",
      "train step 02833 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0074 diff={max=05.5717, min=00.0053, mean=00.8588} policy_loss=-6.2847 policy updated! \n",
      "train step 02834 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7621 diff={max=04.9809, min=00.0390, mean=00.9394} policy_loss=-6.8621 policy updated! \n",
      "train step 02835 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.2787 diff={max=03.5172, min=00.0041, mean=00.7782} policy_loss=-6.2609 policy updated! \n",
      "train step 02836 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9844 diff={max=04.5153, min=00.0022, mean=00.9500} policy_loss=-7.4250 policy updated! \n",
      "train step 02837 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1124 diff={max=05.0798, min=00.0007, mean=00.7130} policy_loss=-6.2646 policy updated! \n",
      "train step 02838 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6883 diff={max=04.8070, min=00.0417, mean=00.8850} policy_loss=-7.8698 policy updated! \n",
      "train step 02839 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9016 diff={max=05.6839, min=00.0006, mean=00.8797} policy_loss=-6.6307 policy updated! \n",
      "train step 02840 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.2946 diff={max=03.3962, min=00.0229, mean=00.7931} policy_loss=-6.8538 policy updated! \n",
      "train step 02841 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=01.8600 diff={max=05.1481, min=00.0111, mean=00.9556} policy_loss=-6.5647 policy updated! \n",
      "train step 02842 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8963 diff={max=06.3096, min=00.0085, mean=00.8293} policy_loss=-7.1641 policy updated! \n",
      "train step 02843 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5921 diff={max=06.4903, min=00.0105, mean=00.9605} policy_loss=-7.9973 policy updated! \n",
      "train step 02844 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4698 diff={max=03.8592, min=00.0053, mean=00.8449} policy_loss=-7.2541 policy updated! \n",
      "train step 02845 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=02.1151 diff={max=05.0673, min=00.0036, mean=00.9813} policy_loss=-8.0000 policy updated! \n",
      "train step 02846 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6563 diff={max=04.8179, min=00.0413, mean=00.8940} policy_loss=-7.2306 policy updated! \n",
      "train step 02847 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7465 diff={max=04.8865, min=00.0034, mean=00.8218} policy_loss=-6.6982 policy updated! \n",
      "train step 02848 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9245 diff={max=04.2568, min=00.0319, mean=00.9447} policy_loss=-7.5269 policy updated! \n",
      "train step 02849 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9567 diff={max=06.0769, min=00.0615, mean=00.9064} policy_loss=-8.0646 policy updated! \n",
      "train step 02850 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.4874 diff={max=03.8182, min=00.0067, mean=00.8381} policy_loss=-6.9422 policy updated! \n",
      "train step 02851 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8910 diff={max=03.7095, min=00.0258, mean=01.0393} policy_loss=-6.9173 policy updated! \n",
      "train step 02852 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9240 diff={max=03.9695, min=00.0063, mean=00.9888} policy_loss=-6.6870 policy updated! \n",
      "train step 02853 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9724 diff={max=03.5224, min=00.0139, mean=00.7358} policy_loss=-6.5292 policy updated! \n",
      "train step 02854 reward={max=08.0000, min=00.0000, mean=06.2000} optimizing loss=01.6661 diff={max=04.3382, min=00.0443, mean=00.9102} policy_loss=-6.5281 policy updated! \n",
      "train step 02855 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.2999 diff={max=07.5208, min=00.0040, mean=01.0079} policy_loss=-7.6293 policy updated! \n",
      "train step 02856 reward={max=08.0000, min=00.0000, mean=04.4000} optimizing loss=02.6592 diff={max=06.0074, min=00.0179, mean=01.0164} policy_loss=-6.9738 policy updated! \n",
      "train step 02857 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7106 diff={max=05.6286, min=00.0208, mean=00.8895} policy_loss=-6.9741 policy updated! \n",
      "train step 02858 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8021 diff={max=07.4972, min=00.0036, mean=01.0360} policy_loss=-6.6811 policy updated! \n",
      "train step 02859 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4250 diff={max=05.7029, min=00.0247, mean=01.0303} policy_loss=-5.9573 policy updated! \n",
      "train step 02860 reward={max=08.0000, min=00.0000, mean=03.0000} optimizing loss=02.7491 diff={max=05.7559, min=00.0092, mean=01.0108} policy_loss=-6.7636 policy updated! \n",
      "train step 02861 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0233 diff={max=03.0002, min=00.0079, mean=00.7845} policy_loss=-8.2875 policy updated! \n",
      "train step 02862 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6673 diff={max=06.3458, min=00.0018, mean=01.3140} policy_loss=-6.5319 policy updated! \n",
      "train step 02863 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3632 diff={max=03.5517, min=00.0091, mean=00.8683} policy_loss=-8.1411 policy updated! \n",
      "train step 02864 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=01.4028 diff={max=03.8820, min=00.0033, mean=00.8206} policy_loss=-6.6547 policy updated! \n",
      "train step 02865 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.9006 diff={max=04.8948, min=00.0383, mean=00.9500} policy_loss=-7.6204 policy updated! \n",
      "train step 02866 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0251 diff={max=04.9471, min=00.0063, mean=00.9394} policy_loss=-7.1211 policy updated! \n",
      "train step 02867 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9606 diff={max=07.3034, min=00.0390, mean=00.8606} policy_loss=-6.2673 policy updated! \n",
      "train step 02868 reward={max=08.0000, min=00.0000, mean=03.2000} optimizing loss=02.0301 diff={max=04.3043, min=00.0113, mean=00.9973} policy_loss=-6.6905 policy updated! \n",
      "train step 02869 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6773 diff={max=05.2253, min=00.0055, mean=00.8635} policy_loss=-7.2194 policy updated! \n",
      "train step 02870 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=02.6921 diff={max=06.6119, min=00.0374, mean=01.0429} policy_loss=-7.8653 policy updated! \n",
      "train step 02871 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4075 diff={max=08.1121, min=00.0246, mean=00.8785} policy_loss=-6.6405 policy updated! \n",
      "train step 02872 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4643 diff={max=03.9948, min=00.0096, mean=00.8762} policy_loss=-7.5725 policy updated! \n",
      "train step 02873 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4478 diff={max=05.2889, min=00.0187, mean=01.3227} policy_loss=-7.6828 policy updated! \n",
      "train step 02874 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3706 diff={max=05.5197, min=00.0571, mean=00.9628} policy_loss=-6.4622 policy updated! \n",
      "train step 02875 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.8783 diff={max=04.3189, min=00.0099, mean=00.8871} policy_loss=-6.9858 policy updated! \n",
      "train step 02876 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2540 diff={max=06.8810, min=00.0159, mean=00.8860} policy_loss=-6.2052 policy updated! \n",
      "train step 02877 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1861 diff={max=06.4709, min=00.0258, mean=00.8838} policy_loss=-5.5966 policy updated! \n",
      "train step 02878 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5797 diff={max=07.2926, min=00.0256, mean=01.0300} policy_loss=-6.1548 policy updated! \n",
      "train step 02879 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1994 diff={max=09.6162, min=00.0915, mean=01.2666} policy_loss=-7.6476 policy updated! \n",
      "train step 02880 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=02.2642 diff={max=05.6050, min=00.0193, mean=00.9673} policy_loss=-6.7970 policy updated! \n",
      "train step 02881 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6022 diff={max=05.3410, min=00.0178, mean=00.8074} policy_loss=-6.5457 policy updated! \n",
      "train step 02882 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6459 diff={max=06.9728, min=00.0361, mean=00.9319} policy_loss=-7.2032 policy updated! \n",
      "train step 02883 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0792 diff={max=05.3903, min=00.0171, mean=00.9874} policy_loss=-7.7871 policy updated! \n",
      "train step 02884 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1843 diff={max=06.2196, min=00.0121, mean=01.1774} policy_loss=-8.6949 policy updated! \n",
      "train step 02885 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.2985 diff={max=05.0233, min=00.0227, mean=01.2430} policy_loss=-7.5416 policy updated! \n",
      "train step 02886 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3242 diff={max=05.5349, min=00.0041, mean=01.0304} policy_loss=-6.7876 policy updated! \n",
      "train step 02887 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2190 diff={max=04.9924, min=00.0030, mean=01.0533} policy_loss=-6.5020 policy updated! \n",
      "train step 02888 reward={max=08.0000, min=00.0000, mean=03.2000} optimizing loss=02.1743 diff={max=05.3248, min=00.0007, mean=00.9337} policy_loss=-6.8298 policy updated! \n",
      "train step 02889 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3552 diff={max=09.7375, min=00.0242, mean=01.1048} policy_loss=-7.0475 policy updated! \n",
      "train step 02890 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=02.3485 diff={max=05.6212, min=00.0101, mean=01.0620} policy_loss=-7.0051 policy updated! \n",
      "train step 02891 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2975 diff={max=06.3001, min=00.0011, mean=00.9333} policy_loss=-6.4777 policy updated! \n",
      "train step 02892 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8796 diff={max=05.3540, min=00.0029, mean=00.8819} policy_loss=-5.9119 policy updated! \n",
      "train step 02893 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5049 diff={max=04.5578, min=00.0065, mean=00.8259} policy_loss=-7.1051 policy updated! \n",
      "train step 02894 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=02.9367 diff={max=07.0504, min=00.0273, mean=01.0150} policy_loss=-6.5314 policy updated! \n",
      "train step 02895 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.4992 diff={max=03.2135, min=00.0067, mean=00.8955} policy_loss=-6.2076 policy updated! \n",
      "train step 02896 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3006 diff={max=03.3584, min=00.0081, mean=00.8277} policy_loss=-6.2993 policy updated! \n",
      "train step 02897 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1190 diff={max=04.7236, min=00.1389, mean=01.0555} policy_loss=-6.4506 policy updated! \n",
      "train step 02898 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6249 diff={max=08.0691, min=00.0336, mean=00.9156} policy_loss=-6.1891 policy updated! \n",
      "train step 02899 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5757 diff={max=05.9560, min=00.0038, mean=01.0224} policy_loss=-7.0671 policy updated! \n",
      "train step 02900 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=02.2612 diff={max=04.7464, min=00.0013, mean=00.9701} policy_loss=-6.9736 policy updated! \n",
      "train step 02901 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5062 diff={max=03.8857, min=00.0171, mean=00.8426} policy_loss=-6.0404 policy updated! \n",
      "train step 02902 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7127 diff={max=05.6619, min=00.0159, mean=01.1947} policy_loss=-6.4247 policy updated! \n",
      "train step 02903 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0268 diff={max=06.8340, min=00.0063, mean=01.0351} policy_loss=-7.9841 policy updated! \n",
      "train step 02904 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=02.6900 diff={max=04.1069, min=00.0262, mean=01.1966} policy_loss=-7.4958 policy updated! \n",
      "train step 02905 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.2657 diff={max=08.0741, min=00.0024, mean=00.8800} policy_loss=-7.3315 policy updated! \n",
      "train step 02906 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0819 diff={max=05.1672, min=00.0202, mean=00.9182} policy_loss=-7.2676 policy updated! \n",
      "train step 02907 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5059 diff={max=04.9831, min=00.0065, mean=01.1245} policy_loss=-7.2744 policy updated! \n",
      "train step 02908 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2526 diff={max=07.4253, min=00.0108, mean=01.0642} policy_loss=-6.3210 policy updated! \n",
      "train step 02909 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.6267 diff={max=02.5464, min=00.0105, mean=00.6047} policy_loss=-6.6895 policy updated! \n",
      "train step 02910 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.5092 diff={max=04.5055, min=00.0244, mean=00.7821} policy_loss=-6.6890 policy updated! \n",
      "train step 02911 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0733 diff={max=05.1402, min=00.0009, mean=00.9936} policy_loss=-7.7406 policy updated! \n",
      "train step 02912 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8952 diff={max=06.6953, min=00.0258, mean=01.0429} policy_loss=-6.4733 policy updated! \n",
      "train step 02913 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.8601 diff={max=06.1803, min=00.0074, mean=00.8990} policy_loss=-8.0176 policy updated! \n",
      "train step 02914 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6674 diff={max=03.3730, min=00.0142, mean=00.8791} policy_loss=-7.1070 policy updated! \n",
      "train step 02915 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.1396 diff={max=05.9624, min=00.0581, mean=01.2431} policy_loss=-7.3486 policy updated! \n",
      "train step 02916 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.6736 diff={max=05.5273, min=00.0229, mean=01.0345} policy_loss=-8.0653 policy updated! \n",
      "train step 02917 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6358 diff={max=04.3699, min=00.0458, mean=00.8845} policy_loss=-6.9838 policy updated! \n",
      "train step 02918 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8358 diff={max=05.4122, min=00.0166, mean=00.8468} policy_loss=-7.3783 policy updated! \n",
      "train step 02919 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9010 diff={max=05.8156, min=00.0012, mean=00.8979} policy_loss=-6.4719 policy updated! \n",
      "train step 02920 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.6903 diff={max=05.5358, min=00.0043, mean=01.3122} policy_loss=-6.8907 policy updated! \n",
      "train step 02921 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2401 diff={max=06.6606, min=00.0097, mean=01.2038} policy_loss=-7.1560 policy updated! \n",
      "train step 02922 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8609 diff={max=06.8509, min=00.0266, mean=01.1474} policy_loss=-7.2251 policy updated! \n",
      "train step 02923 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0688 diff={max=06.3681, min=00.0090, mean=01.2198} policy_loss=-7.8867 policy updated! \n",
      "train step 02924 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2848 diff={max=04.1798, min=00.0954, mean=00.7926} policy_loss=-6.6615 policy updated! \n",
      "train step 02925 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.1451 diff={max=04.7134, min=00.0533, mean=01.0866} policy_loss=-6.4139 policy updated! \n",
      "train step 02926 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.8868 diff={max=04.9079, min=00.0032, mean=00.8345} policy_loss=-6.6846 policy updated! \n",
      "train step 02927 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8468 diff={max=03.6166, min=00.0003, mean=00.6518} policy_loss=-6.1684 policy updated! \n",
      "train step 02928 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9686 diff={max=06.7567, min=00.0023, mean=01.2123} policy_loss=-6.0004 policy updated! \n",
      "train step 02929 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.0392 diff={max=02.8896, min=00.0105, mean=00.8206} policy_loss=-7.0328 policy updated! \n",
      "train step 02930 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.4575 diff={max=06.7251, min=00.0207, mean=01.1428} policy_loss=-6.1714 policy updated! \n",
      "train step 02931 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5787 diff={max=03.7932, min=00.0193, mean=00.8939} policy_loss=-6.9801 policy updated! \n",
      "train step 02932 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5108 diff={max=05.9848, min=00.0203, mean=00.7310} policy_loss=-6.9884 policy updated! \n",
      "train step 02933 reward={max=08.0000, min=00.0000, mean=03.2000} optimizing loss=01.1783 diff={max=03.0131, min=00.0081, mean=00.7517} policy_loss=-6.9748 policy updated! \n",
      "train step 02934 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8168 diff={max=04.8122, min=00.0532, mean=01.0068} policy_loss=-7.0696 policy updated! \n",
      "train step 02935 reward={max=08.0000, min=00.0000, mean=05.8000} optimizing loss=02.6631 diff={max=05.9818, min=00.0007, mean=01.1636} policy_loss=-7.7494 policy updated! \n",
      "train step 02936 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2487 diff={max=07.6830, min=00.0290, mean=01.0839} policy_loss=-7.2064 policy updated! \n",
      "train step 02937 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3560 diff={max=05.8430, min=00.0564, mean=00.9786} policy_loss=-7.5374 policy updated! \n",
      "train step 02938 reward={max=08.0000, min=00.0000, mean=03.2000} optimizing loss=01.5768 diff={max=03.4532, min=00.0830, mean=00.8855} policy_loss=-7.2072 policy updated! \n",
      "train step 02939 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9201 diff={max=05.6984, min=00.0119, mean=01.1446} policy_loss=-8.0992 policy updated! \n",
      "train step 02940 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.4809 diff={max=03.6359, min=00.0019, mean=00.8706} policy_loss=-6.9262 policy updated! \n",
      "train step 02941 reward={max=08.0000, min=00.0000, mean=04.4000} optimizing loss=02.3035 diff={max=06.1405, min=00.0346, mean=00.9614} policy_loss=-6.4921 policy updated! \n",
      "train step 02942 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.1650 diff={max=08.1943, min=00.0100, mean=01.1396} policy_loss=-8.5659 policy updated! \n",
      "train step 02943 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9783 diff={max=05.1062, min=00.0294, mean=00.8964} policy_loss=-7.4432 policy updated! \n",
      "train step 02944 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8580 diff={max=04.7429, min=00.0354, mean=01.0354} policy_loss=-8.2413 policy updated! \n",
      "train step 02945 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.4330 diff={max=04.5038, min=00.1272, mean=00.8814} policy_loss=-7.4085 policy updated! \n",
      "train step 02946 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0221 diff={max=03.5154, min=00.0021, mean=00.7193} policy_loss=-6.4029 policy updated! \n",
      "train step 02947 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2348 diff={max=03.3965, min=00.0012, mean=00.8122} policy_loss=-6.3227 policy updated! \n",
      "train step 02948 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5705 diff={max=03.9003, min=00.0054, mean=00.8042} policy_loss=-6.8484 policy updated! \n",
      "train step 02949 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2181 diff={max=07.9272, min=00.0217, mean=01.1385} policy_loss=-6.1972 policy updated! \n",
      "train step 02950 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=00.7951 diff={max=02.2634, min=00.0022, mean=00.6740} policy_loss=-6.9122 policy updated! \n",
      "train step 02951 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1394 diff={max=04.6801, min=00.0115, mean=00.7404} policy_loss=-7.7220 policy updated! \n",
      "train step 02952 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2049 diff={max=06.6390, min=00.0067, mean=01.1069} policy_loss=-6.9094 policy updated! \n",
      "train step 02953 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1220 diff={max=06.8729, min=00.0133, mean=00.8328} policy_loss=-7.2024 policy updated! \n",
      "train step 02954 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7017 diff={max=04.4188, min=00.0482, mean=00.8077} policy_loss=-6.5626 policy updated! \n",
      "train step 02955 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.3141 diff={max=04.3312, min=00.0086, mean=01.0477} policy_loss=-7.8386 policy updated! \n",
      "train step 02956 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2914 diff={max=04.4996, min=00.0433, mean=00.9903} policy_loss=-5.5583 policy updated! \n",
      "train step 02957 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7940 diff={max=07.1994, min=00.0473, mean=01.2280} policy_loss=-6.4479 policy updated! \n",
      "train step 02958 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4485 diff={max=06.3175, min=00.0064, mean=00.8800} policy_loss=-8.0363 policy updated! \n",
      "train step 02959 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=01.0064 diff={max=02.9201, min=00.0513, mean=00.7509} policy_loss=-8.1545 policy updated! \n",
      "train step 02960 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.3363 diff={max=06.6866, min=00.0123, mean=01.1111} policy_loss=-7.0217 policy updated! \n",
      "train step 02961 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5982 diff={max=06.2254, min=00.0164, mean=00.9682} policy_loss=-5.7452 policy updated! \n",
      "train step 02962 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4294 diff={max=03.9220, min=00.0206, mean=00.8411} policy_loss=-7.6791 policy updated! \n",
      "train step 02963 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6503 diff={max=04.9740, min=00.0335, mean=01.1648} policy_loss=-6.9332 policy updated! \n",
      "train step 02964 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6406 diff={max=03.5713, min=00.0304, mean=00.9652} policy_loss=-7.5876 policy updated! \n",
      "train step 02965 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.5351 diff={max=05.5786, min=00.0339, mean=00.8296} policy_loss=-7.4016 policy updated! \n",
      "train step 02966 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=02.0576 diff={max=05.7178, min=00.0183, mean=00.9449} policy_loss=-7.5470 policy updated! \n",
      "train step 02967 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6452 diff={max=07.2239, min=00.0435, mean=01.0081} policy_loss=-6.8596 policy updated! \n",
      "train step 02968 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6194 diff={max=05.5855, min=00.0038, mean=00.7800} policy_loss=-7.4396 policy updated! \n",
      "train step 02969 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7482 diff={max=06.4216, min=00.0016, mean=00.8067} policy_loss=-8.1059 policy updated! \n",
      "train step 02970 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.5347 diff={max=06.8072, min=00.0062, mean=01.2491} policy_loss=-7.8727 policy updated! \n",
      "train step 02971 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9236 diff={max=04.3162, min=00.0079, mean=00.9723} policy_loss=-7.0937 policy updated! \n",
      "train step 02972 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8599 diff={max=06.1073, min=00.0056, mean=00.8802} policy_loss=-7.4072 policy updated! \n",
      "train step 02973 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.1269 diff={max=06.4201, min=00.0076, mean=01.0815} policy_loss=-7.3447 policy updated! \n",
      "train step 02974 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2294 diff={max=03.2183, min=00.0027, mean=00.7345} policy_loss=-6.9683 policy updated! \n",
      "train step 02975 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.6934 diff={max=07.4207, min=00.0522, mean=01.2778} policy_loss=-6.3189 policy updated! \n",
      "train step 02976 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=03.9895 diff={max=06.1471, min=00.0321, mean=01.3477} policy_loss=-7.3700 policy updated! \n",
      "train step 02977 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7187 diff={max=04.8904, min=00.0017, mean=01.0568} policy_loss=-6.1079 policy updated! \n",
      "train step 02978 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5692 diff={max=07.9116, min=00.0076, mean=01.1522} policy_loss=-6.5638 policy updated! \n",
      "train step 02979 reward={max=08.0000, min=00.0000, mean=06.0000} optimizing loss=02.3558 diff={max=04.7090, min=00.0013, mean=00.9685} policy_loss=-6.2482 policy updated! \n",
      "train step 02980 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.4750 diff={max=08.2968, min=00.0307, mean=01.0966} policy_loss=-7.0283 policy updated! \n",
      "train step 02981 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8104 diff={max=04.5965, min=00.0134, mean=00.9590} policy_loss=-7.6573 policy updated! \n",
      "train step 02982 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4335 diff={max=07.3084, min=00.0050, mean=00.9482} policy_loss=-7.0016 policy updated! \n",
      "train step 02983 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3490 diff={max=05.7571, min=00.0525, mean=01.3292} policy_loss=-6.6913 policy updated! \n",
      "train step 02984 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3247 diff={max=08.7814, min=00.0163, mean=01.0164} policy_loss=-7.0487 policy updated! \n",
      "train step 02985 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.0179 diff={max=03.4019, min=00.0021, mean=00.6846} policy_loss=-6.4747 policy updated! \n",
      "train step 02986 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0900 diff={max=02.9830, min=00.0141, mean=00.7811} policy_loss=-6.8619 policy updated! \n",
      "train step 02987 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9216 diff={max=06.2709, min=00.0113, mean=01.0022} policy_loss=-7.5266 policy updated! \n",
      "train step 02988 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8203 diff={max=06.8347, min=00.0132, mean=01.2294} policy_loss=-6.6030 policy updated! \n",
      "train step 02989 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0108 diff={max=05.6900, min=00.0063, mean=01.0598} policy_loss=-7.8676 policy updated! \n",
      "train step 02990 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.9059 diff={max=04.5062, min=00.0045, mean=00.9541} policy_loss=-7.2912 policy updated! \n",
      "train step 02991 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5602 diff={max=07.5734, min=00.0451, mean=01.4525} policy_loss=-7.1166 policy updated! \n",
      "train step 02992 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2989 diff={max=07.7983, min=00.0223, mean=01.2546} policy_loss=-7.9760 policy updated! \n",
      "train step 02993 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6491 diff={max=04.2023, min=00.0015, mean=00.9152} policy_loss=-7.0749 policy updated! \n",
      "train step 02994 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=02.5578 diff={max=05.6077, min=00.0072, mean=01.0393} policy_loss=-7.5349 policy updated! \n",
      "train step 02995 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.0705 diff={max=02.7449, min=00.0099, mean=00.7446} policy_loss=-6.2001 policy updated! \n",
      "train step 02996 reward={max=08.0000, min=07.0000, mean=07.2000} optimizing loss=01.8011 diff={max=04.7552, min=00.0027, mean=00.8521} policy_loss=-6.3082 policy updated! \n",
      "train step 02997 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8560 diff={max=04.6518, min=00.0013, mean=00.8931} policy_loss=-7.5174 policy updated! \n",
      "train step 02998 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.0895 diff={max=05.8467, min=00.0092, mean=01.2316} policy_loss=-7.4449 policy updated! \n",
      "train step 02999 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3446 diff={max=03.8922, min=00.0290, mean=00.8045} policy_loss=-7.3113 policy updated! \n",
      "train step 03000 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.9010 diff={max=08.7341, min=00.0025, mean=01.1629} policy_loss=-7.8103 policy updated! \n",
      "train step 03001 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5953 diff={max=04.5715, min=00.0121, mean=00.8234} policy_loss=-7.2763 policy updated! \n",
      "train step 03002 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9659 diff={max=04.3340, min=00.0083, mean=00.9904} policy_loss=-7.5614 policy updated! \n",
      "train step 03003 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8820 diff={max=06.8727, min=00.0464, mean=01.4285} policy_loss=-7.0630 policy updated! \n",
      "train step 03004 reward={max=08.0000, min=08.0000, mean=08.0000} optimizing loss=01.0748 diff={max=04.4812, min=00.0116, mean=00.7262} policy_loss=-7.2888 policy updated! \n",
      "train step 03005 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.5228 diff={max=08.5782, min=00.0278, mean=01.1361} policy_loss=-6.9235 policy updated! \n",
      "train step 03006 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7064 diff={max=07.3526, min=00.0441, mean=01.2217} policy_loss=-5.6287 policy updated! \n",
      "train step 03007 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5271 diff={max=05.6499, min=00.0134, mean=01.0561} policy_loss=-6.2200 policy updated! \n",
      "train step 03008 reward={max=08.0000, min=00.0000, mean=03.2000} optimizing loss=01.6951 diff={max=05.2046, min=00.0125, mean=00.8620} policy_loss=-6.9593 policy updated! \n",
      "train step 03009 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4014 diff={max=06.7555, min=00.0450, mean=00.9650} policy_loss=-7.4088 policy updated! \n",
      "train step 03010 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.7707 diff={max=04.5577, min=00.0289, mean=00.8742} policy_loss=-7.0891 policy updated! \n",
      "train step 03011 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6991 diff={max=04.8949, min=00.0086, mean=00.9466} policy_loss=-6.4678 policy updated! \n",
      "train step 03012 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7653 diff={max=05.7283, min=00.0055, mean=00.9392} policy_loss=-6.6841 policy updated! \n",
      "train step 03013 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5692 diff={max=06.1886, min=00.0318, mean=01.1091} policy_loss=-7.5100 policy updated! \n",
      "train step 03014 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5217 diff={max=08.2115, min=00.0178, mean=01.4178} policy_loss=-7.3179 policy updated! \n",
      "train step 03015 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.9117 diff={max=04.9950, min=00.0067, mean=00.9584} policy_loss=-7.4770 policy updated! \n",
      "train step 03016 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3312 diff={max=07.5696, min=00.0049, mean=01.0715} policy_loss=-7.2740 policy updated! \n",
      "train step 03017 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1528 diff={max=05.0648, min=00.0432, mean=00.9852} policy_loss=-6.5179 policy updated! \n",
      "train step 03018 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9199 diff={max=05.2199, min=00.1421, mean=00.9974} policy_loss=-6.7391 policy updated! \n",
      "train step 03019 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4725 diff={max=06.4821, min=00.0125, mean=01.3553} policy_loss=-6.8081 policy updated! \n",
      "train step 03020 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.0087 diff={max=06.1345, min=00.0022, mean=01.1462} policy_loss=-7.3499 policy updated! \n",
      "train step 03021 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=04.8555 diff={max=08.0856, min=00.0015, mean=01.4295} policy_loss=-7.3603 policy updated! \n",
      "train step 03022 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0401 diff={max=06.2725, min=00.0212, mean=01.0618} policy_loss=-8.5956 policy updated! \n",
      "train step 03023 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1100 diff={max=06.1074, min=00.0202, mean=01.1985} policy_loss=-7.3330 policy updated! \n",
      "train step 03024 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9240 diff={max=05.0054, min=00.0060, mean=00.9664} policy_loss=-6.8311 policy updated! \n",
      "train step 03025 reward={max=08.0000, min=08.0000, mean=08.0000} optimizing loss=03.9562 diff={max=05.9675, min=00.0245, mean=01.3872} policy_loss=-7.0146 policy updated! \n",
      "train step 03026 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0813 diff={max=04.3635, min=00.0091, mean=01.0374} policy_loss=-7.2324 policy updated! \n",
      "train step 03027 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5065 diff={max=09.1035, min=00.0195, mean=01.3382} policy_loss=-6.2695 policy updated! \n",
      "train step 03028 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8827 diff={max=08.1554, min=00.0191, mean=01.0018} policy_loss=-5.8443 policy updated! \n",
      "train step 03029 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5858 diff={max=03.2469, min=00.0042, mean=00.9635} policy_loss=-6.5798 policy updated! \n",
      "train step 03030 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.0883 diff={max=06.8282, min=00.0517, mean=01.1525} policy_loss=-7.3131 policy updated! \n",
      "train step 03031 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3815 diff={max=03.9025, min=00.0280, mean=00.8296} policy_loss=-6.4205 policy updated! \n",
      "train step 03032 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2834 diff={max=07.3584, min=00.0164, mean=00.9405} policy_loss=-7.3911 policy updated! \n",
      "train step 03033 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7026 diff={max=07.3276, min=00.0576, mean=01.0001} policy_loss=-7.5011 policy updated! \n",
      "train step 03034 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2503 diff={max=07.0962, min=00.0313, mean=01.5071} policy_loss=-7.2633 policy updated! \n",
      "train step 03035 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.0686 diff={max=05.4996, min=00.0229, mean=01.2293} policy_loss=-7.7249 policy updated! \n",
      "train step 03036 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7733 diff={max=07.2434, min=00.0322, mean=00.9936} policy_loss=-6.4614 policy updated! \n",
      "train step 03037 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6410 diff={max=05.6613, min=00.0122, mean=01.1737} policy_loss=-7.4824 policy updated! \n",
      "train step 03038 reward={max=08.0000, min=00.0000, mean=03.2000} optimizing loss=03.5690 diff={max=06.4364, min=00.0307, mean=01.2421} policy_loss=-6.6518 policy updated! \n",
      "train step 03039 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3569 diff={max=02.9659, min=00.0047, mean=00.8975} policy_loss=-6.7035 policy updated! \n",
      "train step 03040 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.4078 diff={max=08.3511, min=00.0019, mean=01.0618} policy_loss=-7.9002 policy updated! \n",
      "train step 03041 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1740 diff={max=05.0000, min=00.0050, mean=01.0644} policy_loss=-8.2314 policy updated! \n",
      "train step 03042 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6471 diff={max=04.7955, min=00.0152, mean=00.9108} policy_loss=-7.5193 policy updated! \n",
      "train step 03043 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4491 diff={max=07.1889, min=00.0311, mean=01.0292} policy_loss=-7.0618 policy updated! \n",
      "train step 03044 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=03.0076 diff={max=04.1234, min=00.0222, mean=01.2727} policy_loss=-7.0099 policy updated! \n",
      "train step 03045 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=05.1542 diff={max=06.6991, min=00.0077, mean=01.6623} policy_loss=-8.5140 policy updated! \n",
      "train step 03046 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8425 diff={max=04.1282, min=00.0056, mean=00.9526} policy_loss=-6.5674 policy updated! \n",
      "train step 03047 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1959 diff={max=03.5935, min=00.0001, mean=00.8381} policy_loss=-5.8587 policy updated! \n",
      "train step 03048 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8262 diff={max=04.4857, min=00.0051, mean=00.9324} policy_loss=-6.8823 policy updated! \n",
      "train step 03049 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3733 diff={max=04.1908, min=00.0166, mean=00.8216} policy_loss=-6.3363 policy updated! \n",
      "train step 03050 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=02.5023 diff={max=03.9084, min=00.0036, mean=01.1601} policy_loss=-7.2335 policy updated! \n",
      "train step 03051 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7829 diff={max=04.6878, min=00.0071, mean=00.8141} policy_loss=-7.0921 policy updated! \n",
      "train step 03052 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4647 diff={max=07.8576, min=00.0413, mean=00.9918} policy_loss=-7.6469 policy updated! \n",
      "train step 03053 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6066 diff={max=05.2796, min=00.0098, mean=01.1730} policy_loss=-7.6584 policy updated! \n",
      "train step 03054 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7202 diff={max=03.5896, min=00.0014, mean=00.9657} policy_loss=-7.0337 policy updated! \n",
      "train step 03055 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=04.6367 diff={max=07.6474, min=00.0462, mean=01.3380} policy_loss=-7.1512 policy updated! \n",
      "train step 03056 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8318 diff={max=04.0929, min=00.0086, mean=00.9688} policy_loss=-6.9239 policy updated! \n",
      "train step 03057 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2716 diff={max=05.1480, min=00.1313, mean=01.0306} policy_loss=-6.8725 policy updated! \n",
      "train step 03058 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1821 diff={max=04.9077, min=00.0347, mean=01.2485} policy_loss=-6.6409 policy updated! \n",
      "train step 03059 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0855 diff={max=05.9490, min=00.0234, mean=01.0131} policy_loss=-6.4931 policy updated! \n",
      "train step 03060 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.3138 diff={max=04.5866, min=00.0267, mean=00.7974} policy_loss=-7.4525 policy updated! \n",
      "train step 03061 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7346 diff={max=06.7147, min=00.0377, mean=01.1026} policy_loss=-7.0178 policy updated! \n",
      "train step 03062 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1807 diff={max=05.9467, min=00.0143, mean=01.1378} policy_loss=-7.8992 policy updated! \n",
      "train step 03063 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3912 diff={max=07.3360, min=00.0082, mean=01.1698} policy_loss=-7.1995 policy updated! \n",
      "train step 03064 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3786 diff={max=04.1679, min=00.0001, mean=00.7887} policy_loss=-7.4987 policy updated! \n",
      "train step 03065 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.5639 diff={max=04.4917, min=00.0123, mean=00.8839} policy_loss=-7.3022 policy updated! \n",
      "train step 03066 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7446 diff={max=04.2654, min=00.0232, mean=00.8980} policy_loss=-6.5910 policy updated! \n",
      "train step 03067 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6412 diff={max=03.8135, min=00.0133, mean=00.9733} policy_loss=-6.3745 policy updated! \n",
      "train step 03068 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0973 diff={max=07.6260, min=00.0433, mean=01.0428} policy_loss=-7.3949 policy updated! \n",
      "train step 03069 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5984 diff={max=03.8647, min=00.0029, mean=00.8649} policy_loss=-7.1229 policy updated! \n",
      "train step 03070 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=05.2534 diff={max=06.5673, min=00.0186, mean=01.4455} policy_loss=-6.8159 policy updated! \n",
      "train step 03071 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6652 diff={max=05.0104, min=00.0531, mean=00.8600} policy_loss=-7.4268 policy updated! \n",
      "train step 03072 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4959 diff={max=04.9994, min=00.0019, mean=00.8013} policy_loss=-8.1435 policy updated! \n",
      "train step 03073 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6684 diff={max=04.0451, min=00.0041, mean=00.8648} policy_loss=-7.1256 policy updated! \n",
      "train step 03074 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=01.5698 diff={max=05.2059, min=00.0656, mean=00.8872} policy_loss=-7.5950 policy updated! \n",
      "train step 03075 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=02.8538 diff={max=06.2272, min=00.0066, mean=01.0628} policy_loss=-7.3882 policy updated! \n",
      "train step 03076 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1546 diff={max=05.9791, min=00.0170, mean=00.9046} policy_loss=-6.9308 policy updated! \n",
      "train step 03077 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8763 diff={max=04.4247, min=00.0333, mean=00.8913} policy_loss=-6.9936 policy updated! \n",
      "train step 03078 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9239 diff={max=06.6532, min=00.0557, mean=00.8818} policy_loss=-6.2401 policy updated! \n",
      "train step 03079 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9073 diff={max=03.8281, min=00.0021, mean=00.9098} policy_loss=-7.3688 policy updated! \n",
      "train step 03080 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.0243 diff={max=07.4196, min=00.0284, mean=01.1504} policy_loss=-7.1718 policy updated! \n",
      "train step 03081 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6420 diff={max=05.9718, min=00.0047, mean=01.0230} policy_loss=-8.3393 policy updated! \n",
      "train step 03082 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2007 diff={max=03.8368, min=00.0269, mean=00.7750} policy_loss=-7.6318 policy updated! \n",
      "train step 03083 reward={max=08.0000, min=00.0000, mean=03.2000} optimizing loss=03.1037 diff={max=05.8164, min=00.0631, mean=01.1300} policy_loss=-7.1944 policy updated! \n",
      "train step 03084 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6489 diff={max=04.6412, min=00.0146, mean=00.8963} policy_loss=-7.3000 policy updated! \n",
      "train step 03085 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=02.6461 diff={max=05.0097, min=00.0101, mean=01.1140} policy_loss=-6.8067 policy updated! \n",
      "train step 03086 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1306 diff={max=02.7237, min=00.0024, mean=00.8241} policy_loss=-6.9498 policy updated! \n",
      "train step 03087 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5891 diff={max=04.6748, min=00.0225, mean=01.0717} policy_loss=-6.2615 policy updated! \n",
      "train step 03088 reward={max=08.0000, min=00.0000, mean=03.2000} optimizing loss=02.3218 diff={max=06.4526, min=00.0598, mean=00.9594} policy_loss=-6.8085 policy updated! \n",
      "train step 03089 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=01.9291 diff={max=03.8951, min=00.0078, mean=00.9821} policy_loss=-7.3656 policy updated! \n",
      "train step 03090 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=02.6000 diff={max=04.7091, min=00.0049, mean=01.0464} policy_loss=-6.6115 policy updated! \n",
      "train step 03091 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=01.6588 diff={max=03.0688, min=00.0136, mean=00.9663} policy_loss=-6.7313 policy updated! \n",
      "train step 03092 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4730 diff={max=06.3287, min=00.0393, mean=00.9651} policy_loss=-7.1783 policy updated! \n",
      "train step 03093 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0638 diff={max=04.9530, min=00.0278, mean=01.0304} policy_loss=-7.2627 policy updated! \n",
      "train step 03094 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=03.1835 diff={max=07.3920, min=00.0449, mean=01.1817} policy_loss=-6.7350 policy updated! \n",
      "train step 03095 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=02.8137 diff={max=05.7600, min=00.0048, mean=01.1066} policy_loss=-7.4005 policy updated! \n",
      "train step 03096 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6033 diff={max=07.8373, min=00.0065, mean=01.1237} policy_loss=-6.8622 policy updated! \n",
      "train step 03097 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2197 diff={max=03.5294, min=00.0029, mean=00.7567} policy_loss=-6.3300 policy updated! \n",
      "train step 03098 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6330 diff={max=05.8819, min=00.1449, mean=01.3853} policy_loss=-7.3523 policy updated! \n",
      "train step 03099 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6954 diff={max=06.5755, min=00.0078, mean=01.0681} policy_loss=-7.4670 policy updated! \n",
      "train step 03100 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.5158 diff={max=03.9300, min=00.0647, mean=00.8996} policy_loss=-6.9409 policy updated! \n",
      "train step 03101 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1320 diff={max=06.7312, min=00.0019, mean=01.1446} policy_loss=-6.5952 policy updated! \n",
      "train step 03102 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3967 diff={max=05.2214, min=00.0053, mean=01.2971} policy_loss=-8.1822 policy updated! \n",
      "train step 03103 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1809 diff={max=04.1305, min=00.0003, mean=00.7626} policy_loss=-7.5812 policy updated! \n",
      "train step 03104 reward={max=09.0000, min=00.0000, mean=07.0000} optimizing loss=02.5000 diff={max=05.3106, min=00.0114, mean=00.9985} policy_loss=-7.2670 policy updated! \n",
      "train step 03105 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.5288 diff={max=03.1972, min=00.0360, mean=00.9642} policy_loss=-7.9204 policy updated! \n",
      "train step 03106 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5003 diff={max=06.7631, min=00.0270, mean=00.9853} policy_loss=-6.7426 policy updated! \n",
      "train step 03107 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0621 diff={max=03.1826, min=00.0649, mean=00.7912} policy_loss=-7.0315 policy updated! \n",
      "train step 03108 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5761 diff={max=06.3174, min=00.0183, mean=00.7413} policy_loss=-6.4897 policy updated! \n",
      "train step 03109 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0009 diff={max=04.4683, min=00.0131, mean=01.0244} policy_loss=-5.8657 policy updated! \n",
      "train step 03110 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=02.9468 diff={max=06.8962, min=00.0195, mean=01.1538} policy_loss=-6.5602 policy updated! \n",
      "train step 03111 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3268 diff={max=06.1199, min=00.0213, mean=01.2121} policy_loss=-7.3516 policy updated! \n",
      "train step 03112 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7182 diff={max=04.5453, min=00.0113, mean=00.8630} policy_loss=-7.1020 policy updated! \n",
      "train step 03113 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4976 diff={max=08.7637, min=00.0271, mean=01.5276} policy_loss=-7.2024 policy updated! \n",
      "train step 03114 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3834 diff={max=05.3504, min=00.0001, mean=00.7176} policy_loss=-6.7211 policy updated! \n",
      "train step 03115 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.9596 diff={max=07.7668, min=00.0013, mean=01.1905} policy_loss=-7.8348 policy updated! \n",
      "train step 03116 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0971 diff={max=03.5481, min=00.0090, mean=00.7741} policy_loss=-7.5387 policy updated! \n",
      "train step 03117 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2502 diff={max=09.5976, min=00.0103, mean=01.1137} policy_loss=-7.3906 policy updated! \n",
      "train step 03118 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8695 diff={max=03.7300, min=00.0045, mean=00.6130} policy_loss=-7.0279 policy updated! \n",
      "train step 03119 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5859 diff={max=06.1374, min=00.0005, mean=01.2223} policy_loss=-7.4974 policy updated! \n",
      "train step 03120 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.6785 diff={max=08.5071, min=00.0032, mean=00.9460} policy_loss=-8.6103 policy updated! \n",
      "train step 03121 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6444 diff={max=06.4717, min=00.0358, mean=01.0137} policy_loss=-7.6984 policy updated! \n",
      "train step 03122 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7585 diff={max=05.9964, min=00.0144, mean=00.8649} policy_loss=-6.1328 policy updated! \n",
      "train step 03123 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=02.3732 diff={max=05.6552, min=00.0008, mean=00.9793} policy_loss=-7.7320 policy updated! \n",
      "train step 03124 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1237 diff={max=03.9606, min=00.0115, mean=00.7898} policy_loss=-8.1612 policy updated! \n",
      "train step 03125 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.8621 diff={max=06.4729, min=00.0023, mean=01.2548} policy_loss=-6.6478 policy updated! \n",
      "train step 03126 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0484 diff={max=06.2629, min=00.0245, mean=01.0470} policy_loss=-7.7181 policy updated! \n",
      "train step 03127 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3509 diff={max=04.2675, min=00.0072, mean=00.7132} policy_loss=-7.5895 policy updated! \n",
      "train step 03128 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6042 diff={max=06.1200, min=00.0536, mean=01.0168} policy_loss=-8.3679 policy updated! \n",
      "train step 03129 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4034 diff={max=06.8389, min=00.0065, mean=01.0576} policy_loss=-6.8960 policy updated! \n",
      "train step 03130 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.4120 diff={max=06.2385, min=00.0391, mean=01.2474} policy_loss=-7.4270 policy updated! \n",
      "train step 03131 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1376 diff={max=03.7232, min=00.0224, mean=00.7214} policy_loss=-6.0997 policy updated! \n",
      "train step 03132 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6917 diff={max=04.7292, min=00.0229, mean=00.9016} policy_loss=-6.2743 policy updated! \n",
      "train step 03133 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1751 diff={max=02.7863, min=00.0285, mean=00.8000} policy_loss=-8.3266 policy updated! \n",
      "train step 03134 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3181 diff={max=02.7814, min=00.0197, mean=00.8210} policy_loss=-8.3896 policy updated! \n",
      "train step 03135 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.1152 diff={max=04.1649, min=00.0270, mean=00.6315} policy_loss=-6.4277 policy updated! \n",
      "train step 03136 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8344 diff={max=04.5840, min=00.0055, mean=00.8341} policy_loss=-7.4002 policy updated! \n",
      "train step 03137 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8375 diff={max=03.9323, min=00.0063, mean=00.9524} policy_loss=-7.2756 policy updated! \n",
      "train step 03138 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6029 diff={max=05.0349, min=00.0009, mean=00.8010} policy_loss=-6.5700 policy updated! \n",
      "train step 03139 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2532 diff={max=07.1293, min=00.0016, mean=01.0829} policy_loss=-7.4640 policy updated! \n",
      "train step 03140 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.5958 diff={max=03.6820, min=00.0006, mean=00.8519} policy_loss=-6.7648 policy updated! \n",
      "train step 03141 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8683 diff={max=05.5070, min=00.0007, mean=01.0323} policy_loss=-6.6695 policy updated! \n",
      "train step 03142 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.4802 diff={max=14.1695, min=00.0199, mean=01.4202} policy_loss=-6.8048 policy updated! \n",
      "train step 03143 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4965 diff={max=04.0137, min=00.0667, mean=00.9259} policy_loss=-5.9313 policy updated! \n",
      "train step 03144 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5414 diff={max=04.0845, min=00.0288, mean=00.9124} policy_loss=-7.1219 policy updated! \n",
      "train step 03145 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.8814 diff={max=06.3314, min=00.0212, mean=01.0696} policy_loss=-7.9625 policy updated! \n",
      "train step 03146 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4979 diff={max=07.5637, min=00.1082, mean=01.5143} policy_loss=-9.1918 policy updated! \n",
      "train step 03147 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0952 diff={max=06.5191, min=00.0006, mean=01.0084} policy_loss=-8.1898 policy updated! \n",
      "train step 03148 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=03.7041 diff={max=06.7962, min=00.0528, mean=01.4236} policy_loss=-8.1418 policy updated! \n",
      "train step 03149 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9372 diff={max=07.0157, min=00.0233, mean=01.1677} policy_loss=-7.0351 policy updated! \n",
      "train step 03150 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.1526 diff={max=06.3198, min=00.0170, mean=01.2274} policy_loss=-6.6575 policy updated! \n",
      "train step 03151 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6968 diff={max=04.9436, min=00.0469, mean=01.1393} policy_loss=-6.6471 policy updated! \n",
      "train step 03152 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0443 diff={max=05.0822, min=00.0828, mean=01.2191} policy_loss=-6.6878 policy updated! \n",
      "train step 03153 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7704 diff={max=06.8813, min=00.0142, mean=00.9744} policy_loss=-5.9977 policy updated! \n",
      "train step 03154 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9051 diff={max=05.4751, min=00.0318, mean=00.9058} policy_loss=-7.2947 policy updated! \n",
      "train step 03155 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.2346 diff={max=03.3066, min=00.0094, mean=00.8076} policy_loss=-6.5719 policy updated! \n",
      "train step 03156 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5320 diff={max=03.3223, min=00.0010, mean=00.9466} policy_loss=-7.6507 policy updated! \n",
      "train step 03157 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9699 diff={max=03.7791, min=00.0062, mean=01.0771} policy_loss=-5.7858 policy updated! \n",
      "train step 03158 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3579 diff={max=04.8534, min=00.0576, mean=01.1460} policy_loss=-7.5983 policy updated! \n",
      "train step 03159 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2463 diff={max=04.5695, min=00.0011, mean=01.0631} policy_loss=-8.2386 policy updated! \n",
      "train step 03160 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=05.5315 diff={max=09.2983, min=00.0104, mean=01.2870} policy_loss=-5.9412 policy updated! \n",
      "train step 03161 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4980 diff={max=05.9995, min=00.0075, mean=01.2543} policy_loss=-6.6966 policy updated! \n",
      "train step 03162 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5957 diff={max=07.0465, min=00.0009, mean=00.9694} policy_loss=-6.9843 policy updated! \n",
      "train step 03163 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2113 diff={max=06.0399, min=00.0114, mean=00.9273} policy_loss=-6.9439 policy updated! \n",
      "train step 03164 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=03.4365 diff={max=06.9873, min=00.0292, mean=01.1540} policy_loss=-6.5781 policy updated! \n",
      "train step 03165 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.6982 diff={max=06.2207, min=00.0118, mean=01.0718} policy_loss=-6.5427 policy updated! \n",
      "train step 03166 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9715 diff={max=07.2522, min=00.0045, mean=01.1342} policy_loss=-7.1481 policy updated! \n",
      "train step 03167 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8397 diff={max=06.7644, min=00.0258, mean=01.3703} policy_loss=-7.2012 policy updated! \n",
      "train step 03168 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5315 diff={max=04.7768, min=00.0045, mean=01.0955} policy_loss=-7.3198 policy updated! \n",
      "train step 03169 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2348 diff={max=04.1289, min=00.0100, mean=00.7522} policy_loss=-7.3142 policy updated! \n",
      "train step 03170 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.4502 diff={max=06.8249, min=00.0076, mean=01.0216} policy_loss=-6.3213 policy updated! \n",
      "train step 03171 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6726 diff={max=05.4255, min=00.0122, mean=00.9146} policy_loss=-6.9002 policy updated! \n",
      "train step 03172 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2409 diff={max=06.9819, min=00.0090, mean=01.0986} policy_loss=-6.6427 policy updated! \n",
      "train step 03173 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0529 diff={max=06.8863, min=00.0010, mean=01.1082} policy_loss=-8.6046 policy updated! \n",
      "train step 03174 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=03.1044 diff={max=05.5632, min=00.0054, mean=01.1365} policy_loss=-6.3218 policy updated! \n",
      "train step 03175 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.2765 diff={max=04.7510, min=00.0016, mean=00.7560} policy_loss=-7.1671 policy updated! \n",
      "train step 03176 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6473 diff={max=05.0398, min=00.0714, mean=01.1498} policy_loss=-7.4381 policy updated! \n",
      "train step 03177 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3326 diff={max=06.2546, min=00.0014, mean=00.8966} policy_loss=-6.8001 policy updated! \n",
      "train step 03178 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=02.4893 diff={max=05.0112, min=00.0048, mean=01.1206} policy_loss=-7.5958 policy updated! \n",
      "train step 03179 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2757 diff={max=08.7040, min=00.0254, mean=01.2697} policy_loss=-7.0496 policy updated! \n",
      "train step 03180 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.5141 diff={max=03.6993, min=00.0132, mean=00.8732} policy_loss=-6.9806 policy updated! \n",
      "train step 03181 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8562 diff={max=06.9458, min=00.0600, mean=01.1135} policy_loss=-7.2711 policy updated! \n",
      "train step 03182 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0835 diff={max=03.7178, min=00.0137, mean=01.0628} policy_loss=-6.8464 policy updated! \n",
      "train step 03183 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=02.0898 diff={max=06.1000, min=00.0304, mean=00.9584} policy_loss=-6.1104 policy updated! \n",
      "train step 03184 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7629 diff={max=05.9010, min=00.0323, mean=00.8773} policy_loss=-7.5062 policy updated! \n",
      "train step 03185 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.5703 diff={max=06.0984, min=00.0059, mean=01.2535} policy_loss=-8.7315 policy updated! \n",
      "train step 03186 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8865 diff={max=04.4890, min=00.0441, mean=00.9021} policy_loss=-7.2121 policy updated! \n",
      "train step 03187 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4952 diff={max=03.0295, min=00.0140, mean=00.9801} policy_loss=-7.6934 policy updated! \n",
      "train step 03188 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6625 diff={max=05.7358, min=00.0442, mean=01.0331} policy_loss=-7.0455 policy updated! \n",
      "train step 03189 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2283 diff={max=03.7820, min=00.0766, mean=00.7979} policy_loss=-6.6219 policy updated! \n",
      "train step 03190 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.3303 diff={max=06.1044, min=00.0012, mean=01.2373} policy_loss=-7.3413 policy updated! \n",
      "train step 03191 reward={max=09.0000, min=09.0000, mean=09.0000} optimizing loss=02.4888 diff={max=06.3426, min=00.0138, mean=01.0132} policy_loss=-7.3106 policy updated! \n",
      "train step 03192 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7065 diff={max=07.6934, min=00.0534, mean=00.9641} policy_loss=-6.7082 policy updated! \n",
      "train step 03193 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8889 diff={max=03.3010, min=00.0293, mean=00.6671} policy_loss=-7.0464 policy updated! \n",
      "train step 03194 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0158 diff={max=06.9880, min=00.0272, mean=01.1924} policy_loss=-7.9246 policy updated! \n",
      "train step 03195 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.4587 diff={max=05.4032, min=00.0206, mean=01.0011} policy_loss=-7.0228 policy updated! \n",
      "train step 03196 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4760 diff={max=04.0832, min=00.0222, mean=00.8622} policy_loss=-7.9510 policy updated! \n",
      "train step 03197 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6668 diff={max=08.1707, min=00.0121, mean=01.2214} policy_loss=-7.9794 policy updated! \n",
      "train step 03198 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7646 diff={max=06.7355, min=00.0047, mean=01.2080} policy_loss=-6.5838 policy updated! \n",
      "train step 03199 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8492 diff={max=06.0248, min=00.0084, mean=00.9701} policy_loss=-7.0818 policy updated! \n",
      "train step 03200 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.8262 diff={max=06.0892, min=00.0005, mean=01.0572} policy_loss=-6.1710 policy updated! \n",
      "train step 03201 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1069 diff={max=08.1230, min=00.0020, mean=01.1292} policy_loss=-6.8305 policy updated! \n",
      "train step 03202 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6214 diff={max=04.3405, min=00.0297, mean=00.8699} policy_loss=-6.4187 policy updated! \n",
      "train step 03203 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1573 diff={max=06.6296, min=00.0147, mean=01.0328} policy_loss=-6.3667 policy updated! \n",
      "train step 03204 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5850 diff={max=06.2757, min=00.0760, mean=01.1190} policy_loss=-7.1160 policy updated! \n",
      "train step 03205 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.0443 diff={max=05.5390, min=00.0342, mean=00.9330} policy_loss=-7.7368 policy updated! \n",
      "train step 03206 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4756 diff={max=08.0732, min=00.0116, mean=01.1067} policy_loss=-7.3171 policy updated! \n",
      "train step 03207 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4524 diff={max=03.7680, min=00.0078, mean=00.8368} policy_loss=-7.6060 policy updated! \n",
      "train step 03208 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6212 diff={max=06.5366, min=00.0031, mean=00.9857} policy_loss=-6.6831 policy updated! \n",
      "train step 03209 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8578 diff={max=02.7498, min=00.0225, mean=00.7019} policy_loss=-6.6433 policy updated! \n",
      "train step 03210 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.4538 diff={max=05.7696, min=00.0067, mean=01.0326} policy_loss=-6.9112 policy updated! \n",
      "train step 03211 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5609 diff={max=04.8021, min=00.0064, mean=00.7730} policy_loss=-8.0808 policy updated! \n",
      "train step 03212 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8180 diff={max=05.5367, min=00.0161, mean=01.0946} policy_loss=-7.1902 policy updated! \n",
      "train step 03213 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5484 diff={max=03.5930, min=00.0243, mean=00.8928} policy_loss=-7.5278 policy updated! \n",
      "train step 03214 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=01.9355 diff={max=07.0110, min=00.0083, mean=00.8600} policy_loss=-8.2105 policy updated! \n",
      "train step 03215 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.4527 diff={max=04.9705, min=00.0220, mean=00.7786} policy_loss=-5.8986 policy updated! \n",
      "train step 03216 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9964 diff={max=03.4586, min=00.0259, mean=00.7191} policy_loss=-7.5169 policy updated! \n",
      "train step 03217 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3230 diff={max=07.7712, min=00.0810, mean=01.1457} policy_loss=-7.9155 policy updated! \n",
      "train step 03218 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6509 diff={max=05.5623, min=00.0057, mean=00.7981} policy_loss=-6.3008 policy updated! \n",
      "train step 03219 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1617 diff={max=05.7308, min=00.0644, mean=01.2077} policy_loss=-8.3586 policy updated! \n",
      "train step 03220 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.2598 diff={max=03.2988, min=00.0230, mean=00.8186} policy_loss=-7.3226 policy updated! \n",
      "train step 03221 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0276 diff={max=05.4179, min=00.0007, mean=00.9268} policy_loss=-7.2890 policy updated! \n",
      "train step 03222 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7203 diff={max=04.1488, min=00.0407, mean=00.8915} policy_loss=-7.0106 policy updated! \n",
      "train step 03223 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9726 diff={max=05.3103, min=00.0202, mean=00.9484} policy_loss=-8.0718 policy updated! \n",
      "train step 03224 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8268 diff={max=06.7723, min=00.0123, mean=00.9996} policy_loss=-7.0137 policy updated! \n",
      "train step 03225 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.2432 diff={max=04.0675, min=00.0070, mean=01.0770} policy_loss=-7.7674 policy updated! \n",
      "train step 03226 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3122 diff={max=06.5437, min=00.0242, mean=00.9197} policy_loss=-7.3529 policy updated! \n",
      "train step 03227 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3109 diff={max=04.1261, min=00.0016, mean=00.8542} policy_loss=-7.4342 policy updated! \n",
      "train step 03228 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6933 diff={max=05.0107, min=00.0160, mean=00.8960} policy_loss=-8.0916 policy updated! \n",
      "train step 03229 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=01.2326 diff={max=04.3623, min=00.0062, mean=00.8416} policy_loss=-8.2342 policy updated! \n",
      "train step 03230 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.4760 diff={max=04.7923, min=00.0583, mean=00.7528} policy_loss=-6.6950 policy updated! \n",
      "train step 03231 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6098 diff={max=04.9123, min=00.0229, mean=00.8690} policy_loss=-6.8575 policy updated! \n",
      "train step 03232 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1381 diff={max=07.4417, min=00.0275, mean=01.2856} policy_loss=-7.4245 policy updated! \n",
      "train step 03233 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0152 diff={max=07.5893, min=00.0668, mean=00.9875} policy_loss=-6.8573 policy updated! \n",
      "train step 03234 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6038 diff={max=08.4889, min=00.0184, mean=01.3024} policy_loss=-7.0410 policy updated! \n",
      "train step 03235 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=00.9606 diff={max=03.5393, min=00.0012, mean=00.7028} policy_loss=-6.7471 policy updated! \n",
      "train step 03236 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9814 diff={max=07.1002, min=00.0103, mean=01.3467} policy_loss=-7.4325 policy updated! \n",
      "train step 03237 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7443 diff={max=05.6834, min=00.0006, mean=00.9159} policy_loss=-8.9284 policy updated! \n",
      "train step 03238 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5001 diff={max=04.2193, min=00.0243, mean=00.8688} policy_loss=-7.7120 policy updated! \n",
      "train step 03239 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5332 diff={max=07.8215, min=00.0245, mean=01.3611} policy_loss=-6.7180 policy updated! \n",
      "train step 03240 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.3829 diff={max=05.7419, min=00.0268, mean=00.7043} policy_loss=-6.6779 policy updated! \n",
      "train step 03241 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4816 diff={max=05.3804, min=00.0160, mean=00.8044} policy_loss=-7.0312 policy updated! \n",
      "train step 03242 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5643 diff={max=07.3865, min=00.0141, mean=01.0838} policy_loss=-7.7374 policy updated! \n",
      "train step 03243 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6044 diff={max=06.7404, min=00.0134, mean=01.2792} policy_loss=-6.9562 policy updated! \n",
      "train step 03244 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5493 diff={max=05.5402, min=00.0378, mean=01.0243} policy_loss=-7.4950 policy updated! \n",
      "train step 03245 reward={max=09.0000, min=09.0000, mean=09.0000} optimizing loss=03.4390 diff={max=05.3439, min=00.0031, mean=01.3871} policy_loss=-7.8382 policy updated! \n",
      "train step 03246 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=02.3167 diff={max=03.7830, min=00.0522, mean=01.1136} policy_loss=-7.8775 policy updated! \n",
      "train step 03247 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5241 diff={max=03.5881, min=00.0197, mean=00.8900} policy_loss=-7.3319 policy updated! \n",
      "train step 03248 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=01.4176 diff={max=03.2289, min=00.0040, mean=00.9363} policy_loss=-7.8685 policy updated! \n",
      "train step 03249 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4259 diff={max=07.5294, min=00.0080, mean=00.8837} policy_loss=-7.1091 policy updated! \n",
      "train step 03250 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.7546 diff={max=04.3850, min=00.0107, mean=00.9799} policy_loss=-7.1128 policy updated! \n",
      "train step 03251 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=02.4130 diff={max=05.6759, min=00.1044, mean=01.0369} policy_loss=-6.9589 policy updated! \n",
      "train step 03252 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2622 diff={max=03.8938, min=00.0113, mean=00.8419} policy_loss=-6.6416 policy updated! \n",
      "train step 03253 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7205 diff={max=05.1979, min=00.0181, mean=01.1283} policy_loss=-6.0830 policy updated! \n",
      "train step 03254 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0792 diff={max=03.4036, min=00.0239, mean=00.7344} policy_loss=-6.7233 policy updated! \n",
      "train step 03255 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.5027 diff={max=06.6489, min=00.0173, mean=01.0641} policy_loss=-6.9294 policy updated! \n",
      "train step 03256 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4704 diff={max=05.6732, min=00.0671, mean=01.0436} policy_loss=-7.1010 policy updated! \n",
      "train step 03257 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6310 diff={max=04.0058, min=00.0205, mean=00.8741} policy_loss=-7.9682 policy updated! \n",
      "train step 03258 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8839 diff={max=06.1406, min=00.0011, mean=00.8846} policy_loss=-7.4390 policy updated! \n",
      "train step 03259 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0354 diff={max=06.5298, min=00.0007, mean=01.1514} policy_loss=-8.1960 policy updated! \n",
      "train step 03260 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.8623 diff={max=05.9378, min=00.0203, mean=01.0232} policy_loss=-7.5097 policy updated! \n",
      "train step 03261 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0578 diff={max=08.6411, min=00.0078, mean=01.1939} policy_loss=-7.4411 policy updated! \n",
      "train step 03262 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0652 diff={max=06.3448, min=00.0146, mean=00.8499} policy_loss=-7.0560 policy updated! \n",
      "train step 03263 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2284 diff={max=06.2830, min=00.0110, mean=00.9333} policy_loss=-7.7533 policy updated! \n",
      "train step 03264 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7348 diff={max=06.3691, min=00.0241, mean=01.6844} policy_loss=-9.0047 policy updated! \n",
      "train step 03265 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.6293 diff={max=05.9430, min=00.0393, mean=00.8614} policy_loss=-7.4468 policy updated! \n",
      "train step 03266 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=02.7306 diff={max=04.5130, min=00.0401, mean=01.1868} policy_loss=-6.8137 policy updated! \n",
      "train step 03267 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7753 diff={max=04.7180, min=00.0090, mean=00.9084} policy_loss=-6.6831 policy updated! \n",
      "train step 03268 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9197 diff={max=06.2817, min=00.0640, mean=00.8822} policy_loss=-7.7196 policy updated! \n",
      "train step 03269 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1878 diff={max=06.6737, min=00.0057, mean=00.8811} policy_loss=-6.9010 policy updated! \n",
      "train step 03270 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.3432 diff={max=06.8677, min=00.0070, mean=01.0388} policy_loss=-7.3018 policy updated! \n",
      "train step 03271 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5105 diff={max=07.3150, min=00.0063, mean=01.3294} policy_loss=-6.1867 policy updated! \n",
      "train step 03272 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6256 diff={max=05.1737, min=00.0364, mean=00.8551} policy_loss=-5.8291 policy updated! \n",
      "train step 03273 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2802 diff={max=06.3312, min=00.0374, mean=01.1838} policy_loss=-5.6022 policy updated! \n",
      "train step 03274 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2877 diff={max=05.4521, min=00.0285, mean=01.0776} policy_loss=-7.7693 policy updated! \n",
      "train step 03275 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.8846 diff={max=06.9762, min=00.0024, mean=00.7686} policy_loss=-7.7730 policy updated! \n",
      "train step 03276 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1351 diff={max=08.5439, min=00.0058, mean=01.2096} policy_loss=-7.4358 policy updated! \n",
      "train step 03277 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8215 diff={max=05.4978, min=00.0077, mean=00.9440} policy_loss=-7.6229 policy updated! \n",
      "train step 03278 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=01.1669 diff={max=04.2914, min=00.0001, mean=00.7748} policy_loss=-7.6929 policy updated! \n",
      "train step 03279 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6150 diff={max=07.8909, min=00.0091, mean=01.1476} policy_loss=-7.8263 policy updated! \n",
      "train step 03280 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.5961 diff={max=04.8766, min=00.0237, mean=00.9476} policy_loss=-8.4160 policy updated! \n",
      "train step 03281 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6711 diff={max=07.7494, min=00.0772, mean=01.0933} policy_loss=-6.7335 policy updated! \n",
      "train step 03282 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2405 diff={max=03.2965, min=00.0211, mean=00.8003} policy_loss=-7.8033 policy updated! \n",
      "train step 03283 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8978 diff={max=05.2043, min=00.0104, mean=00.9567} policy_loss=-8.3316 policy updated! \n",
      "train step 03284 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7890 diff={max=03.9925, min=00.0037, mean=00.9052} policy_loss=-8.1550 policy updated! \n",
      "train step 03285 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.2972 diff={max=06.3628, min=00.0044, mean=00.9174} policy_loss=-6.9170 policy updated! \n",
      "train step 03286 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1552 diff={max=03.3643, min=00.0234, mean=00.7290} policy_loss=-8.3356 policy updated! \n",
      "train step 03287 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4766 diff={max=05.8383, min=00.0112, mean=00.8984} policy_loss=-6.2892 policy updated! \n",
      "train step 03288 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7637 diff={max=05.3460, min=00.0117, mean=01.0804} policy_loss=-7.6890 policy updated! \n",
      "train step 03289 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9778 diff={max=03.4357, min=00.0022, mean=00.7191} policy_loss=-7.2619 policy updated! \n",
      "train step 03290 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.7664 diff={max=04.8507, min=00.0164, mean=00.9004} policy_loss=-7.8331 policy updated! \n",
      "train step 03291 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5188 diff={max=05.4501, min=00.0143, mean=01.1381} policy_loss=-7.6257 policy updated! \n",
      "train step 03292 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4074 diff={max=04.5789, min=00.0128, mean=00.8617} policy_loss=-7.0001 policy updated! \n",
      "train step 03293 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1784 diff={max=06.0217, min=00.0130, mean=00.9136} policy_loss=-7.0476 policy updated! \n",
      "train step 03294 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=02.8828 diff={max=06.8643, min=00.0052, mean=00.9536} policy_loss=-6.8384 policy updated! \n",
      "train step 03295 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.3499 diff={max=07.8256, min=00.0046, mean=00.9484} policy_loss=-7.9248 policy updated! \n",
      "train step 03296 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3837 diff={max=06.2094, min=00.0253, mean=00.9539} policy_loss=-7.6006 policy updated! \n",
      "train step 03297 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4654 diff={max=07.3657, min=00.0101, mean=00.9825} policy_loss=-8.0293 policy updated! \n",
      "train step 03298 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3135 diff={max=06.7401, min=00.0067, mean=01.1180} policy_loss=-7.8735 policy updated! \n",
      "train step 03299 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2546 diff={max=05.5760, min=00.0651, mean=00.9845} policy_loss=-8.9004 policy updated! \n",
      "train step 03300 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.8801 diff={max=04.7260, min=00.0345, mean=00.9597} policy_loss=-8.8282 policy updated! \n",
      "train step 03301 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1460 diff={max=04.3529, min=00.0069, mean=01.0362} policy_loss=-7.0020 policy updated! \n",
      "train step 03302 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8410 diff={max=05.7235, min=00.0222, mean=01.1058} policy_loss=-7.4711 policy updated! \n",
      "train step 03303 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8832 diff={max=04.5714, min=00.0285, mean=00.9097} policy_loss=-6.8578 policy updated! \n",
      "train step 03304 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=02.8260 diff={max=08.6348, min=00.0072, mean=00.8994} policy_loss=-5.2695 policy updated! \n",
      "train step 03305 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.6052 diff={max=08.6030, min=00.0288, mean=01.1580} policy_loss=-6.6018 policy updated! \n",
      "train step 03306 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1353 diff={max=06.4142, min=00.0014, mean=00.9825} policy_loss=-6.7479 policy updated! \n",
      "train step 03307 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8879 diff={max=04.2680, min=00.0142, mean=00.9789} policy_loss=-7.3813 policy updated! \n",
      "train step 03308 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7762 diff={max=04.6238, min=00.0176, mean=00.9600} policy_loss=-7.1731 policy updated! \n",
      "train step 03309 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3814 diff={max=04.2489, min=00.0278, mean=00.8574} policy_loss=-8.0690 policy updated! \n",
      "train step 03310 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.7905 diff={max=04.9836, min=00.0103, mean=01.0666} policy_loss=-8.3647 policy updated! \n",
      "train step 03311 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=02.6011 diff={max=05.1082, min=00.0035, mean=01.0504} policy_loss=-7.4865 policy updated! \n",
      "train step 03312 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8666 diff={max=07.4333, min=00.0181, mean=01.5044} policy_loss=-7.6021 policy updated! \n",
      "train step 03313 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9457 diff={max=05.4212, min=00.0025, mean=00.7831} policy_loss=-6.8075 policy updated! \n",
      "train step 03314 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4436 diff={max=07.1777, min=00.0056, mean=01.1749} policy_loss=-8.1990 policy updated! \n",
      "train step 03315 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.0298 diff={max=05.6903, min=00.0767, mean=00.9889} policy_loss=-7.7473 policy updated! \n",
      "train step 03316 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0843 diff={max=04.4970, min=00.0049, mean=00.6979} policy_loss=-7.5091 policy updated! \n",
      "train step 03317 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1361 diff={max=06.8239, min=00.0041, mean=00.9045} policy_loss=-6.9317 policy updated! \n",
      "train step 03318 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5894 diff={max=03.8508, min=00.0194, mean=00.8992} policy_loss=-7.3744 policy updated! \n",
      "train step 03319 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4623 diff={max=06.7824, min=00.0033, mean=01.0067} policy_loss=-7.3896 policy updated! \n",
      "train step 03320 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.2243 diff={max=05.2427, min=00.0016, mean=00.9075} policy_loss=-7.1779 policy updated! \n",
      "train step 03321 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8484 diff={max=06.2081, min=00.0619, mean=01.0899} policy_loss=-8.4014 policy updated! \n",
      "train step 03322 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9571 diff={max=02.7686, min=00.0270, mean=00.7727} policy_loss=-6.4073 policy updated! \n",
      "train step 03323 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6663 diff={max=05.9344, min=00.0211, mean=01.3760} policy_loss=-8.1650 policy updated! \n",
      "train step 03324 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2083 diff={max=05.9492, min=00.0549, mean=01.1018} policy_loss=-8.3046 policy updated! \n",
      "train step 03325 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.0186 diff={max=05.7367, min=00.0201, mean=00.9432} policy_loss=-6.6793 policy updated! \n",
      "train step 03326 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6398 diff={max=06.8675, min=00.0030, mean=00.7422} policy_loss=-8.7312 policy updated! \n",
      "train step 03327 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3514 diff={max=06.3434, min=00.0068, mean=00.9266} policy_loss=-6.7747 policy updated! \n",
      "train step 03328 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7840 diff={max=13.4123, min=00.0031, mean=01.0053} policy_loss=-8.0956 policy updated! \n",
      "train step 03329 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5556 diff={max=03.5749, min=00.0159, mean=00.9142} policy_loss=-7.3289 policy updated! \n",
      "train step 03330 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.4477 diff={max=05.6322, min=00.0249, mean=01.0824} policy_loss=-7.7720 policy updated! \n",
      "train step 03331 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=03.4202 diff={max=06.8079, min=00.0005, mean=01.2660} policy_loss=-6.9191 policy updated! \n",
      "train step 03332 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4464 diff={max=08.8665, min=00.0077, mean=01.2992} policy_loss=-6.7924 policy updated! \n",
      "train step 03333 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0855 diff={max=05.3744, min=00.0251, mean=01.1412} policy_loss=-7.2050 policy updated! \n",
      "train step 03334 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8332 diff={max=06.8531, min=00.0172, mean=00.9539} policy_loss=-7.4728 policy updated! \n",
      "train step 03335 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.3712 diff={max=05.8527, min=00.0184, mean=01.2050} policy_loss=-6.3959 policy updated! \n",
      "train step 03336 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2987 diff={max=04.6846, min=00.0112, mean=00.7715} policy_loss=-7.8554 policy updated! \n",
      "train step 03337 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4613 diff={max=08.8424, min=00.0214, mean=01.0019} policy_loss=-6.7375 policy updated! \n",
      "train step 03338 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3182 diff={max=03.6314, min=00.0210, mean=00.8060} policy_loss=-7.1311 policy updated! \n",
      "train step 03339 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8530 diff={max=06.9263, min=00.0037, mean=01.0187} policy_loss=-7.7523 policy updated! \n",
      "train step 03340 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.4844 diff={max=09.6724, min=00.0365, mean=01.0533} policy_loss=-8.3172 policy updated! \n",
      "train step 03341 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=01.7401 diff={max=03.4754, min=00.0036, mean=00.9390} policy_loss=-6.6261 policy updated! \n",
      "train step 03342 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7880 diff={max=07.8907, min=00.0375, mean=01.1937} policy_loss=-6.7620 policy updated! \n",
      "train step 03343 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6474 diff={max=06.8878, min=00.0032, mean=01.0582} policy_loss=-8.3612 policy updated! \n",
      "train step 03344 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=03.7089 diff={max=11.4672, min=00.0058, mean=00.8134} policy_loss=-7.5300 policy updated! \n",
      "train step 03345 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.5967 diff={max=05.8111, min=00.0030, mean=00.9218} policy_loss=-6.1230 policy updated! \n",
      "train step 03346 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7504 diff={max=05.1527, min=00.0009, mean=00.9159} policy_loss=-7.1715 policy updated! \n",
      "train step 03347 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9072 diff={max=03.7488, min=00.0066, mean=00.6916} policy_loss=-8.0953 policy updated! \n",
      "train step 03348 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4383 diff={max=05.3417, min=00.0235, mean=00.7992} policy_loss=-7.7426 policy updated! \n",
      "train step 03349 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2769 diff={max=06.5893, min=00.0208, mean=00.9357} policy_loss=-8.2116 policy updated! \n",
      "train step 03350 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.0801 diff={max=06.0006, min=00.0055, mean=00.8081} policy_loss=-7.0234 policy updated! \n",
      "train step 03351 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8156 diff={max=05.5105, min=00.0603, mean=00.8795} policy_loss=-7.5291 policy updated! \n",
      "train step 03352 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6326 diff={max=04.4551, min=00.0077, mean=00.9302} policy_loss=-7.0580 policy updated! \n",
      "train step 03353 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8409 diff={max=05.1061, min=00.0056, mean=00.9539} policy_loss=-7.8049 policy updated! \n",
      "train step 03354 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9754 diff={max=02.6231, min=00.0214, mean=00.7489} policy_loss=-7.6855 policy updated! \n",
      "train step 03355 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.8956 diff={max=04.8516, min=00.0305, mean=00.8278} policy_loss=-5.6801 policy updated! \n",
      "train step 03356 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5178 diff={max=03.3231, min=00.0254, mean=00.9591} policy_loss=-8.4908 policy updated! \n",
      "train step 03357 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3872 diff={max=03.7315, min=00.0051, mean=00.8395} policy_loss=-7.9843 policy updated! \n",
      "train step 03358 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1702 diff={max=05.9852, min=00.0154, mean=00.9068} policy_loss=-7.1722 policy updated! \n",
      "train step 03359 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=01.6707 diff={max=04.4293, min=00.0529, mean=00.9156} policy_loss=-6.7439 policy updated! \n",
      "train step 03360 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.3696 diff={max=06.1995, min=00.0392, mean=00.8993} policy_loss=-7.5742 policy updated! \n",
      "train step 03361 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4482 diff={max=05.8566, min=00.0089, mean=00.6919} policy_loss=-6.9352 policy updated! \n",
      "train step 03362 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3565 diff={max=04.5155, min=00.0025, mean=00.7945} policy_loss=-8.3121 policy updated! \n",
      "train step 03363 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=01.6794 diff={max=04.9775, min=00.0074, mean=00.8303} policy_loss=-7.8416 policy updated! \n",
      "train step 03364 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.4357 diff={max=01.7517, min=00.0012, mean=00.5057} policy_loss=-6.2075 policy updated! \n",
      "train step 03365 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.0040 diff={max=03.4710, min=00.0134, mean=00.7008} policy_loss=-6.9841 policy updated! \n",
      "train step 03366 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2398 diff={max=08.5533, min=00.0147, mean=01.0120} policy_loss=-6.8132 policy updated! \n",
      "train step 03367 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0475 diff={max=08.8998, min=00.0064, mean=01.0005} policy_loss=-6.0454 policy updated! \n",
      "train step 03368 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4480 diff={max=06.8530, min=00.0052, mean=01.2259} policy_loss=-6.9209 policy updated! \n",
      "train step 03369 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0425 diff={max=06.7524, min=00.0350, mean=00.8455} policy_loss=-7.8075 policy updated! \n",
      "train step 03370 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.8369 diff={max=08.3182, min=00.0167, mean=00.9537} policy_loss=-8.1919 policy updated! \n",
      "train step 03371 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4652 diff={max=05.3075, min=00.0024, mean=01.3625} policy_loss=-7.1629 policy updated! \n",
      "train step 03372 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6592 diff={max=06.6585, min=00.0730, mean=01.2829} policy_loss=-7.3021 policy updated! \n",
      "train step 03373 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0026 diff={max=06.9540, min=00.0158, mean=00.9162} policy_loss=-7.5065 policy updated! \n",
      "train step 03374 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4248 diff={max=06.9088, min=00.0024, mean=00.9937} policy_loss=-8.4010 policy updated! \n",
      "train step 03375 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.8539 diff={max=03.9378, min=00.0458, mean=01.0104} policy_loss=-8.7161 policy updated! \n",
      "train step 03376 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5107 diff={max=09.0497, min=00.0185, mean=01.0814} policy_loss=-6.5803 policy updated! \n",
      "train step 03377 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0955 diff={max=07.6574, min=00.0049, mean=01.1084} policy_loss=-7.2273 policy updated! \n",
      "train step 03378 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3482 diff={max=06.1796, min=00.0229, mean=00.9792} policy_loss=-6.5864 policy updated! \n",
      "train step 03379 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4817 diff={max=03.6095, min=00.0497, mean=00.8641} policy_loss=-7.0643 policy updated! \n",
      "train step 03380 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=00.8761 diff={max=02.6965, min=00.0011, mean=00.7085} policy_loss=-7.0330 policy updated! \n",
      "train step 03381 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0750 diff={max=06.4719, min=00.0294, mean=01.1694} policy_loss=-7.1669 policy updated! \n",
      "train step 03382 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9771 diff={max=06.0349, min=00.0025, mean=00.8958} policy_loss=-7.9268 policy updated! \n",
      "train step 03383 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4032 diff={max=05.7304, min=00.0623, mean=01.2625} policy_loss=-8.4166 policy updated! \n",
      "train step 03384 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=02.7881 diff={max=05.5863, min=00.0119, mean=01.1468} policy_loss=-8.3824 policy updated! \n",
      "train step 03385 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.6205 diff={max=05.2597, min=00.0048, mean=00.8097} policy_loss=-7.4423 policy updated! \n",
      "train step 03386 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7672 diff={max=06.7344, min=00.0400, mean=01.0146} policy_loss=-7.2592 policy updated! \n",
      "train step 03387 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8675 diff={max=07.9852, min=00.0102, mean=00.7408} policy_loss=-7.9978 policy updated! \n",
      "train step 03388 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8476 diff={max=07.3372, min=00.0004, mean=01.1057} policy_loss=-8.4918 policy updated! \n",
      "train step 03389 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=02.9144 diff={max=06.8010, min=00.0026, mean=01.0797} policy_loss=-8.1138 policy updated! \n",
      "train step 03390 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=00.8519 diff={max=02.3715, min=00.0100, mean=00.7140} policy_loss=-6.1596 policy updated! \n",
      "train step 03391 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7628 diff={max=05.8031, min=00.0112, mean=00.8753} policy_loss=-7.1636 policy updated! \n",
      "train step 03392 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9882 diff={max=04.1120, min=00.0561, mean=01.0491} policy_loss=-8.7359 policy updated! \n",
      "train step 03393 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=02.0145 diff={max=04.8575, min=00.0259, mean=00.8940} policy_loss=-7.9278 policy updated! \n",
      "train step 03394 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3043 diff={max=06.4206, min=00.0240, mean=01.1400} policy_loss=-6.4448 policy updated! \n",
      "train step 03395 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=00.6817 diff={max=02.5355, min=00.0019, mean=00.6135} policy_loss=-7.1609 policy updated! \n",
      "train step 03396 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6657 diff={max=06.4505, min=00.0119, mean=01.2216} policy_loss=-7.0830 policy updated! \n",
      "train step 03397 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7300 diff={max=03.9169, min=00.0031, mean=00.9425} policy_loss=-7.4270 policy updated! \n",
      "train step 03398 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2628 diff={max=07.3078, min=00.0183, mean=01.1376} policy_loss=-7.1621 policy updated! \n",
      "train step 03399 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=03.2432 diff={max=07.8005, min=00.0592, mean=01.1451} policy_loss=-6.9608 policy updated! \n",
      "train step 03400 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.6670 diff={max=06.3990, min=00.0075, mean=01.0708} policy_loss=-7.6897 policy updated! \n",
      "train step 03401 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2122 diff={max=05.3757, min=00.0076, mean=01.1591} policy_loss=-8.8867 policy updated! \n",
      "train step 03402 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5295 diff={max=04.9162, min=00.0123, mean=01.0280} policy_loss=-8.5892 policy updated! \n",
      "train step 03403 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2774 diff={max=06.5841, min=00.0134, mean=01.1397} policy_loss=-7.9814 policy updated! \n",
      "train step 03404 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9761 diff={max=02.5263, min=00.0015, mean=00.7325} policy_loss=-7.1855 policy updated! \n",
      "train step 03405 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.3351 diff={max=06.3133, min=00.0461, mean=00.9876} policy_loss=-8.2221 policy updated! \n",
      "train step 03406 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2555 diff={max=06.0218, min=00.0395, mean=01.0374} policy_loss=-8.7169 policy updated! \n",
      "train step 03407 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1241 diff={max=03.3575, min=00.0163, mean=00.8272} policy_loss=-7.2636 policy updated! \n",
      "train step 03408 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2853 diff={max=07.3390, min=00.0031, mean=01.1287} policy_loss=-6.8452 policy updated! \n",
      "train step 03409 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2383 diff={max=06.2691, min=00.0004, mean=01.3687} policy_loss=-6.9681 policy updated! \n",
      "train step 03410 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.5306 diff={max=04.7751, min=00.0011, mean=01.1079} policy_loss=-6.8088 policy updated! \n",
      "train step 03411 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8995 diff={max=04.1889, min=00.0124, mean=00.9726} policy_loss=-7.2302 policy updated! \n",
      "train step 03412 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5798 diff={max=09.2805, min=00.0139, mean=01.2109} policy_loss=-6.9887 policy updated! \n",
      "train step 03413 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0685 diff={max=06.7810, min=00.0496, mean=01.4102} policy_loss=-7.3286 policy updated! \n",
      "train step 03414 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3397 diff={max=03.4341, min=00.0056, mean=00.8646} policy_loss=-7.6987 policy updated! \n",
      "train step 03415 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.9046 diff={max=06.7015, min=00.0132, mean=01.0760} policy_loss=-7.5109 policy updated! \n",
      "train step 03416 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9512 diff={max=04.8414, min=00.0042, mean=00.8703} policy_loss=-7.2326 policy updated! \n",
      "train step 03417 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3415 diff={max=05.3897, min=00.0505, mean=01.2964} policy_loss=-7.8516 policy updated! \n",
      "train step 03418 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2644 diff={max=05.7262, min=00.0072, mean=00.9780} policy_loss=-7.1389 policy updated! \n",
      "train step 03419 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1622 diff={max=02.7645, min=00.0179, mean=00.8151} policy_loss=-7.4193 policy updated! \n",
      "train step 03420 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.7160 diff={max=06.4691, min=00.0013, mean=00.7736} policy_loss=-6.7681 policy updated! \n",
      "train step 03421 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6977 diff={max=08.2417, min=00.0038, mean=01.0482} policy_loss=-6.5533 policy updated! \n",
      "train step 03422 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9596 diff={max=06.5425, min=00.0346, mean=01.0408} policy_loss=-7.3613 policy updated! \n",
      "train step 03423 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=03.2687 diff={max=06.1037, min=00.0714, mean=01.1104} policy_loss=-7.7167 policy updated! \n",
      "train step 03424 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8193 diff={max=04.6487, min=00.0254, mean=00.9537} policy_loss=-7.2017 policy updated! \n",
      "train step 03425 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.9461 diff={max=06.4270, min=00.0077, mean=00.8305} policy_loss=-7.3987 policy updated! \n",
      "train step 03426 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7217 diff={max=08.8651, min=00.0341, mean=01.1280} policy_loss=-7.0222 policy updated! \n",
      "train step 03427 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4548 diff={max=07.9479, min=00.0153, mean=01.1938} policy_loss=-7.5403 policy updated! \n",
      "train step 03428 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5601 diff={max=04.7813, min=00.0371, mean=01.0789} policy_loss=-6.8818 policy updated! \n",
      "train step 03429 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7667 diff={max=05.8445, min=00.0496, mean=01.0822} policy_loss=-6.3660 policy updated! \n",
      "train step 03430 reward={max=09.0000, min=09.0000, mean=09.0000} optimizing loss=02.9972 diff={max=06.9823, min=00.0009, mean=01.0635} policy_loss=-6.8781 policy updated! \n",
      "train step 03431 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8888 diff={max=05.9430, min=00.0419, mean=01.1830} policy_loss=-6.7269 policy updated! \n",
      "train step 03432 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9839 diff={max=06.0419, min=00.0261, mean=01.0976} policy_loss=-6.3803 policy updated! \n",
      "train step 03433 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2648 diff={max=05.7427, min=00.0019, mean=00.9865} policy_loss=-7.1450 policy updated! \n",
      "train step 03434 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7052 diff={max=07.2753, min=00.0058, mean=01.4220} policy_loss=-8.7093 policy updated! \n",
      "train step 03435 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.2263 diff={max=06.4195, min=00.0935, mean=01.3950} policy_loss=-7.7777 policy updated! \n",
      "train step 03436 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5333 diff={max=05.6952, min=00.0362, mean=01.0364} policy_loss=-7.7759 policy updated! \n",
      "train step 03437 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4647 diff={max=04.9461, min=00.0063, mean=01.1303} policy_loss=-8.0129 policy updated! \n",
      "train step 03438 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=02.4082 diff={max=06.9803, min=00.0141, mean=00.8446} policy_loss=-7.1446 policy updated! \n",
      "train step 03439 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7142 diff={max=05.9282, min=00.0053, mean=01.0697} policy_loss=-7.6575 policy updated! \n",
      "train step 03440 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=00.7171 diff={max=01.9770, min=00.0134, mean=00.6574} policy_loss=-6.9553 policy updated! \n",
      "train step 03441 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=02.6890 diff={max=07.2659, min=00.0488, mean=01.0019} policy_loss=-6.7388 policy updated! \n",
      "train step 03442 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7351 diff={max=03.9189, min=00.0739, mean=01.0187} policy_loss=-6.5753 policy updated! \n",
      "train step 03443 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6950 diff={max=04.1145, min=00.0006, mean=00.9725} policy_loss=-7.5591 policy updated! \n",
      "train step 03444 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7473 diff={max=03.9721, min=00.0596, mean=00.9519} policy_loss=-7.7992 policy updated! \n",
      "train step 03445 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.1460 diff={max=02.7020, min=00.0300, mean=00.8102} policy_loss=-7.9757 policy updated! \n",
      "train step 03446 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6191 diff={max=03.9563, min=00.0010, mean=00.9377} policy_loss=-7.4110 policy updated! \n",
      "train step 03447 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4646 diff={max=09.9453, min=00.0160, mean=01.3826} policy_loss=-6.9399 policy updated! \n",
      "train step 03448 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6926 diff={max=05.7099, min=00.0150, mean=00.7796} policy_loss=-6.6474 policy updated! \n",
      "train step 03449 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2330 diff={max=05.3442, min=00.0243, mean=01.0039} policy_loss=-8.0383 policy updated! \n",
      "train step 03450 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.9395 diff={max=04.6942, min=00.0536, mean=01.0291} policy_loss=-8.0073 policy updated! \n",
      "train step 03451 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=01.0747 diff={max=02.8282, min=00.0105, mean=00.8186} policy_loss=-8.8680 policy updated! \n",
      "train step 03452 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8328 diff={max=08.3162, min=00.0010, mean=01.1709} policy_loss=-7.6855 policy updated! \n",
      "train step 03453 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4737 diff={max=06.2678, min=00.0518, mean=01.1086} policy_loss=-6.9755 policy updated! \n",
      "train step 03454 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4324 diff={max=04.3101, min=00.0082, mean=01.0869} policy_loss=-6.9226 policy updated! \n",
      "train step 03455 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.6496 diff={max=03.2286, min=00.0114, mean=00.9902} policy_loss=-6.0585 policy updated! \n",
      "train step 03456 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5717 diff={max=04.8838, min=00.0128, mean=00.9465} policy_loss=-6.5005 policy updated! \n",
      "train step 03457 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4286 diff={max=05.1464, min=00.0102, mean=00.7848} policy_loss=-7.9136 policy updated! \n",
      "train step 03458 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9813 diff={max=05.1191, min=00.0007, mean=00.8461} policy_loss=-7.6925 policy updated! \n",
      "train step 03459 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8252 diff={max=06.7364, min=00.0003, mean=01.1978} policy_loss=-7.8967 policy updated! \n",
      "train step 03460 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.8246 diff={max=06.5606, min=00.0258, mean=01.3561} policy_loss=-7.7733 policy updated! \n",
      "train step 03461 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2226 diff={max=06.0229, min=00.0231, mean=00.9433} policy_loss=-6.5487 policy updated! \n",
      "train step 03462 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5024 diff={max=03.9908, min=00.0360, mean=00.8277} policy_loss=-8.6989 policy updated! \n",
      "train step 03463 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7072 diff={max=05.5815, min=00.0035, mean=00.7655} policy_loss=-7.2880 policy updated! \n",
      "train step 03464 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8764 diff={max=06.3079, min=00.0025, mean=00.8606} policy_loss=-7.0380 policy updated! \n",
      "train step 03465 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.1021 diff={max=09.1234, min=00.0280, mean=00.9760} policy_loss=-6.7692 policy updated! \n",
      "train step 03466 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9180 diff={max=03.6041, min=00.0496, mean=00.6866} policy_loss=-6.8893 policy updated! \n",
      "train step 03467 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8253 diff={max=06.6794, min=00.0123, mean=01.0908} policy_loss=-7.4400 policy updated! \n",
      "train step 03468 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=02.0352 diff={max=04.4533, min=00.0092, mean=00.9959} policy_loss=-7.8464 policy updated! \n",
      "train step 03469 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3006 diff={max=05.4782, min=00.0009, mean=00.9475} policy_loss=-7.6788 policy updated! \n",
      "train step 03470 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=00.9952 diff={max=02.6739, min=00.0090, mean=00.7060} policy_loss=-5.8377 policy updated! \n",
      "train step 03471 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4964 diff={max=04.7686, min=00.0482, mean=00.8218} policy_loss=-7.1905 policy updated! \n",
      "train step 03472 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6938 diff={max=07.9239, min=00.0270, mean=01.2160} policy_loss=-7.5405 policy updated! \n",
      "train step 03473 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8405 diff={max=05.9093, min=00.0022, mean=00.6948} policy_loss=-7.9993 policy updated! \n",
      "train step 03474 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6477 diff={max=03.7897, min=00.0097, mean=00.8666} policy_loss=-7.6739 policy updated! \n",
      "train step 03475 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.9532 diff={max=06.0496, min=00.0091, mean=00.8500} policy_loss=-7.2388 policy updated! \n",
      "train step 03476 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0296 diff={max=06.9232, min=00.0080, mean=01.0706} policy_loss=-7.7154 policy updated! \n",
      "train step 03477 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0458 diff={max=07.4833, min=00.0027, mean=01.1856} policy_loss=-7.6415 policy updated! \n",
      "train step 03478 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=02.6757 diff={max=07.8507, min=00.0110, mean=00.8549} policy_loss=-7.4363 policy updated! \n",
      "train step 03479 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8726 diff={max=06.4650, min=00.0505, mean=01.1254} policy_loss=-6.6763 policy updated! \n",
      "train step 03480 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.5017 diff={max=06.7196, min=00.0040, mean=01.0883} policy_loss=-7.0382 policy updated! \n",
      "train step 03481 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3127 diff={max=06.4308, min=00.0004, mean=01.0123} policy_loss=-7.7583 policy updated! \n",
      "train step 03482 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2565 diff={max=05.3858, min=00.0027, mean=00.7447} policy_loss=-6.8209 policy updated! \n",
      "train step 03483 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4121 diff={max=04.8030, min=00.0385, mean=01.0261} policy_loss=-8.1682 policy updated! \n",
      "train step 03484 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=02.1789 diff={max=06.1761, min=00.0026, mean=01.0409} policy_loss=-8.5175 policy updated! \n",
      "train step 03485 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=05.3180 diff={max=07.6357, min=00.1328, mean=01.6744} policy_loss=-8.6584 policy updated! \n",
      "train step 03486 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1158 diff={max=04.9456, min=00.0106, mean=01.0533} policy_loss=-7.3975 policy updated! \n",
      "train step 03487 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0698 diff={max=05.5657, min=00.0007, mean=00.8573} policy_loss=-6.5557 policy updated! \n",
      "train step 03488 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1364 diff={max=04.1357, min=00.0279, mean=01.0772} policy_loss=-7.5392 policy updated! \n",
      "train step 03489 reward={max=09.0000, min=09.0000, mean=09.0000} optimizing loss=03.6746 diff={max=06.7529, min=00.0244, mean=01.3316} policy_loss=-6.3647 policy updated! \n",
      "train step 03490 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.5919 diff={max=08.0873, min=00.0257, mean=00.9987} policy_loss=-6.3751 policy updated! \n",
      "train step 03491 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0772 diff={max=05.8405, min=00.0202, mean=00.8529} policy_loss=-6.5595 policy updated! \n",
      "train step 03492 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0086 diff={max=07.0156, min=00.0193, mean=01.0194} policy_loss=-7.5102 policy updated! \n",
      "train step 03493 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0419 diff={max=05.2076, min=00.0476, mean=00.9813} policy_loss=-7.1180 policy updated! \n",
      "train step 03494 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9694 diff={max=07.3747, min=00.0035, mean=01.2242} policy_loss=-8.5293 policy updated! \n",
      "train step 03495 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.1213 diff={max=06.1765, min=00.0109, mean=01.1047} policy_loss=-7.1205 policy updated! \n",
      "train step 03496 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1927 diff={max=08.0539, min=00.0044, mean=01.2877} policy_loss=-8.7525 policy updated! \n",
      "train step 03497 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0884 diff={max=06.7792, min=00.0144, mean=01.1287} policy_loss=-7.0668 policy updated! \n",
      "train step 03498 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1121 diff={max=06.3985, min=00.0210, mean=00.9249} policy_loss=-6.6181 policy updated! \n",
      "train step 03499 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4388 diff={max=04.9638, min=00.0648, mean=01.0817} policy_loss=-6.8915 policy updated! \n",
      "train step 03500 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.1906 diff={max=05.4807, min=00.0266, mean=01.3271} policy_loss=-6.7969 policy updated! \n",
      "train step 03501 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2624 diff={max=11.2974, min=00.0196, mean=01.3632} policy_loss=-7.2349 policy updated! \n",
      "train step 03502 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6455 diff={max=05.5137, min=00.0129, mean=01.0927} policy_loss=-7.8955 policy updated! \n",
      "train step 03503 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6516 diff={max=12.4729, min=00.0032, mean=01.0551} policy_loss=-8.5587 policy updated! \n",
      "train step 03504 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2514 diff={max=06.4090, min=00.0255, mean=00.9669} policy_loss=-7.8500 policy updated! \n",
      "train step 03505 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.5810 diff={max=04.2555, min=00.0166, mean=00.9080} policy_loss=-6.8803 policy updated! \n",
      "train step 03506 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7286 diff={max=03.7190, min=00.0097, mean=00.9555} policy_loss=-8.0821 policy updated! \n",
      "train step 03507 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9296 diff={max=10.3397, min=00.0171, mean=00.7999} policy_loss=-7.3571 policy updated! \n",
      "train step 03508 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8358 diff={max=07.3737, min=00.0028, mean=00.9711} policy_loss=-6.7266 policy updated! \n",
      "train step 03509 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5997 diff={max=04.4446, min=00.0186, mean=00.9320} policy_loss=-7.4293 policy updated! \n",
      "train step 03510 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.8255 diff={max=06.0181, min=00.0197, mean=01.1314} policy_loss=-7.6446 policy updated! \n",
      "train step 03511 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5806 diff={max=08.9099, min=00.0260, mean=01.6934} policy_loss=-7.4581 policy updated! \n",
      "train step 03512 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1664 diff={max=05.9912, min=00.0383, mean=01.3578} policy_loss=-8.3137 policy updated! \n",
      "train step 03513 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=01.0252 diff={max=02.6825, min=00.0145, mean=00.7698} policy_loss=-8.5345 policy updated! \n",
      "train step 03514 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2111 diff={max=04.7129, min=00.0392, mean=01.0816} policy_loss=-7.4037 policy updated! \n",
      "train step 03515 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.2409 diff={max=08.3393, min=00.0170, mean=00.7779} policy_loss=-6.6278 policy updated! \n",
      "train step 03516 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3443 diff={max=05.7762, min=00.0010, mean=00.9798} policy_loss=-7.6325 policy updated! \n",
      "train step 03517 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6770 diff={max=06.8950, min=00.0394, mean=01.0176} policy_loss=-7.7754 policy updated! \n",
      "train step 03518 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8218 diff={max=05.2170, min=00.1264, mean=00.9248} policy_loss=-6.5254 policy updated! \n",
      "train step 03519 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7042 diff={max=04.7923, min=00.0359, mean=00.9612} policy_loss=-6.5293 policy updated! \n",
      "train step 03520 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.5761 diff={max=04.0504, min=00.0211, mean=00.8923} policy_loss=-6.1247 policy updated! \n",
      "train step 03521 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6171 diff={max=06.8871, min=00.0012, mean=01.5124} policy_loss=-8.2585 policy updated! \n",
      "train step 03522 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1363 diff={max=07.7237, min=00.0099, mean=01.0302} policy_loss=-7.6972 policy updated! \n",
      "train step 03523 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=04.6777 diff={max=08.2516, min=00.0028, mean=01.3419} policy_loss=-7.8614 policy updated! \n",
      "train step 03524 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5798 diff={max=07.7273, min=00.0250, mean=01.2198} policy_loss=-7.9544 policy updated! \n",
      "train step 03525 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.5967 diff={max=06.5810, min=00.0512, mean=01.1422} policy_loss=-8.6316 policy updated! \n",
      "train step 03526 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8371 diff={max=11.4405, min=00.0018, mean=01.2733} policy_loss=-6.8936 policy updated! \n",
      "train step 03527 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5923 diff={max=05.7262, min=00.0067, mean=01.0214} policy_loss=-7.2711 policy updated! \n",
      "train step 03528 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1858 diff={max=06.8420, min=00.0256, mean=01.3864} policy_loss=-6.8225 policy updated! \n",
      "train step 03529 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1482 diff={max=05.1220, min=00.0108, mean=01.0373} policy_loss=-6.1476 policy updated! \n",
      "train step 03530 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.1954 diff={max=06.1836, min=00.0076, mean=01.1546} policy_loss=-7.0034 policy updated! \n",
      "train step 03531 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0888 diff={max=05.6049, min=00.0045, mean=01.2829} policy_loss=-7.8689 policy updated! \n",
      "train step 03532 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8351 diff={max=04.7748, min=00.0084, mean=00.9699} policy_loss=-8.5476 policy updated! \n",
      "train step 03533 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1224 diff={max=06.0484, min=00.0038, mean=01.0569} policy_loss=-7.3695 policy updated! \n",
      "train step 03534 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5788 diff={max=08.2733, min=00.0164, mean=01.1812} policy_loss=-8.0081 policy updated! \n",
      "train step 03535 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.2334 diff={max=05.2563, min=00.0026, mean=01.2663} policy_loss=-8.3391 policy updated! \n",
      "train step 03536 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4828 diff={max=05.3690, min=00.0115, mean=00.9648} policy_loss=-7.0613 policy updated! \n",
      "train step 03537 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7670 diff={max=07.8372, min=00.0358, mean=01.0852} policy_loss=-7.6734 policy updated! \n",
      "train step 03538 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5232 diff={max=05.8420, min=00.0266, mean=01.2314} policy_loss=-7.1373 policy updated! \n",
      "train step 03539 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0853 diff={max=04.0593, min=00.0203, mean=00.7782} policy_loss=-5.9245 policy updated! \n",
      "train step 03540 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.3085 diff={max=06.3410, min=00.0179, mean=01.0699} policy_loss=-8.7238 policy updated! \n",
      "train step 03541 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8317 diff={max=07.6957, min=00.0034, mean=01.0369} policy_loss=-6.7891 policy updated! \n",
      "train step 03542 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4648 diff={max=06.3523, min=00.0116, mean=01.0052} policy_loss=-6.5956 policy updated! \n",
      "train step 03543 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4558 diff={max=06.4180, min=00.0022, mean=01.0027} policy_loss=-7.2477 policy updated! \n",
      "train step 03544 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5652 diff={max=04.7021, min=00.0122, mean=01.0726} policy_loss=-6.8742 policy updated! \n",
      "train step 03545 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.3159 diff={max=06.5137, min=00.0023, mean=01.4197} policy_loss=-7.1936 policy updated! \n",
      "train step 03546 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6642 diff={max=08.3297, min=00.0155, mean=01.4527} policy_loss=-8.1535 policy updated! \n",
      "train step 03547 reward={max=09.0000, min=09.0000, mean=09.0000} optimizing loss=02.6922 diff={max=06.3323, min=00.0086, mean=00.9846} policy_loss=-6.6754 policy updated! \n",
      "train step 03548 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=02.3335 diff={max=05.6447, min=00.0161, mean=01.0211} policy_loss=-7.1593 policy updated! \n",
      "train step 03549 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0108 diff={max=06.2183, min=00.0167, mean=01.2340} policy_loss=-7.4397 policy updated! \n",
      "train step 03550 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=05.0760 diff={max=08.0186, min=00.0565, mean=01.3665} policy_loss=-6.5902 policy updated! \n",
      "train step 03551 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=04.2581 diff={max=05.6210, min=00.0487, mean=01.4487} policy_loss=-7.2204 policy updated! \n",
      "train step 03552 reward={max=09.0000, min=09.0000, mean=09.0000} optimizing loss=01.6767 diff={max=03.8956, min=00.0032, mean=00.9862} policy_loss=-8.4402 policy updated! \n",
      "train step 03553 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4211 diff={max=05.8857, min=00.0585, mean=01.0780} policy_loss=-8.2465 policy updated! \n",
      "train step 03554 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9649 diff={max=03.5916, min=00.0939, mean=01.1047} policy_loss=-8.4560 policy updated! \n",
      "train step 03555 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.4780 diff={max=04.6261, min=00.0086, mean=01.2127} policy_loss=-7.6692 policy updated! \n",
      "train step 03556 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6632 diff={max=11.7298, min=00.0012, mean=01.2405} policy_loss=-7.7490 policy updated! \n",
      "train step 03557 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3563 diff={max=04.4742, min=00.0038, mean=00.7521} policy_loss=-7.4240 policy updated! \n",
      "train step 03558 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5419 diff={max=03.9077, min=00.0006, mean=00.8967} policy_loss=-6.5903 policy updated! \n",
      "train step 03559 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1133 diff={max=07.3767, min=00.0057, mean=01.0892} policy_loss=-6.9029 policy updated! \n",
      "train step 03560 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.1783 diff={max=07.0620, min=00.0041, mean=01.2499} policy_loss=-6.3626 policy updated! \n",
      "train step 03561 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5474 diff={max=09.3435, min=00.0149, mean=01.2388} policy_loss=-8.2569 policy updated! \n",
      "train step 03562 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7845 diff={max=04.4951, min=00.0278, mean=00.9283} policy_loss=-8.6411 policy updated! \n",
      "train step 03563 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4409 diff={max=05.1983, min=00.0159, mean=01.0724} policy_loss=-7.7409 policy updated! \n",
      "train step 03564 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1156 diff={max=08.3497, min=00.0089, mean=01.1261} policy_loss=-8.0852 policy updated! \n",
      "train step 03565 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.7047 diff={max=07.5432, min=00.0114, mean=01.2030} policy_loss=-8.1053 policy updated! \n",
      "train step 03566 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9114 diff={max=08.7058, min=00.0780, mean=01.3745} policy_loss=-7.9705 policy updated! \n",
      "train step 03567 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1521 diff={max=03.7438, min=00.0004, mean=00.7610} policy_loss=-6.0510 policy updated! \n",
      "train step 03568 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9166 diff={max=02.3398, min=00.0160, mean=00.7586} policy_loss=-8.1423 policy updated! \n",
      "train step 03569 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4708 diff={max=06.9548, min=00.0074, mean=01.1097} policy_loss=-6.9566 policy updated! \n",
      "train step 03570 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.1494 diff={max=04.9014, min=00.0047, mean=01.0769} policy_loss=-7.1508 policy updated! \n",
      "train step 03571 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5588 diff={max=07.1256, min=00.0225, mean=01.4581} policy_loss=-9.0752 policy updated! \n",
      "train step 03572 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1838 diff={max=05.7143, min=00.0117, mean=01.1611} policy_loss=-7.2270 policy updated! \n",
      "train step 03573 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3903 diff={max=10.8156, min=00.0027, mean=01.2803} policy_loss=-8.2331 policy updated! \n",
      "train step 03574 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=03.0823 diff={max=06.5384, min=00.0117, mean=01.0361} policy_loss=-8.0295 policy updated! \n",
      "train step 03575 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.7063 diff={max=06.8558, min=00.0174, mean=01.3476} policy_loss=-8.3207 policy updated! \n",
      "train step 03576 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7563 diff={max=09.3810, min=00.0042, mean=01.0934} policy_loss=-6.3059 policy updated! \n",
      "train step 03577 reward={max=09.0000, min=09.0000, mean=09.0000} optimizing loss=02.5063 diff={max=04.8320, min=00.0795, mean=01.1573} policy_loss=-8.0930 policy updated! \n",
      "train step 03578 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4672 diff={max=05.8310, min=00.0025, mean=01.2666} policy_loss=-8.0748 policy updated! \n",
      "train step 03579 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.6342 diff={max=02.7502, min=00.0029, mean=00.5868} policy_loss=-7.8350 policy updated! \n",
      "train step 03580 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.3660 diff={max=08.5872, min=00.0090, mean=00.9317} policy_loss=-6.4865 policy updated! \n",
      "train step 03581 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3036 diff={max=06.4936, min=00.0461, mean=01.3423} policy_loss=-7.8166 policy updated! \n",
      "train step 03582 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5760 diff={max=06.6831, min=00.0103, mean=00.9703} policy_loss=-8.1332 policy updated! \n",
      "train step 03583 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2225 diff={max=05.3374, min=00.0475, mean=01.1767} policy_loss=-7.5303 policy updated! \n",
      "train step 03584 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0123 diff={max=04.5179, min=00.0154, mean=00.8707} policy_loss=-8.2081 policy updated! \n",
      "train step 03585 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.8031 diff={max=06.0156, min=00.0074, mean=01.1397} policy_loss=-8.0888 policy updated! \n",
      "train step 03586 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7000 diff={max=04.3808, min=00.0473, mean=00.9583} policy_loss=-7.4722 policy updated! \n",
      "train step 03587 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2348 diff={max=06.1303, min=00.0287, mean=01.1465} policy_loss=-7.5158 policy updated! \n",
      "train step 03588 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7059 diff={max=05.1077, min=00.0180, mean=00.7850} policy_loss=-7.0086 policy updated! \n",
      "train step 03589 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5815 diff={max=06.3466, min=00.0675, mean=01.1204} policy_loss=-7.4460 policy updated! \n",
      "train step 03590 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.5998 diff={max=05.2176, min=00.0081, mean=01.0730} policy_loss=-7.8829 policy updated! \n",
      "train step 03591 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6079 diff={max=04.3775, min=00.0089, mean=00.8659} policy_loss=-7.7922 policy updated! \n",
      "train step 03592 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2827 diff={max=07.9920, min=00.0052, mean=01.0713} policy_loss=-7.4816 policy updated! \n",
      "train step 03593 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2616 diff={max=04.7149, min=00.0493, mean=01.1109} policy_loss=-7.4160 policy updated! \n",
      "train step 03594 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8957 diff={max=04.2144, min=00.0049, mean=00.6164} policy_loss=-6.8556 policy updated! \n",
      "train step 03595 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.9516 diff={max=05.4252, min=00.0085, mean=00.8706} policy_loss=-7.3419 policy updated! \n",
      "train step 03596 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8359 diff={max=05.0202, min=00.0022, mean=00.9108} policy_loss=-8.1119 policy updated! \n",
      "train step 03597 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6952 diff={max=07.2831, min=00.0098, mean=01.3842} policy_loss=-8.2253 policy updated! \n",
      "train step 03598 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=03.1028 diff={max=05.3734, min=00.0244, mean=01.2407} policy_loss=-7.6500 policy updated! \n",
      "train step 03599 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2851 diff={max=05.6889, min=00.0041, mean=00.9690} policy_loss=-8.6248 policy updated! \n",
      "train step 03600 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.0331 diff={max=09.0760, min=00.0511, mean=00.9430} policy_loss=-7.5220 policy updated! \n",
      "train step 03601 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4679 diff={max=05.3570, min=00.0421, mean=01.0312} policy_loss=-7.1862 policy updated! \n",
      "train step 03602 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5396 diff={max=08.2603, min=00.0205, mean=01.1947} policy_loss=-7.9972 policy updated! \n",
      "train step 03603 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8379 diff={max=10.0166, min=00.0195, mean=01.3133} policy_loss=-7.5713 policy updated! \n",
      "train step 03604 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4837 diff={max=06.1682, min=00.0036, mean=00.9484} policy_loss=-6.8784 policy updated! \n",
      "train step 03605 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.2136 diff={max=05.9016, min=00.0466, mean=01.1459} policy_loss=-8.1272 policy updated! \n",
      "train step 03606 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=02.2900 diff={max=04.6051, min=00.0109, mean=00.9975} policy_loss=-7.6619 policy updated! \n",
      "train step 03607 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7019 diff={max=05.0980, min=00.0158, mean=00.9988} policy_loss=-7.7517 policy updated! \n",
      "train step 03608 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3473 diff={max=04.7788, min=00.0179, mean=00.9938} policy_loss=-8.4970 policy updated! \n",
      "train step 03609 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5620 diff={max=05.2634, min=00.0035, mean=00.8107} policy_loss=-6.8156 policy updated! \n",
      "train step 03610 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.8603 diff={max=04.8882, min=00.0005, mean=00.8309} policy_loss=-7.7338 policy updated! \n",
      "train step 03611 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9369 diff={max=05.9062, min=00.0306, mean=00.8906} policy_loss=-7.8279 policy updated! \n",
      "train step 03612 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3900 diff={max=05.9577, min=00.0190, mean=01.0085} policy_loss=-7.6958 policy updated! \n",
      "train step 03613 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9509 diff={max=03.1815, min=00.0241, mean=00.6510} policy_loss=-6.8684 policy updated! \n",
      "train step 03614 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8775 diff={max=06.7716, min=00.0272, mean=00.9963} policy_loss=-6.9211 policy updated! \n",
      "train step 03615 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.3650 diff={max=05.0439, min=00.0120, mean=00.7423} policy_loss=-8.3211 policy updated! \n",
      "train step 03616 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7126 diff={max=04.1239, min=00.0038, mean=00.8785} policy_loss=-6.6212 policy updated! \n",
      "train step 03617 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7531 diff={max=05.4013, min=00.0029, mean=00.9076} policy_loss=-7.3329 policy updated! \n",
      "train step 03618 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7980 diff={max=04.6699, min=00.0209, mean=00.9137} policy_loss=-7.4751 policy updated! \n",
      "train step 03619 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1940 diff={max=04.3374, min=00.0314, mean=00.7727} policy_loss=-6.9801 policy updated! \n",
      "train step 03620 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.4555 diff={max=07.8539, min=00.0039, mean=01.0397} policy_loss=-7.1918 policy updated! \n",
      "train step 03621 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6706 diff={max=04.3208, min=00.0506, mean=00.9108} policy_loss=-6.8431 policy updated! \n",
      "train step 03622 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6441 diff={max=06.7852, min=00.0257, mean=01.1556} policy_loss=-7.4711 policy updated! \n",
      "train step 03623 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8524 diff={max=05.0358, min=00.0249, mean=00.8340} policy_loss=-7.5313 policy updated! \n",
      "train step 03624 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9786 diff={max=05.0641, min=00.0271, mean=00.9060} policy_loss=-8.6540 policy updated! \n",
      "train step 03625 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.2471 diff={max=07.4834, min=00.0041, mean=01.2526} policy_loss=-7.6121 policy updated! \n",
      "train step 03626 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1904 diff={max=03.7087, min=00.0496, mean=00.8217} policy_loss=-7.6474 policy updated! \n",
      "train step 03627 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3901 diff={max=03.3776, min=00.0290, mean=00.8851} policy_loss=-8.6468 policy updated! \n",
      "train step 03628 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8561 diff={max=07.7415, min=00.0156, mean=01.0388} policy_loss=-8.2017 policy updated! \n",
      "train step 03629 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8663 diff={max=05.6727, min=00.0281, mean=00.8854} policy_loss=-6.8536 policy updated! \n",
      "train step 03630 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.6086 diff={max=04.8753, min=00.0017, mean=00.8595} policy_loss=-8.0243 policy updated! \n",
      "train step 03631 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5974 diff={max=06.9181, min=00.0455, mean=01.1670} policy_loss=-8.5206 policy updated! \n",
      "train step 03632 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3687 diff={max=03.6736, min=00.0008, mean=00.8112} policy_loss=-6.4709 policy updated! \n",
      "train step 03633 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=01.9332 diff={max=04.9387, min=00.0301, mean=00.9483} policy_loss=-8.5914 policy updated! \n",
      "train step 03634 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0250 diff={max=06.0644, min=00.0167, mean=00.9810} policy_loss=-8.3073 policy updated! \n",
      "train step 03635 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.0999 diff={max=05.6479, min=00.0287, mean=00.9497} policy_loss=-8.2458 policy updated! \n",
      "train step 03636 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9458 diff={max=04.6403, min=00.0070, mean=00.9935} policy_loss=-8.2623 policy updated! \n",
      "train step 03637 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2962 diff={max=04.3189, min=00.0164, mean=01.0530} policy_loss=-6.7980 policy updated! \n",
      "train step 03638 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8703 diff={max=04.5478, min=00.0251, mean=00.9167} policy_loss=-8.5022 policy updated! \n",
      "train step 03639 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4607 diff={max=06.9977, min=00.0116, mean=01.1171} policy_loss=-7.5516 policy updated! \n",
      "train step 03640 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.3609 diff={max=05.5756, min=00.0327, mean=01.0132} policy_loss=-7.5106 policy updated! \n",
      "train step 03641 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5577 diff={max=06.6775, min=00.0264, mean=00.9568} policy_loss=-7.8350 policy updated! \n",
      "train step 03642 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8770 diff={max=06.4148, min=00.0212, mean=00.8399} policy_loss=-6.2776 policy updated! \n",
      "train step 03643 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5586 diff={max=05.5941, min=00.0915, mean=01.0493} policy_loss=-8.1096 policy updated! \n",
      "train step 03644 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1325 diff={max=04.9578, min=00.0200, mean=00.6853} policy_loss=-7.5032 policy updated! \n",
      "train step 03645 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.4076 diff={max=04.7734, min=00.0278, mean=01.0797} policy_loss=-6.5457 policy updated! \n",
      "train step 03646 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4417 diff={max=04.4243, min=00.0195, mean=00.8029} policy_loss=-6.7499 policy updated! \n",
      "train step 03647 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3059 diff={max=04.9710, min=00.0213, mean=00.7928} policy_loss=-8.0243 policy updated! \n",
      "train step 03648 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7517 diff={max=08.4121, min=00.0041, mean=00.9423} policy_loss=-6.9679 policy updated! \n",
      "train step 03649 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8079 diff={max=05.3813, min=00.0079, mean=01.0268} policy_loss=-8.2740 policy updated! \n",
      "train step 03650 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.6356 diff={max=06.6813, min=00.0245, mean=00.9617} policy_loss=-5.9524 policy updated! \n",
      "train step 03651 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=02.1109 diff={max=05.1493, min=00.0277, mean=00.9366} policy_loss=-8.3568 policy updated! \n",
      "train step 03652 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6646 diff={max=05.9321, min=00.0025, mean=00.7918} policy_loss=-7.1231 policy updated! \n",
      "train step 03653 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4348 diff={max=04.4294, min=00.0241, mean=00.7319} policy_loss=-6.7524 policy updated! \n",
      "train step 03654 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=02.3816 diff={max=05.9191, min=00.0175, mean=00.9624} policy_loss=-6.5672 policy updated! \n",
      "train step 03655 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.2439 diff={max=08.0159, min=00.0039, mean=00.9829} policy_loss=-7.8797 policy updated! \n",
      "train step 03656 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5868 diff={max=08.9113, min=00.0019, mean=01.0052} policy_loss=-7.2828 policy updated! \n",
      "train step 03657 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9222 diff={max=07.9992, min=00.0007, mean=01.0198} policy_loss=-7.4630 policy updated! \n",
      "train step 03658 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9944 diff={max=04.0103, min=00.0328, mean=00.9755} policy_loss=-9.1338 policy updated! \n",
      "train step 03659 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3931 diff={max=08.0658, min=00.0045, mean=00.8786} policy_loss=-7.4120 policy updated! \n",
      "train step 03660 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.0162 diff={max=07.7087, min=00.0034, mean=01.1310} policy_loss=-8.1114 policy updated! \n",
      "train step 03661 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6504 diff={max=05.6271, min=00.0079, mean=01.2457} policy_loss=-8.3069 policy updated! \n",
      "train step 03662 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3865 diff={max=06.9344, min=00.0152, mean=01.2366} policy_loss=-8.3085 policy updated! \n",
      "train step 03663 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8760 diff={max=07.3691, min=00.0132, mean=01.4092} policy_loss=-7.3548 policy updated! \n",
      "train step 03664 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8952 diff={max=06.5421, min=00.0013, mean=01.0190} policy_loss=-7.2100 policy updated! \n",
      "train step 03665 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.1971 diff={max=05.2277, min=00.0082, mean=00.9847} policy_loss=-8.3101 policy updated! \n",
      "train step 03666 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8194 diff={max=08.8105, min=00.0259, mean=01.0604} policy_loss=-6.9242 policy updated! \n",
      "train step 03667 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3442 diff={max=07.4533, min=00.0120, mean=00.9779} policy_loss=-7.4850 policy updated! \n",
      "train step 03668 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7936 diff={max=02.6538, min=00.0085, mean=00.6368} policy_loss=-6.6570 policy updated! \n",
      "train step 03669 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2813 diff={max=05.6495, min=00.0219, mean=00.9427} policy_loss=-7.5484 policy updated! \n",
      "train step 03670 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.7480 diff={max=05.1805, min=00.0141, mean=00.9037} policy_loss=-6.8384 policy updated! \n",
      "train step 03671 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=03.1664 diff={max=08.0652, min=00.0064, mean=01.0460} policy_loss=-6.1913 policy updated! \n",
      "train step 03672 reward={max=09.0000, min=09.0000, mean=09.0000} optimizing loss=02.7795 diff={max=06.9874, min=00.0110, mean=01.0029} policy_loss=-7.0079 policy updated! \n",
      "train step 03673 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=02.2061 diff={max=06.9015, min=00.0163, mean=00.8579} policy_loss=-6.5239 policy updated! \n",
      "train step 03674 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8017 diff={max=05.1383, min=00.0286, mean=00.8446} policy_loss=-6.9359 policy updated! \n",
      "train step 03675 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.2899 diff={max=06.4559, min=00.0076, mean=01.0036} policy_loss=-7.8943 policy updated! \n",
      "train step 03676 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7487 diff={max=05.6266, min=00.0009, mean=00.7710} policy_loss=-6.9871 policy updated! \n",
      "train step 03677 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9554 diff={max=06.3445, min=00.0032, mean=01.0816} policy_loss=-8.3377 policy updated! \n",
      "train step 03678 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5019 diff={max=06.7359, min=00.0246, mean=01.0036} policy_loss=-7.2083 policy updated! \n",
      "train step 03679 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0779 diff={max=03.0992, min=00.0119, mean=00.7311} policy_loss=-7.7189 policy updated! \n",
      "train step 03680 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.6321 diff={max=04.8950, min=00.0300, mean=01.1012} policy_loss=-6.8716 policy updated! \n",
      "train step 03681 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2333 diff={max=04.7797, min=00.0127, mean=00.7002} policy_loss=-7.0316 policy updated! \n",
      "train step 03682 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7264 diff={max=07.9182, min=00.0240, mean=01.3721} policy_loss=-7.6961 policy updated! \n",
      "train step 03683 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8294 diff={max=05.9342, min=00.0021, mean=00.8806} policy_loss=-6.7531 policy updated! \n",
      "train step 03684 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7951 diff={max=07.6668, min=00.0197, mean=00.9575} policy_loss=-7.5057 policy updated! \n",
      "train step 03685 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.9498 diff={max=08.4183, min=00.0301, mean=01.2348} policy_loss=-7.0906 policy updated! \n",
      "train step 03686 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7239 diff={max=07.5149, min=00.0352, mean=01.1610} policy_loss=-7.5360 policy updated! \n",
      "train step 03687 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3831 diff={max=06.1980, min=00.0150, mean=01.0431} policy_loss=-7.1870 policy updated! \n",
      "train step 03688 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5587 diff={max=08.6419, min=00.0223, mean=00.7922} policy_loss=-7.6498 policy updated! \n",
      "train step 03689 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9401 diff={max=06.0024, min=00.0610, mean=00.9381} policy_loss=-7.8850 policy updated! \n",
      "train step 03690 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.1524 diff={max=07.7985, min=00.0196, mean=00.9669} policy_loss=-8.5233 policy updated! \n",
      "train step 03691 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6644 diff={max=07.6405, min=00.0098, mean=00.8956} policy_loss=-6.8842 policy updated! \n",
      "train step 03692 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0843 diff={max=05.9895, min=00.0094, mean=00.8677} policy_loss=-6.8214 policy updated! \n",
      "train step 03693 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5146 diff={max=06.9081, min=00.0177, mean=00.9395} policy_loss=-7.8294 policy updated! \n",
      "train step 03694 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9665 diff={max=05.9245, min=00.0318, mean=00.8848} policy_loss=-6.9653 policy updated! \n",
      "train step 03695 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.3834 diff={max=06.9544, min=00.0026, mean=01.1737} policy_loss=-7.7863 policy updated! \n",
      "train step 03696 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8749 diff={max=06.5614, min=00.0094, mean=01.0699} policy_loss=-7.5090 policy updated! \n",
      "train step 03697 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9841 diff={max=03.8083, min=00.0045, mean=00.9873} policy_loss=-7.3753 policy updated! \n",
      "train step 03698 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3416 diff={max=05.7816, min=00.0083, mean=01.0141} policy_loss=-8.0429 policy updated! \n",
      "train step 03699 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9386 diff={max=07.5561, min=00.0084, mean=01.2979} policy_loss=-8.1358 policy updated! \n",
      "train step 03700 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.5421 diff={max=05.4018, min=00.0323, mean=01.0644} policy_loss=-7.8205 policy updated! \n",
      "train step 03701 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2645 diff={max=06.3584, min=00.0098, mean=01.0244} policy_loss=-8.0955 policy updated! \n",
      "train step 03702 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8632 diff={max=04.5593, min=00.0522, mean=01.1742} policy_loss=-7.5823 policy updated! \n",
      "train step 03703 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2612 diff={max=09.0889, min=00.0157, mean=01.2309} policy_loss=-7.0658 policy updated! \n",
      "train step 03704 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0955 diff={max=03.2013, min=00.0006, mean=00.7353} policy_loss=-7.2560 policy updated! \n",
      "train step 03705 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.6800 diff={max=07.5675, min=00.0248, mean=01.1208} policy_loss=-7.6268 policy updated! \n",
      "train step 03706 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6705 diff={max=03.8748, min=00.0052, mean=00.9494} policy_loss=-8.1929 policy updated! \n",
      "train step 03707 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9280 diff={max=07.9033, min=00.0227, mean=01.1681} policy_loss=-6.9004 policy updated! \n",
      "train step 03708 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0960 diff={max=04.6862, min=00.0551, mean=01.0154} policy_loss=-6.6910 policy updated! \n",
      "train step 03709 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6104 diff={max=07.7649, min=00.0049, mean=01.0855} policy_loss=-7.0011 policy updated! \n",
      "train step 03710 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.8427 diff={max=04.3299, min=00.0119, mean=00.9883} policy_loss=-7.3686 policy updated! \n",
      "train step 03711 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8539 diff={max=08.7640, min=00.0067, mean=01.3340} policy_loss=-6.6692 policy updated! \n",
      "train step 03712 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0766 diff={max=07.4512, min=00.0301, mean=01.0246} policy_loss=-7.6766 policy updated! \n",
      "train step 03713 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9661 diff={max=07.4048, min=00.0228, mean=01.2812} policy_loss=-8.6759 policy updated! \n",
      "train step 03714 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0800 diff={max=06.0606, min=00.0468, mean=01.1672} policy_loss=-8.2692 policy updated! \n",
      "train step 03715 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.8071 diff={max=05.0448, min=00.0069, mean=00.9081} policy_loss=-7.4075 policy updated! \n",
      "train step 03716 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8026 diff={max=03.7344, min=00.0032, mean=00.9935} policy_loss=-7.2077 policy updated! \n",
      "train step 03717 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0712 diff={max=05.6628, min=00.0056, mean=00.9273} policy_loss=-7.5191 policy updated! \n",
      "train step 03718 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6109 diff={max=05.4495, min=00.0023, mean=00.8037} policy_loss=-6.3100 policy updated! \n",
      "train step 03719 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=03.4390 diff={max=06.8364, min=00.0308, mean=01.2478} policy_loss=-8.4014 policy updated! \n",
      "train step 03720 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.0338 diff={max=05.2048, min=00.0130, mean=00.8725} policy_loss=-8.1201 policy updated! \n",
      "train step 03721 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2238 diff={max=05.8965, min=00.0110, mean=00.9921} policy_loss=-7.8804 policy updated! \n",
      "train step 03722 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2614 diff={max=08.0965, min=00.0257, mean=01.2370} policy_loss=-7.6602 policy updated! \n",
      "train step 03723 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5067 diff={max=05.9366, min=00.0166, mean=01.2258} policy_loss=-7.7057 policy updated! \n",
      "train step 03724 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3380 diff={max=04.0248, min=00.0545, mean=00.8207} policy_loss=-8.4151 policy updated! \n",
      "train step 03725 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.2991 diff={max=06.9524, min=00.0004, mean=01.0918} policy_loss=-7.4744 policy updated! \n",
      "train step 03726 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=03.4911 diff={max=08.7888, min=00.0142, mean=01.1187} policy_loss=-6.2849 policy updated! \n",
      "train step 03727 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2801 diff={max=05.6816, min=00.0129, mean=00.9552} policy_loss=-7.8082 policy updated! \n",
      "train step 03728 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3095 diff={max=05.3573, min=00.0117, mean=00.9459} policy_loss=-6.8720 policy updated! \n",
      "train step 03729 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4862 diff={max=09.0400, min=00.0145, mean=01.3449} policy_loss=-7.4951 policy updated! \n",
      "train step 03730 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.7776 diff={max=08.7112, min=00.0251, mean=01.0655} policy_loss=-6.9843 policy updated! \n",
      "train step 03731 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1802 diff={max=05.1969, min=00.0207, mean=00.9343} policy_loss=-7.3708 policy updated! \n",
      "train step 03732 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3162 diff={max=03.8125, min=00.0126, mean=00.7565} policy_loss=-7.1999 policy updated! \n",
      "train step 03733 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3229 diff={max=06.1813, min=00.0228, mean=01.1612} policy_loss=-7.7544 policy updated! \n",
      "train step 03734 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7470 diff={max=06.1280, min=00.0098, mean=01.3618} policy_loss=-8.9307 policy updated! \n",
      "train step 03735 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.1933 diff={max=04.4910, min=00.0190, mean=00.7926} policy_loss=-7.8834 policy updated! \n",
      "train step 03736 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2564 diff={max=08.0385, min=00.0178, mean=01.0685} policy_loss=-6.8918 policy updated! \n",
      "train step 03737 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0802 diff={max=07.1053, min=00.0018, mean=01.0277} policy_loss=-7.3164 policy updated! \n",
      "train step 03738 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6620 diff={max=05.9408, min=00.0015, mean=01.3106} policy_loss=-7.5622 policy updated! \n",
      "train step 03739 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0097 diff={max=06.4130, min=00.0025, mean=01.1175} policy_loss=-7.3720 policy updated! \n",
      "train step 03740 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.9665 diff={max=08.5494, min=00.0422, mean=01.3789} policy_loss=-8.0357 policy updated! \n",
      "train step 03741 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=03.9460 diff={max=08.9209, min=00.0025, mean=01.0834} policy_loss=-8.6805 policy updated! \n",
      "train step 03742 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6203 diff={max=08.5814, min=00.0013, mean=00.9678} policy_loss=-7.0239 policy updated! \n",
      "train step 03743 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4772 diff={max=05.6414, min=00.0114, mean=01.0705} policy_loss=-7.9463 policy updated! \n",
      "train step 03744 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4770 diff={max=04.2023, min=00.0230, mean=00.8218} policy_loss=-6.9509 policy updated! \n",
      "train step 03745 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.5995 diff={max=06.7438, min=00.0017, mean=01.2195} policy_loss=-7.3893 policy updated! \n",
      "train step 03746 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3234 diff={max=05.7691, min=00.0031, mean=01.1688} policy_loss=-7.7024 policy updated! \n",
      "train step 03747 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1575 diff={max=03.9354, min=00.0436, mean=00.7410} policy_loss=-7.7296 policy updated! \n",
      "train step 03748 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6695 diff={max=07.1758, min=00.0224, mean=01.5535} policy_loss=-7.5232 policy updated! \n",
      "train step 03749 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2481 diff={max=05.5673, min=00.0033, mean=00.9735} policy_loss=-8.8499 policy updated! \n",
      "train step 03750 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.3648 diff={max=04.7164, min=00.0150, mean=00.8747} policy_loss=-7.9820 policy updated! \n",
      "train step 03751 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7775 diff={max=05.6253, min=00.0034, mean=01.0721} policy_loss=-7.4908 policy updated! \n",
      "train step 03752 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2258 diff={max=03.9687, min=00.0911, mean=00.8092} policy_loss=-7.7760 policy updated! \n",
      "train step 03753 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6821 diff={max=05.2450, min=00.0312, mean=01.2247} policy_loss=-6.9054 policy updated! \n",
      "train step 03754 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6302 diff={max=05.4065, min=00.0181, mean=01.0705} policy_loss=-7.7214 policy updated! \n",
      "train step 03755 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.2797 diff={max=06.1327, min=00.0276, mean=00.8932} policy_loss=-6.4209 policy updated! \n",
      "train step 03756 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8183 diff={max=06.1426, min=00.0322, mean=01.2965} policy_loss=-7.3831 policy updated! \n",
      "train step 03757 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3710 diff={max=07.5636, min=00.0272, mean=01.4267} policy_loss=-7.0665 policy updated! \n",
      "train step 03758 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8175 diff={max=05.3706, min=00.0190, mean=01.0149} policy_loss=-7.3192 policy updated! \n",
      "train step 03759 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8853 diff={max=05.2990, min=00.0241, mean=01.0050} policy_loss=-8.0801 policy updated! \n",
      "train step 03760 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.8412 diff={max=05.7537, min=00.0374, mean=01.1207} policy_loss=-7.8315 policy updated! \n",
      "train step 03761 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2751 diff={max=06.3271, min=00.0116, mean=01.1183} policy_loss=-7.7582 policy updated! \n",
      "train step 03762 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0028 diff={max=06.8373, min=00.0055, mean=01.1239} policy_loss=-8.3551 policy updated! \n",
      "train step 03763 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5344 diff={max=04.8689, min=00.0077, mean=01.0135} policy_loss=-6.7302 policy updated! \n",
      "train step 03764 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2864 diff={max=03.8975, min=00.0148, mean=00.8274} policy_loss=-8.1108 policy updated! \n",
      "train step 03765 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.6900 diff={max=05.5284, min=00.0200, mean=01.0968} policy_loss=-8.1236 policy updated! \n",
      "train step 03766 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5956 diff={max=06.5530, min=00.0047, mean=01.1530} policy_loss=-6.9816 policy updated! \n",
      "train step 03767 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8167 diff={max=02.4911, min=00.0149, mean=00.6851} policy_loss=-7.6386 policy updated! \n",
      "train step 03768 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0079 diff={max=04.0495, min=00.0019, mean=00.9867} policy_loss=-8.5412 policy updated! \n",
      "train step 03769 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4366 diff={max=05.9523, min=00.0015, mean=00.9668} policy_loss=-7.9081 policy updated! \n",
      "train step 03770 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.2521 diff={max=06.4837, min=00.0176, mean=01.0430} policy_loss=-6.6546 policy updated! \n",
      "train step 03771 reward={max=09.0000, min=09.0000, mean=09.0000} optimizing loss=01.8730 diff={max=07.0243, min=00.0288, mean=00.8293} policy_loss=-6.5561 policy updated! \n",
      "train step 03772 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9518 diff={max=04.6645, min=00.0074, mean=01.1853} policy_loss=-8.7062 policy updated! \n",
      "train step 03773 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2915 diff={max=02.7781, min=00.0050, mean=00.8793} policy_loss=-7.1406 policy updated! \n",
      "train step 03774 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=01.7050 diff={max=03.4759, min=00.0296, mean=00.9729} policy_loss=-7.7360 policy updated! \n",
      "train step 03775 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.9866 diff={max=06.0901, min=00.0132, mean=01.0797} policy_loss=-9.2638 policy updated! \n",
      "train step 03776 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6847 diff={max=05.1234, min=00.0201, mean=00.8970} policy_loss=-7.4578 policy updated! \n",
      "train step 03777 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4616 diff={max=05.9203, min=00.0114, mean=01.0895} policy_loss=-7.5869 policy updated! \n",
      "train step 03778 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7194 diff={max=05.3642, min=00.0038, mean=00.8349} policy_loss=-8.4325 policy updated! \n",
      "train step 03779 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2061 diff={max=05.6177, min=00.0041, mean=00.9194} policy_loss=-8.4446 policy updated! \n",
      "train step 03780 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.7606 diff={max=07.3042, min=00.0036, mean=01.0482} policy_loss=-7.5474 policy updated! \n",
      "train step 03781 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0794 diff={max=07.8029, min=00.0460, mean=01.2176} policy_loss=-7.7354 policy updated! \n",
      "train step 03782 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1567 diff={max=06.7716, min=00.0317, mean=00.9042} policy_loss=-7.2146 policy updated! \n",
      "train step 03783 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1414 diff={max=07.6444, min=00.0178, mean=01.1240} policy_loss=-7.5396 policy updated! \n",
      "train step 03784 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0248 diff={max=05.4761, min=00.0061, mean=00.8341} policy_loss=-7.5639 policy updated! \n",
      "train step 03785 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.6252 diff={max=04.1924, min=00.0283, mean=00.9429} policy_loss=-8.0344 policy updated! \n",
      "train step 03786 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7260 diff={max=07.2644, min=00.0090, mean=01.1153} policy_loss=-7.5440 policy updated! \n",
      "train step 03787 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5870 diff={max=06.2885, min=00.0239, mean=01.0345} policy_loss=-7.8136 policy updated! \n",
      "train step 03788 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2666 diff={max=05.7333, min=00.0000, mean=01.0536} policy_loss=-8.1441 policy updated! \n",
      "train step 03789 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0711 diff={max=05.9134, min=00.0080, mean=01.1188} policy_loss=-9.0351 policy updated! \n",
      "train step 03790 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.3473 diff={max=06.6490, min=00.0189, mean=01.2038} policy_loss=-8.4112 policy updated! \n",
      "train step 03791 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8351 diff={max=03.7820, min=00.0157, mean=00.9779} policy_loss=-8.4348 policy updated! \n",
      "train step 03792 reward={max=09.0000, min=09.0000, mean=09.0000} optimizing loss=01.2359 diff={max=02.5589, min=00.0036, mean=00.8499} policy_loss=-7.8048 policy updated! \n",
      "train step 03793 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9835 diff={max=09.4365, min=00.0156, mean=01.0944} policy_loss=-8.1314 policy updated! \n",
      "train step 03794 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6210 diff={max=06.2452, min=00.0011, mean=01.2560} policy_loss=-7.8909 policy updated! \n",
      "train step 03795 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.9303 diff={max=04.7950, min=00.0064, mean=01.1774} policy_loss=-7.2479 policy updated! \n",
      "train step 03796 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0903 diff={max=05.0850, min=00.0029, mean=00.9152} policy_loss=-8.1693 policy updated! \n",
      "train step 03797 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9371 diff={max=09.5333, min=00.0247, mean=01.3954} policy_loss=-6.4265 policy updated! \n",
      "train step 03798 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6542 diff={max=04.7618, min=00.0339, mean=00.8543} policy_loss=-8.5399 policy updated! \n",
      "train step 03799 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=02.6718 diff={max=05.4982, min=00.0808, mean=01.1166} policy_loss=-7.8832 policy updated! \n",
      "train step 03800 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.4537 diff={max=05.0167, min=00.0097, mean=00.8041} policy_loss=-7.0240 policy updated! \n",
      "train step 03801 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=02.5221 diff={max=05.7775, min=00.0025, mean=01.0090} policy_loss=-6.8652 policy updated! \n",
      "train step 03802 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0033 diff={max=03.4581, min=00.0366, mean=00.7414} policy_loss=-8.3149 policy updated! \n",
      "train step 03803 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4203 diff={max=09.5461, min=00.0220, mean=01.1517} policy_loss=-8.7619 policy updated! \n",
      "train step 03804 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7033 diff={max=07.1884, min=00.0798, mean=01.4958} policy_loss=-8.2346 policy updated! \n",
      "train step 03805 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.8089 diff={max=06.1465, min=00.0011, mean=00.8622} policy_loss=-7.5363 policy updated! \n",
      "train step 03806 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7529 diff={max=05.3127, min=00.0043, mean=00.8379} policy_loss=-6.7761 policy updated! \n",
      "train step 03807 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7828 diff={max=05.3760, min=00.0121, mean=00.9545} policy_loss=-8.5409 policy updated! \n",
      "train step 03808 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0589 diff={max=06.0988, min=00.0034, mean=01.2957} policy_loss=-7.0975 policy updated! \n",
      "train step 03809 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6657 diff={max=07.2661, min=00.0054, mean=01.2747} policy_loss=-7.1706 policy updated! \n",
      "train step 03810 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.3106 diff={max=04.2605, min=00.0041, mean=00.7430} policy_loss=-6.9782 policy updated! \n",
      "train step 03811 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0409 diff={max=02.4477, min=00.0005, mean=00.7887} policy_loss=-6.9929 policy updated! \n",
      "train step 03812 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3272 diff={max=08.3189, min=00.0005, mean=01.0930} policy_loss=-7.9768 policy updated! \n",
      "train step 03813 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4189 diff={max=05.8043, min=00.0001, mean=01.1688} policy_loss=-6.5769 policy updated! \n",
      "train step 03814 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=01.7778 diff={max=05.2428, min=00.0052, mean=00.8539} policy_loss=-6.6816 policy updated! \n",
      "train step 03815 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.3470 diff={max=07.8284, min=00.0024, mean=01.0375} policy_loss=-7.8499 policy updated! \n",
      "train step 03816 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2400 diff={max=07.6107, min=00.0068, mean=01.1175} policy_loss=-6.0708 policy updated! \n",
      "train step 03817 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7037 diff={max=08.5759, min=00.0190, mean=01.1461} policy_loss=-6.6781 policy updated! \n",
      "train step 03818 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3315 diff={max=02.6265, min=00.0301, mean=00.9491} policy_loss=-6.7746 policy updated! \n",
      "train step 03819 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4153 diff={max=03.5874, min=00.0615, mean=00.9148} policy_loss=-7.4212 policy updated! \n",
      "train step 03820 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.8485 diff={max=08.7163, min=00.0031, mean=00.9849} policy_loss=-7.6816 policy updated! \n",
      "train step 03821 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3009 diff={max=04.3727, min=00.0117, mean=01.1500} policy_loss=-7.9240 policy updated! \n",
      "train step 03822 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7527 diff={max=05.5179, min=00.0338, mean=00.9029} policy_loss=-7.2106 policy updated! \n",
      "train step 03823 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=01.3567 diff={max=05.3678, min=00.0151, mean=00.7253} policy_loss=-7.7323 policy updated! \n",
      "train step 03824 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7593 diff={max=06.6240, min=00.0609, mean=01.1305} policy_loss=-8.6656 policy updated! \n",
      "train step 03825 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.5774 diff={max=09.5375, min=00.0041, mean=01.2730} policy_loss=-6.6858 policy updated! \n",
      "train step 03826 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=02.2851 diff={max=05.7603, min=00.0152, mean=00.9771} policy_loss=-7.9368 policy updated! \n",
      "train step 03827 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0101 diff={max=05.8897, min=00.0230, mean=01.1187} policy_loss=-7.1077 policy updated! \n",
      "train step 03828 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8425 diff={max=04.8697, min=00.0252, mean=01.1641} policy_loss=-7.9373 policy updated! \n",
      "train step 03829 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1011 diff={max=07.8478, min=00.0399, mean=01.2380} policy_loss=-8.3379 policy updated! \n",
      "train step 03830 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.2601 diff={max=05.4532, min=00.0006, mean=00.7097} policy_loss=-5.8925 policy updated! \n",
      "train step 03831 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4045 diff={max=05.5202, min=00.0676, mean=01.0723} policy_loss=-6.8767 policy updated! \n",
      "train step 03832 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5919 diff={max=04.1161, min=00.0095, mean=00.8639} policy_loss=-8.1719 policy updated! \n",
      "train step 03833 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3105 diff={max=04.9007, min=00.0023, mean=00.9569} policy_loss=-8.1269 policy updated! \n",
      "train step 03834 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2894 diff={max=07.5481, min=00.0053, mean=01.1034} policy_loss=-7.3295 policy updated! \n",
      "train step 03835 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.3845 diff={max=07.1879, min=00.0104, mean=01.1409} policy_loss=-7.4647 policy updated! \n",
      "train step 03836 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6794 diff={max=06.8485, min=00.0363, mean=01.0558} policy_loss=-7.6841 policy updated! \n",
      "train step 03837 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3926 diff={max=04.8864, min=00.0158, mean=00.7836} policy_loss=-8.0574 policy updated! \n",
      "train step 03838 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6595 diff={max=05.0455, min=00.0045, mean=00.8674} policy_loss=-7.6921 policy updated! \n",
      "train step 03839 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4029 diff={max=05.1555, min=00.0121, mean=00.7726} policy_loss=-7.1095 policy updated! \n",
      "train step 03840 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.7307 diff={max=08.5254, min=00.0016, mean=01.3325} policy_loss=-8.4018 policy updated! \n",
      "train step 03841 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1225 diff={max=07.2875, min=00.0237, mean=00.9431} policy_loss=-7.6100 policy updated! \n",
      "train step 03842 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1116 diff={max=05.8758, min=00.0083, mean=01.1936} policy_loss=-7.3836 policy updated! \n",
      "train step 03843 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4495 diff={max=05.4316, min=00.0811, mean=01.3314} policy_loss=-7.5427 policy updated! \n",
      "train step 03844 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8627 diff={max=05.0517, min=00.0084, mean=00.8532} policy_loss=-7.2451 policy updated! \n",
      "train step 03845 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.0294 diff={max=09.1746, min=00.0130, mean=01.0248} policy_loss=-7.0910 policy updated! \n",
      "train step 03846 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0721 diff={max=06.3812, min=00.0118, mean=00.9101} policy_loss=-8.7090 policy updated! \n",
      "train step 03847 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5168 diff={max=04.8587, min=00.0777, mean=01.0306} policy_loss=-8.7251 policy updated! \n",
      "train step 03848 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1901 diff={max=04.7086, min=00.0200, mean=00.9482} policy_loss=-9.7244 policy updated! \n",
      "train step 03849 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5338 diff={max=04.4875, min=00.0228, mean=00.8945} policy_loss=-9.0882 policy updated! \n",
      "train step 03850 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.6402 diff={max=06.8914, min=00.0703, mean=01.3086} policy_loss=-6.6017 policy updated! \n",
      "train step 03851 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0968 diff={max=05.5756, min=00.0018, mean=00.9926} policy_loss=-6.2252 policy updated! \n",
      "train step 03852 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0643 diff={max=05.6460, min=00.0388, mean=00.9239} policy_loss=-7.3235 policy updated! \n",
      "train step 03853 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0886 diff={max=05.8668, min=00.0081, mean=00.8906} policy_loss=-7.2897 policy updated! \n",
      "train step 03854 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=02.0381 diff={max=04.8244, min=00.0181, mean=00.9656} policy_loss=-7.3118 policy updated! \n",
      "train step 03855 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.8131 diff={max=05.2781, min=00.0189, mean=01.1214} policy_loss=-7.0533 policy updated! \n",
      "train step 03856 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5227 diff={max=10.7953, min=00.0147, mean=01.1591} policy_loss=-8.2874 policy updated! \n",
      "train step 03857 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0926 diff={max=12.5579, min=00.0032, mean=01.1697} policy_loss=-8.2137 policy updated! \n",
      "train step 03858 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7258 diff={max=07.8196, min=00.0104, mean=01.3532} policy_loss=-6.3990 policy updated! \n",
      "train step 03859 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8074 diff={max=07.3325, min=00.0287, mean=01.1655} policy_loss=-7.9475 policy updated! \n",
      "train step 03860 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.4883 diff={max=05.4237, min=00.0119, mean=01.0556} policy_loss=-8.0528 policy updated! \n",
      "train step 03861 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=01.0527 diff={max=03.6453, min=00.0013, mean=00.7323} policy_loss=-6.7594 policy updated! \n",
      "train step 03862 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5093 diff={max=04.1633, min=00.0309, mean=00.8595} policy_loss=-8.9511 policy updated! \n",
      "train step 03863 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0325 diff={max=07.0526, min=00.0066, mean=01.1249} policy_loss=-7.3303 policy updated! \n",
      "train step 03864 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0131 diff={max=05.9492, min=00.0071, mean=00.8997} policy_loss=-8.2009 policy updated! \n",
      "train step 03865 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.1661 diff={max=06.6468, min=00.0385, mean=00.9081} policy_loss=-7.7407 policy updated! \n",
      "train step 03866 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7174 diff={max=01.8484, min=00.0928, mean=00.7013} policy_loss=-7.8369 policy updated! \n",
      "train step 03867 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=03.6890 diff={max=07.7384, min=00.0240, mean=01.2814} policy_loss=-6.8447 policy updated! \n",
      "train step 03868 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=02.8440 diff={max=06.0265, min=00.0089, mean=01.1306} policy_loss=-8.1751 policy updated! \n",
      "train step 03869 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8707 diff={max=05.6566, min=00.0357, mean=01.0707} policy_loss=-8.7206 policy updated! \n",
      "train step 03870 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.5945 diff={max=07.6709, min=00.0420, mean=00.9637} policy_loss=-7.3631 policy updated! \n",
      "train step 03871 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2010 diff={max=02.9182, min=00.0181, mean=00.7610} policy_loss=-7.4673 policy updated! \n",
      "train step 03872 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9116 diff={max=03.9001, min=00.0143, mean=00.6580} policy_loss=-8.3198 policy updated! \n",
      "train step 03873 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8129 diff={max=03.1811, min=00.0011, mean=00.6642} policy_loss=-7.6261 policy updated! \n",
      "train step 03874 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=01.5322 diff={max=04.1269, min=00.0081, mean=00.8201} policy_loss=-7.7592 policy updated! \n",
      "train step 03875 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.3044 diff={max=04.7438, min=00.0165, mean=00.7539} policy_loss=-8.5249 policy updated! \n",
      "train step 03876 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7845 diff={max=08.0698, min=00.0111, mean=01.2137} policy_loss=-7.5960 policy updated! \n",
      "train step 03877 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3048 diff={max=05.6865, min=00.0326, mean=00.9870} policy_loss=-7.1133 policy updated! \n",
      "train step 03878 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0854 diff={max=06.7794, min=00.0247, mean=00.9410} policy_loss=-7.7640 policy updated! \n",
      "train step 03879 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6548 diff={max=05.8739, min=00.0322, mean=00.8300} policy_loss=-7.5787 policy updated! \n",
      "train step 03880 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.6874 diff={max=06.5124, min=00.0063, mean=00.7865} policy_loss=-7.6626 policy updated! \n",
      "train step 03881 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3350 diff={max=09.2888, min=00.0152, mean=01.3582} policy_loss=-6.1432 policy updated! \n",
      "train step 03882 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2658 diff={max=04.5969, min=00.0501, mean=01.0323} policy_loss=-7.1329 policy updated! \n",
      "train step 03883 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4873 diff={max=07.3235, min=00.0032, mean=01.1252} policy_loss=-8.3530 policy updated! \n",
      "train step 03884 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4607 diff={max=04.1930, min=00.0011, mean=00.8050} policy_loss=-7.5957 policy updated! \n",
      "train step 03885 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=01.5980 diff={max=06.7640, min=00.0240, mean=00.7397} policy_loss=-7.9276 policy updated! \n",
      "train step 03886 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1907 diff={max=05.7936, min=00.0070, mean=00.8731} policy_loss=-7.6514 policy updated! \n",
      "train step 03887 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0121 diff={max=03.1683, min=00.0058, mean=00.7143} policy_loss=-7.3640 policy updated! \n",
      "train step 03888 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7371 diff={max=04.8903, min=00.0000, mean=00.8002} policy_loss=-8.7038 policy updated! \n",
      "train step 03889 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=02.7608 diff={max=05.9441, min=00.0238, mean=00.9965} policy_loss=-7.1331 policy updated! \n",
      "train step 03890 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.4367 diff={max=06.7088, min=00.0244, mean=01.2492} policy_loss=-7.9701 policy updated! \n",
      "train step 03891 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=03.8359 diff={max=06.8753, min=00.0243, mean=01.2184} policy_loss=-7.9561 policy updated! \n",
      "train step 03892 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2179 diff={max=07.0585, min=00.0320, mean=01.2583} policy_loss=-9.4630 policy updated! \n",
      "train step 03893 reward={max=08.0000, min=00.0000, mean=03.2000} optimizing loss=01.8232 diff={max=04.4477, min=00.0444, mean=00.9213} policy_loss=-7.5469 policy updated! \n",
      "train step 03894 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=02.3269 diff={max=04.6344, min=00.0048, mean=01.1256} policy_loss=-9.4846 policy updated! \n",
      "train step 03895 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.8072 diff={max=03.7554, min=00.0256, mean=00.9591} policy_loss=-8.2599 policy updated! \n",
      "train step 03896 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3398 diff={max=04.3675, min=00.0096, mean=00.8358} policy_loss=-8.7154 policy updated! \n",
      "train step 03897 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9049 diff={max=05.1307, min=00.0656, mean=01.1923} policy_loss=-8.5965 policy updated! \n",
      "train step 03898 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6883 diff={max=08.7448, min=00.0011, mean=01.1116} policy_loss=-7.0402 policy updated! \n",
      "train step 03899 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0011 diff={max=04.4661, min=00.0519, mean=00.9677} policy_loss=-6.8148 policy updated! \n",
      "train step 03900 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.5573 diff={max=07.2456, min=00.0110, mean=01.2472} policy_loss=-7.9720 policy updated! \n",
      "train step 03901 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3813 diff={max=06.3221, min=00.0079, mean=01.1553} policy_loss=-7.3247 policy updated! \n",
      "train step 03902 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3189 diff={max=03.6561, min=00.0153, mean=00.7549} policy_loss=-8.5552 policy updated! \n",
      "train step 03903 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1979 diff={max=04.7699, min=00.0170, mean=00.9988} policy_loss=-9.0709 policy updated! \n",
      "train step 03904 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3649 diff={max=07.8674, min=00.0298, mean=01.1513} policy_loss=-8.3656 policy updated! \n",
      "train step 03905 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=05.0259 diff={max=07.2423, min=00.0056, mean=01.4579} policy_loss=-7.6920 policy updated! \n",
      "train step 03906 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7314 diff={max=08.4386, min=00.0689, mean=01.2878} policy_loss=-9.2225 policy updated! \n",
      "train step 03907 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1951 diff={max=07.5124, min=00.0799, mean=01.1665} policy_loss=-7.6708 policy updated! \n",
      "train step 03908 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7849 diff={max=07.5682, min=00.0103, mean=00.9873} policy_loss=-6.4243 policy updated! \n",
      "train step 03909 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=02.8472 diff={max=07.1345, min=00.0414, mean=01.0365} policy_loss=-7.0900 policy updated! \n",
      "train step 03910 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=02.2777 diff={max=03.9953, min=00.0403, mean=01.0830} policy_loss=-7.2397 policy updated! \n",
      "train step 03911 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1662 diff={max=06.8463, min=00.0031, mean=01.3808} policy_loss=-8.0156 policy updated! \n",
      "train step 03912 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6804 diff={max=04.4646, min=00.0017, mean=01.1486} policy_loss=-8.1800 policy updated! \n",
      "train step 03913 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2749 diff={max=09.2332, min=00.0085, mean=01.2342} policy_loss=-9.4145 policy updated! \n",
      "train step 03914 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6461 diff={max=06.5076, min=00.0120, mean=01.2407} policy_loss=-9.0051 policy updated! \n",
      "train step 03915 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.3411 diff={max=04.7938, min=00.0189, mean=01.0498} policy_loss=-6.9689 policy updated! \n",
      "train step 03916 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7296 diff={max=05.7108, min=00.0192, mean=01.3928} policy_loss=-7.6433 policy updated! \n",
      "train step 03917 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9213 diff={max=06.2944, min=00.0228, mean=01.4223} policy_loss=-7.2821 policy updated! \n",
      "train step 03918 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7449 diff={max=05.1967, min=00.0316, mean=01.1223} policy_loss=-6.7924 policy updated! \n",
      "train step 03919 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1592 diff={max=07.8718, min=00.0237, mean=01.2901} policy_loss=-6.4260 policy updated! \n",
      "train step 03920 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.3812 diff={max=05.5944, min=00.0343, mean=01.0359} policy_loss=-7.5316 policy updated! \n",
      "train step 03921 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2411 diff={max=07.0663, min=00.0133, mean=01.1620} policy_loss=-10.0123 policy updated! \n",
      "train step 03922 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6270 diff={max=03.8289, min=00.0088, mean=00.9140} policy_loss=-7.6100 policy updated! \n",
      "train step 03923 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1513 diff={max=04.5812, min=00.0474, mean=00.9718} policy_loss=-7.9461 policy updated! \n",
      "train step 03924 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8742 diff={max=04.4353, min=00.0023, mean=01.0216} policy_loss=-8.3767 policy updated! \n",
      "train step 03925 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.4077 diff={max=08.6995, min=00.0182, mean=01.0944} policy_loss=-6.7157 policy updated! \n",
      "train step 03926 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9857 diff={max=03.1691, min=00.0352, mean=00.7032} policy_loss=-7.7343 policy updated! \n",
      "train step 03927 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2672 diff={max=03.1640, min=00.0166, mean=00.8203} policy_loss=-6.9778 policy updated! \n",
      "train step 03928 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8826 diff={max=07.4985, min=00.0016, mean=01.2529} policy_loss=-6.8459 policy updated! \n",
      "train step 03929 reward={max=09.0000, min=00.0000, mean=06.8000} optimizing loss=02.2933 diff={max=04.7449, min=00.0078, mean=01.1017} policy_loss=-6.7759 policy updated! \n",
      "train step 03930 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.5027 diff={max=03.4493, min=00.0094, mean=00.9304} policy_loss=-8.0182 policy updated! \n",
      "train step 03931 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=02.9438 diff={max=05.6585, min=00.0390, mean=01.1170} policy_loss=-7.3188 policy updated! \n",
      "train step 03932 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1662 diff={max=05.6626, min=00.0002, mean=01.1709} policy_loss=-8.4786 policy updated! \n",
      "train step 03933 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7355 diff={max=05.4609, min=00.0061, mean=01.1779} policy_loss=-8.3542 policy updated! \n",
      "train step 03934 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8139 diff={max=05.5193, min=00.0119, mean=01.3463} policy_loss=-7.4287 policy updated! \n",
      "train step 03935 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.4227 diff={max=07.7400, min=00.0093, mean=01.1736} policy_loss=-7.1817 policy updated! \n",
      "train step 03936 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7203 diff={max=04.4479, min=00.0009, mean=00.9024} policy_loss=-7.4046 policy updated! \n",
      "train step 03937 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9325 diff={max=03.0766, min=00.0011, mean=00.7008} policy_loss=-7.4581 policy updated! \n",
      "train step 03938 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.1444 diff={max=02.3432, min=00.0389, mean=00.8737} policy_loss=-7.9108 policy updated! \n",
      "train step 03939 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6827 diff={max=07.6854, min=00.0168, mean=01.3121} policy_loss=-6.7848 policy updated! \n",
      "train step 03940 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.8557 diff={max=08.5024, min=00.0056, mean=01.4642} policy_loss=-6.4769 policy updated! \n",
      "train step 03941 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0609 diff={max=08.1845, min=00.0151, mean=01.4900} policy_loss=-7.5251 policy updated! \n",
      "train step 03942 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2664 diff={max=04.4753, min=00.0203, mean=00.9866} policy_loss=-8.6626 policy updated! \n",
      "train step 03943 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1767 diff={max=03.7230, min=00.0004, mean=00.7722} policy_loss=-8.5029 policy updated! \n",
      "train step 03944 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=02.3412 diff={max=05.6561, min=00.0158, mean=01.1092} policy_loss=-9.0362 policy updated! \n",
      "train step 03945 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.1674 diff={max=05.4101, min=00.0057, mean=01.1270} policy_loss=-7.7543 policy updated! \n",
      "train step 03946 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8967 diff={max=06.0158, min=00.0192, mean=00.8822} policy_loss=-8.1222 policy updated! \n",
      "train step 03947 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8677 diff={max=05.4267, min=00.0203, mean=01.0899} policy_loss=-8.3405 policy updated! \n",
      "train step 03948 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4988 diff={max=03.7223, min=00.0131, mean=00.8720} policy_loss=-8.2603 policy updated! \n",
      "train step 03949 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9879 diff={max=04.6713, min=00.0045, mean=00.9565} policy_loss=-6.8604 policy updated! \n",
      "train step 03950 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=02.0525 diff={max=05.1880, min=00.0144, mean=00.8915} policy_loss=-8.3187 policy updated! \n",
      "train step 03951 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6605 diff={max=07.5610, min=00.0001, mean=01.0675} policy_loss=-6.9000 policy updated! \n",
      "train step 03952 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5187 diff={max=09.0136, min=00.0140, mean=01.0859} policy_loss=-6.7163 policy updated! \n",
      "train step 03953 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9855 diff={max=07.3114, min=00.0698, mean=01.1452} policy_loss=-8.0641 policy updated! \n",
      "train step 03954 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8923 diff={max=08.3270, min=00.0021, mean=01.1586} policy_loss=-7.5447 policy updated! \n",
      "train step 03955 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.8624 diff={max=04.8444, min=00.0055, mean=00.9205} policy_loss=-7.6541 policy updated! \n",
      "train step 03956 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6330 diff={max=07.4124, min=00.0042, mean=01.0312} policy_loss=-7.6827 policy updated! \n",
      "train step 03957 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2179 diff={max=07.5914, min=00.0028, mean=01.0442} policy_loss=-7.3884 policy updated! \n",
      "train step 03958 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3009 diff={max=06.9089, min=00.0247, mean=01.1710} policy_loss=-9.6544 policy updated! \n",
      "train step 03959 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2952 diff={max=05.3311, min=00.0006, mean=01.0111} policy_loss=-9.1422 policy updated! \n",
      "train step 03960 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=02.6633 diff={max=06.5459, min=00.0134, mean=01.1041} policy_loss=-8.2595 policy updated! \n",
      "train step 03961 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5615 diff={max=11.3055, min=00.0324, mean=01.1779} policy_loss=-7.9362 policy updated! \n",
      "train step 03962 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0731 diff={max=09.1607, min=00.0219, mean=01.2666} policy_loss=-7.9184 policy updated! \n",
      "train step 03963 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8965 diff={max=05.9149, min=00.0054, mean=00.8798} policy_loss=-7.1613 policy updated! \n",
      "train step 03964 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=02.7218 diff={max=06.8327, min=00.0254, mean=01.1204} policy_loss=-6.3693 policy updated! \n",
      "train step 03965 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.9903 diff={max=04.5942, min=00.0245, mean=00.9973} policy_loss=-7.4192 policy updated! \n",
      "train step 03966 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=02.4078 diff={max=05.6049, min=00.0155, mean=01.1334} policy_loss=-7.7152 policy updated! \n",
      "train step 03967 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2485 diff={max=03.5242, min=00.0050, mean=00.8287} policy_loss=-8.0774 policy updated! \n",
      "train step 03968 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0163 diff={max=06.5366, min=00.0040, mean=01.1101} policy_loss=-8.6736 policy updated! \n",
      "train step 03969 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6777 diff={max=04.4366, min=00.0021, mean=01.0862} policy_loss=-7.1698 policy updated! \n",
      "train step 03970 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=04.4200 diff={max=07.2872, min=00.0066, mean=01.2660} policy_loss=-7.9491 policy updated! \n",
      "train step 03971 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4407 diff={max=04.1624, min=00.0609, mean=01.1395} policy_loss=-8.7946 policy updated! \n",
      "train step 03972 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7034 diff={max=04.6647, min=00.0116, mean=00.8680} policy_loss=-7.9057 policy updated! \n",
      "train step 03973 reward={max=08.0000, min=00.0000, mean=03.2000} optimizing loss=04.7015 diff={max=08.2214, min=00.0038, mean=01.3765} policy_loss=-6.8310 policy updated! \n",
      "train step 03974 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0213 diff={max=08.6091, min=00.0350, mean=01.3527} policy_loss=-6.8823 policy updated! \n",
      "train step 03975 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=02.5864 diff={max=04.6661, min=00.0070, mean=01.1655} policy_loss=-8.0558 policy updated! \n",
      "train step 03976 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7799 diff={max=03.3374, min=00.0128, mean=00.9292} policy_loss=-7.5092 policy updated! \n",
      "train step 03977 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2953 diff={max=03.5509, min=00.0452, mean=00.8862} policy_loss=-8.0787 policy updated! \n",
      "train step 03978 reward={max=09.0000, min=09.0000, mean=09.0000} optimizing loss=02.6278 diff={max=06.4863, min=00.0099, mean=01.0027} policy_loss=-8.9009 policy updated! \n",
      "train step 03979 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7632 diff={max=05.7308, min=00.0147, mean=01.1086} policy_loss=-8.3622 policy updated! \n",
      "train step 03980 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.9595 diff={max=04.5186, min=00.0051, mean=00.9410} policy_loss=-6.9565 policy updated! \n",
      "train step 03981 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6412 diff={max=03.7445, min=00.0094, mean=00.9052} policy_loss=-6.6592 policy updated! \n",
      "train step 03982 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8039 diff={max=06.6035, min=00.0129, mean=01.1569} policy_loss=-7.5526 policy updated! \n",
      "train step 03983 reward={max=08.0000, min=00.0000, mean=03.2000} optimizing loss=04.2883 diff={max=07.9171, min=00.0137, mean=01.2945} policy_loss=-7.9811 policy updated! \n",
      "train step 03984 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=02.3984 diff={max=05.3337, min=00.0095, mean=01.0518} policy_loss=-7.7008 policy updated! \n",
      "train step 03985 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.2546 diff={max=06.3395, min=00.0266, mean=01.0341} policy_loss=-7.1623 policy updated! \n",
      "train step 03986 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1776 diff={max=06.8573, min=00.0273, mean=01.1606} policy_loss=-6.9511 policy updated! \n",
      "train step 03987 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8570 diff={max=02.9974, min=00.0133, mean=00.6836} policy_loss=-7.6451 policy updated! \n",
      "train step 03988 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3684 diff={max=08.3574, min=00.0507, mean=01.2973} policy_loss=-7.8424 policy updated! \n",
      "train step 03989 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6253 diff={max=09.5003, min=00.0039, mean=01.2800} policy_loss=-7.5866 policy updated! \n",
      "train step 03990 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.7846 diff={max=06.8522, min=00.0000, mean=01.1355} policy_loss=-8.1853 policy updated! \n",
      "train step 03991 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0018 diff={max=07.2019, min=00.0334, mean=01.1749} policy_loss=-7.7336 policy updated! \n",
      "train step 03992 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1955 diff={max=03.8584, min=00.0034, mean=00.7680} policy_loss=-9.4522 policy updated! \n",
      "train step 03993 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5181 diff={max=05.4913, min=00.0559, mean=01.0466} policy_loss=-8.2088 policy updated! \n",
      "train step 03994 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2439 diff={max=06.6458, min=00.0080, mean=01.1538} policy_loss=-7.5570 policy updated! \n",
      "train step 03995 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.0046 diff={max=06.3574, min=00.0253, mean=00.8991} policy_loss=-7.5851 policy updated! \n",
      "train step 03996 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5567 diff={max=07.5318, min=00.0139, mean=00.9016} policy_loss=-8.2254 policy updated! \n",
      "train step 03997 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1384 diff={max=07.9117, min=00.0017, mean=01.0960} policy_loss=-7.6842 policy updated! \n",
      "train step 03998 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6300 diff={max=04.0924, min=00.0023, mean=00.8941} policy_loss=-7.2913 policy updated! \n",
      "train step 03999 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7526 diff={max=06.2315, min=00.0065, mean=00.9922} policy_loss=-8.6914 policy updated! \n",
      "train step 04000 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.6386 diff={max=05.3180, min=00.0386, mean=01.2620} policy_loss=-7.8088 policy updated! \n",
      "train step 04001 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2928 diff={max=04.9425, min=00.0060, mean=00.7783} policy_loss=-8.3749 policy updated! \n",
      "train step 04002 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2619 diff={max=06.3310, min=00.0158, mean=01.1557} policy_loss=-8.2508 policy updated! \n",
      "train step 04003 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9685 diff={max=07.3111, min=00.0206, mean=01.0568} policy_loss=-7.5469 policy updated! \n",
      "train step 04004 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7432 diff={max=06.1560, min=00.0143, mean=00.8070} policy_loss=-7.8865 policy updated! \n",
      "train step 04005 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.5273 diff={max=06.2180, min=00.0176, mean=01.2419} policy_loss=-6.7235 policy updated! \n",
      "train step 04006 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6296 diff={max=07.7563, min=00.0052, mean=01.2318} policy_loss=-8.5406 policy updated! \n",
      "train step 04007 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0195 diff={max=02.9281, min=00.0486, mean=00.7619} policy_loss=-7.5605 policy updated! \n",
      "train step 04008 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7793 diff={max=04.8416, min=00.0037, mean=00.9398} policy_loss=-7.6215 policy updated! \n",
      "train step 04009 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5458 diff={max=04.6541, min=00.0231, mean=00.7855} policy_loss=-6.8400 policy updated! \n",
      "train step 04010 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.2623 diff={max=03.7298, min=00.0259, mean=00.7878} policy_loss=-6.7814 policy updated! \n",
      "train step 04011 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3940 diff={max=05.9111, min=00.0476, mean=00.9206} policy_loss=-7.2144 policy updated! \n",
      "train step 04012 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3514 diff={max=05.3063, min=00.0282, mean=00.9674} policy_loss=-8.1467 policy updated! \n",
      "train step 04013 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9030 diff={max=07.7101, min=00.0046, mean=01.6633} policy_loss=-7.7443 policy updated! \n",
      "train step 04014 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2880 diff={max=08.6034, min=00.0328, mean=01.3645} policy_loss=-8.4728 policy updated! \n",
      "train step 04015 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.9269 diff={max=05.4769, min=00.0140, mean=00.9495} policy_loss=-6.2221 policy updated! \n",
      "train step 04016 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6928 diff={max=04.6777, min=00.0467, mean=00.9732} policy_loss=-8.8304 policy updated! \n",
      "train step 04017 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0459 diff={max=09.0868, min=00.0141, mean=01.1569} policy_loss=-8.1052 policy updated! \n",
      "train step 04018 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7664 diff={max=04.7798, min=00.0082, mean=00.8549} policy_loss=-8.2822 policy updated! \n",
      "train step 04019 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7645 diff={max=04.2647, min=00.0311, mean=00.9518} policy_loss=-9.5539 policy updated! \n",
      "train step 04020 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.8032 diff={max=08.0718, min=00.0145, mean=01.2379} policy_loss=-7.7382 policy updated! \n",
      "train step 04021 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6101 diff={max=04.7712, min=00.0079, mean=00.8504} policy_loss=-7.0025 policy updated! \n",
      "train step 04022 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7656 diff={max=05.0135, min=00.0079, mean=00.7765} policy_loss=-8.0962 policy updated! \n",
      "train step 04023 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=02.0152 diff={max=04.4842, min=00.0195, mean=00.9851} policy_loss=-7.9589 policy updated! \n",
      "train step 04024 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9296 diff={max=08.7401, min=00.0190, mean=00.9746} policy_loss=-7.2643 policy updated! \n",
      "train step 04025 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.1902 diff={max=06.3447, min=00.0090, mean=01.1225} policy_loss=-7.4383 policy updated! \n",
      "train step 04026 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5399 diff={max=04.8230, min=00.0026, mean=00.7828} policy_loss=-7.4772 policy updated! \n",
      "train step 04027 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9798 diff={max=08.4267, min=00.0154, mean=01.1916} policy_loss=-7.2141 policy updated! \n",
      "train step 04028 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8355 diff={max=05.4164, min=00.0039, mean=00.8561} policy_loss=-7.1467 policy updated! \n",
      "train step 04029 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2817 diff={max=04.6227, min=00.0060, mean=01.0167} policy_loss=-7.1791 policy updated! \n",
      "train step 04030 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.6300 diff={max=05.0109, min=00.0002, mean=01.1805} policy_loss=-8.3572 policy updated! \n",
      "train step 04031 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=03.2394 diff={max=07.6560, min=00.0230, mean=01.1291} policy_loss=-8.9928 policy updated! \n",
      "train step 04032 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1499 diff={max=08.3351, min=00.0320, mean=01.2183} policy_loss=-8.1759 policy updated! \n",
      "train step 04033 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0534 diff={max=08.1642, min=00.0075, mean=01.0232} policy_loss=-7.6541 policy updated! \n",
      "train step 04034 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2451 diff={max=07.9144, min=00.0177, mean=01.1533} policy_loss=-7.5445 policy updated! \n",
      "train step 04035 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.4302 diff={max=06.2485, min=00.0002, mean=01.0066} policy_loss=-8.1190 policy updated! \n",
      "train step 04036 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2688 diff={max=04.3412, min=00.0040, mean=01.0629} policy_loss=-8.5234 policy updated! \n",
      "train step 04037 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8323 diff={max=04.7542, min=00.0018, mean=00.9137} policy_loss=-7.7244 policy updated! \n",
      "train step 04038 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6341 diff={max=05.5720, min=00.0554, mean=00.8112} policy_loss=-7.1873 policy updated! \n",
      "train step 04039 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0606 diff={max=07.2009, min=00.0282, mean=00.8530} policy_loss=-7.2646 policy updated! \n",
      "train step 04040 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=05.6937 diff={max=09.0498, min=00.0033, mean=01.4659} policy_loss=-7.7451 policy updated! \n",
      "train step 04041 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3412 diff={max=05.8713, min=00.0662, mean=00.9690} policy_loss=-8.0573 policy updated! \n",
      "train step 04042 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4272 diff={max=07.5742, min=00.0427, mean=01.2127} policy_loss=-7.6043 policy updated! \n",
      "train step 04043 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4211 diff={max=04.1382, min=00.0006, mean=00.8536} policy_loss=-7.7965 policy updated! \n",
      "train step 04044 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8618 diff={max=05.7173, min=00.0149, mean=01.0423} policy_loss=-7.6020 policy updated! \n",
      "train step 04045 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.4585 diff={max=06.1624, min=00.0117, mean=01.3570} policy_loss=-7.8567 policy updated! \n",
      "train step 04046 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0465 diff={max=04.0555, min=00.0254, mean=00.7224} policy_loss=-7.1458 policy updated! \n",
      "train step 04047 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6339 diff={max=05.3031, min=00.0367, mean=00.8266} policy_loss=-7.9134 policy updated! \n",
      "train step 04048 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3862 diff={max=05.8356, min=00.0267, mean=00.9790} policy_loss=-6.9603 policy updated! \n",
      "train step 04049 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3263 diff={max=03.7986, min=00.0034, mean=00.8069} policy_loss=-7.5502 policy updated! \n",
      "train step 04050 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.7824 diff={max=05.9758, min=00.0223, mean=01.0810} policy_loss=-6.4628 policy updated! \n",
      "train step 04051 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6052 diff={max=05.4827, min=00.0315, mean=01.0205} policy_loss=-7.4637 policy updated! \n",
      "train step 04052 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0379 diff={max=08.4727, min=00.0132, mean=01.3752} policy_loss=-7.0370 policy updated! \n",
      "train step 04053 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=02.4270 diff={max=05.3608, min=00.0853, mean=01.0385} policy_loss=-7.1986 policy updated! \n",
      "train step 04054 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6730 diff={max=07.9124, min=00.0034, mean=00.9313} policy_loss=-7.9029 policy updated! \n",
      "train step 04055 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.1961 diff={max=05.8378, min=00.0213, mean=00.9406} policy_loss=-7.8550 policy updated! \n",
      "train step 04056 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2017 diff={max=06.8765, min=00.0197, mean=01.1021} policy_loss=-9.1572 policy updated! \n",
      "train step 04057 reward={max=08.0000, min=00.0000, mean=03.2000} optimizing loss=02.0352 diff={max=05.8519, min=00.0297, mean=00.8330} policy_loss=-7.5866 policy updated! \n",
      "train step 04058 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5017 diff={max=06.9675, min=00.0251, mean=00.8984} policy_loss=-7.1689 policy updated! \n",
      "train step 04059 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9464 diff={max=05.8373, min=00.0254, mean=01.2685} policy_loss=-8.1706 policy updated! \n",
      "train step 04060 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.3622 diff={max=07.7521, min=00.0025, mean=01.0329} policy_loss=-7.5053 policy updated! \n",
      "train step 04061 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2259 diff={max=08.1303, min=00.0022, mean=01.1495} policy_loss=-7.5416 policy updated! \n",
      "train step 04062 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1273 diff={max=05.5415, min=00.0019, mean=00.9493} policy_loss=-7.1939 policy updated! \n",
      "train step 04063 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3830 diff={max=07.2108, min=00.0002, mean=01.3790} policy_loss=-8.1737 policy updated! \n",
      "train step 04064 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4357 diff={max=09.0259, min=00.0210, mean=01.0187} policy_loss=-8.2583 policy updated! \n",
      "train step 04065 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.2332 diff={max=05.9922, min=00.0363, mean=00.9080} policy_loss=-8.1787 policy updated! \n",
      "train step 04066 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2335 diff={max=07.7929, min=00.0043, mean=01.5996} policy_loss=-8.1256 policy updated! \n",
      "train step 04067 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9261 diff={max=08.7585, min=00.0189, mean=01.4076} policy_loss=-9.0246 policy updated! \n",
      "train step 04068 reward={max=08.0000, min=00.0000, mean=03.2000} optimizing loss=02.7358 diff={max=05.2050, min=00.0518, mean=01.2035} policy_loss=-7.0915 policy updated! \n",
      "train step 04069 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8385 diff={max=03.8117, min=00.0210, mean=00.9730} policy_loss=-9.0458 policy updated! \n",
      "train step 04070 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=04.8499 diff={max=09.4634, min=00.0250, mean=01.1343} policy_loss=-7.2622 policy updated! \n",
      "train step 04071 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9101 diff={max=09.2444, min=00.0077, mean=01.4796} policy_loss=-7.4923 policy updated! \n",
      "train step 04072 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6478 diff={max=06.4720, min=00.0241, mean=01.2209} policy_loss=-7.4961 policy updated! \n",
      "train step 04073 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3579 diff={max=05.0726, min=00.0106, mean=00.7567} policy_loss=-9.3899 policy updated! \n",
      "train step 04074 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7732 diff={max=02.7746, min=00.0573, mean=00.6971} policy_loss=-6.9375 policy updated! \n",
      "train step 04075 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.1743 diff={max=06.2475, min=00.0094, mean=01.0849} policy_loss=-7.0223 policy updated! \n",
      "train step 04076 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4211 diff={max=03.1055, min=00.0157, mean=00.8457} policy_loss=-7.7764 policy updated! \n",
      "train step 04077 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4508 diff={max=05.5376, min=00.0372, mean=01.1113} policy_loss=-8.0756 policy updated! \n",
      "train step 04078 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1042 diff={max=06.7555, min=00.0063, mean=01.2554} policy_loss=-8.6221 policy updated! \n",
      "train step 04079 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7771 diff={max=03.2430, min=00.0076, mean=00.6479} policy_loss=-7.8375 policy updated! \n",
      "train step 04080 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.9250 diff={max=08.3381, min=00.0004, mean=01.0276} policy_loss=-8.6647 policy updated! \n",
      "train step 04081 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4855 diff={max=04.7112, min=00.0046, mean=00.8102} policy_loss=-8.0951 policy updated! \n",
      "train step 04082 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9078 diff={max=03.7194, min=00.0325, mean=00.9836} policy_loss=-7.8192 policy updated! \n",
      "train step 04083 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4601 diff={max=06.1013, min=00.0116, mean=00.9709} policy_loss=-7.7269 policy updated! \n",
      "train step 04084 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6778 diff={max=09.6774, min=00.0248, mean=01.2210} policy_loss=-7.3521 policy updated! \n",
      "train step 04085 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.0628 diff={max=05.9481, min=00.0066, mean=00.8302} policy_loss=-8.8448 policy updated! \n",
      "train step 04086 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=05.7194 diff={max=06.7773, min=00.0632, mean=01.6371} policy_loss=-8.4192 policy updated! \n",
      "train step 04087 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1999 diff={max=08.2755, min=00.0791, mean=00.9124} policy_loss=-9.1280 policy updated! \n",
      "train step 04088 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8765 diff={max=07.4945, min=00.0109, mean=01.0868} policy_loss=-6.7844 policy updated! \n",
      "train step 04089 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5490 diff={max=03.5808, min=00.0010, mean=00.8898} policy_loss=-8.0134 policy updated! \n",
      "train step 04090 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=06.0557 diff={max=13.8889, min=00.0602, mean=01.1893} policy_loss=-6.4177 policy updated! \n",
      "train step 04091 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3470 diff={max=05.2563, min=00.0217, mean=01.3274} policy_loss=-7.6822 policy updated! \n",
      "train step 04092 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0625 diff={max=07.7866, min=00.0001, mean=01.2689} policy_loss=-7.0571 policy updated! \n",
      "train step 04093 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4000 diff={max=05.9274, min=00.1568, mean=01.2822} policy_loss=-8.2533 policy updated! \n",
      "train step 04094 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9453 diff={max=07.0808, min=00.0071, mean=00.9619} policy_loss=-8.6330 policy updated! \n",
      "train step 04095 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.1604 diff={max=06.5702, min=00.0485, mean=01.2654} policy_loss=-7.8688 policy updated! \n",
      "train step 04096 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5187 diff={max=07.2917, min=00.0014, mean=00.9918} policy_loss=-8.6492 policy updated! \n",
      "train step 04097 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0352 diff={max=06.7551, min=00.0067, mean=01.3522} policy_loss=-8.1715 policy updated! \n",
      "train step 04098 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9757 diff={max=06.1644, min=00.0109, mean=00.9038} policy_loss=-8.2630 policy updated! \n",
      "train step 04099 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1614 diff={max=06.1609, min=00.0050, mean=01.1722} policy_loss=-8.5415 policy updated! \n",
      "train step 04100 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.9883 diff={max=04.9764, min=00.0116, mean=00.9755} policy_loss=-7.1600 policy updated! \n",
      "train step 04101 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6709 diff={max=06.5930, min=00.0099, mean=01.1839} policy_loss=-8.0232 policy updated! \n",
      "train step 04102 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3798 diff={max=07.6755, min=00.0047, mean=01.0838} policy_loss=-7.6217 policy updated! \n",
      "train step 04103 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5552 diff={max=05.9410, min=00.0184, mean=01.1909} policy_loss=-7.9334 policy updated! \n",
      "train step 04104 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5804 diff={max=06.3084, min=00.0007, mean=01.1004} policy_loss=-8.3933 policy updated! \n",
      "train step 04105 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.7603 diff={max=06.0750, min=00.0289, mean=00.8993} policy_loss=-7.8179 policy updated! \n",
      "train step 04106 reward={max=08.0000, min=00.0000, mean=03.2000} optimizing loss=06.0378 diff={max=09.2095, min=00.0266, mean=01.5309} policy_loss=-8.2686 policy updated! \n",
      "train step 04107 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0867 diff={max=07.4158, min=00.0022, mean=01.0887} policy_loss=-7.0942 policy updated! \n",
      "train step 04108 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1950 diff={max=06.3474, min=00.0283, mean=01.1416} policy_loss=-7.1149 policy updated! \n",
      "train step 04109 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8317 diff={max=05.1880, min=00.0215, mean=00.8964} policy_loss=-6.1946 policy updated! \n",
      "train step 04110 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.3605 diff={max=03.2850, min=00.0131, mean=00.8656} policy_loss=-6.4812 policy updated! \n",
      "train step 04111 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3844 diff={max=06.0693, min=00.0128, mean=01.2284} policy_loss=-6.8080 policy updated! \n",
      "train step 04112 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1555 diff={max=04.9037, min=00.0187, mean=01.1020} policy_loss=-8.7118 policy updated! \n",
      "train step 04113 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3540 diff={max=03.9781, min=00.0019, mean=00.7597} policy_loss=-7.6739 policy updated! \n",
      "train step 04114 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0364 diff={max=08.5843, min=00.0249, mean=01.0524} policy_loss=-7.9109 policy updated! \n",
      "train step 04115 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.8100 diff={max=06.4018, min=00.0177, mean=01.0020} policy_loss=-6.5809 policy updated! \n",
      "train step 04116 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1966 diff={max=07.0170, min=00.0159, mean=01.1920} policy_loss=-9.0328 policy updated! \n",
      "train step 04117 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8088 diff={max=06.9680, min=00.0546, mean=01.2289} policy_loss=-7.2486 policy updated! \n",
      "train step 04118 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5460 diff={max=04.2662, min=00.0014, mean=00.9139} policy_loss=-7.6875 policy updated! \n",
      "train step 04119 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4991 diff={max=06.4457, min=00.0209, mean=00.9292} policy_loss=-5.8109 policy updated! \n",
      "train step 04120 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=05.5259 diff={max=08.5573, min=00.0061, mean=01.6479} policy_loss=-7.5401 policy updated! \n",
      "train step 04121 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=03.7005 diff={max=08.6578, min=00.0030, mean=01.1234} policy_loss=-7.8053 policy updated! \n",
      "train step 04122 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3825 diff={max=04.7671, min=00.0293, mean=00.8236} policy_loss=-10.2226 policy updated! \n",
      "train step 04123 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=05.5665 diff={max=07.6968, min=00.0046, mean=01.5570} policy_loss=-8.9684 policy updated! \n",
      "train step 04124 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5454 diff={max=03.7994, min=00.0647, mean=00.9172} policy_loss=-8.2663 policy updated! \n",
      "train step 04125 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=05.5151 diff={max=07.3058, min=00.0150, mean=01.5655} policy_loss=-9.1714 policy updated! \n",
      "train step 04126 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3244 diff={max=03.3925, min=00.0181, mean=01.2175} policy_loss=-8.3222 policy updated! \n",
      "train step 04127 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4319 diff={max=09.4587, min=00.0039, mean=01.4895} policy_loss=-7.4629 policy updated! \n",
      "train step 04128 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5606 diff={max=05.2816, min=00.0209, mean=01.0745} policy_loss=-7.8725 policy updated! \n",
      "train step 04129 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2238 diff={max=09.4623, min=00.1104, mean=01.3474} policy_loss=-8.9310 policy updated! \n",
      "train step 04130 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.0360 diff={max=06.7256, min=00.0421, mean=01.1669} policy_loss=-7.4632 policy updated! \n",
      "train step 04131 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0017 diff={max=04.8440, min=00.0015, mean=01.0356} policy_loss=-8.0438 policy updated! \n",
      "train step 04132 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5798 diff={max=04.8637, min=00.0285, mean=01.1316} policy_loss=-9.1976 policy updated! \n",
      "train step 04133 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7061 diff={max=07.3243, min=00.0071, mean=01.2624} policy_loss=-6.7702 policy updated! \n",
      "train step 04134 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=03.3812 diff={max=07.7485, min=00.1090, mean=01.1622} policy_loss=-8.7310 policy updated! \n",
      "train step 04135 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.8768 diff={max=09.5143, min=00.0094, mean=01.3420} policy_loss=-6.8107 policy updated! \n",
      "train step 04136 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6141 diff={max=07.9127, min=00.0030, mean=01.0951} policy_loss=-7.3237 policy updated! \n",
      "train step 04137 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2801 diff={max=04.6094, min=00.0203, mean=00.9652} policy_loss=-7.2399 policy updated! \n",
      "train step 04138 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4598 diff={max=10.0613, min=00.0008, mean=01.0431} policy_loss=-7.3755 policy updated! \n",
      "train step 04139 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8639 diff={max=05.4104, min=00.0114, mean=01.1296} policy_loss=-8.2699 policy updated! \n",
      "train step 04140 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.1898 diff={max=05.9291, min=00.0257, mean=01.2105} policy_loss=-6.9341 policy updated! \n",
      "train step 04141 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=03.0939 diff={max=06.1907, min=00.0571, mean=01.1575} policy_loss=-8.4140 policy updated! \n",
      "train step 04142 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4401 diff={max=09.7937, min=00.0832, mean=01.3673} policy_loss=-8.2769 policy updated! \n",
      "train step 04143 reward={max=09.0000, min=09.0000, mean=09.0000} optimizing loss=03.6578 diff={max=05.9490, min=00.0253, mean=01.2813} policy_loss=-7.4871 policy updated! \n",
      "train step 04144 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2664 diff={max=09.8554, min=00.0579, mean=01.3220} policy_loss=-7.5998 policy updated! \n",
      "train step 04145 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.8812 diff={max=07.1225, min=00.0071, mean=01.3879} policy_loss=-7.7547 policy updated! \n",
      "train step 04146 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2562 diff={max=05.4923, min=00.0666, mean=00.9984} policy_loss=-7.8281 policy updated! \n",
      "train step 04147 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8844 diff={max=04.8122, min=00.0017, mean=00.9778} policy_loss=-8.3721 policy updated! \n",
      "train step 04148 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7351 diff={max=05.1751, min=00.0073, mean=00.9896} policy_loss=-6.3500 policy updated! \n",
      "train step 04149 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=02.7268 diff={max=06.2496, min=00.0049, mean=01.0510} policy_loss=-6.5182 policy updated! \n",
      "train step 04150 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.7493 diff={max=08.3471, min=00.0230, mean=01.1907} policy_loss=-7.7836 policy updated! \n",
      "train step 04151 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9340 diff={max=04.9391, min=00.0075, mean=00.9803} policy_loss=-7.8473 policy updated! \n",
      "train step 04152 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8249 diff={max=08.2817, min=00.0332, mean=01.4693} policy_loss=-9.0614 policy updated! \n",
      "train step 04153 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1623 diff={max=05.6057, min=00.0047, mean=01.1266} policy_loss=-7.4806 policy updated! \n",
      "train step 04154 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8525 diff={max=04.9694, min=00.0195, mean=01.1294} policy_loss=-7.9936 policy updated! \n",
      "train step 04155 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.5807 diff={max=07.3268, min=00.0167, mean=01.5179} policy_loss=-8.1035 policy updated! \n",
      "train step 04156 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4333 diff={max=06.4451, min=00.0160, mean=01.0840} policy_loss=-8.2862 policy updated! \n",
      "train step 04157 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0653 diff={max=05.9411, min=00.0169, mean=01.2102} policy_loss=-8.1592 policy updated! \n",
      "train step 04158 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4654 diff={max=08.7023, min=00.0104, mean=01.1923} policy_loss=-7.9058 policy updated! \n",
      "train step 04159 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0896 diff={max=07.6513, min=00.0447, mean=01.1744} policy_loss=-8.3920 policy updated! \n",
      "train step 04160 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.3233 diff={max=05.3013, min=00.0469, mean=01.0677} policy_loss=-7.3365 policy updated! \n",
      "train step 04161 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5516 diff={max=06.4961, min=00.0048, mean=01.0040} policy_loss=-8.0714 policy updated! \n",
      "train step 04162 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5171 diff={max=06.8428, min=00.0067, mean=01.4071} policy_loss=-8.7301 policy updated! \n",
      "train step 04163 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9086 diff={max=06.5697, min=00.0034, mean=01.1505} policy_loss=-7.9657 policy updated! \n",
      "train step 04164 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5731 diff={max=06.7640, min=00.0117, mean=01.1890} policy_loss=-9.2007 policy updated! \n",
      "train step 04165 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.8929 diff={max=06.8154, min=00.0151, mean=01.0414} policy_loss=-7.8928 policy updated! \n",
      "train step 04166 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6760 diff={max=06.7719, min=00.0170, mean=00.9726} policy_loss=-8.6248 policy updated! \n",
      "train step 04167 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1877 diff={max=05.0239, min=00.0441, mean=01.3381} policy_loss=-7.9413 policy updated! \n",
      "train step 04168 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8729 diff={max=08.3702, min=00.0130, mean=01.1033} policy_loss=-7.4865 policy updated! \n",
      "train step 04169 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4381 diff={max=04.6491, min=00.0025, mean=00.7968} policy_loss=-6.6673 policy updated! \n",
      "train step 04170 reward={max=09.0000, min=09.0000, mean=09.0000} optimizing loss=01.9406 diff={max=04.8593, min=00.0369, mean=00.8560} policy_loss=-7.0326 policy updated! \n",
      "train step 04171 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8620 diff={max=07.1944, min=00.0110, mean=00.7885} policy_loss=-6.9946 policy updated! \n",
      "train step 04172 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6188 diff={max=08.0144, min=00.0448, mean=01.1400} policy_loss=-7.2450 policy updated! \n",
      "train step 04173 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4348 diff={max=08.1999, min=00.0078, mean=00.9117} policy_loss=-8.1082 policy updated! \n",
      "train step 04174 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5364 diff={max=05.7040, min=00.0032, mean=00.9911} policy_loss=-8.4567 policy updated! \n",
      "train step 04175 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.2817 diff={max=06.7222, min=00.0003, mean=01.0512} policy_loss=-8.9232 policy updated! \n",
      "train step 04176 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0156 diff={max=07.4900, min=00.0530, mean=01.2809} policy_loss=-8.6992 policy updated! \n",
      "train step 04177 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8563 diff={max=05.8223, min=00.0135, mean=01.2092} policy_loss=-7.7298 policy updated! \n",
      "train step 04178 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2964 diff={max=06.3922, min=00.0120, mean=01.2128} policy_loss=-7.3061 policy updated! \n",
      "train step 04179 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=01.3316 diff={max=04.7043, min=00.0104, mean=00.7450} policy_loss=-7.8972 policy updated! \n",
      "train step 04180 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.4699 diff={max=05.1902, min=00.0060, mean=01.3369} policy_loss=-7.8422 policy updated! \n",
      "train step 04181 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2892 diff={max=06.2327, min=00.0154, mean=00.9620} policy_loss=-8.9632 policy updated! \n",
      "train step 04182 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2527 diff={max=06.3562, min=00.0852, mean=01.1877} policy_loss=-7.2797 policy updated! \n",
      "train step 04183 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5859 diff={max=06.0743, min=00.0280, mean=01.0081} policy_loss=-8.7684 policy updated! \n",
      "train step 04184 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9334 diff={max=06.6252, min=00.0864, mean=01.0692} policy_loss=-7.0736 policy updated! \n",
      "train step 04185 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.1857 diff={max=07.1999, min=00.0114, mean=00.8526} policy_loss=-7.7279 policy updated! \n",
      "train step 04186 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0395 diff={max=08.3119, min=00.0053, mean=00.9957} policy_loss=-7.0114 policy updated! \n",
      "train step 04187 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5536 diff={max=05.8987, min=00.0522, mean=01.3359} policy_loss=-9.1358 policy updated! \n",
      "train step 04188 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=03.4160 diff={max=07.9422, min=00.0035, mean=01.1479} policy_loss=-8.1786 policy updated! \n",
      "train step 04189 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9059 diff={max=04.9101, min=00.0134, mean=00.9491} policy_loss=-7.8408 policy updated! \n",
      "train step 04190 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.6497 diff={max=06.8194, min=00.0298, mean=01.1687} policy_loss=-8.1937 policy updated! \n",
      "train step 04191 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6097 diff={max=09.5416, min=00.0191, mean=01.2228} policy_loss=-8.1512 policy updated! \n",
      "train step 04192 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8102 diff={max=06.1539, min=00.0004, mean=01.0503} policy_loss=-8.1290 policy updated! \n",
      "train step 04193 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9101 diff={max=02.5353, min=00.0023, mean=00.6889} policy_loss=-7.5190 policy updated! \n",
      "train step 04194 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2901 diff={max=07.9669, min=00.0018, mean=01.1610} policy_loss=-6.7361 policy updated! \n",
      "train step 04195 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.6818 diff={max=03.7569, min=00.0117, mean=00.9454} policy_loss=-7.5784 policy updated! \n",
      "train step 04196 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=02.5695 diff={max=05.4646, min=00.0070, mean=01.0596} policy_loss=-8.3025 policy updated! \n",
      "train step 04197 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6997 diff={max=06.6251, min=00.0688, mean=01.2423} policy_loss=-7.5396 policy updated! \n",
      "train step 04198 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6967 diff={max=06.8836, min=00.0579, mean=01.0274} policy_loss=-8.1722 policy updated! \n",
      "train step 04199 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3170 diff={max=05.2958, min=00.0071, mean=01.2517} policy_loss=-9.0494 policy updated! \n",
      "train step 04200 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.4286 diff={max=06.6898, min=00.0069, mean=01.0416} policy_loss=-8.5735 policy updated! \n",
      "train step 04201 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=02.8795 diff={max=05.6943, min=00.0115, mean=01.1319} policy_loss=-7.2808 policy updated! \n",
      "train step 04202 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3154 diff={max=08.0343, min=00.0655, mean=01.2019} policy_loss=-8.3663 policy updated! \n",
      "train step 04203 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2416 diff={max=04.8838, min=00.0004, mean=00.7302} policy_loss=-6.3660 policy updated! \n",
      "train step 04204 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2538 diff={max=05.1261, min=00.0242, mean=01.2168} policy_loss=-7.0450 policy updated! \n",
      "train step 04205 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=00.9534 diff={max=02.9128, min=00.0077, mean=00.7331} policy_loss=-6.5386 policy updated! \n",
      "train step 04206 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8520 diff={max=08.0355, min=00.0088, mean=01.2332} policy_loss=-6.7009 policy updated! \n",
      "train step 04207 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9134 diff={max=05.7317, min=00.0206, mean=01.3534} policy_loss=-7.5049 policy updated! \n",
      "train step 04208 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3814 diff={max=07.8958, min=00.0328, mean=01.1065} policy_loss=-8.2498 policy updated! \n",
      "train step 04209 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4132 diff={max=04.8234, min=00.0029, mean=01.0008} policy_loss=-8.2717 policy updated! \n",
      "train step 04210 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.3477 diff={max=09.8420, min=00.0117, mean=01.0379} policy_loss=-8.3389 policy updated! \n",
      "train step 04211 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1114 diff={max=09.9041, min=00.0046, mean=01.2047} policy_loss=-8.1963 policy updated! \n",
      "train step 04212 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5997 diff={max=06.7456, min=00.0041, mean=00.9965} policy_loss=-7.7518 policy updated! \n",
      "train step 04213 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2470 diff={max=07.4563, min=00.0183, mean=00.9095} policy_loss=-8.1871 policy updated! \n",
      "train step 04214 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8617 diff={max=04.2841, min=00.0363, mean=00.6307} policy_loss=-7.5936 policy updated! \n",
      "train step 04215 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.3260 diff={max=04.6029, min=00.0146, mean=00.8126} policy_loss=-8.5290 policy updated! \n",
      "train step 04216 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1143 diff={max=07.5601, min=00.0004, mean=01.1566} policy_loss=-6.5230 policy updated! \n",
      "train step 04217 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6424 diff={max=05.3089, min=00.0001, mean=01.0273} policy_loss=-9.7097 policy updated! \n",
      "train step 04218 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1765 diff={max=02.5421, min=00.0337, mean=00.8386} policy_loss=-9.5307 policy updated! \n",
      "train step 04219 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9752 diff={max=07.3548, min=00.0168, mean=01.0438} policy_loss=-8.6149 policy updated! \n",
      "train step 04220 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.9464 diff={max=07.0231, min=00.0287, mean=01.0600} policy_loss=-8.2168 policy updated! \n",
      "train step 04221 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8127 diff={max=08.0933, min=00.0121, mean=01.0512} policy_loss=-7.2911 policy updated! \n",
      "train step 04222 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6279 diff={max=05.7556, min=00.0001, mean=00.9986} policy_loss=-7.6352 policy updated! \n",
      "train step 04223 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7396 diff={max=07.5594, min=00.0263, mean=01.0872} policy_loss=-7.0355 policy updated! \n",
      "train step 04224 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=01.2792 diff={max=02.5982, min=00.0088, mean=00.9413} policy_loss=-7.8418 policy updated! \n",
      "train step 04225 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.1633 diff={max=09.4944, min=00.0060, mean=01.1578} policy_loss=-7.6740 policy updated! \n",
      "train step 04226 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8185 diff={max=05.4872, min=00.0091, mean=01.1490} policy_loss=-8.5813 policy updated! \n",
      "train step 04227 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7977 diff={max=04.4454, min=00.0023, mean=00.9466} policy_loss=-8.6947 policy updated! \n",
      "train step 04228 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6773 diff={max=03.4553, min=00.0376, mean=00.8754} policy_loss=-6.5195 policy updated! \n",
      "train step 04229 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0631 diff={max=05.5690, min=00.0556, mean=01.0066} policy_loss=-7.0508 policy updated! \n",
      "train step 04230 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.4929 diff={max=08.2928, min=00.0515, mean=01.1766} policy_loss=-8.8522 policy updated! \n",
      "train step 04231 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3047 diff={max=05.5671, min=00.0127, mean=00.9462} policy_loss=-7.1065 policy updated! \n",
      "train step 04232 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0998 diff={max=04.8441, min=00.0364, mean=00.9885} policy_loss=-8.0481 policy updated! \n",
      "train step 04233 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1854 diff={max=04.4111, min=00.0071, mean=00.6821} policy_loss=-8.1244 policy updated! \n",
      "train step 04234 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7532 diff={max=08.3201, min=00.0151, mean=01.0935} policy_loss=-7.6855 policy updated! \n",
      "train step 04235 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.7398 diff={max=05.3944, min=00.0046, mean=00.7927} policy_loss=-7.8634 policy updated! \n",
      "train step 04236 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2776 diff={max=07.2329, min=00.0042, mean=00.9454} policy_loss=-7.9483 policy updated! \n",
      "train step 04237 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1849 diff={max=03.3822, min=00.0030, mean=00.8163} policy_loss=-9.2306 policy updated! \n",
      "train step 04238 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5240 diff={max=04.7809, min=00.0404, mean=00.8503} policy_loss=-7.2922 policy updated! \n",
      "train step 04239 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6274 diff={max=07.4511, min=00.0018, mean=00.9345} policy_loss=-8.4870 policy updated! \n",
      "train step 04240 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.8836 diff={max=10.2042, min=00.0439, mean=01.3522} policy_loss=-7.9973 policy updated! \n",
      "train step 04241 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9996 diff={max=06.4451, min=00.0723, mean=01.1331} policy_loss=-8.1465 policy updated! \n",
      "train step 04242 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2071 diff={max=06.2560, min=00.0576, mean=01.3773} policy_loss=-7.7688 policy updated! \n",
      "train step 04243 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7394 diff={max=06.3983, min=00.0062, mean=01.0187} policy_loss=-7.1257 policy updated! \n",
      "train step 04244 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4985 diff={max=06.8536, min=00.0177, mean=00.9011} policy_loss=-6.4130 policy updated! \n",
      "train step 04245 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.0135 diff={max=07.5075, min=00.0059, mean=01.0761} policy_loss=-7.2411 policy updated! \n",
      "train step 04246 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3676 diff={max=04.5978, min=00.0956, mean=00.8568} policy_loss=-8.0665 policy updated! \n",
      "train step 04247 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6291 diff={max=06.4550, min=00.0250, mean=01.0014} policy_loss=-7.7512 policy updated! \n",
      "train step 04248 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=01.1823 diff={max=02.8946, min=00.0507, mean=00.8195} policy_loss=-6.9373 policy updated! \n",
      "train step 04249 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3900 diff={max=04.7788, min=00.0183, mean=01.0553} policy_loss=-8.8537 policy updated! \n",
      "train step 04250 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.7064 diff={max=05.3682, min=00.0012, mean=00.7579} policy_loss=-8.2955 policy updated! \n",
      "train step 04251 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3918 diff={max=05.6376, min=00.0230, mean=00.9640} policy_loss=-9.0552 policy updated! \n",
      "train step 04252 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7074 diff={max=06.3033, min=00.0263, mean=01.4021} policy_loss=-7.6694 policy updated! \n",
      "train step 04253 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4666 diff={max=06.8959, min=00.0009, mean=00.9684} policy_loss=-7.9257 policy updated! \n",
      "train step 04254 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9619 diff={max=04.2317, min=00.0193, mean=01.0071} policy_loss=-8.9352 policy updated! \n",
      "train step 04255 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.6976 diff={max=05.5605, min=00.0088, mean=00.8384} policy_loss=-7.9653 policy updated! \n",
      "train step 04256 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6592 diff={max=08.4645, min=00.0203, mean=01.1818} policy_loss=-8.8899 policy updated! \n",
      "train step 04257 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1009 diff={max=04.0221, min=00.0008, mean=00.6659} policy_loss=-7.7716 policy updated! \n",
      "train step 04258 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3685 diff={max=04.1455, min=00.0075, mean=00.7869} policy_loss=-6.3317 policy updated! \n",
      "train step 04259 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9156 diff={max=04.9238, min=00.0140, mean=00.9977} policy_loss=-8.7961 policy updated! \n",
      "train step 04260 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.5499 diff={max=05.4084, min=00.0100, mean=01.0745} policy_loss=-7.3870 policy updated! \n",
      "train step 04261 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8898 diff={max=07.8881, min=00.0288, mean=01.2452} policy_loss=-8.4223 policy updated! \n",
      "train step 04262 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3201 diff={max=03.8499, min=00.0144, mean=00.7943} policy_loss=-9.9672 policy updated! \n",
      "train step 04263 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9553 diff={max=05.9692, min=00.0249, mean=00.9387} policy_loss=-7.5703 policy updated! \n",
      "train step 04264 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5462 diff={max=03.6317, min=00.0516, mean=01.2858} policy_loss=-9.8500 policy updated! \n",
      "train step 04265 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.6408 diff={max=07.4165, min=00.0370, mean=01.2967} policy_loss=-7.9094 policy updated! \n",
      "train step 04266 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9537 diff={max=04.1604, min=00.0387, mean=01.0153} policy_loss=-7.4852 policy updated! \n",
      "train step 04267 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9975 diff={max=04.0131, min=00.0106, mean=00.9451} policy_loss=-7.9774 policy updated! \n",
      "train step 04268 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2888 diff={max=07.8190, min=00.0423, mean=01.3352} policy_loss=-7.7828 policy updated! \n",
      "train step 04269 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9987 diff={max=08.5200, min=00.0087, mean=01.1769} policy_loss=-7.4526 policy updated! \n",
      "train step 04270 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.8174 diff={max=06.3499, min=00.0012, mean=01.0410} policy_loss=-7.6464 policy updated! \n",
      "train step 04271 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8194 diff={max=07.5542, min=00.0065, mean=00.9517} policy_loss=-7.8875 policy updated! \n",
      "train step 04272 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1785 diff={max=06.7058, min=00.0150, mean=01.0795} policy_loss=-8.1611 policy updated! \n",
      "train step 04273 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3201 diff={max=09.0462, min=00.0106, mean=01.4195} policy_loss=-9.4115 policy updated! \n",
      "train step 04274 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4930 diff={max=07.0610, min=00.0082, mean=00.6976} policy_loss=-8.7438 policy updated! \n",
      "train step 04275 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=05.6141 diff={max=11.0278, min=00.0131, mean=01.2635} policy_loss=-7.4965 policy updated! \n",
      "train step 04276 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6822 diff={max=03.8063, min=00.0559, mean=00.9118} policy_loss=-7.9656 policy updated! \n",
      "train step 04277 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5477 diff={max=06.1337, min=00.0177, mean=00.7972} policy_loss=-9.3402 policy updated! \n",
      "train step 04278 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=02.7583 diff={max=07.0730, min=00.0015, mean=00.9745} policy_loss=-7.3029 policy updated! \n",
      "train step 04279 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5479 diff={max=07.2457, min=00.0121, mean=00.9586} policy_loss=-7.9163 policy updated! \n",
      "train step 04280 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.8186 diff={max=05.1169, min=00.0108, mean=00.9244} policy_loss=-8.4861 policy updated! \n",
      "train step 04281 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1823 diff={max=05.8516, min=00.0031, mean=01.0651} policy_loss=-8.4730 policy updated! \n",
      "train step 04282 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1169 diff={max=06.2485, min=00.0727, mean=01.0670} policy_loss=-7.7910 policy updated! \n",
      "train step 04283 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7212 diff={max=06.8219, min=00.0139, mean=01.1336} policy_loss=-8.3148 policy updated! \n",
      "train step 04284 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0876 diff={max=07.3852, min=00.0048, mean=00.9133} policy_loss=-5.8918 policy updated! \n",
      "train step 04285 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.2472 diff={max=04.8609, min=00.0009, mean=00.9353} policy_loss=-7.3120 policy updated! \n",
      "train step 04286 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1251 diff={max=07.0909, min=00.0528, mean=01.0720} policy_loss=-7.1567 policy updated! \n",
      "train step 04287 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1622 diff={max=07.0274, min=00.0538, mean=01.0960} policy_loss=-7.5199 policy updated! \n",
      "train step 04288 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5367 diff={max=06.6049, min=00.0155, mean=01.1700} policy_loss=-8.4324 policy updated! \n",
      "train step 04289 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4493 diff={max=05.0168, min=00.0058, mean=01.1322} policy_loss=-9.1427 policy updated! \n",
      "train step 04290 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.2047 diff={max=04.8404, min=00.0439, mean=00.9653} policy_loss=-8.6370 policy updated! \n",
      "train step 04291 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=01.9161 diff={max=06.1681, min=00.0023, mean=00.7589} policy_loss=-6.9694 policy updated! \n",
      "train step 04292 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2258 diff={max=06.9887, min=00.0187, mean=01.0143} policy_loss=-8.0092 policy updated! \n",
      "train step 04293 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0977 diff={max=06.3887, min=00.0115, mean=00.9157} policy_loss=-8.0104 policy updated! \n",
      "train step 04294 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0694 diff={max=03.0131, min=00.0177, mean=00.7337} policy_loss=-7.1972 policy updated! \n",
      "train step 04295 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.8550 diff={max=06.2133, min=00.0065, mean=01.1026} policy_loss=-8.4733 policy updated! \n",
      "train step 04296 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2644 diff={max=04.7967, min=00.0126, mean=01.0268} policy_loss=-8.5956 policy updated! \n",
      "train step 04297 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3884 diff={max=05.6712, min=00.0001, mean=01.1712} policy_loss=-8.2837 policy updated! \n",
      "train step 04298 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0714 diff={max=04.6843, min=00.0099, mean=00.9766} policy_loss=-8.9162 policy updated! \n",
      "train step 04299 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0441 diff={max=09.7378, min=00.0288, mean=01.2502} policy_loss=-8.8989 policy updated! \n",
      "train step 04300 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.9986 diff={max=07.1371, min=00.0092, mean=01.1305} policy_loss=-7.4872 policy updated! \n",
      "train step 04301 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4381 diff={max=08.0173, min=00.0124, mean=01.0355} policy_loss=-8.2444 policy updated! \n",
      "train step 04302 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2017 diff={max=05.6442, min=00.0043, mean=00.9641} policy_loss=-9.2408 policy updated! \n",
      "train step 04303 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2416 diff={max=05.0963, min=00.0313, mean=01.0603} policy_loss=-7.3207 policy updated! \n",
      "train step 04304 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9028 diff={max=08.3813, min=00.0467, mean=01.2998} policy_loss=-7.3062 policy updated! \n",
      "train step 04305 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.2183 diff={max=06.3859, min=00.0154, mean=01.0562} policy_loss=-8.5604 policy updated! \n",
      "train step 04306 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1507 diff={max=06.9774, min=00.0005, mean=00.9475} policy_loss=-7.2339 policy updated! \n",
      "train step 04307 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2718 diff={max=03.4953, min=00.0322, mean=00.7996} policy_loss=-8.1100 policy updated! \n",
      "train step 04308 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0927 diff={max=05.2086, min=00.0063, mean=00.9756} policy_loss=-7.8531 policy updated! \n",
      "train step 04309 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9358 diff={max=06.4853, min=00.0015, mean=00.8935} policy_loss=-7.4732 policy updated! \n",
      "train step 04310 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.0972 diff={max=03.8157, min=00.0005, mean=00.6697} policy_loss=-7.4109 policy updated! \n",
      "train step 04311 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2908 diff={max=05.0942, min=00.0036, mean=00.6525} policy_loss=-7.5498 policy updated! \n",
      "train step 04312 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4180 diff={max=06.2795, min=00.0257, mean=01.1830} policy_loss=-8.9822 policy updated! \n",
      "train step 04313 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5443 diff={max=06.3438, min=00.0361, mean=01.0549} policy_loss=-6.0555 policy updated! \n",
      "train step 04314 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2940 diff={max=07.5665, min=00.0151, mean=01.3030} policy_loss=-8.4854 policy updated! \n",
      "train step 04315 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.1156 diff={max=04.6557, min=00.0024, mean=00.9313} policy_loss=-7.7864 policy updated! \n",
      "train step 04316 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5436 diff={max=06.0771, min=00.0327, mean=01.0262} policy_loss=-8.6186 policy updated! \n",
      "train step 04317 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0209 diff={max=04.8040, min=00.0221, mean=00.6159} policy_loss=-7.1150 policy updated! \n",
      "train step 04318 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1606 diff={max=07.2219, min=00.0015, mean=01.0905} policy_loss=-9.0944 policy updated! \n",
      "train step 04319 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6828 diff={max=07.5178, min=00.0091, mean=01.0595} policy_loss=-7.3296 policy updated! \n",
      "train step 04320 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.5300 diff={max=07.3439, min=00.0076, mean=01.0233} policy_loss=-9.4304 policy updated! \n",
      "train step 04321 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0216 diff={max=03.6067, min=00.0056, mean=00.6931} policy_loss=-9.5381 policy updated! \n",
      "train step 04322 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5503 diff={max=06.5446, min=00.0091, mean=01.2603} policy_loss=-7.7064 policy updated! \n",
      "train step 04323 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5583 diff={max=08.6077, min=00.0047, mean=01.0486} policy_loss=-8.8099 policy updated! \n",
      "train step 04324 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5538 diff={max=08.3211, min=00.0228, mean=01.1734} policy_loss=-8.6498 policy updated! \n",
      "train step 04325 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.6912 diff={max=05.0300, min=00.0366, mean=00.8476} policy_loss=-8.9772 policy updated! \n",
      "train step 04326 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4618 diff={max=07.7017, min=00.0162, mean=00.9829} policy_loss=-7.6193 policy updated! \n",
      "train step 04327 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6974 diff={max=08.8181, min=00.0343, mean=01.2712} policy_loss=-7.9825 policy updated! \n",
      "train step 04328 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3973 diff={max=08.5332, min=00.0189, mean=00.9734} policy_loss=-6.5581 policy updated! \n",
      "train step 04329 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=01.6292 diff={max=06.1749, min=00.0296, mean=00.7458} policy_loss=-7.3723 policy updated! \n",
      "train step 04330 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.5934 diff={max=07.0992, min=00.0048, mean=01.3785} policy_loss=-7.9956 policy updated! \n",
      "train step 04331 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=03.5190 diff={max=07.4756, min=00.0317, mean=00.9659} policy_loss=-7.8194 policy updated! \n",
      "train step 04332 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6271 diff={max=02.9975, min=00.0238, mean=00.9337} policy_loss=-7.7640 policy updated! \n",
      "train step 04333 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4264 diff={max=07.5061, min=00.0219, mean=01.4617} policy_loss=-8.0601 policy updated! \n",
      "train step 04334 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8422 diff={max=06.3238, min=00.0172, mean=01.2622} policy_loss=-8.1682 policy updated! \n",
      "train step 04335 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.9760 diff={max=07.6762, min=00.0088, mean=01.0638} policy_loss=-8.3023 policy updated! \n",
      "train step 04336 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5799 diff={max=05.6357, min=00.0017, mean=01.0375} policy_loss=-8.8583 policy updated! \n",
      "train step 04337 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3021 diff={max=04.8575, min=00.0195, mean=01.0640} policy_loss=-7.7066 policy updated! \n",
      "train step 04338 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8365 diff={max=04.9448, min=00.0071, mean=01.1858} policy_loss=-7.2617 policy updated! \n",
      "train step 04339 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0466 diff={max=05.5503, min=00.0327, mean=01.0019} policy_loss=-7.3729 policy updated! \n",
      "train step 04340 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.5580 diff={max=05.3173, min=00.0134, mean=01.2385} policy_loss=-6.9871 policy updated! \n",
      "train step 04341 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5702 diff={max=08.3782, min=00.0291, mean=01.2349} policy_loss=-8.8047 policy updated! \n",
      "train step 04342 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7987 diff={max=10.8071, min=00.0192, mean=01.1313} policy_loss=-8.1524 policy updated! \n",
      "train step 04343 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4461 diff={max=07.8843, min=00.0011, mean=00.8833} policy_loss=-7.4232 policy updated! \n",
      "train step 04344 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8426 diff={max=06.0825, min=00.0173, mean=01.2444} policy_loss=-9.0479 policy updated! \n",
      "train step 04345 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.7566 diff={max=05.5489, min=00.0204, mean=01.0583} policy_loss=-8.3373 policy updated! \n",
      "train step 04346 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=01.5599 diff={max=07.0495, min=00.0233, mean=00.7128} policy_loss=-7.9436 policy updated! \n",
      "train step 04347 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2424 diff={max=09.2467, min=00.0200, mean=01.3220} policy_loss=-8.7444 policy updated! \n",
      "train step 04348 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2135 diff={max=03.5694, min=00.0048, mean=00.7926} policy_loss=-6.6607 policy updated! \n",
      "train step 04349 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0002 diff={max=06.3454, min=00.0209, mean=01.0975} policy_loss=-7.9498 policy updated! \n",
      "train step 04350 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.3097 diff={max=07.3811, min=00.0630, mean=01.0187} policy_loss=-7.3269 policy updated! \n",
      "train step 04351 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=04.4586 diff={max=09.7409, min=00.0405, mean=01.2804} policy_loss=-7.2169 policy updated! \n",
      "train step 04352 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0852 diff={max=08.2463, min=00.0051, mean=00.8587} policy_loss=-7.8011 policy updated! \n",
      "train step 04353 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4906 diff={max=03.5139, min=00.0336, mean=00.9297} policy_loss=-8.9223 policy updated! \n",
      "train step 04354 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1239 diff={max=09.1899, min=00.0031, mean=01.2155} policy_loss=-7.4705 policy updated! \n",
      "train step 04355 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.7796 diff={max=05.0825, min=00.0116, mean=00.8358} policy_loss=-7.7848 policy updated! \n",
      "train step 04356 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1839 diff={max=06.3961, min=00.0146, mean=01.1217} policy_loss=-8.8756 policy updated! \n",
      "train step 04357 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1808 diff={max=05.8561, min=00.0021, mean=00.8665} policy_loss=-7.5446 policy updated! \n",
      "train step 04358 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1716 diff={max=05.6394, min=00.0056, mean=01.1035} policy_loss=-7.9431 policy updated! \n",
      "train step 04359 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4085 diff={max=07.0914, min=00.0035, mean=00.8991} policy_loss=-7.0408 policy updated! \n",
      "train step 04360 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.1170 diff={max=05.9477, min=00.0020, mean=00.9420} policy_loss=-8.8029 policy updated! \n",
      "train step 04361 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6265 diff={max=06.4917, min=00.0386, mean=01.0479} policy_loss=-8.9746 policy updated! \n",
      "train step 04362 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5293 diff={max=05.6532, min=00.0015, mean=00.7966} policy_loss=-7.2833 policy updated! \n",
      "train step 04363 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1677 diff={max=04.5146, min=00.0049, mean=00.6946} policy_loss=-8.3683 policy updated! \n",
      "train step 04364 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=04.4748 diff={max=08.7959, min=00.0129, mean=01.3985} policy_loss=-8.6875 policy updated! \n",
      "train step 04365 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.6342 diff={max=08.0413, min=00.0097, mean=01.1954} policy_loss=-9.0001 policy updated! \n",
      "train step 04366 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4512 diff={max=07.2261, min=00.0116, mean=01.1120} policy_loss=-8.5331 policy updated! \n",
      "train step 04367 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0142 diff={max=06.6992, min=00.0260, mean=01.2330} policy_loss=-8.2333 policy updated! \n",
      "train step 04368 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9615 diff={max=06.8168, min=00.0261, mean=00.7724} policy_loss=-8.0923 policy updated! \n",
      "train step 04369 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5454 diff={max=09.8082, min=00.0029, mean=01.3376} policy_loss=-7.4511 policy updated! \n",
      "train step 04370 reward={max=09.0000, min=09.0000, mean=09.0000} optimizing loss=03.7891 diff={max=08.1306, min=00.0535, mean=01.2352} policy_loss=-8.4343 policy updated! \n",
      "train step 04371 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0263 diff={max=05.4281, min=00.0274, mean=00.9462} policy_loss=-9.2169 policy updated! \n",
      "train step 04372 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2792 diff={max=05.5353, min=00.0128, mean=01.1874} policy_loss=-7.6494 policy updated! \n",
      "train step 04373 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6408 diff={max=03.3308, min=00.0539, mean=00.9812} policy_loss=-9.5531 policy updated! \n",
      "train step 04374 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9996 diff={max=05.7766, min=00.0286, mean=01.0136} policy_loss=-9.6955 policy updated! \n",
      "train step 04375 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.6593 diff={max=05.5107, min=00.0230, mean=01.1098} policy_loss=-8.5475 policy updated! \n",
      "train step 04376 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3189 diff={max=05.5870, min=00.0133, mean=01.0456} policy_loss=-7.2265 policy updated! \n",
      "train step 04377 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5637 diff={max=05.9440, min=00.0177, mean=01.0848} policy_loss=-6.7976 policy updated! \n",
      "train step 04378 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4633 diff={max=05.4648, min=00.0154, mean=01.1110} policy_loss=-8.0449 policy updated! \n",
      "train step 04379 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2077 diff={max=02.9532, min=00.0121, mean=00.8288} policy_loss=-6.8757 policy updated! \n",
      "train step 04380 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.9060 diff={max=06.0958, min=00.0220, mean=01.2176} policy_loss=-7.8071 policy updated! \n",
      "train step 04381 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4011 diff={max=05.1543, min=00.0024, mean=00.7920} policy_loss=-8.2575 policy updated! \n",
      "train step 04382 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1966 diff={max=03.9229, min=00.0202, mean=00.7229} policy_loss=-9.8235 policy updated! \n",
      "train step 04383 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8531 diff={max=06.0736, min=00.0213, mean=01.0489} policy_loss=-9.3077 policy updated! \n",
      "train step 04384 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=01.6852 diff={max=03.5873, min=00.0001, mean=00.8737} policy_loss=-9.2233 policy updated! \n",
      "train step 04385 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.8608 diff={max=09.3172, min=00.0174, mean=01.2416} policy_loss=-8.6396 policy updated! \n",
      "train step 04386 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0788 diff={max=07.0085, min=00.0064, mean=00.8759} policy_loss=-8.5084 policy updated! \n",
      "train step 04387 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2278 diff={max=06.0370, min=00.0060, mean=01.1082} policy_loss=-7.5224 policy updated! \n",
      "train step 04388 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8561 diff={max=06.5522, min=00.0011, mean=00.9611} policy_loss=-7.4266 policy updated! \n",
      "train step 04389 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6736 diff={max=07.2639, min=00.0128, mean=01.1306} policy_loss=-10.0671 policy updated! \n",
      "train step 04390 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.3392 diff={max=05.1342, min=00.0232, mean=00.8131} policy_loss=-8.2558 policy updated! \n",
      "train step 04391 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=02.3138 diff={max=05.8814, min=00.0500, mean=01.0031} policy_loss=-9.2763 policy updated! \n",
      "train step 04392 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3878 diff={max=02.8622, min=00.0091, mean=00.9108} policy_loss=-8.4834 policy updated! \n",
      "train step 04393 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=01.0275 diff={max=03.1068, min=00.0424, mean=00.7060} policy_loss=-7.5237 policy updated! \n",
      "train step 04394 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9881 diff={max=06.2737, min=00.0377, mean=00.9172} policy_loss=-7.4725 policy updated! \n",
      "train step 04395 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.7781 diff={max=04.8817, min=00.0333, mean=01.1188} policy_loss=-7.1804 policy updated! \n",
      "train step 04396 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9501 diff={max=08.6635, min=00.0193, mean=01.0217} policy_loss=-7.2334 policy updated! \n",
      "train step 04397 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5421 diff={max=07.2310, min=00.0747, mean=01.1588} policy_loss=-8.3858 policy updated! \n",
      "train step 04398 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8854 diff={max=07.9639, min=00.0114, mean=00.9431} policy_loss=-6.9135 policy updated! \n",
      "train step 04399 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7111 diff={max=01.9248, min=00.0100, mean=00.6360} policy_loss=-7.8830 policy updated! \n",
      "train step 04400 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.1438 diff={max=05.5377, min=00.0278, mean=01.0151} policy_loss=-9.0152 policy updated! \n",
      "train step 04401 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8451 diff={max=07.4682, min=00.0332, mean=00.9182} policy_loss=-7.0019 policy updated! \n",
      "train step 04402 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6151 diff={max=04.1528, min=00.0096, mean=00.8993} policy_loss=-6.6275 policy updated! \n",
      "train step 04403 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1408 diff={max=04.9379, min=00.0272, mean=01.0166} policy_loss=-8.9593 policy updated! \n",
      "train step 04404 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6256 diff={max=10.2267, min=00.0195, mean=01.3107} policy_loss=-9.1605 policy updated! \n",
      "train step 04405 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.2957 diff={max=04.8054, min=00.0567, mean=01.0459} policy_loss=-8.9289 policy updated! \n",
      "train step 04406 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4198 diff={max=03.5526, min=00.0542, mean=00.7738} policy_loss=-8.8379 policy updated! \n",
      "train step 04407 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1474 diff={max=08.3650, min=00.0095, mean=01.4173} policy_loss=-8.0662 policy updated! \n",
      "train step 04408 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8031 diff={max=06.9396, min=00.0237, mean=01.0119} policy_loss=-8.6533 policy updated! \n",
      "train step 04409 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8267 diff={max=04.9226, min=00.0313, mean=00.9585} policy_loss=-7.6396 policy updated! \n",
      "train step 04410 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.0198 diff={max=03.1698, min=00.0220, mean=00.7454} policy_loss=-7.5731 policy updated! \n",
      "train step 04411 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5899 diff={max=05.7471, min=00.0559, mean=01.0310} policy_loss=-6.9767 policy updated! \n",
      "train step 04412 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3011 diff={max=05.4598, min=00.0019, mean=00.9471} policy_loss=-6.9910 policy updated! \n",
      "train step 04413 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4693 diff={max=04.6605, min=00.0457, mean=01.1097} policy_loss=-8.6146 policy updated! \n",
      "train step 04414 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=02.4210 diff={max=06.8218, min=00.0271, mean=00.9283} policy_loss=-6.4920 policy updated! \n",
      "train step 04415 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.4832 diff={max=03.8345, min=00.0001, mean=00.8245} policy_loss=-7.7641 policy updated! \n",
      "train step 04416 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5612 diff={max=05.0059, min=00.0117, mean=00.8768} policy_loss=-7.9037 policy updated! \n",
      "train step 04417 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3871 diff={max=08.3630, min=00.0096, mean=01.2029} policy_loss=-7.9726 policy updated! \n",
      "train step 04418 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1378 diff={max=04.4820, min=00.0156, mean=00.7656} policy_loss=-8.4569 policy updated! \n",
      "train step 04419 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3417 diff={max=05.3608, min=00.0345, mean=00.9703} policy_loss=-7.7082 policy updated! \n",
      "train step 04420 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.3735 diff={max=05.6152, min=00.0034, mean=01.2284} policy_loss=-8.3399 policy updated! \n",
      "train step 04421 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0179 diff={max=04.0425, min=00.0058, mean=00.6070} policy_loss=-7.7165 policy updated! \n",
      "train step 04422 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1260 diff={max=07.3675, min=00.0002, mean=00.9389} policy_loss=-7.7080 policy updated! \n",
      "train step 04423 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9451 diff={max=06.2071, min=00.0190, mean=00.9252} policy_loss=-9.1076 policy updated! \n",
      "train step 04424 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1553 diff={max=09.7791, min=00.0261, mean=01.2485} policy_loss=-8.3219 policy updated! \n",
      "train step 04425 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.7668 diff={max=08.2437, min=00.0022, mean=01.2941} policy_loss=-7.7398 policy updated! \n",
      "train step 04426 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0463 diff={max=03.7742, min=00.0046, mean=00.7801} policy_loss=-8.5388 policy updated! \n",
      "train step 04427 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8499 diff={max=05.1381, min=00.0248, mean=00.9613} policy_loss=-8.1856 policy updated! \n",
      "train step 04428 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=06.1221 diff={max=07.8379, min=00.0017, mean=01.5113} policy_loss=-7.5751 policy updated! \n",
      "train step 04429 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1203 diff={max=09.3409, min=00.0267, mean=00.9839} policy_loss=-8.2424 policy updated! \n",
      "train step 04430 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.9968 diff={max=08.1256, min=00.0217, mean=00.9762} policy_loss=-8.1143 policy updated! \n",
      "train step 04431 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=01.4474 diff={max=03.8314, min=00.0149, mean=00.8783} policy_loss=-8.1381 policy updated! \n",
      "train step 04432 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8277 diff={max=05.3882, min=00.0173, mean=00.8461} policy_loss=-8.4495 policy updated! \n",
      "train step 04433 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6629 diff={max=06.1785, min=00.0193, mean=01.1920} policy_loss=-8.7845 policy updated! \n",
      "train step 04434 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8057 diff={max=06.5490, min=00.0097, mean=01.3277} policy_loss=-7.6542 policy updated! \n",
      "train step 04435 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.7939 diff={max=05.6585, min=00.0052, mean=00.7997} policy_loss=-7.1406 policy updated! \n",
      "train step 04436 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0265 diff={max=05.4329, min=00.0044, mean=00.8377} policy_loss=-8.6540 policy updated! \n",
      "train step 04437 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1249 diff={max=05.4346, min=00.0122, mean=00.9216} policy_loss=-7.7994 policy updated! \n",
      "train step 04438 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1954 diff={max=03.8445, min=00.0239, mean=00.7266} policy_loss=-7.7021 policy updated! \n",
      "train step 04439 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0028 diff={max=05.4050, min=00.0207, mean=00.8934} policy_loss=-8.8172 policy updated! \n",
      "train step 04440 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.6300 diff={max=06.5729, min=00.0051, mean=00.7019} policy_loss=-8.7209 policy updated! \n",
      "train step 04441 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9520 diff={max=06.6830, min=00.0121, mean=01.0416} policy_loss=-8.6078 policy updated! \n",
      "train step 04442 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6346 diff={max=08.2745, min=00.0862, mean=01.2940} policy_loss=-7.5717 policy updated! \n",
      "train step 04443 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5044 diff={max=06.7859, min=00.0125, mean=01.0879} policy_loss=-7.4670 policy updated! \n",
      "train step 04444 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9987 diff={max=07.1183, min=00.0065, mean=01.0360} policy_loss=-7.7271 policy updated! \n",
      "train step 04445 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.9505 diff={max=07.3787, min=00.0063, mean=01.0161} policy_loss=-7.1019 policy updated! \n",
      "train step 04446 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3380 diff={max=05.2559, min=00.0043, mean=00.9812} policy_loss=-6.6224 policy updated! \n",
      "train step 04447 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6081 diff={max=05.2238, min=00.0084, mean=00.7706} policy_loss=-8.6726 policy updated! \n",
      "train step 04448 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1918 diff={max=09.5234, min=00.0286, mean=01.2338} policy_loss=-8.1092 policy updated! \n",
      "train step 04449 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8384 diff={max=04.2973, min=00.0088, mean=00.5817} policy_loss=-6.4144 policy updated! \n",
      "train step 04450 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.5437 diff={max=03.8232, min=00.0226, mean=00.8343} policy_loss=-7.4687 policy updated! \n",
      "train step 04451 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7781 diff={max=09.3923, min=00.0526, mean=01.6248} policy_loss=-7.0740 policy updated! \n",
      "train step 04452 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2920 diff={max=05.5558, min=00.0147, mean=00.9686} policy_loss=-7.8643 policy updated! \n",
      "train step 04453 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2671 diff={max=05.2667, min=00.0302, mean=01.2531} policy_loss=-8.8824 policy updated! \n",
      "train step 04454 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4880 diff={max=06.9514, min=00.0287, mean=01.2099} policy_loss=-8.1834 policy updated! \n",
      "train step 04455 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.6401 diff={max=09.3611, min=00.0153, mean=01.0715} policy_loss=-8.4964 policy updated! \n",
      "train step 04456 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7328 diff={max=08.1212, min=00.0286, mean=01.0031} policy_loss=-7.2601 policy updated! \n",
      "train step 04457 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2183 diff={max=05.0125, min=00.0076, mean=01.0146} policy_loss=-6.9822 policy updated! \n",
      "train step 04458 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8724 diff={max=08.7757, min=00.0477, mean=01.1707} policy_loss=-7.4438 policy updated! \n",
      "train step 04459 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3914 diff={max=07.5445, min=00.0113, mean=01.5565} policy_loss=-8.5935 policy updated! \n",
      "train step 04460 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.7929 diff={max=05.7440, min=00.0051, mean=00.8523} policy_loss=-7.6249 policy updated! \n",
      "train step 04461 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3741 diff={max=07.2648, min=00.0039, mean=01.0986} policy_loss=-7.4201 policy updated! \n",
      "train step 04462 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1996 diff={max=04.7497, min=00.0073, mean=01.0319} policy_loss=-8.5734 policy updated! \n",
      "train step 04463 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1148 diff={max=04.2535, min=00.0218, mean=00.8143} policy_loss=-8.6607 policy updated! \n",
      "train step 04464 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1596 diff={max=05.7243, min=00.0010, mean=01.0391} policy_loss=-8.6344 policy updated! \n",
      "train step 04465 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=00.8088 diff={max=03.4959, min=00.0086, mean=00.5624} policy_loss=-7.0437 policy updated! \n",
      "train step 04466 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2603 diff={max=06.3360, min=00.0492, mean=01.1391} policy_loss=-7.7753 policy updated! \n",
      "train step 04467 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8371 diff={max=04.9270, min=00.0031, mean=00.8937} policy_loss=-7.6183 policy updated! \n",
      "train step 04468 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8356 diff={max=07.3545, min=00.0236, mean=01.2114} policy_loss=-7.8389 policy updated! \n",
      "train step 04469 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3683 diff={max=03.6501, min=00.0137, mean=00.7759} policy_loss=-6.7143 policy updated! \n",
      "train step 04470 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.9209 diff={max=06.5957, min=00.0724, mean=01.4071} policy_loss=-8.3824 policy updated! \n",
      "train step 04471 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4617 diff={max=07.3129, min=00.0263, mean=00.8209} policy_loss=-10.1307 policy updated! \n",
      "train step 04472 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5740 diff={max=07.1676, min=00.0270, mean=01.4042} policy_loss=-8.5434 policy updated! \n",
      "train step 04473 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2499 diff={max=04.3566, min=00.0391, mean=00.7338} policy_loss=-8.2076 policy updated! \n",
      "train step 04474 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5699 diff={max=07.3691, min=00.0283, mean=01.0266} policy_loss=-7.0524 policy updated! \n",
      "train step 04475 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.0911 diff={max=07.4669, min=00.0159, mean=00.8794} policy_loss=-7.8687 policy updated! \n",
      "train step 04476 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1690 diff={max=04.8580, min=00.0332, mean=01.0102} policy_loss=-7.7442 policy updated! \n",
      "train step 04477 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8627 diff={max=02.5517, min=00.0572, mean=00.7406} policy_loss=-6.5570 policy updated! \n",
      "train step 04478 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2746 diff={max=06.9351, min=00.0111, mean=01.2035} policy_loss=-8.0948 policy updated! \n",
      "train step 04479 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3153 diff={max=08.3340, min=00.0129, mean=01.0318} policy_loss=-7.1070 policy updated! \n",
      "train step 04480 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.5463 diff={max=05.4914, min=00.0268, mean=01.0972} policy_loss=-8.1090 policy updated! \n",
      "train step 04481 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=01.5878 diff={max=04.2360, min=00.0057, mean=00.8898} policy_loss=-7.7470 policy updated! \n",
      "train step 04482 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1317 diff={max=05.3326, min=00.0145, mean=01.0862} policy_loss=-7.4168 policy updated! \n",
      "train step 04483 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6713 diff={max=06.3937, min=00.0014, mean=00.8973} policy_loss=-9.6658 policy updated! \n",
      "train step 04484 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0733 diff={max=05.1511, min=00.0128, mean=01.0093} policy_loss=-9.3970 policy updated! \n",
      "train step 04485 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=06.2068 diff={max=10.3775, min=00.0589, mean=01.5027} policy_loss=-8.7533 policy updated! \n",
      "train step 04486 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8036 diff={max=06.8957, min=00.0164, mean=01.1690} policy_loss=-8.6554 policy updated! \n",
      "train step 04487 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8178 diff={max=04.1781, min=00.0206, mean=00.8732} policy_loss=-8.3434 policy updated! \n",
      "train step 04488 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3784 diff={max=08.2328, min=00.0105, mean=00.9951} policy_loss=-6.1012 policy updated! \n",
      "train step 04489 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2618 diff={max=03.1534, min=00.0443, mean=00.8538} policy_loss=-7.3336 policy updated! \n",
      "train step 04490 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.0837 diff={max=07.0116, min=00.0211, mean=01.1535} policy_loss=-7.1430 policy updated! \n",
      "train step 04491 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=04.2295 diff={max=07.6547, min=00.0068, mean=01.2317} policy_loss=-6.8203 policy updated! \n",
      "train step 04492 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0292 diff={max=04.0268, min=00.0039, mean=01.0065} policy_loss=-7.2334 policy updated! \n",
      "train step 04493 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0959 diff={max=05.4802, min=00.0421, mean=01.1455} policy_loss=-6.9484 policy updated! \n",
      "train step 04494 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=02.7030 diff={max=04.9553, min=00.0011, mean=01.0749} policy_loss=-8.4967 policy updated! \n",
      "train step 04495 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.4700 diff={max=06.3756, min=00.0082, mean=01.0462} policy_loss=-9.0191 policy updated! \n",
      "train step 04496 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9700 diff={max=06.7289, min=00.0043, mean=01.4428} policy_loss=-7.0665 policy updated! \n",
      "train step 04497 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0694 diff={max=09.6185, min=00.0115, mean=01.1486} policy_loss=-8.8639 policy updated! \n",
      "train step 04498 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0817 diff={max=05.4199, min=00.0101, mean=01.0116} policy_loss=-8.6070 policy updated! \n",
      "train step 04499 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1087 diff={max=05.5058, min=00.0601, mean=00.9981} policy_loss=-7.4017 policy updated! \n",
      "train step 04500 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.3778 diff={max=05.9727, min=00.0406, mean=00.9839} policy_loss=-8.2080 policy updated! \n",
      "train step 04501 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=02.1089 diff={max=04.8523, min=00.0270, mean=00.9533} policy_loss=-8.0143 policy updated! \n",
      "train step 04502 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.5621 diff={max=13.1847, min=00.0434, mean=01.2768} policy_loss=-7.1849 policy updated! \n",
      "train step 04503 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=03.5394 diff={max=06.9306, min=00.0105, mean=01.2504} policy_loss=-7.4721 policy updated! \n",
      "train step 04504 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8613 diff={max=08.8198, min=00.1058, mean=01.2103} policy_loss=-7.8562 policy updated! \n",
      "train step 04505 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.5886 diff={max=03.7272, min=00.0338, mean=00.8888} policy_loss=-8.2525 policy updated! \n",
      "train step 04506 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6211 diff={max=08.2031, min=00.0475, mean=01.0610} policy_loss=-8.2245 policy updated! \n",
      "train step 04507 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3554 diff={max=07.4323, min=00.0282, mean=01.0605} policy_loss=-7.4898 policy updated! \n",
      "train step 04508 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7731 diff={max=03.4413, min=00.0116, mean=00.6326} policy_loss=-8.2014 policy updated! \n",
      "train step 04509 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7961 diff={max=06.4566, min=00.0044, mean=01.2767} policy_loss=-8.2585 policy updated! \n",
      "train step 04510 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.5278 diff={max=07.0401, min=00.0062, mean=01.2079} policy_loss=-9.1852 policy updated! \n",
      "train step 04511 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1611 diff={max=08.1592, min=00.0031, mean=01.0034} policy_loss=-8.1000 policy updated! \n",
      "train step 04512 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5690 diff={max=06.1680, min=00.0240, mean=01.0475} policy_loss=-6.8867 policy updated! \n",
      "train step 04513 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3755 diff={max=04.9637, min=00.0191, mean=01.0269} policy_loss=-9.0609 policy updated! \n",
      "train step 04514 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4439 diff={max=06.2359, min=00.0030, mean=01.0154} policy_loss=-7.3508 policy updated! \n",
      "train step 04515 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.5286 diff={max=07.2246, min=00.0071, mean=01.1355} policy_loss=-7.6488 policy updated! \n",
      "train step 04516 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5227 diff={max=06.6953, min=00.0157, mean=01.0458} policy_loss=-8.2362 policy updated! \n",
      "train step 04517 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3324 diff={max=03.4980, min=00.0037, mean=00.8230} policy_loss=-7.9642 policy updated! \n",
      "train step 04518 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2024 diff={max=09.1378, min=00.0212, mean=01.2486} policy_loss=-7.9031 policy updated! \n",
      "train step 04519 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3331 diff={max=06.9938, min=00.0528, mean=01.1023} policy_loss=-6.7437 policy updated! \n",
      "train step 04520 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.5623 diff={max=08.4687, min=00.0120, mean=01.1910} policy_loss=-7.9408 policy updated! \n",
      "train step 04521 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3096 diff={max=06.9670, min=00.0143, mean=00.8901} policy_loss=-6.9016 policy updated! \n",
      "train step 04522 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6436 diff={max=04.7429, min=00.0562, mean=01.1480} policy_loss=-8.7987 policy updated! \n",
      "train step 04523 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4105 diff={max=08.7280, min=00.0016, mean=01.0506} policy_loss=-8.5694 policy updated! \n",
      "train step 04524 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8241 diff={max=07.1595, min=00.0234, mean=01.1675} policy_loss=-8.2949 policy updated! \n",
      "train step 04525 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.3095 diff={max=03.3238, min=00.0548, mean=00.8397} policy_loss=-7.9049 policy updated! \n",
      "train step 04526 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1805 diff={max=08.6226, min=00.0002, mean=01.1954} policy_loss=-8.6405 policy updated! \n",
      "train step 04527 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1346 diff={max=06.8033, min=00.0222, mean=01.1869} policy_loss=-9.0450 policy updated! \n",
      "train step 04528 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4168 diff={max=05.1394, min=00.0051, mean=01.0493} policy_loss=-8.4088 policy updated! \n",
      "train step 04529 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9819 diff={max=06.2390, min=00.0090, mean=01.0652} policy_loss=-6.9037 policy updated! \n",
      "train step 04530 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.4258 diff={max=04.0837, min=00.0027, mean=01.1317} policy_loss=-6.8093 policy updated! \n",
      "train step 04531 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2021 diff={max=06.4195, min=00.0393, mean=01.7324} policy_loss=-7.9281 policy updated! \n",
      "train step 04532 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2774 diff={max=04.8667, min=00.0646, mean=01.1508} policy_loss=-10.2823 policy updated! \n",
      "train step 04533 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3302 diff={max=06.8854, min=00.0001, mean=01.1886} policy_loss=-8.0239 policy updated! \n",
      "train step 04534 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8322 diff={max=09.7181, min=00.0013, mean=01.4001} policy_loss=-8.8488 policy updated! \n",
      "train step 04535 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.9462 diff={max=07.5304, min=00.0097, mean=01.4549} policy_loss=-8.9345 policy updated! \n",
      "train step 04536 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5650 diff={max=08.0798, min=00.0299, mean=00.8594} policy_loss=-7.5866 policy updated! \n",
      "train step 04537 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1932 diff={max=10.0187, min=00.0256, mean=01.3130} policy_loss=-7.4784 policy updated! \n",
      "train step 04538 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0131 diff={max=08.6009, min=00.0122, mean=01.4520} policy_loss=-7.4492 policy updated! \n",
      "train step 04539 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=01.6726 diff={max=04.3773, min=00.0150, mean=00.9519} policy_loss=-8.5664 policy updated! \n",
      "train step 04540 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.0070 diff={max=07.3264, min=00.0211, mean=01.0735} policy_loss=-9.0420 policy updated! \n",
      "train step 04541 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1560 diff={max=05.2281, min=00.0185, mean=01.0499} policy_loss=-8.7393 policy updated! \n",
      "train step 04542 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2985 diff={max=08.4445, min=00.1401, mean=01.3667} policy_loss=-9.5886 policy updated! \n",
      "train step 04543 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8485 diff={max=04.1644, min=00.0426, mean=01.0015} policy_loss=-7.7256 policy updated! \n",
      "train step 04544 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3243 diff={max=07.6214, min=00.0630, mean=01.2305} policy_loss=-7.7684 policy updated! \n",
      "train step 04545 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=05.7821 diff={max=07.3471, min=00.0004, mean=01.6590} policy_loss=-8.3704 policy updated! \n",
      "train step 04546 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8301 diff={max=07.3645, min=00.0145, mean=01.1808} policy_loss=-6.4563 policy updated! \n",
      "train step 04547 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3930 diff={max=06.3669, min=00.0352, mean=01.2144} policy_loss=-7.7339 policy updated! \n",
      "train step 04548 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0725 diff={max=03.8666, min=00.0129, mean=01.0289} policy_loss=-6.9262 policy updated! \n",
      "train step 04549 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5777 diff={max=05.6217, min=00.0084, mean=01.3244} policy_loss=-7.5538 policy updated! \n",
      "train step 04550 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.2049 diff={max=07.1263, min=00.0286, mean=01.4152} policy_loss=-7.2833 policy updated! \n",
      "train step 04551 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=03.7280 diff={max=06.6092, min=00.0023, mean=01.2284} policy_loss=-7.1602 policy updated! \n",
      "train step 04552 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2476 diff={max=04.7377, min=00.0093, mean=00.7780} policy_loss=-7.0211 policy updated! \n",
      "train step 04553 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4540 diff={max=10.9743, min=00.0181, mean=01.2470} policy_loss=-8.0219 policy updated! \n",
      "train step 04554 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8140 diff={max=03.0978, min=00.0048, mean=00.6506} policy_loss=-8.8893 policy updated! \n",
      "train step 04555 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.1330 diff={max=04.8499, min=00.0279, mean=01.0546} policy_loss=-7.5321 policy updated! \n",
      "train step 04556 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1777 diff={max=06.4393, min=00.0418, mean=00.9237} policy_loss=-7.9801 policy updated! \n",
      "train step 04557 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6446 diff={max=09.8658, min=00.0000, mean=01.2535} policy_loss=-8.5298 policy updated! \n",
      "train step 04558 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2539 diff={max=03.2875, min=00.0194, mean=00.7905} policy_loss=-7.4745 policy updated! \n",
      "train step 04559 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6871 diff={max=06.6994, min=00.0189, mean=01.1592} policy_loss=-7.3438 policy updated! \n",
      "train step 04560 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=05.3117 diff={max=07.1572, min=00.0115, mean=01.4830} policy_loss=-8.1504 policy updated! \n",
      "train step 04561 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4351 diff={max=08.2369, min=00.0255, mean=00.9490} policy_loss=-8.3396 policy updated! \n",
      "train step 04562 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6892 diff={max=04.9674, min=00.0416, mean=00.8468} policy_loss=-7.9480 policy updated! \n",
      "train step 04563 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6698 diff={max=08.3307, min=00.0352, mean=01.5451} policy_loss=-8.4180 policy updated! \n",
      "train step 04564 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9661 diff={max=04.7338, min=00.0022, mean=00.9863} policy_loss=-9.2223 policy updated! \n",
      "train step 04565 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.7595 diff={max=05.1990, min=00.0332, mean=01.3932} policy_loss=-8.2576 policy updated! \n",
      "train step 04566 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2717 diff={max=07.9839, min=00.0047, mean=00.9182} policy_loss=-7.8108 policy updated! \n",
      "train step 04567 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4618 diff={max=07.8974, min=00.0235, mean=01.1294} policy_loss=-7.5676 policy updated! \n",
      "train step 04568 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0587 diff={max=02.7912, min=00.0030, mean=00.7649} policy_loss=-8.0458 policy updated! \n",
      "train step 04569 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=02.9465 diff={max=07.2438, min=00.0123, mean=01.0612} policy_loss=-9.0034 policy updated! \n",
      "train step 04570 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.1413 diff={max=05.1463, min=00.0149, mean=01.2253} policy_loss=-9.6751 policy updated! \n",
      "train step 04571 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7100 diff={max=05.8926, min=00.0790, mean=01.3611} policy_loss=-9.4375 policy updated! \n",
      "train step 04572 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6556 diff={max=04.0568, min=00.0130, mean=00.9162} policy_loss=-8.3280 policy updated! \n",
      "train step 04573 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5017 diff={max=05.7655, min=00.0107, mean=01.4092} policy_loss=-8.5455 policy updated! \n",
      "train step 04574 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6924 diff={max=07.7381, min=00.0234, mean=00.9701} policy_loss=-7.2682 policy updated! \n",
      "train step 04575 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.9097 diff={max=04.9940, min=00.0066, mean=00.9228} policy_loss=-6.9546 policy updated! \n",
      "train step 04576 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=05.3873 diff={max=09.4534, min=00.0053, mean=01.4405} policy_loss=-8.5416 policy updated! \n",
      "train step 04577 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3179 diff={max=03.3666, min=00.0013, mean=00.8093} policy_loss=-7.6850 policy updated! \n",
      "train step 04578 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8866 diff={max=06.3305, min=00.0140, mean=00.9170} policy_loss=-8.8432 policy updated! \n",
      "train step 04579 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1716 diff={max=06.9480, min=00.0082, mean=01.0865} policy_loss=-9.0294 policy updated! \n",
      "train step 04580 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.3670 diff={max=06.4379, min=00.0432, mean=01.4616} policy_loss=-8.5727 policy updated! \n",
      "train step 04581 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1115 diff={max=05.6605, min=00.0336, mean=00.9907} policy_loss=-8.9192 policy updated! \n",
      "train step 04582 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2570 diff={max=06.3658, min=00.0618, mean=01.0232} policy_loss=-9.3940 policy updated! \n",
      "train step 04583 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1539 diff={max=06.7428, min=00.0146, mean=00.9164} policy_loss=-8.7628 policy updated! \n",
      "train step 04584 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0762 diff={max=05.6928, min=00.0074, mean=01.1008} policy_loss=-8.8889 policy updated! \n",
      "train step 04585 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.4187 diff={max=06.8604, min=00.0524, mean=01.0327} policy_loss=-8.5359 policy updated! \n",
      "train step 04586 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9849 diff={max=06.3327, min=00.0116, mean=01.0806} policy_loss=-7.5026 policy updated! \n",
      "train step 04587 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4293 diff={max=08.5864, min=00.0372, mean=01.3249} policy_loss=-8.7100 policy updated! \n",
      "train step 04588 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0753 diff={max=08.8727, min=00.0157, mean=01.4513} policy_loss=-8.9448 policy updated! \n",
      "train step 04589 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1551 diff={max=09.9815, min=00.0149, mean=00.8847} policy_loss=-7.7899 policy updated! \n",
      "train step 04590 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.5679 diff={max=06.5642, min=00.0639, mean=00.7403} policy_loss=-7.9516 policy updated! \n",
      "train step 04591 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2248 diff={max=08.1040, min=00.0187, mean=00.8044} policy_loss=-7.8369 policy updated! \n",
      "train step 04592 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9874 diff={max=07.1853, min=00.0064, mean=01.2652} policy_loss=-8.1675 policy updated! \n",
      "train step 04593 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2448 diff={max=05.8612, min=00.0808, mean=01.1751} policy_loss=-7.5456 policy updated! \n",
      "train step 04594 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9006 diff={max=07.2319, min=00.0013, mean=01.0955} policy_loss=-7.2120 policy updated! \n",
      "train step 04595 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=03.6287 diff={max=09.1484, min=00.0694, mean=01.2047} policy_loss=-8.1616 policy updated! \n",
      "train step 04596 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8707 diff={max=05.9895, min=00.0301, mean=00.9099} policy_loss=-8.6900 policy updated! \n",
      "train step 04597 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6306 diff={max=07.5682, min=00.0062, mean=00.9580} policy_loss=-8.2208 policy updated! \n",
      "train step 04598 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7206 diff={max=08.8430, min=00.0218, mean=01.1821} policy_loss=-8.6942 policy updated! \n",
      "train step 04599 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7667 diff={max=10.5277, min=00.0039, mean=01.1799} policy_loss=-7.2601 policy updated! \n",
      "train step 04600 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=05.9013 diff={max=08.0907, min=00.0231, mean=01.4835} policy_loss=-8.4641 policy updated! \n",
      "train step 04601 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6840 diff={max=04.7443, min=00.0418, mean=00.9097} policy_loss=-8.4643 policy updated! \n",
      "train step 04602 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1917 diff={max=02.9013, min=00.0175, mean=00.8260} policy_loss=-8.8935 policy updated! \n",
      "train step 04603 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1063 diff={max=06.6891, min=00.0115, mean=01.0247} policy_loss=-6.7942 policy updated! \n",
      "train step 04604 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5118 diff={max=08.8864, min=00.0944, mean=01.4388} policy_loss=-9.2505 policy updated! \n",
      "train step 04605 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=01.2098 diff={max=04.3524, min=00.0021, mean=00.7052} policy_loss=-7.2808 policy updated! \n",
      "train step 04606 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4011 diff={max=06.2371, min=00.0819, mean=00.9410} policy_loss=-6.4579 policy updated! \n",
      "train step 04607 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0865 diff={max=06.8561, min=00.0608, mean=01.5252} policy_loss=-8.0030 policy updated! \n",
      "train step 04608 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4438 diff={max=04.9730, min=00.0326, mean=00.7310} policy_loss=-7.3211 policy updated! \n",
      "train step 04609 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8900 diff={max=07.6481, min=00.0153, mean=01.1468} policy_loss=-8.4825 policy updated! \n",
      "train step 04610 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=06.5185 diff={max=12.3270, min=00.0055, mean=01.4283} policy_loss=-8.2805 policy updated! \n",
      "train step 04611 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4139 diff={max=05.2872, min=00.0195, mean=00.7874} policy_loss=-9.7046 policy updated! \n",
      "train step 04612 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3831 diff={max=07.6914, min=00.0019, mean=01.3543} policy_loss=-8.2503 policy updated! \n",
      "train step 04613 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1134 diff={max=07.8382, min=00.0068, mean=01.6019} policy_loss=-8.9871 policy updated! \n",
      "train step 04614 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2703 diff={max=06.8156, min=00.0211, mean=01.1160} policy_loss=-8.1547 policy updated! \n",
      "train step 04615 reward={max=10.0000, min=10.0000, mean=10.0000} optimizing loss=03.4838 diff={max=08.3883, min=00.0027, mean=01.0624} policy_loss=-8.3127 policy updated! \n",
      "train step 04616 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4254 diff={max=07.4535, min=00.0303, mean=00.9459} policy_loss=-8.5574 policy updated! \n",
      "train step 04617 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7515 diff={max=04.9689, min=00.0021, mean=01.1228} policy_loss=-8.4189 policy updated! \n",
      "train step 04618 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8629 diff={max=06.4900, min=00.0102, mean=01.0705} policy_loss=-9.6006 policy updated! \n",
      "train step 04619 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2856 diff={max=05.1412, min=00.0119, mean=00.7050} policy_loss=-8.3164 policy updated! \n",
      "train step 04620 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=03.7180 diff={max=07.0350, min=00.0255, mean=01.2449} policy_loss=-8.4328 policy updated! \n",
      "train step 04621 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6578 diff={max=03.7257, min=00.0140, mean=00.9440} policy_loss=-9.5132 policy updated! \n",
      "train step 04622 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4246 diff={max=06.5516, min=00.0176, mean=01.0759} policy_loss=-8.3700 policy updated! \n",
      "train step 04623 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9528 diff={max=04.3770, min=00.0201, mean=00.9157} policy_loss=-7.4387 policy updated! \n",
      "train step 04624 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3309 diff={max=05.8515, min=00.0178, mean=00.9015} policy_loss=-7.4049 policy updated! \n",
      "train step 04625 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=02.3871 diff={max=06.0943, min=00.0166, mean=01.0471} policy_loss=-7.3242 policy updated! \n",
      "train step 04626 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2354 diff={max=07.1277, min=00.0379, mean=00.9072} policy_loss=-7.6031 policy updated! \n",
      "train step 04627 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1851 diff={max=06.2138, min=00.0201, mean=01.1177} policy_loss=-7.9416 policy updated! \n",
      "train step 04628 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5669 diff={max=04.6209, min=00.0128, mean=01.1512} policy_loss=-7.6083 policy updated! \n",
      "train step 04629 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5901 diff={max=08.0973, min=00.0046, mean=01.2380} policy_loss=-6.5795 policy updated! \n",
      "train step 04630 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=03.1822 diff={max=08.3614, min=00.0265, mean=01.0846} policy_loss=-9.3136 policy updated! \n",
      "train step 04631 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9323 diff={max=12.1776, min=00.0083, mean=01.1151} policy_loss=-7.7810 policy updated! \n",
      "train step 04632 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1896 diff={max=03.8660, min=00.0028, mean=00.7232} policy_loss=-8.2543 policy updated! \n",
      "train step 04633 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8085 diff={max=05.3732, min=00.0028, mean=01.0616} policy_loss=-8.0761 policy updated! \n",
      "train step 04634 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6833 diff={max=04.7526, min=00.0255, mean=00.8467} policy_loss=-7.8919 policy updated! \n",
      "train step 04635 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=01.9998 diff={max=05.7145, min=00.0123, mean=00.9001} policy_loss=-7.7385 policy updated! \n",
      "train step 04636 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4013 diff={max=07.9118, min=00.0174, mean=00.9639} policy_loss=-8.3145 policy updated! \n",
      "train step 04637 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9806 diff={max=04.6241, min=00.0035, mean=00.9362} policy_loss=-8.0703 policy updated! \n",
      "train step 04638 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=05.2136 diff={max=09.1074, min=00.0132, mean=01.3387} policy_loss=-8.3002 policy updated! \n",
      "train step 04639 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=04.4625 diff={max=09.1772, min=00.0055, mean=01.2192} policy_loss=-8.3194 policy updated! \n",
      "train step 04640 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.3236 diff={max=07.6048, min=00.0194, mean=01.1040} policy_loss=-6.7278 policy updated! \n",
      "train step 04641 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7689 diff={max=01.9279, min=00.0058, mean=00.7139} policy_loss=-7.4701 policy updated! \n",
      "train step 04642 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1568 diff={max=08.2239, min=00.0360, mean=00.9486} policy_loss=-7.2831 policy updated! \n",
      "train step 04643 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=01.8670 diff={max=04.2263, min=00.0027, mean=00.9058} policy_loss=-8.7749 policy updated! \n",
      "train step 04644 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6350 diff={max=03.5424, min=00.0105, mean=00.9628} policy_loss=-8.7937 policy updated! \n",
      "train step 04645 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.5921 diff={max=05.8685, min=00.0034, mean=01.2394} policy_loss=-8.9107 policy updated! \n",
      "train step 04646 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6928 diff={max=08.1494, min=00.0013, mean=01.0532} policy_loss=-7.9376 policy updated! \n",
      "train step 04647 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7652 diff={max=10.4041, min=00.0604, mean=01.1492} policy_loss=-8.0333 policy updated! \n",
      "train step 04648 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8878 diff={max=02.7169, min=00.0189, mean=00.7539} policy_loss=-7.3286 policy updated! \n",
      "train step 04649 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8923 diff={max=05.8337, min=00.0865, mean=01.3732} policy_loss=-9.0717 policy updated! \n",
      "train step 04650 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.6610 diff={max=08.4426, min=00.0097, mean=01.0900} policy_loss=-8.1331 policy updated! \n",
      "train step 04651 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0615 diff={max=04.0899, min=00.0027, mean=00.9547} policy_loss=-8.0971 policy updated! \n",
      "train step 04652 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1238 diff={max=06.4797, min=00.0189, mean=00.9580} policy_loss=-8.4660 policy updated! \n",
      "train step 04653 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1167 diff={max=05.4704, min=00.0346, mean=00.9002} policy_loss=-6.8700 policy updated! \n",
      "train step 04654 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2894 diff={max=04.1561, min=00.0001, mean=01.0452} policy_loss=-8.1481 policy updated! \n",
      "train step 04655 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.3453 diff={max=05.7035, min=00.0064, mean=00.9899} policy_loss=-7.6621 policy updated! \n",
      "train step 04656 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5654 diff={max=06.8571, min=00.0111, mean=01.0277} policy_loss=-8.8461 policy updated! \n",
      "train step 04657 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8756 diff={max=08.6908, min=00.0339, mean=01.3598} policy_loss=-9.3021 policy updated! \n",
      "train step 04658 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2746 diff={max=05.3183, min=00.0155, mean=00.8795} policy_loss=-7.9602 policy updated! \n",
      "train step 04659 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0784 diff={max=10.4052, min=00.0076, mean=01.3180} policy_loss=-7.6855 policy updated! \n",
      "train step 04660 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.1484 diff={max=03.3816, min=00.0157, mean=00.7920} policy_loss=-7.7959 policy updated! \n",
      "train step 04661 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8071 diff={max=06.0022, min=00.0058, mean=01.0011} policy_loss=-9.2668 policy updated! \n",
      "train step 04662 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6014 diff={max=07.5933, min=00.0430, mean=01.1630} policy_loss=-9.1420 policy updated! \n",
      "train step 04663 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1262 diff={max=03.9562, min=00.0247, mean=00.7299} policy_loss=-8.2374 policy updated! \n",
      "train step 04664 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6676 diff={max=03.5841, min=00.0006, mean=00.8362} policy_loss=-8.6421 policy updated! \n",
      "train step 04665 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.3845 diff={max=07.8357, min=00.0013, mean=01.1691} policy_loss=-7.7890 policy updated! \n",
      "train step 04666 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3360 diff={max=04.8368, min=00.0024, mean=00.7716} policy_loss=-8.8287 policy updated! \n",
      "train step 04667 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9441 diff={max=08.8971, min=00.0391, mean=01.0273} policy_loss=-8.6945 policy updated! \n",
      "train step 04668 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5429 diff={max=04.0704, min=00.0228, mean=00.8338} policy_loss=-8.7808 policy updated! \n",
      "train step 04669 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3859 diff={max=04.6788, min=00.0014, mean=00.7537} policy_loss=-8.7304 policy updated! \n",
      "train step 04670 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.7458 diff={max=04.1626, min=00.0019, mean=00.8270} policy_loss=-6.9478 policy updated! \n",
      "train step 04671 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5307 diff={max=07.2468, min=00.0267, mean=01.0590} policy_loss=-8.0929 policy updated! \n",
      "train step 04672 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2554 diff={max=05.3041, min=00.0023, mean=00.9472} policy_loss=-9.8255 policy updated! \n",
      "train step 04673 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2923 diff={max=06.7528, min=00.0881, mean=01.0229} policy_loss=-8.7948 policy updated! \n",
      "train step 04674 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8523 diff={max=06.3372, min=00.0406, mean=00.9931} policy_loss=-7.7989 policy updated! \n",
      "train step 04675 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.5287 diff={max=06.6618, min=00.0185, mean=01.0924} policy_loss=-8.5213 policy updated! \n",
      "train step 04676 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7469 diff={max=02.4788, min=00.0300, mean=00.6573} policy_loss=-7.5592 policy updated! \n",
      "train step 04677 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4845 diff={max=08.2625, min=00.0143, mean=00.9022} policy_loss=-7.7703 policy updated! \n",
      "train step 04678 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1055 diff={max=02.9035, min=00.0049, mean=00.8342} policy_loss=-8.0497 policy updated! \n",
      "train step 04679 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5366 diff={max=07.9085, min=00.0097, mean=01.1553} policy_loss=-9.3963 policy updated! \n",
      "train step 04680 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=04.1445 diff={max=08.1290, min=00.0695, mean=01.1149} policy_loss=-8.5179 policy updated! \n",
      "train step 04681 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2095 diff={max=05.8426, min=00.0256, mean=01.1306} policy_loss=-8.8280 policy updated! \n",
      "train step 04682 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8309 diff={max=06.7372, min=00.0197, mean=01.0162} policy_loss=-8.4696 policy updated! \n",
      "train step 04683 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9078 diff={max=05.9674, min=00.0042, mean=01.0933} policy_loss=-8.8563 policy updated! \n",
      "train step 04684 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6087 diff={max=06.7684, min=00.0207, mean=01.0678} policy_loss=-8.8660 policy updated! \n",
      "train step 04685 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.4110 diff={max=03.5792, min=00.0183, mean=00.8790} policy_loss=-8.5092 policy updated! \n",
      "train step 04686 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0000 diff={max=04.9293, min=00.0078, mean=01.2554} policy_loss=-7.8486 policy updated! \n",
      "train step 04687 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9976 diff={max=08.0059, min=00.0325, mean=01.1608} policy_loss=-6.7691 policy updated! \n",
      "train step 04688 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7719 diff={max=02.9620, min=00.0023, mean=00.6345} policy_loss=-7.1518 policy updated! \n",
      "train step 04689 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=01.3088 diff={max=03.0726, min=00.0039, mean=00.8479} policy_loss=-6.9900 policy updated! \n",
      "train step 04690 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.3882 diff={max=05.0318, min=00.0004, mean=00.7376} policy_loss=-6.4721 policy updated! \n",
      "train step 04691 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9575 diff={max=06.5878, min=00.0330, mean=01.2578} policy_loss=-8.8856 policy updated! \n",
      "train step 04692 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9167 diff={max=09.8113, min=00.0212, mean=00.8674} policy_loss=-8.4131 policy updated! \n",
      "train step 04693 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=02.2026 diff={max=06.1804, min=00.0086, mean=00.9653} policy_loss=-9.9121 policy updated! \n",
      "train step 04694 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8591 diff={max=08.7394, min=00.0389, mean=01.5798} policy_loss=-8.7739 policy updated! \n",
      "train step 04695 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.6347 diff={max=06.0008, min=00.0284, mean=01.0056} policy_loss=-7.2791 policy updated! \n",
      "train step 04696 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6795 diff={max=04.3866, min=00.0015, mean=01.0856} policy_loss=-8.1743 policy updated! \n",
      "train step 04697 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5258 diff={max=07.3247, min=00.0134, mean=01.3825} policy_loss=-8.7399 policy updated! \n",
      "train step 04698 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9313 diff={max=12.0770, min=00.0129, mean=01.3733} policy_loss=-7.7256 policy updated! \n",
      "train step 04699 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0349 diff={max=09.8691, min=00.0058, mean=01.2295} policy_loss=-7.6269 policy updated! \n",
      "train step 04700 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.5389 diff={max=07.0367, min=00.0129, mean=01.1570} policy_loss=-7.4773 policy updated! \n",
      "train step 04701 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0089 diff={max=10.2460, min=00.0031, mean=01.2329} policy_loss=-7.8693 policy updated! \n",
      "train step 04702 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2426 diff={max=03.9416, min=00.0350, mean=01.0945} policy_loss=-7.7221 policy updated! \n",
      "train step 04703 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7074 diff={max=08.5448, min=00.0058, mean=01.1216} policy_loss=-7.2410 policy updated! \n",
      "train step 04704 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2043 diff={max=03.2797, min=00.0145, mean=00.8147} policy_loss=-8.1462 policy updated! \n",
      "train step 04705 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=02.4416 diff={max=05.6303, min=00.0112, mean=00.9669} policy_loss=-9.0607 policy updated! \n",
      "train step 04706 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0560 diff={max=04.7734, min=00.0159, mean=00.9536} policy_loss=-8.6144 policy updated! \n",
      "train step 04707 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1204 diff={max=06.6579, min=00.0254, mean=01.0613} policy_loss=-7.3165 policy updated! \n",
      "train step 04708 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8797 diff={max=07.0620, min=00.0038, mean=01.4910} policy_loss=-9.2043 policy updated! \n",
      "train step 04709 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0773 diff={max=04.3085, min=00.0238, mean=00.9850} policy_loss=-8.2877 policy updated! \n",
      "train step 04710 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.5775 diff={max=07.3815, min=00.0183, mean=01.0729} policy_loss=-7.5682 policy updated! \n",
      "train step 04711 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5674 diff={max=08.1729, min=00.0014, mean=00.9858} policy_loss=-9.9114 policy updated! \n",
      "train step 04712 reward={max=09.0000, min=09.0000, mean=09.0000} optimizing loss=02.7508 diff={max=07.2412, min=00.0236, mean=01.0386} policy_loss=-7.7799 policy updated! \n",
      "train step 04713 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9647 diff={max=08.5016, min=00.0400, mean=01.2468} policy_loss=-8.2973 policy updated! \n",
      "train step 04714 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2442 diff={max=02.8140, min=00.0152, mean=00.8140} policy_loss=-8.0416 policy updated! \n",
      "train step 04715 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.1431 diff={max=08.9469, min=00.0058, mean=01.2303} policy_loss=-7.5080 policy updated! \n",
      "train step 04716 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0344 diff={max=10.7450, min=00.0001, mean=00.9932} policy_loss=-8.4206 policy updated! \n",
      "train step 04717 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1974 diff={max=05.8745, min=00.0015, mean=01.0291} policy_loss=-6.4728 policy updated! \n",
      "train step 04718 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3704 diff={max=09.9422, min=00.0087, mean=01.0784} policy_loss=-9.1938 policy updated! \n",
      "train step 04719 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0302 diff={max=04.9050, min=00.0400, mean=01.0450} policy_loss=-7.7154 policy updated! \n",
      "train step 04720 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.6081 diff={max=06.4995, min=00.0030, mean=01.1166} policy_loss=-7.9552 policy updated! \n",
      "train step 04721 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8374 diff={max=06.1350, min=00.0009, mean=01.1701} policy_loss=-7.8067 policy updated! \n",
      "train step 04722 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4865 diff={max=06.2591, min=00.0017, mean=01.2428} policy_loss=-8.1945 policy updated! \n",
      "train step 04723 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6205 diff={max=06.9923, min=00.0101, mean=01.2481} policy_loss=-8.6637 policy updated! \n",
      "train step 04724 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2459 diff={max=04.0843, min=00.0065, mean=01.1478} policy_loss=-9.1978 policy updated! \n",
      "train step 04725 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.1345 diff={max=02.6968, min=00.0347, mean=00.8441} policy_loss=-7.3886 policy updated! \n",
      "train step 04726 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4595 diff={max=04.3833, min=00.0009, mean=01.1714} policy_loss=-8.2108 policy updated! \n",
      "train step 04727 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0097 diff={max=05.9835, min=00.0803, mean=01.1332} policy_loss=-7.9505 policy updated! \n",
      "train step 04728 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6134 diff={max=11.2239, min=00.0540, mean=01.4622} policy_loss=-7.4565 policy updated! \n",
      "train step 04729 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7341 diff={max=10.8506, min=00.0263, mean=01.3038} policy_loss=-7.7977 policy updated! \n",
      "train step 04730 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.8507 diff={max=03.7086, min=00.0050, mean=00.9605} policy_loss=-6.5873 policy updated! \n",
      "train step 04731 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2157 diff={max=08.8862, min=00.0087, mean=01.2571} policy_loss=-6.3339 policy updated! \n",
      "train step 04732 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2323 diff={max=04.1776, min=00.0565, mean=01.1226} policy_loss=-8.0867 policy updated! \n",
      "train step 04733 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3526 diff={max=06.1032, min=00.0301, mean=01.1116} policy_loss=-7.7342 policy updated! \n",
      "train step 04734 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9338 diff={max=04.6094, min=00.0228, mean=01.0268} policy_loss=-9.7293 policy updated! \n",
      "train step 04735 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.4996 diff={max=07.8649, min=00.0202, mean=01.3138} policy_loss=-9.6607 policy updated! \n",
      "train step 04736 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5758 diff={max=09.3470, min=00.0224, mean=01.1867} policy_loss=-8.6984 policy updated! \n",
      "train step 04737 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1517 diff={max=06.1655, min=00.0212, mean=01.2534} policy_loss=-9.9187 policy updated! \n",
      "train step 04738 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6376 diff={max=08.0949, min=00.0177, mean=01.4413} policy_loss=-8.0571 policy updated! \n",
      "train step 04739 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=03.2171 diff={max=04.9788, min=00.0266, mean=01.2827} policy_loss=-7.8648 policy updated! \n",
      "train step 04740 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.1282 diff={max=07.7462, min=00.0054, mean=01.1590} policy_loss=-8.3803 policy updated! \n",
      "train step 04741 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6639 diff={max=05.1642, min=00.0006, mean=01.0879} policy_loss=-7.7699 policy updated! \n",
      "train step 04742 reward={max=09.0000, min=09.0000, mean=09.0000} optimizing loss=03.3487 diff={max=06.8014, min=00.0259, mean=01.0354} policy_loss=-6.5142 policy updated! \n",
      "train step 04743 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1079 diff={max=09.6924, min=00.0343, mean=01.3246} policy_loss=-7.2350 policy updated! \n",
      "train step 04744 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=01.6000 diff={max=04.1563, min=00.0085, mean=00.8901} policy_loss=-7.6350 policy updated! \n",
      "train step 04745 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.5668 diff={max=06.7541, min=00.0051, mean=00.7319} policy_loss=-8.1862 policy updated! \n",
      "train step 04746 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4673 diff={max=08.4304, min=00.0074, mean=01.0015} policy_loss=-7.6731 policy updated! \n",
      "train step 04747 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0890 diff={max=06.1206, min=00.0034, mean=01.1003} policy_loss=-9.5574 policy updated! \n",
      "train step 04748 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6722 diff={max=03.3508, min=00.0483, mean=00.9403} policy_loss=-9.0015 policy updated! \n",
      "train step 04749 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3940 diff={max=05.8803, min=00.0014, mean=01.0120} policy_loss=-8.9831 policy updated! \n",
      "train step 04750 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.2950 diff={max=10.6663, min=00.0186, mean=00.9370} policy_loss=-9.4387 policy updated! \n",
      "train step 04751 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=01.4431 diff={max=03.9489, min=00.0162, mean=00.8102} policy_loss=-8.2170 policy updated! \n",
      "train step 04752 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0836 diff={max=06.0879, min=00.0275, mean=00.9921} policy_loss=-7.4936 policy updated! \n",
      "train step 04753 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=02.8953 diff={max=08.2040, min=00.0283, mean=00.9866} policy_loss=-7.5629 policy updated! \n",
      "train step 04754 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2843 diff={max=04.6858, min=00.0048, mean=01.0129} policy_loss=-7.0972 policy updated! \n",
      "train step 04755 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.4442 diff={max=04.3053, min=00.0179, mean=01.1105} policy_loss=-7.9232 policy updated! \n",
      "train step 04756 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4749 diff={max=06.4185, min=00.0314, mean=01.1858} policy_loss=-9.0546 policy updated! \n",
      "train step 04757 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2464 diff={max=07.8470, min=00.0084, mean=00.8759} policy_loss=-8.0550 policy updated! \n",
      "train step 04758 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5763 diff={max=10.3924, min=00.0107, mean=01.4567} policy_loss=-7.5598 policy updated! \n",
      "train step 04759 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8121 diff={max=07.5951, min=00.0151, mean=01.0126} policy_loss=-8.3616 policy updated! \n",
      "train step 04760 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.8431 diff={max=04.2746, min=00.0737, mean=00.9559} policy_loss=-8.3632 policy updated! \n",
      "train step 04761 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7176 diff={max=05.5988, min=00.0372, mean=01.0621} policy_loss=-7.9349 policy updated! \n",
      "train step 04762 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8615 diff={max=03.5312, min=00.0185, mean=01.0600} policy_loss=-8.8363 policy updated! \n",
      "train step 04763 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0844 diff={max=07.4339, min=00.0134, mean=01.2088} policy_loss=-9.1069 policy updated! \n",
      "train step 04764 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5299 diff={max=08.8853, min=00.0026, mean=01.1212} policy_loss=-7.4266 policy updated! \n",
      "train step 04765 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.1068 diff={max=10.9604, min=00.0389, mean=01.0368} policy_loss=-9.6959 policy updated! \n",
      "train step 04766 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2596 diff={max=06.7622, min=00.0312, mean=01.2465} policy_loss=-7.6401 policy updated! \n",
      "train step 04767 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4876 diff={max=11.2648, min=00.0132, mean=01.3222} policy_loss=-8.5002 policy updated! \n",
      "train step 04768 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7854 diff={max=06.7633, min=00.0853, mean=01.0332} policy_loss=-8.8977 policy updated! \n",
      "train step 04769 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3927 diff={max=07.9433, min=00.0228, mean=00.9315} policy_loss=-7.5879 policy updated! \n",
      "train step 04770 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.0529 diff={max=07.2301, min=00.0344, mean=01.1547} policy_loss=-7.2187 policy updated! \n",
      "train step 04771 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=03.3689 diff={max=05.4616, min=00.0198, mean=01.2106} policy_loss=-7.9151 policy updated! \n",
      "train step 04772 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4860 diff={max=09.6026, min=00.0429, mean=01.4755} policy_loss=-8.1849 policy updated! \n",
      "train step 04773 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6648 diff={max=11.4168, min=00.0198, mean=01.2968} policy_loss=-8.4561 policy updated! \n",
      "train step 04774 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1329 diff={max=08.2554, min=00.0444, mean=01.2630} policy_loss=-7.9903 policy updated! \n",
      "train step 04775 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=02.9683 diff={max=05.6272, min=00.0312, mean=01.0981} policy_loss=-8.1459 policy updated! \n",
      "train step 04776 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8181 diff={max=05.7001, min=00.0133, mean=01.0643} policy_loss=-8.0102 policy updated! \n",
      "train step 04777 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4892 diff={max=14.7306, min=00.0319, mean=01.3365} policy_loss=-7.6009 policy updated! \n",
      "train step 04778 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5625 diff={max=09.2276, min=00.0979, mean=01.5697} policy_loss=-8.7797 policy updated! \n",
      "train step 04779 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3199 diff={max=05.3237, min=00.0184, mean=01.3448} policy_loss=-9.4375 policy updated! \n",
      "train step 04780 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=04.6288 diff={max=06.7226, min=00.0569, mean=01.4198} policy_loss=-8.6733 policy updated! \n",
      "train step 04781 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9893 diff={max=08.0473, min=00.0040, mean=01.3579} policy_loss=-7.6700 policy updated! \n",
      "train step 04782 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2672 diff={max=03.9445, min=00.0387, mean=00.7895} policy_loss=-7.7077 policy updated! \n",
      "train step 04783 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8517 diff={max=04.1218, min=00.0011, mean=00.9838} policy_loss=-7.7981 policy updated! \n",
      "train step 04784 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4582 diff={max=06.5888, min=00.0969, mean=01.0629} policy_loss=-8.8475 policy updated! \n",
      "train step 04785 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=02.1402 diff={max=05.2808, min=00.0149, mean=00.9995} policy_loss=-7.5678 policy updated! \n",
      "train step 04786 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1903 diff={max=08.2024, min=00.0031, mean=01.1562} policy_loss=-6.3824 policy updated! \n",
      "train step 04787 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2793 diff={max=04.5983, min=00.0647, mean=01.1590} policy_loss=-7.0604 policy updated! \n",
      "train step 04788 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1672 diff={max=06.0260, min=00.0208, mean=00.9487} policy_loss=-7.2177 policy updated! \n",
      "train step 04789 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6531 diff={max=08.8220, min=00.0016, mean=01.3459} policy_loss=-7.3006 policy updated! \n",
      "train step 04790 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=01.7802 diff={max=04.6278, min=00.0067, mean=00.8846} policy_loss=-7.2027 policy updated! \n",
      "train step 04791 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3920 diff={max=04.5667, min=00.0080, mean=01.2617} policy_loss=-7.5219 policy updated! \n",
      "train step 04792 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6206 diff={max=07.1869, min=00.0109, mean=01.3571} policy_loss=-9.6748 policy updated! \n",
      "train step 04793 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5690 diff={max=08.2757, min=00.0040, mean=01.2755} policy_loss=-8.7153 policy updated! \n",
      "train step 04794 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1150 diff={max=07.3530, min=00.0536, mean=01.2152} policy_loss=-8.7404 policy updated! \n",
      "train step 04795 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.4497 diff={max=04.8499, min=00.0276, mean=01.3599} policy_loss=-9.1233 policy updated! \n",
      "train step 04796 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0426 diff={max=05.6212, min=00.0107, mean=00.9870} policy_loss=-7.6823 policy updated! \n",
      "train step 04797 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9799 diff={max=06.7010, min=00.0111, mean=01.3790} policy_loss=-7.6520 policy updated! \n",
      "train step 04798 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7396 diff={max=06.0263, min=00.0356, mean=01.2683} policy_loss=-8.2025 policy updated! \n",
      "train step 04799 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1662 diff={max=08.6602, min=00.0031, mean=01.3889} policy_loss=-8.0435 policy updated! \n",
      "train step 04800 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.3212 diff={max=08.6982, min=00.0688, mean=01.2406} policy_loss=-7.8771 policy updated! \n",
      "train step 04801 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5089 diff={max=08.2142, min=00.0179, mean=01.2362} policy_loss=-7.2869 policy updated! \n",
      "train step 04802 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2703 diff={max=04.2634, min=00.0118, mean=00.7223} policy_loss=-7.1506 policy updated! \n",
      "train step 04803 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9054 diff={max=08.3251, min=00.0518, mean=01.0818} policy_loss=-9.4003 policy updated! \n",
      "train step 04804 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9741 diff={max=05.9699, min=00.0033, mean=00.9300} policy_loss=-8.8626 policy updated! \n",
      "train step 04805 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.2200 diff={max=05.7651, min=00.0013, mean=00.9732} policy_loss=-7.7194 policy updated! \n",
      "train step 04806 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2315 diff={max=07.2217, min=00.0078, mean=01.1643} policy_loss=-8.1033 policy updated! \n",
      "train step 04807 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4207 diff={max=12.7828, min=00.0117, mean=01.3041} policy_loss=-7.4777 policy updated! \n",
      "train step 04808 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8157 diff={max=06.4978, min=00.0153, mean=00.8347} policy_loss=-7.7279 policy updated! \n",
      "train step 04809 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2478 diff={max=04.9399, min=00.0006, mean=01.2281} policy_loss=-7.7093 policy updated! \n",
      "train step 04810 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.2777 diff={max=03.8415, min=00.0199, mean=00.7537} policy_loss=-8.0480 policy updated! \n",
      "train step 04811 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3703 diff={max=07.2492, min=00.0056, mean=01.2103} policy_loss=-8.9750 policy updated! \n",
      "train step 04812 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9828 diff={max=04.8885, min=00.0014, mean=00.9058} policy_loss=-7.6370 policy updated! \n",
      "train step 04813 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7863 diff={max=04.2565, min=00.0582, mean=01.0091} policy_loss=-7.5045 policy updated! \n",
      "train step 04814 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=04.8441 diff={max=09.8796, min=00.0964, mean=01.3510} policy_loss=-9.1575 policy updated! \n",
      "train step 04815 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.0011 diff={max=08.4185, min=00.0062, mean=01.0392} policy_loss=-8.6710 policy updated! \n",
      "train step 04816 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2498 diff={max=06.3451, min=00.0421, mean=01.1854} policy_loss=-9.7427 policy updated! \n",
      "train step 04817 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1280 diff={max=04.2630, min=00.0315, mean=01.0964} policy_loss=-8.9848 policy updated! \n",
      "train step 04818 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.2463 diff={max=09.3985, min=00.0001, mean=01.7269} policy_loss=-9.7383 policy updated! \n",
      "train step 04819 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=02.5245 diff={max=05.3230, min=00.0181, mean=01.1177} policy_loss=-8.1885 policy updated! \n",
      "train step 04820 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.0040 diff={max=07.3591, min=00.0112, mean=01.0206} policy_loss=-7.4895 policy updated! \n",
      "train step 04821 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3593 diff={max=11.5322, min=00.0117, mean=01.3682} policy_loss=-7.8524 policy updated! \n",
      "train step 04822 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6021 diff={max=06.0757, min=00.0066, mean=01.2933} policy_loss=-7.8122 policy updated! \n",
      "train step 04823 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3436 diff={max=06.5463, min=00.0606, mean=01.2122} policy_loss=-7.6083 policy updated! \n",
      "train step 04824 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8274 diff={max=09.5465, min=00.0007, mean=01.3569} policy_loss=-8.2417 policy updated! \n",
      "train step 04825 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.6164 diff={max=07.9098, min=00.0032, mean=01.3401} policy_loss=-7.7739 policy updated! \n",
      "train step 04826 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0801 diff={max=05.9868, min=00.0120, mean=01.1788} policy_loss=-7.8810 policy updated! \n",
      "train step 04827 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7606 diff={max=04.4001, min=00.0090, mean=00.9743} policy_loss=-8.1622 policy updated! \n",
      "train step 04828 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4234 diff={max=07.2101, min=00.0097, mean=01.1795} policy_loss=-10.3624 policy updated! \n",
      "train step 04829 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4417 diff={max=05.8934, min=00.0219, mean=01.2683} policy_loss=-9.1152 policy updated! \n",
      "train step 04830 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=06.3536 diff={max=07.8008, min=00.0009, mean=01.4861} policy_loss=-8.4280 policy updated! \n",
      "train step 04831 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5486 diff={max=11.2498, min=00.0150, mean=01.3197} policy_loss=-7.9652 policy updated! \n",
      "train step 04832 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6685 diff={max=06.4560, min=00.0184, mean=01.2766} policy_loss=-7.8342 policy updated! \n",
      "train step 04833 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8810 diff={max=10.3350, min=00.0010, mean=01.4388} policy_loss=-7.8480 policy updated! \n",
      "train step 04834 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1359 diff={max=05.3872, min=00.0253, mean=01.2644} policy_loss=-7.2226 policy updated! \n",
      "train step 04835 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.7716 diff={max=07.0507, min=00.0481, mean=01.3281} policy_loss=-6.9949 policy updated! \n",
      "train step 04836 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2249 diff={max=06.3320, min=00.0140, mean=01.2259} policy_loss=-8.4156 policy updated! \n",
      "train step 04837 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2202 diff={max=08.7122, min=00.0048, mean=01.0233} policy_loss=-8.7687 policy updated! \n",
      "train step 04838 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5168 diff={max=06.2274, min=00.0147, mean=01.0950} policy_loss=-7.2614 policy updated! \n",
      "train step 04839 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9296 diff={max=06.7589, min=00.0046, mean=01.1256} policy_loss=-8.8101 policy updated! \n",
      "train step 04840 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.2790 diff={max=06.7147, min=00.0037, mean=01.0973} policy_loss=-6.9152 policy updated! \n",
      "train step 04841 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4208 diff={max=05.2901, min=00.0284, mean=01.0454} policy_loss=-8.3213 policy updated! \n",
      "train step 04842 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7518 diff={max=04.2491, min=00.0021, mean=01.0008} policy_loss=-8.9503 policy updated! \n",
      "train step 04843 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0974 diff={max=04.9228, min=00.0153, mean=00.9532} policy_loss=-9.6561 policy updated! \n",
      "train step 04844 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4688 diff={max=04.4464, min=00.0410, mean=00.8043} policy_loss=-8.1378 policy updated! \n",
      "train step 04845 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.7726 diff={max=08.6512, min=00.0490, mean=01.2373} policy_loss=-6.2585 policy updated! \n",
      "train step 04846 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7239 diff={max=05.2007, min=00.0041, mean=00.8744} policy_loss=-9.0017 policy updated! \n",
      "train step 04847 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0949 diff={max=06.5181, min=00.0012, mean=01.1611} policy_loss=-8.3045 policy updated! \n",
      "train step 04848 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7435 diff={max=05.9585, min=00.0227, mean=01.1431} policy_loss=-8.5130 policy updated! \n",
      "train step 04849 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1803 diff={max=04.5042, min=00.0042, mean=01.0040} policy_loss=-8.0911 policy updated! \n",
      "train step 04850 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.0153 diff={max=04.8722, min=00.0084, mean=01.0138} policy_loss=-7.8610 policy updated! \n",
      "train step 04851 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8246 diff={max=08.8789, min=00.0160, mean=01.4786} policy_loss=-8.7837 policy updated! \n",
      "train step 04852 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2466 diff={max=03.6936, min=00.0063, mean=00.7435} policy_loss=-7.6948 policy updated! \n",
      "train step 04853 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5705 diff={max=06.9406, min=00.0423, mean=01.1548} policy_loss=-9.1973 policy updated! \n",
      "train step 04854 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5424 diff={max=08.4172, min=00.0147, mean=00.9228} policy_loss=-7.8954 policy updated! \n",
      "train step 04855 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=04.4546 diff={max=08.9989, min=00.0130, mean=01.1797} policy_loss=-7.6928 policy updated! \n",
      "train step 04856 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7602 diff={max=07.7514, min=00.0164, mean=00.8912} policy_loss=-8.1910 policy updated! \n",
      "train step 04857 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7931 diff={max=04.7624, min=00.0332, mean=01.1145} policy_loss=-8.9481 policy updated! \n",
      "train step 04858 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6745 diff={max=07.4086, min=00.0004, mean=01.1124} policy_loss=-8.5321 policy updated! \n",
      "train step 04859 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3182 diff={max=05.1707, min=00.0027, mean=00.9738} policy_loss=-6.9831 policy updated! \n",
      "train step 04860 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=03.2261 diff={max=05.9628, min=00.0102, mean=01.1195} policy_loss=-9.0860 policy updated! \n",
      "train step 04861 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2065 diff={max=03.2725, min=00.0071, mean=00.8025} policy_loss=-9.3864 policy updated! \n",
      "train step 04862 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1237 diff={max=05.7331, min=00.0046, mean=01.0716} policy_loss=-8.8733 policy updated! \n",
      "train step 04863 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1471 diff={max=07.0713, min=00.0085, mean=01.0649} policy_loss=-8.4101 policy updated! \n",
      "train step 04864 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2897 diff={max=07.9487, min=00.0298, mean=01.3365} policy_loss=-8.6627 policy updated! \n",
      "train step 04865 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=01.5243 diff={max=05.4254, min=00.0028, mean=00.8010} policy_loss=-6.8851 policy updated! \n",
      "train step 04866 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8290 diff={max=09.2962, min=00.0139, mean=01.1227} policy_loss=-8.2349 policy updated! \n",
      "train step 04867 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7759 diff={max=08.1196, min=00.0305, mean=01.1537} policy_loss=-8.9817 policy updated! \n",
      "train step 04868 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8744 diff={max=06.1443, min=00.0427, mean=01.1372} policy_loss=-7.5961 policy updated! \n",
      "train step 04869 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6736 diff={max=06.7372, min=00.0035, mean=00.7709} policy_loss=-9.2852 policy updated! \n",
      "train step 04870 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=02.2814 diff={max=06.0726, min=00.0227, mean=00.9536} policy_loss=-9.7668 policy updated! \n",
      "train step 04871 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2599 diff={max=10.1799, min=00.0097, mean=01.3029} policy_loss=-10.4971 policy updated! \n",
      "train step 04872 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.2346 diff={max=09.4006, min=00.0213, mean=01.0222} policy_loss=-7.9768 policy updated! \n",
      "train step 04873 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3414 diff={max=07.0768, min=00.0020, mean=01.2509} policy_loss=-8.3931 policy updated! \n",
      "train step 04874 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9626 diff={max=07.4878, min=00.0141, mean=00.7732} policy_loss=-8.0930 policy updated! \n",
      "train step 04875 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=02.5010 diff={max=06.5436, min=00.0218, mean=01.0392} policy_loss=-9.2612 policy updated! \n",
      "train step 04876 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7009 diff={max=09.0763, min=00.0177, mean=01.0909} policy_loss=-8.7405 policy updated! \n",
      "train step 04877 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4826 diff={max=06.3084, min=00.0092, mean=00.7944} policy_loss=-7.5406 policy updated! \n",
      "train step 04878 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4942 diff={max=07.2667, min=00.0053, mean=01.1365} policy_loss=-8.6801 policy updated! \n",
      "train step 04879 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6864 diff={max=06.3057, min=00.0033, mean=01.2165} policy_loss=-8.8285 policy updated! \n",
      "train step 04880 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=01.5196 diff={max=03.6002, min=00.0071, mean=00.7837} policy_loss=-7.9025 policy updated! \n",
      "train step 04881 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1047 diff={max=07.3847, min=00.0065, mean=00.8040} policy_loss=-7.8873 policy updated! \n",
      "train step 04882 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5873 diff={max=07.7068, min=00.0192, mean=01.1360} policy_loss=-7.0540 policy updated! \n",
      "train step 04883 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7290 diff={max=08.3030, min=00.0519, mean=01.2445} policy_loss=-8.5011 policy updated! \n",
      "train step 04884 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0484 diff={max=05.7454, min=00.0062, mean=01.1056} policy_loss=-7.8487 policy updated! \n",
      "train step 04885 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=02.9064 diff={max=06.3607, min=00.0065, mean=00.9568} policy_loss=-6.9609 policy updated! \n",
      "train step 04886 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1732 diff={max=02.9040, min=00.0249, mean=00.7435} policy_loss=-8.2553 policy updated! \n",
      "train step 04887 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9561 diff={max=04.5621, min=00.0103, mean=00.9845} policy_loss=-8.4555 policy updated! \n",
      "train step 04888 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3544 diff={max=06.4029, min=00.0277, mean=00.9355} policy_loss=-5.9616 policy updated! \n",
      "train step 04889 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8134 diff={max=05.5519, min=00.0073, mean=00.8710} policy_loss=-7.2231 policy updated! \n",
      "train step 04890 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=03.8694 diff={max=07.6048, min=00.0071, mean=01.1574} policy_loss=-8.9018 policy updated! \n",
      "train step 04891 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2033 diff={max=07.0721, min=00.0102, mean=01.0947} policy_loss=-8.8352 policy updated! \n",
      "train step 04892 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1960 diff={max=04.6466, min=00.0110, mean=01.2401} policy_loss=-8.7189 policy updated! \n",
      "train step 04893 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3258 diff={max=07.4438, min=00.0021, mean=01.3639} policy_loss=-8.3642 policy updated! \n",
      "train step 04894 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9054 diff={max=10.6718, min=00.0117, mean=01.1381} policy_loss=-8.0420 policy updated! \n",
      "train step 04895 reward={max=10.0000, min=09.0000, mean=09.4000} optimizing loss=01.8828 diff={max=04.7578, min=00.0007, mean=00.9013} policy_loss=-8.9065 policy updated! \n",
      "train step 04896 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=02.4090 diff={max=07.8755, min=00.0049, mean=00.8162} policy_loss=-6.8248 policy updated! \n",
      "train step 04897 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3780 diff={max=07.0706, min=00.0074, mean=01.4116} policy_loss=-8.7146 policy updated! \n",
      "train step 04898 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8157 diff={max=10.1045, min=00.0201, mean=01.2117} policy_loss=-8.5660 policy updated! \n",
      "train step 04899 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6947 diff={max=09.2294, min=00.0045, mean=01.0930} policy_loss=-8.6622 policy updated! \n",
      "train step 04900 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.3390 diff={max=08.0734, min=00.0691, mean=01.1260} policy_loss=-8.6860 policy updated! \n",
      "train step 04901 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4431 diff={max=05.9922, min=00.0096, mean=01.2096} policy_loss=-8.1185 policy updated! \n",
      "train step 04902 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8076 diff={max=05.6816, min=00.0048, mean=01.0938} policy_loss=-8.3834 policy updated! \n",
      "train step 04903 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7656 diff={max=07.1791, min=00.0248, mean=01.1420} policy_loss=-8.0884 policy updated! \n",
      "train step 04904 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2047 diff={max=07.6696, min=00.0063, mean=01.2108} policy_loss=-7.9583 policy updated! \n",
      "train step 04905 reward={max=10.0000, min=00.0000, mean=03.8000} optimizing loss=01.4599 diff={max=03.3394, min=00.0198, mean=00.9454} policy_loss=-8.6033 policy updated! \n",
      "train step 04906 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8622 diff={max=06.6337, min=00.0024, mean=01.1103} policy_loss=-7.4511 policy updated! \n",
      "train step 04907 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6818 diff={max=08.3141, min=00.0118, mean=01.0322} policy_loss=-7.4422 policy updated! \n",
      "train step 04908 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1302 diff={max=10.6486, min=00.0096, mean=01.0963} policy_loss=-8.3911 policy updated! \n",
      "train step 04909 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4508 diff={max=07.9485, min=00.0008, mean=01.0516} policy_loss=-7.8470 policy updated! \n",
      "train step 04910 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=05.1900 diff={max=11.7891, min=00.0596, mean=01.2951} policy_loss=-8.5615 policy updated! \n",
      "train step 04911 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3854 diff={max=06.4084, min=00.0486, mean=01.4075} policy_loss=-7.7568 policy updated! \n",
      "train step 04912 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9043 diff={max=06.4296, min=00.0011, mean=01.0827} policy_loss=-8.6733 policy updated! \n",
      "train step 04913 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0693 diff={max=05.1861, min=00.0262, mean=01.2272} policy_loss=-8.0106 policy updated! \n",
      "train step 04914 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5227 diff={max=04.1337, min=00.0051, mean=00.8820} policy_loss=-6.7692 policy updated! \n",
      "train step 04915 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.3663 diff={max=06.5996, min=00.0077, mean=01.2099} policy_loss=-8.5831 policy updated! \n",
      "train step 04916 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=02.8711 diff={max=04.9382, min=00.0130, mean=01.1470} policy_loss=-8.9943 policy updated! \n",
      "train step 04917 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6065 diff={max=09.1735, min=00.0055, mean=00.9778} policy_loss=-8.3379 policy updated! \n",
      "train step 04918 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6366 diff={max=05.1746, min=00.0338, mean=00.8078} policy_loss=-7.7315 policy updated! \n",
      "train step 04919 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0368 diff={max=07.2106, min=00.0103, mean=01.0764} policy_loss=-8.2398 policy updated! \n",
      "train step 04920 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.2658 diff={max=07.6308, min=00.0363, mean=01.1391} policy_loss=-8.4247 policy updated! \n",
      "train step 04921 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1499 diff={max=02.2687, min=00.0643, mean=00.8481} policy_loss=-10.1887 policy updated! \n",
      "train step 04922 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2893 diff={max=03.1753, min=00.0400, mean=00.9032} policy_loss=-7.9651 policy updated! \n",
      "train step 04923 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=03.4570 diff={max=06.7116, min=00.0008, mean=01.1085} policy_loss=-8.5827 policy updated! \n",
      "train step 04924 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1373 diff={max=08.4637, min=00.0199, mean=01.0382} policy_loss=-9.8053 policy updated! \n",
      "train step 04925 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.1459 diff={max=06.6892, min=00.0643, mean=00.9464} policy_loss=-7.8402 policy updated! \n",
      "train step 04926 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5735 diff={max=06.1632, min=00.0157, mean=01.1355} policy_loss=-7.8258 policy updated! \n",
      "train step 04927 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3214 diff={max=09.7173, min=00.0009, mean=01.2138} policy_loss=-8.4220 policy updated! \n",
      "train step 04928 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0622 diff={max=05.8497, min=00.0017, mean=01.0197} policy_loss=-6.8720 policy updated! \n",
      "train step 04929 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5296 diff={max=03.7602, min=00.0381, mean=00.8204} policy_loss=-7.0593 policy updated! \n",
      "train step 04930 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.5413 diff={max=11.5106, min=00.0286, mean=01.0446} policy_loss=-7.7008 policy updated! \n",
      "train step 04931 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1251 diff={max=08.3998, min=00.0164, mean=01.1902} policy_loss=-6.7567 policy updated! \n",
      "train step 04932 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1538 diff={max=05.0862, min=00.0041, mean=00.6739} policy_loss=-8.5999 policy updated! \n",
      "train step 04933 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1681 diff={max=05.9093, min=00.0132, mean=00.9662} policy_loss=-7.7659 policy updated! \n",
      "train step 04934 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6992 diff={max=08.7515, min=00.0310, mean=01.4303} policy_loss=-9.1491 policy updated! \n",
      "train step 04935 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.6193 diff={max=04.0860, min=00.0025, mean=00.8488} policy_loss=-7.7304 policy updated! \n",
      "train step 04936 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=03.5769 diff={max=05.9086, min=00.0289, mean=01.1976} policy_loss=-7.8598 policy updated! \n",
      "train step 04937 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5458 diff={max=05.3623, min=00.0076, mean=01.0698} policy_loss=-8.7531 policy updated! \n",
      "train step 04938 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7621 diff={max=05.2821, min=00.0068, mean=01.1115} policy_loss=-8.5795 policy updated! \n",
      "train step 04939 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9400 diff={max=06.0159, min=00.0043, mean=01.0942} policy_loss=-7.4475 policy updated! \n",
      "train step 04940 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.2732 diff={max=05.6576, min=00.0028, mean=00.6863} policy_loss=-8.4917 policy updated! \n",
      "train step 04941 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0426 diff={max=03.4401, min=00.0087, mean=00.7389} policy_loss=-8.2299 policy updated! \n",
      "train step 04942 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3745 diff={max=04.5672, min=00.0126, mean=00.7934} policy_loss=-8.9186 policy updated! \n",
      "train step 04943 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3017 diff={max=08.7653, min=00.0052, mean=00.9847} policy_loss=-8.1588 policy updated! \n",
      "train step 04944 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1062 diff={max=05.3843, min=00.0105, mean=00.9526} policy_loss=-7.0059 policy updated! \n",
      "train step 04945 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.6320 diff={max=04.7138, min=00.0358, mean=00.8693} policy_loss=-7.0588 policy updated! \n",
      "train step 04946 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2056 diff={max=03.5950, min=00.0257, mean=00.7871} policy_loss=-7.0134 policy updated! \n",
      "train step 04947 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2483 diff={max=08.3998, min=00.0042, mean=01.1758} policy_loss=-9.1020 policy updated! \n",
      "train step 04948 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=02.0708 diff={max=04.5281, min=00.0035, mean=01.0292} policy_loss=-9.6439 policy updated! \n",
      "train step 04949 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5464 diff={max=03.9242, min=00.0034, mean=00.8623} policy_loss=-8.2896 policy updated! \n",
      "train step 04950 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.6948 diff={max=07.7950, min=00.0179, mean=01.2123} policy_loss=-8.1014 policy updated! \n",
      "train step 04951 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8777 diff={max=06.4941, min=00.0154, mean=01.2255} policy_loss=-7.9426 policy updated! \n",
      "train step 04952 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8266 diff={max=05.5596, min=00.0043, mean=00.8748} policy_loss=-9.0852 policy updated! \n",
      "train step 04953 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=03.3739 diff={max=09.6005, min=00.0063, mean=01.0445} policy_loss=-8.0838 policy updated! \n",
      "train step 04954 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4760 diff={max=06.1298, min=00.0282, mean=00.9278} policy_loss=-8.2425 policy updated! \n",
      "train step 04955 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.5669 diff={max=05.0060, min=00.0161, mean=00.8364} policy_loss=-8.0674 policy updated! \n",
      "train step 04956 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6023 diff={max=07.0484, min=00.0013, mean=01.1550} policy_loss=-8.7771 policy updated! \n",
      "train step 04957 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6655 diff={max=07.0185, min=00.0362, mean=01.1979} policy_loss=-7.4586 policy updated! \n",
      "train step 04958 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1215 diff={max=07.9445, min=00.0052, mean=01.2926} policy_loss=-7.3013 policy updated! \n",
      "train step 04959 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7253 diff={max=06.3928, min=00.0320, mean=01.3117} policy_loss=-8.5198 policy updated! \n",
      "train step 04960 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.0900 diff={max=03.9948, min=00.0007, mean=01.0512} policy_loss=-6.9965 policy updated! \n",
      "train step 04961 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0135 diff={max=05.6090, min=00.0237, mean=01.0042} policy_loss=-7.6624 policy updated! \n",
      "train step 04962 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1228 diff={max=04.6762, min=00.0029, mean=01.1990} policy_loss=-8.2140 policy updated! \n",
      "train step 04963 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6150 diff={max=05.3656, min=00.0012, mean=01.0672} policy_loss=-8.3532 policy updated! \n",
      "train step 04964 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9462 diff={max=07.5690, min=00.0180, mean=00.9896} policy_loss=-6.9746 policy updated! \n",
      "train step 04965 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.7813 diff={max=03.5271, min=00.0314, mean=00.9402} policy_loss=-7.7629 policy updated! \n",
      "train step 04966 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3677 diff={max=06.3113, min=00.0243, mean=01.1612} policy_loss=-8.3181 policy updated! \n",
      "train step 04967 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7101 diff={max=03.6436, min=00.0187, mean=01.0072} policy_loss=-8.6576 policy updated! \n",
      "train step 04968 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6569 diff={max=06.8335, min=00.0352, mean=01.0719} policy_loss=-7.9802 policy updated! \n",
      "train step 04969 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=02.9000 diff={max=05.3127, min=00.0102, mean=01.2209} policy_loss=-8.3149 policy updated! \n",
      "train step 04970 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.1044 diff={max=08.7436, min=00.0135, mean=01.2388} policy_loss=-8.0171 policy updated! \n",
      "train step 04971 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6142 diff={max=10.9376, min=00.0046, mean=01.3382} policy_loss=-8.2660 policy updated! \n",
      "train step 04972 reward={max=09.0000, min=09.0000, mean=09.0000} optimizing loss=05.7698 diff={max=08.4102, min=00.0591, mean=01.5682} policy_loss=-9.1497 policy updated! \n",
      "train step 04973 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0283 diff={max=03.8942, min=00.0016, mean=01.0903} policy_loss=-8.7939 policy updated! \n",
      "train step 04974 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7723 diff={max=05.1199, min=00.0016, mean=00.8043} policy_loss=-8.2298 policy updated! \n",
      "train step 04975 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.1210 diff={max=05.5655, min=00.0111, mean=00.9916} policy_loss=-8.8186 policy updated! \n",
      "train step 04976 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4178 diff={max=07.3906, min=00.0110, mean=01.1057} policy_loss=-9.1238 policy updated! \n",
      "train step 04977 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6176 diff={max=04.1591, min=00.0199, mean=01.2200} policy_loss=-8.1371 policy updated! \n",
      "train step 04978 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=02.5602 diff={max=06.3909, min=00.0541, mean=01.1109} policy_loss=-8.1466 policy updated! \n",
      "train step 04979 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7301 diff={max=07.1320, min=00.0063, mean=00.9711} policy_loss=-6.6005 policy updated! \n",
      "train step 04980 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.9449 diff={max=04.7077, min=00.0004, mean=00.8717} policy_loss=-7.6100 policy updated! \n",
      "train step 04981 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1773 diff={max=06.7774, min=00.0008, mean=01.1446} policy_loss=-9.0816 policy updated! \n",
      "train step 04982 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0497 diff={max=10.8390, min=00.0275, mean=01.2020} policy_loss=-9.2798 policy updated! \n",
      "train step 04983 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6122 diff={max=04.3670, min=00.0101, mean=01.1004} policy_loss=-7.5525 policy updated! \n",
      "train step 04984 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8741 diff={max=06.4651, min=00.0894, mean=01.1168} policy_loss=-7.8890 policy updated! \n",
      "train step 04985 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=07.5458 diff={max=11.5653, min=00.0092, mean=01.6476} policy_loss=-7.3692 policy updated! \n",
      "train step 04986 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=03.0522 diff={max=07.2192, min=00.0155, mean=01.1318} policy_loss=-7.5370 policy updated! \n",
      "train step 04987 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1134 diff={max=06.5468, min=00.0261, mean=00.9166} policy_loss=-9.1081 policy updated! \n",
      "train step 04988 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=04.2357 diff={max=07.2031, min=00.0141, mean=01.2485} policy_loss=-7.5272 policy updated! \n",
      "train step 04989 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=03.7379 diff={max=08.4322, min=00.0815, mean=01.2728} policy_loss=-8.9182 policy updated! \n",
      "train step 04990 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.7135 diff={max=07.8119, min=00.0261, mean=01.1938} policy_loss=-8.3412 policy updated! \n",
      "train step 04991 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7746 diff={max=04.9874, min=00.0071, mean=00.9372} policy_loss=-8.2646 policy updated! \n",
      "train step 04992 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0660 diff={max=07.0038, min=00.0076, mean=01.1607} policy_loss=-8.8220 policy updated! \n",
      "train step 04993 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2315 diff={max=08.8994, min=00.0005, mean=01.2269} policy_loss=-8.1946 policy updated! \n",
      "train step 04994 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3580 diff={max=06.2454, min=00.0240, mean=01.0465} policy_loss=-7.7204 policy updated! \n",
      "train step 04995 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.1784 diff={max=05.4955, min=00.0249, mean=01.2073} policy_loss=-8.1471 policy updated! \n",
      "train step 04996 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1777 diff={max=04.7320, min=00.0276, mean=01.1497} policy_loss=-8.7120 policy updated! \n",
      "train step 04997 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9140 diff={max=05.4262, min=00.0171, mean=01.2065} policy_loss=-7.3524 policy updated! \n",
      "train step 04998 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4526 diff={max=09.0152, min=00.0077, mean=01.1888} policy_loss=-8.3819 policy updated! \n",
      "train step 04999 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3501 diff={max=05.5506, min=00.0038, mean=00.9583} policy_loss=-7.9895 policy updated! \n",
      "train step 05000 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.5793 diff={max=05.0836, min=00.0309, mean=00.8309} policy_loss=-8.6313 policy updated! \n",
      "train step 05001 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1954 diff={max=09.1014, min=00.0312, mean=01.5244} policy_loss=-8.5391 policy updated! \n",
      "train step 05002 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5284 diff={max=03.3363, min=00.0051, mean=00.9674} policy_loss=-8.8669 policy updated! \n",
      "train step 05003 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5650 diff={max=08.0264, min=00.0446, mean=01.4736} policy_loss=-9.1584 policy updated! \n",
      "train step 05004 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2494 diff={max=08.9096, min=00.0040, mean=01.3201} policy_loss=-8.5111 policy updated! \n",
      "train step 05005 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.1530 diff={max=05.4450, min=00.0127, mean=01.2196} policy_loss=-7.2942 policy updated! \n",
      "train step 05006 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=01.5694 diff={max=04.4531, min=00.0531, mean=00.8292} policy_loss=-7.8351 policy updated! \n",
      "train step 05007 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6046 diff={max=05.8308, min=00.0039, mean=01.2125} policy_loss=-8.5113 policy updated! \n",
      "train step 05008 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5460 diff={max=07.3680, min=00.0256, mean=01.2705} policy_loss=-7.9898 policy updated! \n",
      "train step 05009 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1214 diff={max=04.8489, min=00.0262, mean=00.9974} policy_loss=-7.6389 policy updated! \n",
      "train step 05010 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.6924 diff={max=05.7838, min=00.0171, mean=00.8252} policy_loss=-9.5425 policy updated! \n",
      "train step 05011 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4755 diff={max=05.3036, min=00.0101, mean=00.9391} policy_loss=-7.6062 policy updated! \n",
      "train step 05012 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8881 diff={max=09.5314, min=00.0057, mean=00.9076} policy_loss=-9.1648 policy updated! \n",
      "train step 05013 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2591 diff={max=05.5452, min=00.0052, mean=00.9742} policy_loss=-8.9518 policy updated! \n",
      "train step 05014 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9526 diff={max=07.8775, min=00.0168, mean=00.7572} policy_loss=-9.1742 policy updated! \n",
      "train step 05015 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.7381 diff={max=04.8388, min=00.0327, mean=00.9173} policy_loss=-8.0940 policy updated! \n",
      "train step 05016 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7011 diff={max=03.9281, min=00.0173, mean=00.9808} policy_loss=-9.1560 policy updated! \n",
      "train step 05017 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9746 diff={max=03.5023, min=00.0252, mean=00.7058} policy_loss=-9.5471 policy updated! \n",
      "train step 05018 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7046 diff={max=05.3889, min=00.0004, mean=00.8173} policy_loss=-8.4753 policy updated! \n",
      "train step 05019 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5479 diff={max=09.1970, min=00.0320, mean=01.1947} policy_loss=-8.9133 policy updated! \n",
      "train step 05020 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.4348 diff={max=05.9836, min=00.0015, mean=00.9747} policy_loss=-7.7085 policy updated! \n",
      "train step 05021 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5351 diff={max=07.2843, min=00.0084, mean=01.2824} policy_loss=-10.5200 policy updated! \n",
      "train step 05022 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6264 diff={max=06.7735, min=00.0197, mean=00.9792} policy_loss=-6.4031 policy updated! \n",
      "train step 05023 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2221 diff={max=08.9400, min=00.0303, mean=01.1372} policy_loss=-8.1384 policy updated! \n",
      "train step 05024 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0705 diff={max=06.2005, min=00.0184, mean=01.1099} policy_loss=-8.6872 policy updated! \n",
      "train step 05025 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.8179 diff={max=08.2819, min=00.0057, mean=01.3750} policy_loss=-7.9423 policy updated! \n",
      "train step 05026 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5647 diff={max=08.3996, min=00.0150, mean=00.9470} policy_loss=-7.5183 policy updated! \n",
      "train step 05027 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1326 diff={max=05.3347, min=00.0094, mean=00.6737} policy_loss=-7.0635 policy updated! \n",
      "train step 05028 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0942 diff={max=10.0627, min=00.0039, mean=01.0326} policy_loss=-7.9510 policy updated! \n",
      "train step 05029 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6421 diff={max=05.3906, min=00.0269, mean=00.8856} policy_loss=-8.8916 policy updated! \n",
      "train step 05030 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.8623 diff={max=05.1693, min=00.0203, mean=01.1305} policy_loss=-7.4383 policy updated! \n",
      "train step 05031 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0178 diff={max=08.5237, min=00.0284, mean=01.1608} policy_loss=-6.9624 policy updated! \n",
      "train step 05032 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4546 diff={max=03.9502, min=00.0309, mean=00.8853} policy_loss=-7.8287 policy updated! \n",
      "train step 05033 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9381 diff={max=06.4871, min=00.0069, mean=01.0848} policy_loss=-8.8239 policy updated! \n",
      "train step 05034 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6716 diff={max=09.3757, min=00.0154, mean=01.0387} policy_loss=-9.0808 policy updated! \n",
      "train step 05035 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.0006 diff={max=06.7975, min=00.0366, mean=01.1808} policy_loss=-8.9229 policy updated! \n",
      "train step 05036 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8971 diff={max=07.2095, min=00.0050, mean=01.0256} policy_loss=-8.6579 policy updated! \n",
      "train step 05037 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5789 diff={max=13.5843, min=00.0095, mean=01.1618} policy_loss=-8.4112 policy updated! \n",
      "train step 05038 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0711 diff={max=06.2124, min=00.0068, mean=00.9277} policy_loss=-7.4056 policy updated! \n",
      "train step 05039 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6194 diff={max=07.2527, min=00.0061, mean=01.2534} policy_loss=-7.4160 policy updated! \n",
      "train step 05040 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=03.9141 diff={max=07.0705, min=00.0086, mean=01.2205} policy_loss=-8.7734 policy updated! \n",
      "train step 05041 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3029 diff={max=05.2856, min=00.0028, mean=00.9969} policy_loss=-8.1659 policy updated! \n",
      "train step 05042 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2297 diff={max=08.5675, min=00.0285, mean=01.0398} policy_loss=-9.6759 policy updated! \n",
      "train step 05043 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4375 diff={max=08.7516, min=00.0076, mean=01.0992} policy_loss=-7.7674 policy updated! \n",
      "train step 05044 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8692 diff={max=04.8232, min=00.0109, mean=01.1424} policy_loss=-8.0185 policy updated! \n",
      "train step 05045 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=03.2026 diff={max=06.4995, min=00.0209, mean=01.1175} policy_loss=-9.0862 policy updated! \n",
      "train step 05046 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4421 diff={max=03.1421, min=00.0081, mean=00.9101} policy_loss=-8.0207 policy updated! \n",
      "train step 05047 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7133 diff={max=04.0524, min=00.0222, mean=00.9182} policy_loss=-8.5120 policy updated! \n",
      "train step 05048 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9222 diff={max=09.4252, min=00.0350, mean=01.0589} policy_loss=-8.2624 policy updated! \n",
      "train step 05049 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9351 diff={max=06.5805, min=00.0037, mean=00.8059} policy_loss=-8.4769 policy updated! \n",
      "train step 05050 reward={max=10.0000, min=09.0000, mean=09.8000} optimizing loss=01.7120 diff={max=05.8926, min=00.0334, mean=00.8607} policy_loss=-8.9674 policy updated! \n",
      "train step 05051 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9817 diff={max=08.6596, min=00.0023, mean=01.0193} policy_loss=-8.5293 policy updated! \n",
      "train step 05052 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8185 diff={max=06.3800, min=00.0219, mean=00.8405} policy_loss=-8.6271 policy updated! \n",
      "train step 05053 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.1161 diff={max=08.8174, min=00.0192, mean=01.1800} policy_loss=-7.6747 policy updated! \n",
      "train step 05054 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9849 diff={max=04.8460, min=00.0169, mean=00.9780} policy_loss=-7.9096 policy updated! \n",
      "train step 05055 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.8541 diff={max=07.4282, min=00.0172, mean=00.9836} policy_loss=-9.3046 policy updated! \n",
      "train step 05056 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0238 diff={max=10.3481, min=00.0161, mean=01.3094} policy_loss=-7.6482 policy updated! \n",
      "train step 05057 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9035 diff={max=07.4987, min=00.0035, mean=01.3455} policy_loss=-9.4173 policy updated! \n",
      "train step 05058 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6220 diff={max=05.8480, min=00.0056, mean=01.1282} policy_loss=-8.0924 policy updated! \n",
      "train step 05059 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.5969 diff={max=04.0749, min=00.0126, mean=00.8767} policy_loss=-7.5200 policy updated! \n",
      "train step 05060 reward={max=10.0000, min=00.0000, mean=03.8000} optimizing loss=03.7990 diff={max=07.0226, min=00.0156, mean=01.1331} policy_loss=-9.2637 policy updated! \n",
      "train step 05061 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=01.6858 diff={max=04.0573, min=00.0279, mean=00.9093} policy_loss=-8.3609 policy updated! \n",
      "train step 05062 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5625 diff={max=05.3100, min=00.0197, mean=01.0005} policy_loss=-7.6611 policy updated! \n",
      "train step 05063 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9586 diff={max=06.9786, min=00.0233, mean=01.3728} policy_loss=-8.2819 policy updated! \n",
      "train step 05064 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4846 diff={max=10.5674, min=00.0405, mean=01.2212} policy_loss=-9.0172 policy updated! \n",
      "train step 05065 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=03.5937 diff={max=06.6411, min=00.0167, mean=01.2840} policy_loss=-9.2983 policy updated! \n",
      "train step 05066 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3146 diff={max=07.1314, min=00.0105, mean=01.1945} policy_loss=-7.7580 policy updated! \n",
      "train step 05067 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2581 diff={max=07.1195, min=00.0304, mean=01.3205} policy_loss=-8.8517 policy updated! \n",
      "train step 05068 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1109 diff={max=05.2875, min=00.0148, mean=01.1967} policy_loss=-7.1478 policy updated! \n",
      "train step 05069 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6031 diff={max=05.0207, min=00.0076, mean=00.7762} policy_loss=-7.6957 policy updated! \n",
      "train step 05070 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=04.5237 diff={max=10.3309, min=00.0248, mean=01.1862} policy_loss=-8.7950 policy updated! \n",
      "train step 05071 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4475 diff={max=06.9357, min=00.0024, mean=01.3459} policy_loss=-8.7425 policy updated! \n",
      "train step 05072 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5318 diff={max=04.7757, min=00.0088, mean=00.9018} policy_loss=-7.3920 policy updated! \n",
      "train step 05073 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9270 diff={max=05.6556, min=00.0055, mean=01.3727} policy_loss=-8.5618 policy updated! \n",
      "train step 05074 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6790 diff={max=04.1217, min=00.0051, mean=00.9573} policy_loss=-8.2332 policy updated! \n",
      "train step 05075 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=04.4295 diff={max=07.8127, min=00.0127, mean=01.3140} policy_loss=-8.3287 policy updated! \n",
      "train step 05076 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0619 diff={max=05.8468, min=00.0008, mean=01.1070} policy_loss=-8.1522 policy updated! \n",
      "train step 05077 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4270 diff={max=07.3302, min=00.0133, mean=01.1934} policy_loss=-8.3541 policy updated! \n",
      "train step 05078 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5708 diff={max=06.5036, min=00.0426, mean=01.0975} policy_loss=-8.9370 policy updated! \n",
      "train step 05079 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0546 diff={max=07.1049, min=00.0012, mean=01.3200} policy_loss=-7.4390 policy updated! \n",
      "train step 05080 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=02.6757 diff={max=04.6426, min=00.0020, mean=01.2001} policy_loss=-9.0323 policy updated! \n",
      "train step 05081 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5230 diff={max=04.0452, min=00.0042, mean=00.8216} policy_loss=-7.3593 policy updated! \n",
      "train step 05082 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0236 diff={max=08.9777, min=00.1025, mean=01.2900} policy_loss=-9.3139 policy updated! \n",
      "train step 05083 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4992 diff={max=07.2974, min=00.0206, mean=01.0157} policy_loss=-8.0016 policy updated! \n",
      "train step 05084 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8435 diff={max=08.9607, min=00.0311, mean=01.4987} policy_loss=-8.7332 policy updated! \n",
      "train step 05085 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=03.7223 diff={max=08.3278, min=00.0157, mean=01.1234} policy_loss=-8.4936 policy updated! \n",
      "train step 05086 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7650 diff={max=08.3717, min=00.0186, mean=01.2757} policy_loss=-7.5978 policy updated! \n",
      "train step 05087 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0422 diff={max=11.3482, min=00.0412, mean=01.4253} policy_loss=-9.6658 policy updated! \n",
      "train step 05088 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0078 diff={max=04.8268, min=00.0295, mean=01.2297} policy_loss=-6.7880 policy updated! \n",
      "train step 05089 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7746 diff={max=05.4060, min=00.0043, mean=01.1560} policy_loss=-8.5879 policy updated! \n",
      "train step 05090 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=05.3033 diff={max=08.0259, min=00.0810, mean=01.3696} policy_loss=-8.7350 policy updated! \n",
      "train step 05091 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4369 diff={max=06.3191, min=00.0150, mean=00.9852} policy_loss=-8.0536 policy updated! \n",
      "train step 05092 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4658 diff={max=08.5867, min=00.0022, mean=01.1961} policy_loss=-9.3963 policy updated! \n",
      "train step 05093 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1081 diff={max=04.0794, min=00.0020, mean=00.6750} policy_loss=-7.1296 policy updated! \n",
      "train step 05094 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0040 diff={max=04.2155, min=00.0007, mean=01.0350} policy_loss=-8.9573 policy updated! \n",
      "train step 05095 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=06.3024 diff={max=07.9121, min=00.0244, mean=01.6510} policy_loss=-9.8332 policy updated! \n",
      "train step 05096 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0403 diff={max=08.7242, min=00.0437, mean=00.9916} policy_loss=-10.2259 policy updated! \n",
      "train step 05097 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0536 diff={max=06.8179, min=00.0046, mean=01.2221} policy_loss=-7.3635 policy updated! \n",
      "train step 05098 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2234 diff={max=05.5144, min=00.0147, mean=00.9944} policy_loss=-9.1405 policy updated! \n",
      "train step 05099 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2244 diff={max=04.5018, min=00.0002, mean=01.0067} policy_loss=-8.1370 policy updated! \n",
      "train step 05100 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=04.4543 diff={max=08.1884, min=00.0235, mean=01.3582} policy_loss=-9.7669 policy updated! \n",
      "train step 05101 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7419 diff={max=08.1191, min=00.0062, mean=01.3441} policy_loss=-8.9649 policy updated! \n",
      "train step 05102 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7685 diff={max=06.0027, min=00.0361, mean=01.0562} policy_loss=-8.4639 policy updated! \n",
      "train step 05103 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6149 diff={max=06.4573, min=00.0302, mean=01.4878} policy_loss=-8.8516 policy updated! \n",
      "train step 05104 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1383 diff={max=08.1944, min=00.0187, mean=00.9473} policy_loss=-7.9939 policy updated! \n",
      "train step 05105 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=06.6570 diff={max=11.5438, min=00.0365, mean=01.5081} policy_loss=-8.5696 policy updated! \n",
      "train step 05106 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8790 diff={max=06.5819, min=00.0017, mean=00.8480} policy_loss=-7.4435 policy updated! \n",
      "train step 05107 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2517 diff={max=08.7049, min=00.0000, mean=01.0176} policy_loss=-9.6290 policy updated! \n",
      "train step 05108 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2585 diff={max=09.9276, min=00.0076, mean=01.2608} policy_loss=-9.1501 policy updated! \n",
      "train step 05109 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6165 diff={max=03.3512, min=00.0007, mean=00.9689} policy_loss=-9.1487 policy updated! \n",
      "train step 05110 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=04.0933 diff={max=07.3441, min=00.0335, mean=01.3097} policy_loss=-9.5012 policy updated! \n",
      "train step 05111 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4155 diff={max=08.0293, min=00.0195, mean=01.1109} policy_loss=-7.8270 policy updated! \n",
      "train step 05112 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2589 diff={max=05.0247, min=00.0056, mean=00.9666} policy_loss=-8.0080 policy updated! \n",
      "train step 05113 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6271 diff={max=04.9375, min=00.0045, mean=00.7935} policy_loss=-6.7727 policy updated! \n",
      "train step 05114 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3127 diff={max=05.9376, min=00.0004, mean=01.1606} policy_loss=-8.8839 policy updated! \n",
      "train step 05115 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=03.8068 diff={max=08.1486, min=00.0450, mean=01.2465} policy_loss=-8.0197 policy updated! \n",
      "train step 05116 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8788 diff={max=05.9097, min=00.0075, mean=00.9388} policy_loss=-7.0854 policy updated! \n",
      "train step 05117 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0402 diff={max=06.3605, min=00.0073, mean=01.0461} policy_loss=-9.7763 policy updated! \n",
      "train step 05118 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1690 diff={max=05.3010, min=00.0104, mean=01.0100} policy_loss=-9.4691 policy updated! \n",
      "train step 05119 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0469 diff={max=06.3902, min=00.0265, mean=01.2138} policy_loss=-9.3502 policy updated! \n",
      "train step 05120 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=02.7983 diff={max=07.8915, min=00.0006, mean=00.9670} policy_loss=-8.0216 policy updated! \n",
      "train step 05121 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5669 diff={max=04.3766, min=00.0009, mean=00.8551} policy_loss=-8.1690 policy updated! \n",
      "train step 05122 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2610 diff={max=03.4747, min=00.0197, mean=00.7641} policy_loss=-7.0238 policy updated! \n",
      "train step 05123 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9030 diff={max=07.6704, min=00.0165, mean=01.0427} policy_loss=-8.2802 policy updated! \n",
      "train step 05124 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3762 diff={max=09.3363, min=00.0089, mean=01.0687} policy_loss=-8.5870 policy updated! \n",
      "train step 05125 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=05.2173 diff={max=07.8748, min=00.0150, mean=01.4192} policy_loss=-8.5305 policy updated! \n",
      "train step 05126 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0900 diff={max=06.2661, min=00.0041, mean=01.0989} policy_loss=-7.9933 policy updated! \n",
      "train step 05127 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1168 diff={max=06.8918, min=00.0038, mean=01.3617} policy_loss=-8.7096 policy updated! \n",
      "train step 05128 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6468 diff={max=07.8145, min=00.0678, mean=01.5819} policy_loss=-8.5899 policy updated! \n",
      "train step 05129 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0586 diff={max=04.8794, min=00.0048, mean=01.2418} policy_loss=-9.6173 policy updated! \n",
      "train step 05130 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=02.1842 diff={max=04.7639, min=00.0112, mean=01.0218} policy_loss=-7.9321 policy updated! \n",
      "train step 05131 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7984 diff={max=07.1658, min=00.0073, mean=01.1397} policy_loss=-9.0202 policy updated! \n",
      "train step 05132 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3995 diff={max=08.2378, min=00.0100, mean=01.0374} policy_loss=-6.8021 policy updated! \n",
      "train step 05133 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4111 diff={max=04.2761, min=00.0273, mean=00.8692} policy_loss=-8.4982 policy updated! \n",
      "train step 05134 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1246 diff={max=07.6291, min=00.0492, mean=01.0542} policy_loss=-8.6633 policy updated! \n",
      "train step 05135 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=05.1773 diff={max=08.9984, min=00.0093, mean=01.2702} policy_loss=-7.0797 policy updated! \n",
      "train step 05136 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2589 diff={max=03.4432, min=00.0036, mean=00.8329} policy_loss=-9.7789 policy updated! \n",
      "train step 05137 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0275 diff={max=08.3154, min=00.0154, mean=01.3195} policy_loss=-8.0266 policy updated! \n",
      "train step 05138 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0486 diff={max=06.5178, min=00.0864, mean=01.0948} policy_loss=-8.3568 policy updated! \n",
      "train step 05139 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8898 diff={max=09.6118, min=00.0139, mean=01.0517} policy_loss=-8.7612 policy updated! \n",
      "train step 05140 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=04.4111 diff={max=07.9037, min=00.0163, mean=01.1775} policy_loss=-8.5183 policy updated! \n",
      "train step 05141 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6050 diff={max=04.0032, min=00.0175, mean=00.8985} policy_loss=-7.9805 policy updated! \n",
      "train step 05142 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1809 diff={max=07.8970, min=00.0172, mean=01.3542} policy_loss=-7.8351 policy updated! \n",
      "train step 05143 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3863 diff={max=06.3201, min=00.0367, mean=01.2058} policy_loss=-9.4365 policy updated! \n",
      "train step 05144 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0237 diff={max=07.0863, min=00.0027, mean=01.5019} policy_loss=-8.1187 policy updated! \n",
      "train step 05145 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=03.3118 diff={max=06.7882, min=00.0283, mean=01.1420} policy_loss=-7.4300 policy updated! \n",
      "train step 05146 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1571 diff={max=08.2807, min=00.0028, mean=00.8927} policy_loss=-8.5882 policy updated! \n",
      "train step 05147 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4252 diff={max=07.7019, min=00.0146, mean=00.9899} policy_loss=-9.0293 policy updated! \n",
      "train step 05148 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7475 diff={max=08.2902, min=00.0044, mean=01.0879} policy_loss=-8.4316 policy updated! \n",
      "train step 05149 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2210 diff={max=09.0649, min=00.0325, mean=01.1089} policy_loss=-8.1355 policy updated! \n",
      "train step 05150 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=02.0617 diff={max=05.0578, min=00.0248, mean=00.9440} policy_loss=-9.2092 policy updated! \n",
      "train step 05151 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2526 diff={max=05.1062, min=00.0468, mean=01.1878} policy_loss=-8.8327 policy updated! \n",
      "train step 05152 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5178 diff={max=06.5376, min=00.0479, mean=01.2333} policy_loss=-9.3945 policy updated! \n",
      "train step 05153 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8057 diff={max=05.0385, min=00.0102, mean=00.9747} policy_loss=-9.9096 policy updated! \n",
      "train step 05154 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8383 diff={max=07.6569, min=00.0217, mean=01.3500} policy_loss=-7.5921 policy updated! \n",
      "train step 05155 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=05.5492 diff={max=10.6292, min=00.0877, mean=01.2952} policy_loss=-9.3493 policy updated! \n",
      "train step 05156 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2526 diff={max=08.4329, min=00.0319, mean=01.3612} policy_loss=-8.6418 policy updated! \n",
      "train step 05157 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2297 diff={max=07.5381, min=00.0075, mean=01.1824} policy_loss=-7.5154 policy updated! \n",
      "train step 05158 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0103 diff={max=07.8557, min=00.0216, mean=00.9686} policy_loss=-7.6052 policy updated! \n",
      "train step 05159 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6330 diff={max=09.0120, min=00.0108, mean=01.2394} policy_loss=-7.6959 policy updated! \n",
      "train step 05160 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=02.8650 diff={max=06.6875, min=00.0247, mean=01.0651} policy_loss=-7.8393 policy updated! \n",
      "train step 05161 reward={max=10.0000, min=00.0000, mean=06.0000} optimizing loss=01.2538 diff={max=03.8970, min=00.0026, mean=00.7707} policy_loss=-8.1793 policy updated! \n",
      "train step 05162 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1794 diff={max=02.2959, min=00.0004, mean=00.8734} policy_loss=-10.1543 policy updated! \n",
      "train step 05163 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6669 diff={max=07.4932, min=00.0147, mean=01.2822} policy_loss=-8.4623 policy updated! \n",
      "train step 05164 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1001 diff={max=11.3930, min=00.0527, mean=01.1870} policy_loss=-8.8590 policy updated! \n",
      "train step 05165 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=04.0967 diff={max=06.3137, min=00.0152, mean=01.3819} policy_loss=-9.8199 policy updated! \n",
      "train step 05166 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0463 diff={max=06.1280, min=00.0096, mean=01.0108} policy_loss=-7.5335 policy updated! \n",
      "train step 05167 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0080 diff={max=04.5460, min=00.0094, mean=01.0092} policy_loss=-7.6918 policy updated! \n",
      "train step 05168 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.0600 diff={max=10.0546, min=00.0328, mean=01.6560} policy_loss=-7.4706 policy updated! \n",
      "train step 05169 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2919 diff={max=06.3623, min=00.0318, mean=00.9212} policy_loss=-8.6947 policy updated! \n",
      "train step 05170 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=02.1993 diff={max=05.3962, min=00.0180, mean=00.9112} policy_loss=-7.6600 policy updated! \n",
      "train step 05171 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3912 diff={max=09.5275, min=00.0049, mean=01.2444} policy_loss=-9.4602 policy updated! \n",
      "train step 05172 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9210 diff={max=06.8573, min=00.0148, mean=01.1350} policy_loss=-8.4650 policy updated! \n",
      "train step 05173 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6632 diff={max=07.3012, min=00.0185, mean=01.4447} policy_loss=-8.2928 policy updated! \n",
      "train step 05174 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9237 diff={max=02.9116, min=00.0004, mean=00.6564} policy_loss=-8.0046 policy updated! \n",
      "train step 05175 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=05.2636 diff={max=08.1357, min=00.0445, mean=01.4144} policy_loss=-9.1319 policy updated! \n",
      "train step 05176 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9768 diff={max=09.9158, min=00.0051, mean=01.1668} policy_loss=-7.7771 policy updated! \n",
      "train step 05177 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4773 diff={max=06.2072, min=00.0081, mean=01.0389} policy_loss=-7.7758 policy updated! \n",
      "train step 05178 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1303 diff={max=06.5673, min=00.0276, mean=00.8978} policy_loss=-8.7919 policy updated! \n",
      "train step 05179 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1840 diff={max=08.0138, min=00.0021, mean=01.2537} policy_loss=-8.7398 policy updated! \n",
      "train step 05180 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=01.5957 diff={max=03.8252, min=00.0539, mean=00.9647} policy_loss=-9.4634 policy updated! \n",
      "train step 05181 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6266 diff={max=06.3289, min=00.0349, mean=01.3283} policy_loss=-8.8257 policy updated! \n",
      "train step 05182 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0869 diff={max=08.5959, min=00.0008, mean=00.9978} policy_loss=-8.0624 policy updated! \n",
      "train step 05183 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4371 diff={max=08.9072, min=00.0020, mean=00.9830} policy_loss=-8.8607 policy updated! \n",
      "train step 05184 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2267 diff={max=07.6937, min=00.0073, mean=01.1700} policy_loss=-8.0389 policy updated! \n",
      "train step 05185 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=02.3287 diff={max=05.3467, min=00.0126, mean=00.9892} policy_loss=-9.2237 policy updated! \n",
      "train step 05186 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9426 diff={max=06.3594, min=00.0094, mean=01.0895} policy_loss=-8.5991 policy updated! \n",
      "train step 05187 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5718 diff={max=07.4748, min=00.0264, mean=01.0285} policy_loss=-8.2332 policy updated! \n",
      "train step 05188 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4223 diff={max=06.2873, min=00.0193, mean=01.1181} policy_loss=-8.4690 policy updated! \n",
      "train step 05189 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0269 diff={max=05.5363, min=00.0237, mean=01.0000} policy_loss=-8.7781 policy updated! \n",
      "train step 05190 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=02.5012 diff={max=05.2082, min=00.0175, mean=01.0306} policy_loss=-8.6189 policy updated! \n",
      "train step 05191 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0038 diff={max=05.3416, min=00.0108, mean=00.9527} policy_loss=-8.8752 policy updated! \n",
      "train step 05192 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3144 diff={max=10.8794, min=00.0311, mean=01.0231} policy_loss=-8.3638 policy updated! \n",
      "train step 05193 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1099 diff={max=07.3712, min=00.0032, mean=01.2317} policy_loss=-8.8288 policy updated! \n",
      "train step 05194 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0256 diff={max=04.7592, min=00.0192, mean=00.9783} policy_loss=-7.5887 policy updated! \n",
      "train step 05195 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=02.6405 diff={max=05.9778, min=00.0017, mean=01.0426} policy_loss=-8.4060 policy updated! \n",
      "train step 05196 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7795 diff={max=05.9883, min=00.0212, mean=01.0943} policy_loss=-8.0871 policy updated! \n",
      "train step 05197 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3604 diff={max=08.2409, min=00.0058, mean=01.2937} policy_loss=-8.8315 policy updated! \n",
      "train step 05198 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4778 diff={max=09.7877, min=00.0193, mean=01.0466} policy_loss=-8.6830 policy updated! \n",
      "train step 05199 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2241 diff={max=05.7791, min=00.0003, mean=01.2509} policy_loss=-9.4032 policy updated! \n",
      "train step 05200 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=03.2216 diff={max=07.7062, min=00.0048, mean=01.1580} policy_loss=-10.6935 policy updated! \n",
      "train step 05201 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7932 diff={max=06.5886, min=00.0026, mean=01.1374} policy_loss=-8.4868 policy updated! \n",
      "train step 05202 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1423 diff={max=07.4657, min=00.0316, mean=01.5142} policy_loss=-10.0453 policy updated! \n",
      "train step 05203 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1297 diff={max=05.0472, min=00.0091, mean=00.9547} policy_loss=-7.8355 policy updated! \n",
      "train step 05204 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0376 diff={max=14.0357, min=00.0142, mean=01.5047} policy_loss=-8.5029 policy updated! \n",
      "train step 05205 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=02.6171 diff={max=07.1683, min=00.0027, mean=01.0005} policy_loss=-8.6582 policy updated! \n",
      "train step 05206 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9909 diff={max=07.4182, min=00.0152, mean=01.1932} policy_loss=-8.2259 policy updated! \n",
      "train step 05207 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7440 diff={max=03.6932, min=00.0436, mean=00.9869} policy_loss=-8.5087 policy updated! \n",
      "train step 05208 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8438 diff={max=07.9006, min=00.0026, mean=01.2046} policy_loss=-8.5825 policy updated! \n",
      "train step 05209 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7571 diff={max=08.4457, min=00.0033, mean=01.5445} policy_loss=-8.6313 policy updated! \n",
      "train step 05210 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=02.1042 diff={max=04.8596, min=00.0081, mean=00.9891} policy_loss=-9.3236 policy updated! \n",
      "train step 05211 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4026 diff={max=08.6822, min=00.0187, mean=01.0846} policy_loss=-8.8912 policy updated! \n",
      "train step 05212 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7716 diff={max=05.5166, min=00.0101, mean=00.8501} policy_loss=-9.1300 policy updated! \n",
      "train step 05213 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6892 diff={max=08.4636, min=00.0045, mean=01.2420} policy_loss=-6.2111 policy updated! \n",
      "train step 05214 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6355 diff={max=08.4652, min=00.0267, mean=01.2709} policy_loss=-7.6270 policy updated! \n",
      "train step 05215 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=01.5195 diff={max=05.2175, min=00.0175, mean=00.7625} policy_loss=-6.5691 policy updated! \n",
      "train step 05216 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7520 diff={max=05.2254, min=00.0172, mean=01.1683} policy_loss=-7.6208 policy updated! \n",
      "train step 05217 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2281 diff={max=04.4113, min=00.0080, mean=01.0256} policy_loss=-7.9361 policy updated! \n",
      "train step 05218 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4393 diff={max=03.5154, min=00.0023, mean=00.8654} policy_loss=-8.3991 policy updated! \n",
      "train step 05219 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7211 diff={max=09.3146, min=00.0433, mean=01.1084} policy_loss=-9.4483 policy updated! \n",
      "train step 05220 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=05.9818 diff={max=09.0515, min=00.0154, mean=01.4952} policy_loss=-9.1770 policy updated! \n",
      "train step 05221 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2634 diff={max=09.2462, min=00.0263, mean=01.2952} policy_loss=-7.8881 policy updated! \n",
      "train step 05222 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9849 diff={max=05.0894, min=00.0115, mean=00.8753} policy_loss=-9.7285 policy updated! \n",
      "train step 05223 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5908 diff={max=07.2915, min=00.0192, mean=01.0018} policy_loss=-9.2091 policy updated! \n",
      "train step 05224 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2582 diff={max=07.1989, min=00.0031, mean=01.2398} policy_loss=-7.6271 policy updated! \n",
      "train step 05225 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=01.9463 diff={max=04.5768, min=00.0019, mean=00.9694} policy_loss=-7.8403 policy updated! \n",
      "train step 05226 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5846 diff={max=06.4471, min=00.0396, mean=01.0915} policy_loss=-8.5333 policy updated! \n",
      "train step 05227 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0770 diff={max=05.3508, min=00.0095, mean=00.9039} policy_loss=-7.4816 policy updated! \n",
      "train step 05228 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1812 diff={max=06.9287, min=00.0145, mean=01.1581} policy_loss=-7.2806 policy updated! \n",
      "train step 05229 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6005 diff={max=08.1156, min=00.0208, mean=01.6685} policy_loss=-8.6881 policy updated! \n",
      "train step 05230 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=02.3796 diff={max=05.0131, min=00.0377, mean=01.1377} policy_loss=-7.5374 policy updated! \n",
      "train step 05231 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6454 diff={max=08.2033, min=00.0095, mean=01.4329} policy_loss=-9.3934 policy updated! \n",
      "train step 05232 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8099 diff={max=11.3067, min=00.0252, mean=01.1275} policy_loss=-9.5362 policy updated! \n",
      "train step 05233 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7477 diff={max=05.6948, min=00.0081, mean=01.2823} policy_loss=-8.7481 policy updated! \n",
      "train step 05234 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8123 diff={max=04.5639, min=00.0102, mean=00.9528} policy_loss=-7.7973 policy updated! \n",
      "train step 05235 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=02.6327 diff={max=05.7036, min=00.0157, mean=01.0845} policy_loss=-8.2000 policy updated! \n",
      "train step 05236 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4018 diff={max=08.5518, min=00.0166, mean=01.0637} policy_loss=-8.8850 policy updated! \n",
      "train step 05237 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1079 diff={max=03.9204, min=00.0339, mean=00.6702} policy_loss=-6.8038 policy updated! \n",
      "train step 05238 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2381 diff={max=06.6914, min=00.0014, mean=01.0856} policy_loss=-8.5453 policy updated! \n",
      "train step 05239 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9291 diff={max=08.1419, min=00.0008, mean=00.9832} policy_loss=-8.1824 policy updated! \n",
      "train step 05240 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=04.5350 diff={max=07.0401, min=00.0005, mean=01.2897} policy_loss=-8.6664 policy updated! \n",
      "train step 05241 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3165 diff={max=05.6822, min=00.0032, mean=00.9536} policy_loss=-8.8645 policy updated! \n",
      "train step 05242 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.6736 diff={max=02.3902, min=00.0055, mean=00.6422} policy_loss=-8.5940 policy updated! \n",
      "train step 05243 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1646 diff={max=06.5185, min=00.0005, mean=01.3036} policy_loss=-7.3657 policy updated! \n",
      "train step 05244 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0493 diff={max=08.2604, min=00.0076, mean=01.0277} policy_loss=-7.9659 policy updated! \n",
      "train step 05245 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=06.4122 diff={max=08.8859, min=00.0425, mean=01.4416} policy_loss=-8.7089 policy updated! \n",
      "train step 05246 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5302 diff={max=07.8149, min=00.0017, mean=01.3269} policy_loss=-6.9408 policy updated! \n",
      "train step 05247 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8635 diff={max=10.5877, min=00.0310, mean=01.5420} policy_loss=-8.8781 policy updated! \n",
      "train step 05248 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8411 diff={max=05.6424, min=00.0422, mean=00.8936} policy_loss=-9.1467 policy updated! \n",
      "train step 05249 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1184 diff={max=06.2973, min=00.0484, mean=00.9703} policy_loss=-8.9953 policy updated! \n",
      "train step 05250 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=03.7471 diff={max=07.5094, min=00.0102, mean=01.0582} policy_loss=-8.6363 policy updated! \n",
      "train step 05251 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8151 diff={max=08.7588, min=00.0574, mean=01.4258} policy_loss=-9.5284 policy updated! \n",
      "train step 05252 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3515 diff={max=08.0454, min=00.0391, mean=01.2468} policy_loss=-8.6063 policy updated! \n",
      "train step 05253 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6419 diff={max=09.1723, min=00.0815, mean=01.5440} policy_loss=-7.3966 policy updated! \n",
      "train step 05254 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9777 diff={max=08.8659, min=00.0536, mean=01.4756} policy_loss=-10.7997 policy updated! \n",
      "train step 05255 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=02.5730 diff={max=06.4668, min=00.0375, mean=01.0650} policy_loss=-8.4357 policy updated! \n",
      "train step 05256 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4589 diff={max=05.4070, min=00.0137, mean=01.1579} policy_loss=-7.7023 policy updated! \n",
      "train step 05257 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7638 diff={max=07.9346, min=00.0926, mean=01.2131} policy_loss=-9.4211 policy updated! \n",
      "train step 05258 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4835 diff={max=06.2922, min=00.0068, mean=01.2788} policy_loss=-8.3934 policy updated! \n",
      "train step 05259 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5299 diff={max=05.3313, min=00.0141, mean=01.0163} policy_loss=-7.4442 policy updated! \n",
      "train step 05260 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=03.0367 diff={max=06.4578, min=00.0045, mean=01.0915} policy_loss=-9.5400 policy updated! \n",
      "train step 05261 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0850 diff={max=05.2656, min=00.0105, mean=01.2469} policy_loss=-9.1281 policy updated! \n",
      "train step 05262 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9945 diff={max=09.2317, min=00.0162, mean=01.3425} policy_loss=-8.9195 policy updated! \n",
      "train step 05263 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9648 diff={max=08.8672, min=00.0289, mean=01.2802} policy_loss=-9.3457 policy updated! \n",
      "train step 05264 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8359 diff={max=05.3936, min=00.0905, mean=01.2076} policy_loss=-8.7695 policy updated! \n",
      "train step 05265 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=01.4439 diff={max=03.9245, min=00.0296, mean=00.8503} policy_loss=-10.2705 policy updated! \n",
      "train step 05266 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8489 diff={max=06.0471, min=00.0018, mean=00.8880} policy_loss=-7.7620 policy updated! \n",
      "train step 05267 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4333 diff={max=05.3958, min=00.0288, mean=01.0781} policy_loss=-8.7010 policy updated! \n",
      "train step 05268 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4923 diff={max=06.6231, min=00.0352, mean=01.2010} policy_loss=-8.7681 policy updated! \n",
      "train step 05269 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8189 diff={max=10.4502, min=00.0708, mean=01.1369} policy_loss=-8.3603 policy updated! \n",
      "train step 05270 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=02.6886 diff={max=05.3395, min=00.0668, mean=01.0862} policy_loss=-8.7971 policy updated! \n",
      "train step 05271 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6011 diff={max=10.1524, min=00.0008, mean=01.1147} policy_loss=-7.5135 policy updated! \n",
      "train step 05272 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5130 diff={max=06.1434, min=00.0008, mean=01.0612} policy_loss=-9.5083 policy updated! \n",
      "train step 05273 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2345 diff={max=06.2237, min=00.0122, mean=01.3192} policy_loss=-7.5370 policy updated! \n",
      "train step 05274 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6426 diff={max=06.8699, min=00.0222, mean=00.9086} policy_loss=-8.7668 policy updated! \n",
      "train step 05275 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=02.5166 diff={max=05.5694, min=00.0247, mean=01.0037} policy_loss=-7.7557 policy updated! \n",
      "train step 05276 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7856 diff={max=05.9080, min=00.0070, mean=00.9557} policy_loss=-10.2516 policy updated! \n",
      "train step 05277 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2170 diff={max=07.0365, min=00.0646, mean=01.5631} policy_loss=-8.1366 policy updated! \n",
      "train step 05278 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9382 diff={max=08.5302, min=00.0238, mean=01.0638} policy_loss=-7.9212 policy updated! \n",
      "train step 05279 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8957 diff={max=08.3219, min=00.0716, mean=01.7602} policy_loss=-8.4132 policy updated! \n",
      "train step 05280 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=04.1808 diff={max=11.7208, min=00.0265, mean=00.9993} policy_loss=-8.4082 policy updated! \n",
      "train step 05281 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3475 diff={max=07.8502, min=00.0177, mean=01.0913} policy_loss=-7.6300 policy updated! \n",
      "train step 05282 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3364 diff={max=10.6069, min=00.0103, mean=01.3882} policy_loss=-7.9800 policy updated! \n",
      "train step 05283 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5612 diff={max=10.2038, min=00.0028, mean=01.1438} policy_loss=-9.8449 policy updated! \n",
      "train step 05284 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0510 diff={max=04.4768, min=00.0231, mean=01.0140} policy_loss=-8.9101 policy updated! \n",
      "train step 05285 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=00.8894 diff={max=03.2551, min=00.0100, mean=00.6997} policy_loss=-8.2511 policy updated! \n",
      "train step 05286 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5452 diff={max=05.3635, min=00.0270, mean=00.7634} policy_loss=-7.4249 policy updated! \n",
      "train step 05287 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8574 diff={max=06.2371, min=00.0107, mean=01.0222} policy_loss=-7.2143 policy updated! \n",
      "train step 05288 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4233 diff={max=06.4134, min=00.0137, mean=01.0330} policy_loss=-8.9580 policy updated! \n",
      "train step 05289 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7168 diff={max=09.3110, min=00.0000, mean=01.2771} policy_loss=-9.7067 policy updated! \n",
      "train step 05290 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=01.3775 diff={max=04.7192, min=00.0143, mean=00.7053} policy_loss=-8.5612 policy updated! \n",
      "train step 05291 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6013 diff={max=06.9216, min=00.0047, mean=01.2393} policy_loss=-9.0489 policy updated! \n",
      "train step 05292 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2670 diff={max=04.9519, min=00.0101, mean=01.1432} policy_loss=-9.3223 policy updated! \n",
      "train step 05293 reward={max=10.0000, min=00.0000, mean=04.0000} optimizing loss=01.3441 diff={max=03.6132, min=00.0087, mean=00.7915} policy_loss=-7.9543 policy updated! \n",
      "train step 05294 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5890 diff={max=05.9679, min=00.0069, mean=01.0379} policy_loss=-9.7065 policy updated! \n",
      "train step 05295 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=01.8281 diff={max=05.6609, min=00.0043, mean=00.8927} policy_loss=-7.6457 policy updated! \n",
      "train step 05296 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4593 diff={max=08.2098, min=00.0510, mean=01.1801} policy_loss=-7.9079 policy updated! \n",
      "train step 05297 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3982 diff={max=06.2059, min=00.0194, mean=01.0625} policy_loss=-8.2608 policy updated! \n",
      "train step 05298 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1879 diff={max=06.3130, min=00.0053, mean=01.2340} policy_loss=-8.4876 policy updated! \n",
      "train step 05299 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9897 diff={max=04.9574, min=00.0538, mean=00.9547} policy_loss=-8.1133 policy updated! \n",
      "train step 05300 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=02.6856 diff={max=06.4889, min=00.0141, mean=01.1315} policy_loss=-8.0229 policy updated! \n",
      "train step 05301 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0223 diff={max=09.1218, min=00.0164, mean=01.1900} policy_loss=-9.2713 policy updated! \n",
      "train step 05302 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6137 diff={max=04.3700, min=00.0056, mean=00.8551} policy_loss=-8.3716 policy updated! \n",
      "train step 05303 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7044 diff={max=09.9904, min=00.0357, mean=01.2983} policy_loss=-8.7148 policy updated! \n",
      "train step 05304 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3543 diff={max=05.5739, min=00.0019, mean=00.7956} policy_loss=-9.1525 policy updated! \n",
      "train step 05305 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=03.5535 diff={max=07.9460, min=00.0032, mean=01.0507} policy_loss=-7.5090 policy updated! \n",
      "train step 05306 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7363 diff={max=04.8811, min=00.0164, mean=00.9697} policy_loss=-9.1645 policy updated! \n",
      "train step 05307 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5656 diff={max=10.2001, min=00.0247, mean=01.3527} policy_loss=-8.5670 policy updated! \n",
      "train step 05308 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4463 diff={max=07.7392, min=00.0168, mean=01.3417} policy_loss=-7.1895 policy updated! \n",
      "train step 05309 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1582 diff={max=08.5154, min=00.0116, mean=01.3507} policy_loss=-8.2120 policy updated! \n",
      "train step 05310 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=05.1125 diff={max=09.2802, min=00.0489, mean=01.4053} policy_loss=-9.1905 policy updated! \n",
      "train step 05311 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0763 diff={max=08.3069, min=00.0188, mean=01.1707} policy_loss=-8.4701 policy updated! \n",
      "train step 05312 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4200 diff={max=03.9832, min=00.0227, mean=00.8244} policy_loss=-9.8540 policy updated! \n",
      "train step 05313 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6871 diff={max=07.3908, min=00.0073, mean=01.0962} policy_loss=-8.0883 policy updated! \n",
      "train step 05314 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9876 diff={max=10.7787, min=00.0128, mean=01.1161} policy_loss=-9.5580 policy updated! \n",
      "train step 05315 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=02.4493 diff={max=06.7732, min=00.0017, mean=00.8589} policy_loss=-8.5493 policy updated! \n",
      "train step 05316 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7876 diff={max=07.1221, min=00.0018, mean=00.9704} policy_loss=-7.4769 policy updated! \n",
      "train step 05317 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4575 diff={max=07.5848, min=00.0111, mean=01.1655} policy_loss=-9.8448 policy updated! \n",
      "train step 05318 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8364 diff={max=09.3687, min=00.0040, mean=01.1699} policy_loss=-8.4210 policy updated! \n",
      "train step 05319 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3544 diff={max=07.0556, min=00.0148, mean=01.1273} policy_loss=-8.2717 policy updated! \n",
      "train step 05320 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=02.1755 diff={max=05.4610, min=00.0563, mean=01.0070} policy_loss=-8.9989 policy updated! \n",
      "train step 05321 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1652 diff={max=06.5994, min=00.0244, mean=00.9514} policy_loss=-6.6702 policy updated! \n",
      "train step 05322 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6065 diff={max=06.5533, min=00.0076, mean=00.8923} policy_loss=-9.0420 policy updated! \n",
      "train step 05323 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2519 diff={max=06.3610, min=00.0083, mean=00.9862} policy_loss=-7.9214 policy updated! \n",
      "train step 05324 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3853 diff={max=02.7111, min=00.0321, mean=00.8503} policy_loss=-6.9654 policy updated! \n",
      "train step 05325 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=01.9544 diff={max=04.5046, min=00.0120, mean=00.9667} policy_loss=-8.6353 policy updated! \n",
      "train step 05326 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9562 diff={max=07.3656, min=00.0035, mean=01.1074} policy_loss=-9.1452 policy updated! \n",
      "train step 05327 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5896 diff={max=06.4252, min=00.0285, mean=01.2546} policy_loss=-8.7424 policy updated! \n",
      "train step 05328 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0359 diff={max=06.3579, min=00.0021, mean=00.8667} policy_loss=-8.6100 policy updated! \n",
      "train step 05329 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1191 diff={max=05.7317, min=00.0035, mean=00.9515} policy_loss=-9.1696 policy updated! \n",
      "train step 05330 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=04.0511 diff={max=07.6998, min=00.0216, mean=01.1108} policy_loss=-9.4985 policy updated! \n",
      "train step 05331 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4292 diff={max=07.4511, min=00.0560, mean=01.1436} policy_loss=-10.0041 policy updated! \n",
      "train step 05332 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3262 diff={max=05.9919, min=00.0074, mean=00.9658} policy_loss=-6.7766 policy updated! \n",
      "train step 05333 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0678 diff={max=07.9325, min=00.0084, mean=01.2498} policy_loss=-7.8602 policy updated! \n",
      "train step 05334 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1054 diff={max=09.2654, min=00.0040, mean=01.2785} policy_loss=-8.4473 policy updated! \n",
      "train step 05335 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=04.1730 diff={max=08.1243, min=00.0969, mean=01.3403} policy_loss=-8.7858 policy updated! \n",
      "train step 05336 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9483 diff={max=05.0306, min=00.0125, mean=00.9098} policy_loss=-8.3657 policy updated! \n",
      "train step 05337 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6360 diff={max=05.4184, min=00.0203, mean=00.8266} policy_loss=-6.7642 policy updated! \n",
      "train step 05338 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3757 diff={max=07.0349, min=00.0065, mean=01.0013} policy_loss=-7.9731 policy updated! \n",
      "train step 05339 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2986 diff={max=05.6791, min=00.0002, mean=01.1937} policy_loss=-8.4159 policy updated! \n",
      "train step 05340 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=04.3881 diff={max=09.2248, min=00.0148, mean=01.3275} policy_loss=-11.0992 policy updated! \n",
      "train step 05341 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2500 diff={max=08.1734, min=00.0063, mean=01.1387} policy_loss=-8.9432 policy updated! \n",
      "train step 05342 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.6261 diff={max=02.0312, min=00.0006, mean=00.6140} policy_loss=-7.1556 policy updated! \n",
      "train step 05343 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7640 diff={max=09.6268, min=00.0233, mean=01.4836} policy_loss=-8.2733 policy updated! \n",
      "train step 05344 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5663 diff={max=04.0929, min=00.0908, mean=00.8562} policy_loss=-7.2067 policy updated! \n",
      "train step 05345 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=02.7557 diff={max=05.3233, min=00.0108, mean=01.1143} policy_loss=-8.9942 policy updated! \n",
      "train step 05346 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2057 diff={max=06.0917, min=00.0026, mean=01.1231} policy_loss=-8.9389 policy updated! \n",
      "train step 05347 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9790 diff={max=05.5600, min=00.0244, mean=00.9538} policy_loss=-8.6153 policy updated! \n",
      "train step 05348 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7610 diff={max=11.6827, min=00.0064, mean=01.2556} policy_loss=-8.9189 policy updated! \n",
      "train step 05349 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3258 diff={max=05.4342, min=00.0180, mean=00.9877} policy_loss=-9.3559 policy updated! \n",
      "train step 05350 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=03.2285 diff={max=08.6510, min=00.0316, mean=01.1084} policy_loss=-7.6079 policy updated! \n",
      "train step 05351 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2907 diff={max=09.1750, min=00.0274, mean=01.1233} policy_loss=-8.5787 policy updated! \n",
      "train step 05352 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6363 diff={max=08.8282, min=00.0177, mean=01.2171} policy_loss=-8.0442 policy updated! \n",
      "train step 05353 reward={max=10.0000, min=00.0000, mean=04.0000} optimizing loss=03.9708 diff={max=07.3252, min=00.0017, mean=01.1129} policy_loss=-7.8109 policy updated! \n",
      "train step 05354 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9885 diff={max=04.3650, min=00.0417, mean=01.0120} policy_loss=-9.7634 policy updated! \n",
      "train step 05355 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=06.1692 diff={max=09.3488, min=00.0559, mean=01.4409} policy_loss=-7.5735 policy updated! \n",
      "train step 05356 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4686 diff={max=07.6757, min=00.0133, mean=01.0222} policy_loss=-8.0578 policy updated! \n",
      "train step 05357 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3901 diff={max=06.4405, min=00.0074, mean=01.1470} policy_loss=-8.5808 policy updated! \n",
      "train step 05358 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7910 diff={max=07.3609, min=00.0072, mean=01.0959} policy_loss=-8.4925 policy updated! \n",
      "train step 05359 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7797 diff={max=07.7242, min=00.0450, mean=00.8399} policy_loss=-8.3866 policy updated! \n",
      "train step 05360 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=04.9607 diff={max=08.5697, min=00.0086, mean=01.3475} policy_loss=-8.4081 policy updated! \n",
      "train step 05361 reward={max=10.0000, min=00.0000, mean=06.0000} optimizing loss=05.6230 diff={max=08.6943, min=00.0049, mean=01.4543} policy_loss=-8.4383 policy updated! \n",
      "train step 05362 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9702 diff={max=06.3322, min=00.0320, mean=00.8425} policy_loss=-8.0535 policy updated! \n",
      "train step 05363 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3996 diff={max=09.4143, min=00.0034, mean=00.8970} policy_loss=-6.7809 policy updated! \n",
      "train step 05364 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6665 diff={max=03.7092, min=00.0095, mean=00.9551} policy_loss=-9.9230 policy updated! \n",
      "train step 05365 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=06.4551 diff={max=12.4955, min=00.0482, mean=01.3764} policy_loss=-9.6723 policy updated! \n",
      "train step 05366 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9781 diff={max=07.0869, min=00.0552, mean=01.0644} policy_loss=-7.8586 policy updated! \n",
      "train step 05367 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2570 diff={max=06.6877, min=00.0223, mean=01.1985} policy_loss=-7.6884 policy updated! \n",
      "train step 05368 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5688 diff={max=08.5570, min=00.0372, mean=01.0918} policy_loss=-7.7893 policy updated! \n",
      "train step 05369 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9214 diff={max=04.8917, min=00.0265, mean=01.1830} policy_loss=-7.6569 policy updated! \n",
      "train step 05370 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.5424 diff={max=06.9195, min=00.0170, mean=00.8926} policy_loss=-8.4030 policy updated! \n",
      "train step 05371 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=03.3252 diff={max=06.3406, min=00.0166, mean=01.1325} policy_loss=-8.5901 policy updated! \n",
      "train step 05372 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3403 diff={max=07.0744, min=00.0061, mean=01.2677} policy_loss=-9.6995 policy updated! \n",
      "train step 05373 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7781 diff={max=05.7156, min=00.0140, mean=00.8577} policy_loss=-8.1827 policy updated! \n",
      "train step 05374 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1302 diff={max=05.6427, min=00.0295, mean=00.9457} policy_loss=-9.2266 policy updated! \n",
      "train step 05375 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=05.6208 diff={max=10.3475, min=00.0358, mean=01.2702} policy_loss=-8.7548 policy updated! \n",
      "train step 05376 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7176 diff={max=04.9442, min=00.0347, mean=01.1291} policy_loss=-6.7031 policy updated! \n",
      "train step 05377 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1776 diff={max=06.3049, min=00.0106, mean=00.8698} policy_loss=-8.0550 policy updated! \n",
      "train step 05378 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7612 diff={max=07.6905, min=00.0009, mean=01.1855} policy_loss=-9.2848 policy updated! \n",
      "train step 05379 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3577 diff={max=08.2527, min=00.0471, mean=01.2262} policy_loss=-8.4456 policy updated! \n",
      "train step 05380 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.1883 diff={max=07.6376, min=00.0052, mean=00.8094} policy_loss=-8.0785 policy updated! \n",
      "train step 05381 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8252 diff={max=02.2527, min=00.0460, mean=00.6893} policy_loss=-10.5323 policy updated! \n",
      "train step 05382 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5843 diff={max=05.1348, min=00.0014, mean=01.0361} policy_loss=-9.0678 policy updated! \n",
      "train step 05383 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7336 diff={max=08.8425, min=00.0006, mean=01.1885} policy_loss=-6.6370 policy updated! \n",
      "train step 05384 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7929 diff={max=10.0635, min=00.0608, mean=01.1745} policy_loss=-7.3072 policy updated! \n",
      "train step 05385 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=06.3479 diff={max=08.6643, min=00.0122, mean=01.4212} policy_loss=-7.8895 policy updated! \n",
      "train step 05386 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8408 diff={max=06.4923, min=00.0090, mean=00.7956} policy_loss=-7.6613 policy updated! \n",
      "train step 05387 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7802 diff={max=04.7661, min=00.0089, mean=00.9218} policy_loss=-8.4892 policy updated! \n",
      "train step 05388 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6200 diff={max=05.1996, min=00.0286, mean=00.8740} policy_loss=-8.4167 policy updated! \n",
      "train step 05389 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5675 diff={max=04.7136, min=00.0461, mean=00.8891} policy_loss=-9.0928 policy updated! \n",
      "train step 05390 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.3626 diff={max=03.4690, min=00.0117, mean=00.7932} policy_loss=-7.4119 policy updated! \n",
      "train step 05391 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=02.2943 diff={max=05.1908, min=00.0170, mean=00.9898} policy_loss=-8.4901 policy updated! \n",
      "train step 05392 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7176 diff={max=08.7113, min=00.0091, mean=00.8983} policy_loss=-8.5112 policy updated! \n",
      "train step 05393 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1520 diff={max=09.4109, min=00.0074, mean=00.9851} policy_loss=-7.0137 policy updated! \n",
      "train step 05394 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.2260 diff={max=07.9146, min=00.0177, mean=01.7769} policy_loss=-9.6971 policy updated! \n",
      "train step 05395 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.7571 diff={max=09.2279, min=00.0111, mean=01.1060} policy_loss=-8.7934 policy updated! \n",
      "train step 05396 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3629 diff={max=07.1871, min=00.0093, mean=00.8505} policy_loss=-8.6080 policy updated! \n",
      "train step 05397 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1037 diff={max=07.2722, min=00.0047, mean=01.3319} policy_loss=-9.2812 policy updated! \n",
      "train step 05398 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4196 diff={max=04.8325, min=00.0238, mean=01.1649} policy_loss=-8.1604 policy updated! \n",
      "train step 05399 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6778 diff={max=07.9456, min=00.0481, mean=01.2147} policy_loss=-7.6390 policy updated! \n",
      "train step 05400 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.7818 diff={max=05.6565, min=00.0933, mean=01.1645} policy_loss=-9.7680 policy updated! \n",
      "train step 05401 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=05.6755 diff={max=09.6911, min=00.0105, mean=01.2856} policy_loss=-9.3765 policy updated! \n",
      "train step 05402 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3533 diff={max=03.7598, min=00.0013, mean=00.7446} policy_loss=-7.5160 policy updated! \n",
      "train step 05403 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0545 diff={max=08.4892, min=00.0082, mean=01.1923} policy_loss=-7.8766 policy updated! \n",
      "train step 05404 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8245 diff={max=07.6602, min=00.0948, mean=01.5669} policy_loss=-8.5188 policy updated! \n",
      "train step 05405 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.2783 diff={max=05.8948, min=00.0051, mean=00.9655} policy_loss=-8.6879 policy updated! \n",
      "train step 05406 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9491 diff={max=03.5832, min=00.0002, mean=00.6330} policy_loss=-7.0261 policy updated! \n",
      "train step 05407 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1511 diff={max=08.3756, min=00.0066, mean=01.0312} policy_loss=-7.7989 policy updated! \n",
      "train step 05408 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0449 diff={max=07.0742, min=00.0218, mean=00.9695} policy_loss=-7.3147 policy updated! \n",
      "train step 05409 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7879 diff={max=08.0760, min=00.0245, mean=01.1794} policy_loss=-8.4368 policy updated! \n",
      "train step 05410 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.2388 diff={max=05.2395, min=00.0094, mean=01.0469} policy_loss=-9.2570 policy updated! \n",
      "train step 05411 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.2143 diff={max=08.1638, min=00.0031, mean=01.0431} policy_loss=-8.0995 policy updated! \n",
      "train step 05412 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1269 diff={max=08.1573, min=00.0105, mean=01.3460} policy_loss=-8.9554 policy updated! \n",
      "train step 05413 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5593 diff={max=08.1213, min=00.0420, mean=01.4212} policy_loss=-7.8327 policy updated! \n",
      "train step 05414 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4373 diff={max=08.5446, min=00.0125, mean=00.8858} policy_loss=-8.3348 policy updated! \n",
      "train step 05415 reward={max=10.0000, min=00.0000, mean=07.4000} optimizing loss=02.5884 diff={max=05.1746, min=00.0238, mean=01.0711} policy_loss=-7.7531 policy updated! \n",
      "train step 05416 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2959 diff={max=05.4567, min=00.0162, mean=00.9500} policy_loss=-9.0758 policy updated! \n",
      "train step 05417 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7310 diff={max=05.7565, min=00.0226, mean=01.0560} policy_loss=-9.0571 policy updated! \n",
      "train step 05418 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7335 diff={max=07.9077, min=00.0034, mean=00.9521} policy_loss=-10.3479 policy updated! \n",
      "train step 05419 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8330 diff={max=06.3805, min=00.0022, mean=00.7845} policy_loss=-7.7184 policy updated! \n",
      "train step 05420 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=01.9093 diff={max=03.8837, min=00.0023, mean=00.9850} policy_loss=-6.4125 policy updated! \n",
      "train step 05421 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.5513 diff={max=02.4951, min=00.0354, mean=00.5469} policy_loss=-7.4612 policy updated! \n",
      "train step 05422 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9145 diff={max=07.7984, min=00.0199, mean=00.9582} policy_loss=-7.0414 policy updated! \n",
      "train step 05423 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1586 diff={max=07.5468, min=00.0147, mean=01.0088} policy_loss=-9.9117 policy updated! \n",
      "train step 05424 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.7455 diff={max=09.0450, min=00.0305, mean=01.5686} policy_loss=-7.7654 policy updated! \n",
      "train step 05425 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=02.0184 diff={max=04.0503, min=00.0488, mean=01.0567} policy_loss=-7.9135 policy updated! \n",
      "train step 05426 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0934 diff={max=05.8425, min=00.0070, mean=01.1170} policy_loss=-8.4417 policy updated! \n",
      "train step 05427 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9671 diff={max=11.2346, min=00.0135, mean=01.3364} policy_loss=-7.0813 policy updated! \n",
      "train step 05428 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5872 diff={max=07.4951, min=00.0211, mean=01.2181} policy_loss=-8.3488 policy updated! \n",
      "train step 05429 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4306 diff={max=07.8721, min=00.0135, mean=01.0395} policy_loss=-7.8614 policy updated! \n",
      "train step 05430 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.0010 diff={max=09.1050, min=00.0341, mean=00.8791} policy_loss=-8.1436 policy updated! \n",
      "train step 05431 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3888 diff={max=06.3657, min=00.0294, mean=00.8766} policy_loss=-7.7547 policy updated! \n",
      "train step 05432 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6174 diff={max=06.5646, min=00.0134, mean=01.0121} policy_loss=-8.3684 policy updated! \n",
      "train step 05433 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8059 diff={max=06.4742, min=00.0053, mean=01.1532} policy_loss=-9.2823 policy updated! \n",
      "train step 05434 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7393 diff={max=05.4344, min=00.0187, mean=00.9321} policy_loss=-7.9644 policy updated! \n",
      "train step 05435 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.9104 diff={max=10.1214, min=00.0121, mean=01.3086} policy_loss=-8.8144 policy updated! \n",
      "train step 05436 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4396 diff={max=08.5731, min=00.0256, mean=01.1314} policy_loss=-9.5490 policy updated! \n",
      "train step 05437 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1581 diff={max=04.3817, min=00.0015, mean=01.0663} policy_loss=-8.4079 policy updated! \n",
      "train step 05438 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6160 diff={max=08.2976, min=00.0147, mean=01.3630} policy_loss=-8.8820 policy updated! \n",
      "train step 05439 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5197 diff={max=03.4318, min=00.0080, mean=00.9507} policy_loss=-9.2172 policy updated! \n",
      "train step 05440 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.5414 diff={max=08.6110, min=00.0059, mean=01.0259} policy_loss=-9.2912 policy updated! \n",
      "train step 05441 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9336 diff={max=04.7777, min=00.0275, mean=00.9366} policy_loss=-7.8320 policy updated! \n",
      "train step 05442 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3299 diff={max=06.1790, min=00.0297, mean=01.0184} policy_loss=-8.7297 policy updated! \n",
      "train step 05443 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5197 diff={max=06.8635, min=00.0348, mean=01.0483} policy_loss=-7.6195 policy updated! \n",
      "train step 05444 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5085 diff={max=06.3031, min=00.0158, mean=01.2052} policy_loss=-6.5988 policy updated! \n",
      "train step 05445 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.3221 diff={max=05.5855, min=00.0175, mean=00.8627} policy_loss=-7.1268 policy updated! \n",
      "train step 05446 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9026 diff={max=08.8501, min=00.0161, mean=01.6941} policy_loss=-7.9919 policy updated! \n",
      "train step 05447 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6748 diff={max=05.1440, min=00.0131, mean=00.8552} policy_loss=-8.8692 policy updated! \n",
      "train step 05448 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6871 diff={max=07.9163, min=00.0038, mean=00.9660} policy_loss=-8.1689 policy updated! \n",
      "train step 05449 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0108 diff={max=08.2381, min=00.0020, mean=01.3406} policy_loss=-8.4505 policy updated! \n",
      "train step 05450 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.6536 diff={max=07.8706, min=00.0015, mean=01.1326} policy_loss=-8.8807 policy updated! \n",
      "train step 05451 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6744 diff={max=07.4998, min=00.0013, mean=01.3043} policy_loss=-7.9364 policy updated! \n",
      "train step 05452 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7961 diff={max=07.5518, min=00.0126, mean=01.0598} policy_loss=-9.4696 policy updated! \n",
      "train step 05453 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9892 diff={max=05.9232, min=00.0006, mean=01.3783} policy_loss=-9.0589 policy updated! \n",
      "train step 05454 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3586 diff={max=07.4561, min=00.0554, mean=01.1859} policy_loss=-10.4153 policy updated! \n",
      "train step 05455 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.2262 diff={max=04.9001, min=00.0191, mean=01.3162} policy_loss=-9.4987 policy updated! \n",
      "train step 05456 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9233 diff={max=08.6405, min=00.0468, mean=01.1305} policy_loss=-8.2765 policy updated! \n",
      "train step 05457 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2225 diff={max=06.1161, min=00.0445, mean=01.6323} policy_loss=-9.0749 policy updated! \n",
      "train step 05458 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0913 diff={max=05.9950, min=00.0022, mean=00.9739} policy_loss=-7.5888 policy updated! \n",
      "train step 05459 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7864 diff={max=06.5092, min=00.0141, mean=01.1396} policy_loss=-9.4198 policy updated! \n",
      "train step 05460 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.2650 diff={max=07.0517, min=00.0063, mean=01.2242} policy_loss=-8.6480 policy updated! \n",
      "train step 05461 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2386 diff={max=04.6398, min=00.0325, mean=01.0257} policy_loss=-8.3501 policy updated! \n",
      "train step 05462 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3302 diff={max=05.9277, min=00.0541, mean=01.2098} policy_loss=-8.0916 policy updated! \n",
      "train step 05463 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9867 diff={max=05.3313, min=00.0376, mean=01.1698} policy_loss=-7.8614 policy updated! \n",
      "train step 05464 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=01.7896 diff={max=03.9003, min=00.0749, mean=00.9622} policy_loss=-7.8487 policy updated! \n",
      "train step 05465 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.2816 diff={max=08.5543, min=00.0665, mean=01.1099} policy_loss=-7.7942 policy updated! \n",
      "train step 05466 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7815 diff={max=07.0968, min=00.0346, mean=01.3428} policy_loss=-8.3144 policy updated! \n",
      "train step 05467 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1929 diff={max=09.3374, min=00.0096, mean=01.1547} policy_loss=-8.5337 policy updated! \n",
      "train step 05468 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5876 diff={max=03.5847, min=00.0283, mean=01.1389} policy_loss=-9.7321 policy updated! \n",
      "train step 05469 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5389 diff={max=08.5084, min=00.0257, mean=01.3202} policy_loss=-8.9162 policy updated! \n",
      "train step 05470 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.7271 diff={max=07.1005, min=00.0230, mean=01.1472} policy_loss=-8.0052 policy updated! \n",
      "train step 05471 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6832 diff={max=06.3201, min=00.0081, mean=01.2470} policy_loss=-8.4899 policy updated! \n",
      "train step 05472 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1923 diff={max=09.4630, min=00.0348, mean=01.0410} policy_loss=-9.1565 policy updated! \n",
      "train step 05473 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3975 diff={max=06.1472, min=00.0209, mean=01.2253} policy_loss=-9.4035 policy updated! \n",
      "train step 05474 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9689 diff={max=10.9033, min=00.0355, mean=01.2619} policy_loss=-8.7546 policy updated! \n",
      "train step 05475 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=05.6121 diff={max=07.3591, min=00.0194, mean=01.6087} policy_loss=-8.7927 policy updated! \n",
      "train step 05476 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4337 diff={max=07.7843, min=00.0131, mean=00.8951} policy_loss=-7.7394 policy updated! \n",
      "train step 05477 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6413 diff={max=05.4522, min=00.0211, mean=00.8350} policy_loss=-7.8992 policy updated! \n",
      "train step 05478 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=03.8313 diff={max=09.6259, min=00.0139, mean=01.0747} policy_loss=-8.7285 policy updated! \n",
      "train step 05479 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4508 diff={max=04.6051, min=00.0061, mean=01.1025} policy_loss=-9.1727 policy updated! \n",
      "train step 05480 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=06.0613 diff={max=16.0304, min=00.0046, mean=00.9753} policy_loss=-8.4910 policy updated! \n",
      "train step 05481 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3427 diff={max=05.6190, min=00.0250, mean=01.1549} policy_loss=-9.0147 policy updated! \n",
      "train step 05482 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1049 diff={max=07.3377, min=00.0186, mean=01.3981} policy_loss=-10.1790 policy updated! \n",
      "train step 05483 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6957 diff={max=06.2279, min=00.0193, mean=01.3245} policy_loss=-9.2555 policy updated! \n",
      "train step 05484 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=04.9999 diff={max=07.5479, min=00.0063, mean=01.4598} policy_loss=-9.5604 policy updated! \n",
      "train step 05485 reward={max=10.0000, min=00.0000, mean=02.0000} optimizing loss=04.2773 diff={max=09.8597, min=00.0075, mean=01.1162} policy_loss=-8.3443 policy updated! \n",
      "train step 05486 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7988 diff={max=10.2578, min=00.0066, mean=01.0139} policy_loss=-6.7969 policy updated! \n",
      "train step 05487 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7059 diff={max=08.2003, min=00.0692, mean=01.3296} policy_loss=-10.3718 policy updated! \n",
      "train step 05488 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4638 diff={max=10.5422, min=00.0129, mean=01.4502} policy_loss=-9.1988 policy updated! \n",
      "train step 05489 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0008 diff={max=07.2438, min=00.0229, mean=01.4013} policy_loss=-9.2987 policy updated! \n",
      "train step 05490 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.7214 diff={max=05.5842, min=00.0061, mean=01.1611} policy_loss=-9.0091 policy updated! \n",
      "train step 05491 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.2050 diff={max=12.4041, min=00.0267, mean=01.4283} policy_loss=-8.1199 policy updated! \n",
      "train step 05492 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0708 diff={max=02.9961, min=00.0057, mean=00.7539} policy_loss=-7.4355 policy updated! \n",
      "train step 05493 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3227 diff={max=07.2362, min=00.0240, mean=01.3364} policy_loss=-8.6952 policy updated! \n",
      "train step 05494 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=01.4234 diff={max=05.8081, min=00.0020, mean=00.7472} policy_loss=-8.3055 policy updated! \n",
      "train step 05495 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.4478 diff={max=10.3055, min=00.0246, mean=01.2121} policy_loss=-8.3451 policy updated! \n",
      "train step 05496 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4042 diff={max=05.4865, min=00.0278, mean=01.0839} policy_loss=-8.6361 policy updated! \n",
      "train step 05497 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5206 diff={max=06.3864, min=00.0394, mean=00.9952} policy_loss=-9.0753 policy updated! \n",
      "train step 05498 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=02.2462 diff={max=04.6332, min=00.0250, mean=01.0393} policy_loss=-9.0271 policy updated! \n",
      "train step 05499 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2704 diff={max=06.9823, min=00.0239, mean=01.5858} policy_loss=-9.7877 policy updated! \n",
      "train step 05500 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.8780 diff={max=06.9255, min=00.0016, mean=01.4013} policy_loss=-8.3756 policy updated! \n",
      "train step 05501 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=01.0060 diff={max=03.2632, min=00.0024, mean=00.7083} policy_loss=-8.3977 policy updated! \n",
      "train step 05502 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3695 diff={max=06.5396, min=00.0038, mean=01.1281} policy_loss=-8.2986 policy updated! \n",
      "train step 05503 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1947 diff={max=05.8136, min=00.0029, mean=00.8887} policy_loss=-8.9070 policy updated! \n",
      "train step 05504 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3896 diff={max=09.0122, min=00.0254, mean=01.0635} policy_loss=-7.8999 policy updated! \n",
      "train step 05505 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.8812 diff={max=05.7615, min=00.0342, mean=01.1250} policy_loss=-7.5369 policy updated! \n",
      "train step 05506 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3263 diff={max=07.3097, min=00.0056, mean=01.1551} policy_loss=-8.9030 policy updated! \n",
      "train step 05507 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2669 diff={max=07.0113, min=00.0456, mean=01.1661} policy_loss=-8.1318 policy updated! \n",
      "train step 05508 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=06.8869 diff={max=12.4875, min=00.0181, mean=01.2834} policy_loss=-8.2909 policy updated! \n",
      "train step 05509 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1590 diff={max=02.9712, min=00.0680, mean=00.8246} policy_loss=-8.3922 policy updated! \n",
      "train step 05510 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.9713 diff={max=08.9007, min=00.0351, mean=01.3021} policy_loss=-9.3149 policy updated! \n",
      "train step 05511 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1414 diff={max=08.2146, min=00.0449, mean=01.6677} policy_loss=-9.1424 policy updated! \n",
      "train step 05512 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6809 diff={max=08.5687, min=00.0246, mean=01.2723} policy_loss=-9.6846 policy updated! \n",
      "train step 05513 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=06.3090 diff={max=11.1997, min=00.0041, mean=01.4621} policy_loss=-9.5793 policy updated! \n",
      "train step 05514 reward={max=09.0000, min=09.0000, mean=09.0000} optimizing loss=05.3828 diff={max=10.5057, min=00.0211, mean=01.3467} policy_loss=-8.3317 policy updated! \n",
      "train step 05515 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=05.1022 diff={max=09.0601, min=00.0287, mean=01.3533} policy_loss=-8.9675 policy updated! \n",
      "train step 05516 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=02.3858 diff={max=06.5764, min=00.0362, mean=00.9403} policy_loss=-7.9508 policy updated! \n",
      "train step 05517 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2185 diff={max=05.2474, min=00.0016, mean=01.1974} policy_loss=-9.8248 policy updated! \n",
      "train step 05518 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2035 diff={max=12.4294, min=00.0062, mean=01.2217} policy_loss=-6.7215 policy updated! \n",
      "train step 05519 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9375 diff={max=07.4791, min=00.0067, mean=01.3910} policy_loss=-8.4455 policy updated! \n",
      "train step 05520 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.7286 diff={max=06.3451, min=00.0166, mean=01.2333} policy_loss=-8.6529 policy updated! \n",
      "train step 05521 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5800 diff={max=08.0180, min=00.0345, mean=01.5412} policy_loss=-8.8412 policy updated! \n",
      "train step 05522 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5683 diff={max=06.0367, min=00.0121, mean=01.0291} policy_loss=-8.7463 policy updated! \n",
      "train step 05523 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3633 diff={max=06.0528, min=00.0083, mean=01.1084} policy_loss=-9.4274 policy updated! \n",
      "train step 05524 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5839 diff={max=05.1861, min=00.0076, mean=00.7793} policy_loss=-9.0278 policy updated! \n",
      "train step 05525 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.4042 diff={max=04.0571, min=00.0123, mean=00.8473} policy_loss=-9.5121 policy updated! \n",
      "train step 05526 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3074 diff={max=05.3373, min=00.0008, mean=01.2610} policy_loss=-9.2875 policy updated! \n",
      "train step 05527 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0065 diff={max=08.4013, min=00.0379, mean=01.4352} policy_loss=-7.9107 policy updated! \n",
      "train step 05528 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=06.2592 diff={max=09.1646, min=00.0184, mean=01.5790} policy_loss=-8.1667 policy updated! \n",
      "train step 05529 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7577 diff={max=12.4646, min=00.0105, mean=01.1596} policy_loss=-8.0999 policy updated! \n",
      "train step 05530 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.4060 diff={max=08.5029, min=00.0124, mean=01.0687} policy_loss=-9.5249 policy updated! \n",
      "train step 05531 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=02.0147 diff={max=07.8590, min=00.0029, mean=00.7677} policy_loss=-6.5734 policy updated! \n",
      "train step 05532 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9832 diff={max=07.6972, min=00.0214, mean=01.3537} policy_loss=-8.0821 policy updated! \n",
      "train step 05533 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2341 diff={max=05.7975, min=00.0171, mean=01.2840} policy_loss=-8.9061 policy updated! \n",
      "train step 05534 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4568 diff={max=05.8561, min=00.0095, mean=01.0732} policy_loss=-8.4982 policy updated! \n",
      "train step 05535 reward={max=09.0000, min=09.0000, mean=09.0000} optimizing loss=01.2217 diff={max=03.8196, min=00.0475, mean=00.8046} policy_loss=-10.7546 policy updated! \n",
      "train step 05536 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9654 diff={max=05.9370, min=00.0131, mean=00.9919} policy_loss=-8.2080 policy updated! \n",
      "train step 05537 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3614 diff={max=04.9474, min=00.0023, mean=00.9943} policy_loss=-8.9050 policy updated! \n",
      "train step 05538 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8072 diff={max=08.0213, min=00.0100, mean=01.2818} policy_loss=-8.1002 policy updated! \n",
      "train step 05539 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1298 diff={max=05.6900, min=00.0040, mean=00.9332} policy_loss=-9.8212 policy updated! \n",
      "train step 05540 reward={max=09.0000, min=09.0000, mean=09.0000} optimizing loss=05.7299 diff={max=09.0580, min=00.0171, mean=01.4192} policy_loss=-8.7313 policy updated! \n",
      "train step 05541 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=06.7364 diff={max=11.2743, min=00.0192, mean=01.4820} policy_loss=-8.0650 policy updated! \n",
      "train step 05542 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4420 diff={max=09.4238, min=00.0198, mean=01.1916} policy_loss=-7.5615 policy updated! \n",
      "train step 05543 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=04.6720 diff={max=08.5665, min=00.0120, mean=01.3064} policy_loss=-7.4514 policy updated! \n",
      "train step 05544 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4440 diff={max=08.1559, min=00.0059, mean=01.1716} policy_loss=-7.7988 policy updated! \n",
      "train step 05545 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.1468 diff={max=05.1274, min=00.0046, mean=00.9643} policy_loss=-8.7536 policy updated! \n",
      "train step 05546 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8385 diff={max=07.8387, min=00.0138, mean=01.2636} policy_loss=-9.5705 policy updated! \n",
      "train step 05547 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6507 diff={max=08.7260, min=00.0491, mean=01.4712} policy_loss=-10.6396 policy updated! \n",
      "train step 05548 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0756 diff={max=05.5540, min=00.0611, mean=01.2465} policy_loss=-9.5116 policy updated! \n",
      "train step 05549 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3273 diff={max=09.0693, min=00.0026, mean=01.3321} policy_loss=-9.4969 policy updated! \n",
      "train step 05550 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.8047 diff={max=06.8035, min=00.0066, mean=01.2328} policy_loss=-7.7476 policy updated! \n",
      "train step 05551 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3211 diff={max=06.6486, min=00.0343, mean=00.9816} policy_loss=-7.5052 policy updated! \n",
      "train step 05552 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8495 diff={max=12.7108, min=00.0065, mean=01.3202} policy_loss=-9.6820 policy updated! \n",
      "train step 05553 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0275 diff={max=08.0960, min=00.0038, mean=01.0363} policy_loss=-9.1483 policy updated! \n",
      "train step 05554 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7715 diff={max=07.8339, min=00.0343, mean=01.1001} policy_loss=-7.3053 policy updated! \n",
      "train step 05555 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.7361 diff={max=04.6109, min=00.0202, mean=00.9935} policy_loss=-6.9593 policy updated! \n",
      "train step 05556 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9091 diff={max=05.8033, min=00.0138, mean=01.2488} policy_loss=-8.4990 policy updated! \n",
      "train step 05557 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9261 diff={max=04.9108, min=00.0069, mean=01.2701} policy_loss=-9.3952 policy updated! \n",
      "train step 05558 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7414 diff={max=07.0488, min=00.0561, mean=01.3812} policy_loss=-7.7115 policy updated! \n",
      "train step 05559 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2878 diff={max=03.8672, min=00.0009, mean=00.7701} policy_loss=-7.7164 policy updated! \n",
      "train step 05560 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.4053 diff={max=08.0601, min=00.0047, mean=01.2192} policy_loss=-7.9800 policy updated! \n",
      "train step 05561 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7661 diff={max=11.6707, min=00.0053, mean=01.4077} policy_loss=-9.6632 policy updated! \n",
      "train step 05562 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9362 diff={max=05.4705, min=00.0078, mean=01.2106} policy_loss=-9.0516 policy updated! \n",
      "train step 05563 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=03.8493 diff={max=06.6135, min=00.0006, mean=01.2095} policy_loss=-8.6982 policy updated! \n",
      "train step 05564 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5722 diff={max=05.4056, min=00.0039, mean=00.7989} policy_loss=-7.6452 policy updated! \n",
      "train step 05565 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.5372 diff={max=07.0188, min=00.0066, mean=00.9989} policy_loss=-8.3281 policy updated! \n",
      "train step 05566 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3315 diff={max=06.0466, min=00.0285, mean=01.1797} policy_loss=-8.1244 policy updated! \n",
      "train step 05567 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4101 diff={max=05.0605, min=00.0181, mean=00.7844} policy_loss=-7.6290 policy updated! \n",
      "train step 05568 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2324 diff={max=08.2657, min=00.0165, mean=01.4034} policy_loss=-7.0750 policy updated! \n",
      "train step 05569 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3481 diff={max=06.2644, min=00.0013, mean=01.0127} policy_loss=-9.0172 policy updated! \n",
      "train step 05570 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.8912 diff={max=05.3129, min=00.0196, mean=00.9193} policy_loss=-8.9777 policy updated! \n",
      "train step 05571 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0274 diff={max=06.1827, min=00.0143, mean=01.0802} policy_loss=-11.6127 policy updated! \n",
      "train step 05572 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5097 diff={max=04.3359, min=00.0052, mean=01.1303} policy_loss=-8.7984 policy updated! \n",
      "train step 05573 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3562 diff={max=04.6332, min=00.0293, mean=00.8190} policy_loss=-9.5397 policy updated! \n",
      "train step 05574 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9293 diff={max=07.9114, min=00.0002, mean=01.1200} policy_loss=-9.2740 policy updated! \n",
      "train step 05575 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.8716 diff={max=07.1811, min=00.0095, mean=01.3446} policy_loss=-8.6620 policy updated! \n",
      "train step 05576 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4951 diff={max=08.3016, min=00.0354, mean=01.3384} policy_loss=-8.0494 policy updated! \n",
      "train step 05577 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0822 diff={max=09.4543, min=00.0098, mean=01.2813} policy_loss=-9.1266 policy updated! \n",
      "train step 05578 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1108 diff={max=06.0822, min=00.0003, mean=01.0913} policy_loss=-8.1628 policy updated! \n",
      "train step 05579 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1692 diff={max=06.2484, min=00.0012, mean=01.1483} policy_loss=-7.4621 policy updated! \n",
      "train step 05580 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.1774 diff={max=03.9757, min=00.0187, mean=01.0506} policy_loss=-9.4484 policy updated! \n",
      "train step 05581 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2094 diff={max=09.4588, min=00.0213, mean=01.0755} policy_loss=-8.8497 policy updated! \n",
      "train step 05582 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4599 diff={max=07.2326, min=00.0132, mean=01.2098} policy_loss=-9.0909 policy updated! \n",
      "train step 05583 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3588 diff={max=08.9709, min=00.0204, mean=01.1176} policy_loss=-9.7353 policy updated! \n",
      "train step 05584 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0589 diff={max=05.1951, min=00.0787, mean=01.2580} policy_loss=-9.4152 policy updated! \n",
      "train step 05585 reward={max=09.0000, min=09.0000, mean=09.0000} optimizing loss=02.7408 diff={max=04.7772, min=00.0033, mean=01.2875} policy_loss=-10.0710 policy updated! \n",
      "train step 05586 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9691 diff={max=05.1909, min=00.0073, mean=01.1785} policy_loss=-7.9355 policy updated! \n",
      "train step 05587 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9563 diff={max=08.5733, min=00.0024, mean=01.6016} policy_loss=-10.2160 policy updated! \n",
      "train step 05588 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4669 diff={max=08.5265, min=00.0006, mean=01.1492} policy_loss=-10.4628 policy updated! \n",
      "train step 05589 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=03.8691 diff={max=07.8461, min=00.0076, mean=01.0947} policy_loss=-7.7849 policy updated! \n",
      "train step 05590 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.0157 diff={max=05.4867, min=00.0060, mean=01.0079} policy_loss=-8.9737 policy updated! \n",
      "train step 05591 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2420 diff={max=07.2946, min=00.0267, mean=01.1195} policy_loss=-8.6878 policy updated! \n",
      "train step 05592 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4835 diff={max=07.9591, min=00.0327, mean=01.6070} policy_loss=-7.1340 policy updated! \n",
      "train step 05593 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8617 diff={max=08.6631, min=00.0135, mean=01.1872} policy_loss=-7.9998 policy updated! \n",
      "train step 05594 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=01.2713 diff={max=04.5129, min=00.0030, mean=00.7868} policy_loss=-8.8325 policy updated! \n",
      "train step 05595 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.3975 diff={max=03.7919, min=00.0044, mean=00.7874} policy_loss=-9.1027 policy updated! \n",
      "train step 05596 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6514 diff={max=05.3715, min=00.0077, mean=00.8938} policy_loss=-9.3011 policy updated! \n",
      "train step 05597 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4844 diff={max=05.1819, min=00.0117, mean=00.9958} policy_loss=-8.0384 policy updated! \n",
      "train step 05598 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2029 diff={max=05.1451, min=00.0162, mean=01.1875} policy_loss=-9.1324 policy updated! \n",
      "train step 05599 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4259 diff={max=03.3094, min=00.0071, mean=00.8326} policy_loss=-9.3521 policy updated! \n",
      "train step 05600 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.3748 diff={max=03.8519, min=00.0085, mean=00.8224} policy_loss=-9.5645 policy updated! \n",
      "train step 05601 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0860 diff={max=04.6472, min=00.0175, mean=00.9540} policy_loss=-10.0070 policy updated! \n",
      "train step 05602 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5492 diff={max=06.7786, min=00.0129, mean=00.9117} policy_loss=-8.7193 policy updated! \n",
      "train step 05603 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=02.0966 diff={max=07.6394, min=00.0027, mean=00.8750} policy_loss=-7.6577 policy updated! \n",
      "train step 05604 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7668 diff={max=06.6088, min=00.0183, mean=01.0956} policy_loss=-8.0265 policy updated! \n",
      "train step 05605 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.0789 diff={max=03.4196, min=00.0306, mean=00.7456} policy_loss=-8.6754 policy updated! \n",
      "train step 05606 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=01.9814 diff={max=04.8737, min=00.0205, mean=00.9614} policy_loss=-10.2107 policy updated! \n",
      "train step 05607 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4621 diff={max=10.0199, min=00.0021, mean=01.1419} policy_loss=-7.5955 policy updated! \n",
      "train step 05608 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=02.7748 diff={max=07.2309, min=00.0656, mean=01.0320} policy_loss=-8.2683 policy updated! \n",
      "train step 05609 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1499 diff={max=07.0412, min=00.0015, mean=00.9412} policy_loss=-6.2390 policy updated! \n",
      "train step 05610 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.4095 diff={max=03.9327, min=00.0277, mean=01.1765} policy_loss=-9.1162 policy updated! \n",
      "train step 05611 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6135 diff={max=04.9291, min=00.0326, mean=01.1122} policy_loss=-8.5073 policy updated! \n",
      "train step 05612 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8750 diff={max=04.6434, min=00.0418, mean=00.8650} policy_loss=-8.3225 policy updated! \n",
      "train step 05613 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4065 diff={max=04.9651, min=00.0496, mean=00.7899} policy_loss=-8.6716 policy updated! \n",
      "train step 05614 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=04.1839 diff={max=06.5442, min=00.0129, mean=01.3748} policy_loss=-9.9268 policy updated! \n",
      "train step 05615 reward={max=09.0000, min=09.0000, mean=09.0000} optimizing loss=02.2876 diff={max=07.0650, min=00.0406, mean=00.9161} policy_loss=-7.6681 policy updated! \n",
      "train step 05616 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8025 diff={max=03.8229, min=00.0162, mean=00.9493} policy_loss=-8.5388 policy updated! \n",
      "train step 05617 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3420 diff={max=04.6055, min=00.0052, mean=00.9986} policy_loss=-8.6992 policy updated! \n",
      "train step 05618 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8421 diff={max=03.9191, min=00.0029, mean=00.9037} policy_loss=-8.2545 policy updated! \n",
      "train step 05619 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=03.1158 diff={max=07.5895, min=00.0044, mean=01.0776} policy_loss=-7.5055 policy updated! \n",
      "train step 05620 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.1537 diff={max=06.9737, min=00.0159, mean=01.2913} policy_loss=-10.4864 policy updated! \n",
      "train step 05621 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=05.6001 diff={max=07.8099, min=00.0163, mean=01.4876} policy_loss=-8.0513 policy updated! \n",
      "train step 05622 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3020 diff={max=07.4088, min=00.0144, mean=01.1211} policy_loss=-8.3140 policy updated! \n",
      "train step 05623 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4502 diff={max=08.6133, min=00.0291, mean=01.0289} policy_loss=-8.8236 policy updated! \n",
      "train step 05624 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3807 diff={max=08.0383, min=00.0249, mean=01.1192} policy_loss=-9.4982 policy updated! \n",
      "train step 05625 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.6690 diff={max=07.8906, min=00.0083, mean=00.9822} policy_loss=-8.5153 policy updated! \n",
      "train step 05626 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7801 diff={max=08.4828, min=00.0040, mean=01.1436} policy_loss=-9.1417 policy updated! \n",
      "train step 05627 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9151 diff={max=07.1300, min=00.0693, mean=01.3723} policy_loss=-7.9970 policy updated! \n",
      "train step 05628 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=05.6183 diff={max=07.4424, min=00.0131, mean=01.5052} policy_loss=-8.2120 policy updated! \n",
      "train step 05629 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0263 diff={max=05.3435, min=00.0292, mean=01.1867} policy_loss=-8.7353 policy updated! \n",
      "train step 05630 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.4015 diff={max=03.3243, min=00.0019, mean=00.8299} policy_loss=-9.1246 policy updated! \n",
      "train step 05631 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=01.9290 diff={max=05.5124, min=00.0093, mean=00.8792} policy_loss=-8.6561 policy updated! \n",
      "train step 05632 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3263 diff={max=08.4331, min=00.0077, mean=01.0464} policy_loss=-8.4202 policy updated! \n",
      "train step 05633 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7937 diff={max=05.1888, min=00.0287, mean=00.8071} policy_loss=-7.5511 policy updated! \n",
      "train step 05634 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0327 diff={max=09.0979, min=00.0391, mean=01.2121} policy_loss=-8.7009 policy updated! \n",
      "train step 05635 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.5864 diff={max=09.6099, min=00.0416, mean=01.0435} policy_loss=-7.3967 policy updated! \n",
      "train step 05636 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1542 diff={max=06.0501, min=00.0237, mean=01.1230} policy_loss=-10.3978 policy updated! \n",
      "train step 05637 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9937 diff={max=05.7306, min=00.0155, mean=00.9147} policy_loss=-9.5541 policy updated! \n",
      "train step 05638 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3281 diff={max=05.2140, min=00.0126, mean=01.0553} policy_loss=-9.7240 policy updated! \n",
      "train step 05639 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=03.5883 diff={max=09.2625, min=00.0755, mean=01.1405} policy_loss=-9.0102 policy updated! \n",
      "train step 05640 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.1966 diff={max=04.6048, min=00.0299, mean=01.0275} policy_loss=-8.3277 policy updated! \n",
      "train step 05641 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3380 diff={max=08.3335, min=00.0017, mean=01.2512} policy_loss=-9.3375 policy updated! \n",
      "train step 05642 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2953 diff={max=11.3892, min=00.0004, mean=01.4542} policy_loss=-9.6310 policy updated! \n",
      "train step 05643 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6668 diff={max=05.9691, min=00.0196, mean=01.4043} policy_loss=-9.6823 policy updated! \n",
      "train step 05644 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0865 diff={max=07.9700, min=00.0090, mean=00.9791} policy_loss=-8.4382 policy updated! \n",
      "train step 05645 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.1988 diff={max=06.6920, min=00.0252, mean=01.1738} policy_loss=-10.0738 policy updated! \n",
      "train step 05646 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8972 diff={max=09.9369, min=00.0031, mean=01.2475} policy_loss=-5.9024 policy updated! \n",
      "train step 05647 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1009 diff={max=08.2880, min=00.0144, mean=01.2735} policy_loss=-9.5672 policy updated! \n",
      "train step 05648 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=05.3930 diff={max=09.2326, min=00.0099, mean=01.3808} policy_loss=-9.0859 policy updated! \n",
      "train step 05649 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6622 diff={max=07.8302, min=00.0055, mean=01.3933} policy_loss=-9.8908 policy updated! \n",
      "train step 05650 reward={max=09.0000, min=09.0000, mean=09.0000} optimizing loss=03.6755 diff={max=06.0935, min=00.0167, mean=01.1980} policy_loss=-8.3694 policy updated! \n",
      "train step 05651 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0306 diff={max=06.8633, min=00.0033, mean=01.1759} policy_loss=-9.2956 policy updated! \n",
      "train step 05652 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6893 diff={max=09.3191, min=00.0147, mean=00.8575} policy_loss=-9.4893 policy updated! \n",
      "train step 05653 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6458 diff={max=06.4348, min=00.0112, mean=01.0602} policy_loss=-7.9770 policy updated! \n",
      "train step 05654 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3526 diff={max=04.9508, min=00.0251, mean=01.1029} policy_loss=-9.7727 policy updated! \n",
      "train step 05655 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.1888 diff={max=06.5858, min=00.0031, mean=01.1324} policy_loss=-9.5034 policy updated! \n",
      "train step 05656 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0166 diff={max=05.1556, min=00.0021, mean=01.0892} policy_loss=-8.2805 policy updated! \n",
      "train step 05657 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8071 diff={max=02.9263, min=00.0063, mean=00.6525} policy_loss=-7.8802 policy updated! \n",
      "train step 05658 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1558 diff={max=03.5687, min=00.0125, mean=00.7555} policy_loss=-8.9613 policy updated! \n",
      "train step 05659 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8757 diff={max=08.3074, min=00.0188, mean=01.2373} policy_loss=-8.6574 policy updated! \n",
      "train step 05660 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.8181 diff={max=08.3924, min=00.0074, mean=01.1167} policy_loss=-8.2894 policy updated! \n",
      "train step 05661 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8252 diff={max=10.3912, min=00.0258, mean=01.0561} policy_loss=-9.0304 policy updated! \n",
      "train step 05662 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3593 diff={max=06.7006, min=00.0477, mean=01.1911} policy_loss=-9.8725 policy updated! \n",
      "train step 05663 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5211 diff={max=07.4221, min=00.0405, mean=01.3967} policy_loss=-8.4051 policy updated! \n",
      "train step 05664 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5066 diff={max=07.4170, min=00.0007, mean=01.1349} policy_loss=-9.2748 policy updated! \n",
      "train step 05665 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=00.9049 diff={max=02.7467, min=00.0072, mean=00.6866} policy_loss=-8.6537 policy updated! \n",
      "train step 05666 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8584 diff={max=09.3846, min=00.0023, mean=01.0662} policy_loss=-9.3767 policy updated! \n",
      "train step 05667 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4905 diff={max=05.1327, min=00.0132, mean=01.1228} policy_loss=-9.2379 policy updated! \n",
      "train step 05668 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=01.0450 diff={max=02.6850, min=00.0875, mean=00.7687} policy_loss=-8.7416 policy updated! \n",
      "train step 05669 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2919 diff={max=04.4668, min=00.0059, mean=00.7807} policy_loss=-7.0421 policy updated! \n",
      "train step 05670 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.6972 diff={max=06.8813, min=00.0030, mean=01.3939} policy_loss=-9.8065 policy updated! \n",
      "train step 05671 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5535 diff={max=04.7021, min=00.0042, mean=00.9030} policy_loss=-8.6777 policy updated! \n",
      "train step 05672 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3522 diff={max=06.4032, min=00.0100, mean=00.9430} policy_loss=-9.2103 policy updated! \n",
      "train step 05673 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0244 diff={max=07.7433, min=00.0035, mean=01.1852} policy_loss=-8.1940 policy updated! \n",
      "train step 05674 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1859 diff={max=07.3818, min=00.0665, mean=01.3694} policy_loss=-8.9076 policy updated! \n",
      "train step 05675 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=00.9004 diff={max=02.9469, min=00.0346, mean=00.6630} policy_loss=-8.1664 policy updated! \n",
      "train step 05676 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5563 diff={max=04.2623, min=00.0017, mean=00.7998} policy_loss=-8.5251 policy updated! \n",
      "train step 05677 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0417 diff={max=06.6487, min=00.0338, mean=00.8644} policy_loss=-9.0568 policy updated! \n",
      "train step 05678 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1613 diff={max=07.2438, min=00.0063, mean=01.1251} policy_loss=-9.2998 policy updated! \n",
      "train step 05679 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6323 diff={max=09.2139, min=00.0294, mean=01.0282} policy_loss=-7.9414 policy updated! \n",
      "train step 05680 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.0326 diff={max=06.7252, min=00.0112, mean=01.2431} policy_loss=-10.5394 policy updated! \n",
      "train step 05681 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5672 diff={max=09.4437, min=00.0690, mean=01.1863} policy_loss=-10.0567 policy updated! \n",
      "train step 05682 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0418 diff={max=06.6008, min=00.0136, mean=01.1086} policy_loss=-10.3952 policy updated! \n",
      "train step 05683 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3672 diff={max=05.4629, min=00.0063, mean=01.2160} policy_loss=-9.9695 policy updated! \n",
      "train step 05684 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1598 diff={max=10.2719, min=00.0034, mean=01.3351} policy_loss=-9.5586 policy updated! \n",
      "train step 05685 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.1006 diff={max=06.6863, min=00.0094, mean=01.0891} policy_loss=-8.2954 policy updated! \n",
      "train step 05686 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7331 diff={max=08.3156, min=00.0026, mean=01.1347} policy_loss=-9.0181 policy updated! \n",
      "train step 05687 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1085 diff={max=12.2024, min=00.0358, mean=01.1839} policy_loss=-9.0484 policy updated! \n",
      "train step 05688 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0552 diff={max=06.9390, min=00.0232, mean=01.3624} policy_loss=-9.1747 policy updated! \n",
      "train step 05689 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1837 diff={max=04.2154, min=00.0902, mean=01.1160} policy_loss=-8.8280 policy updated! \n",
      "train step 05690 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.8467 diff={max=08.1516, min=00.0144, mean=01.0703} policy_loss=-8.4139 policy updated! \n",
      "train step 05691 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1118 diff={max=07.5662, min=00.0118, mean=01.0083} policy_loss=-7.8952 policy updated! \n",
      "train step 05692 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8868 diff={max=08.3668, min=00.0387, mean=01.0700} policy_loss=-9.8494 policy updated! \n",
      "train step 05693 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4323 diff={max=04.1532, min=00.0220, mean=00.8487} policy_loss=-9.5331 policy updated! \n",
      "train step 05694 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2659 diff={max=07.6954, min=00.0135, mean=01.0818} policy_loss=-7.2773 policy updated! \n",
      "train step 05695 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.6765 diff={max=07.4927, min=00.0253, mean=01.0136} policy_loss=-9.7831 policy updated! \n",
      "train step 05696 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.4188 diff={max=16.2577, min=00.0234, mean=01.3947} policy_loss=-8.7524 policy updated! \n",
      "train step 05697 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3829 diff={max=06.7852, min=00.0034, mean=00.9783} policy_loss=-8.2039 policy updated! \n",
      "train step 05698 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2147 diff={max=08.8685, min=00.0446, mean=01.2511} policy_loss=-10.6751 policy updated! \n",
      "train step 05699 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6404 diff={max=06.4983, min=00.0002, mean=01.0433} policy_loss=-8.9476 policy updated! \n",
      "train step 05700 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.0153 diff={max=06.4868, min=00.0026, mean=00.9052} policy_loss=-8.1050 policy updated! \n",
      "train step 05701 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1567 diff={max=06.7622, min=00.0084, mean=00.7501} policy_loss=-7.9468 policy updated! \n",
      "train step 05702 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5413 diff={max=07.5338, min=00.0452, mean=01.2129} policy_loss=-8.6402 policy updated! \n",
      "train step 05703 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1464 diff={max=06.7806, min=00.0037, mean=00.9421} policy_loss=-7.2579 policy updated! \n",
      "train step 05704 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5445 diff={max=09.4140, min=00.0380, mean=01.3333} policy_loss=-8.3457 policy updated! \n",
      "train step 05705 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.8247 diff={max=07.9352, min=00.0081, mean=01.0327} policy_loss=-9.3145 policy updated! \n",
      "train step 05706 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1999 diff={max=06.0464, min=00.0402, mean=01.1873} policy_loss=-8.5250 policy updated! \n",
      "train step 05707 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2429 diff={max=07.0828, min=00.0033, mean=00.9331} policy_loss=-8.4068 policy updated! \n",
      "train step 05708 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5862 diff={max=04.5113, min=00.0175, mean=00.7770} policy_loss=-8.0146 policy updated! \n",
      "train step 05709 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=04.0209 diff={max=09.3027, min=00.0115, mean=01.1109} policy_loss=-8.0768 policy updated! \n",
      "train step 05710 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.2088 diff={max=04.7725, min=00.0125, mean=00.9560} policy_loss=-8.6335 policy updated! \n",
      "train step 05711 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4102 diff={max=05.1957, min=00.0265, mean=01.0201} policy_loss=-9.8601 policy updated! \n",
      "train step 05712 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6350 diff={max=06.8430, min=00.0282, mean=01.2879} policy_loss=-8.3332 policy updated! \n",
      "train step 05713 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2450 diff={max=08.3932, min=00.0427, mean=01.1528} policy_loss=-8.7037 policy updated! \n",
      "train step 05714 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8199 diff={max=03.7346, min=00.0260, mean=00.9229} policy_loss=-8.7618 policy updated! \n",
      "train step 05715 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.0834 diff={max=09.4157, min=00.0080, mean=00.9672} policy_loss=-7.6979 policy updated! \n",
      "train step 05716 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6484 diff={max=04.3991, min=00.0017, mean=00.8183} policy_loss=-8.6938 policy updated! \n",
      "train step 05717 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2321 diff={max=06.9393, min=00.0107, mean=01.1703} policy_loss=-9.1825 policy updated! \n",
      "train step 05718 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4657 diff={max=06.0654, min=00.0409, mean=01.0417} policy_loss=-8.6992 policy updated! \n",
      "train step 05719 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7846 diff={max=06.5099, min=00.0003, mean=01.1778} policy_loss=-8.5956 policy updated! \n",
      "train step 05720 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.4849 diff={max=08.6812, min=00.0100, mean=01.2028} policy_loss=-9.9459 policy updated! \n",
      "train step 05721 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2583 diff={max=10.8404, min=00.0035, mean=01.1751} policy_loss=-9.9907 policy updated! \n",
      "train step 05722 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3544 diff={max=03.5032, min=00.0042, mean=00.8310} policy_loss=-7.8983 policy updated! \n",
      "train step 05723 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5960 diff={max=05.6942, min=00.0383, mean=01.1578} policy_loss=-9.9034 policy updated! \n",
      "train step 05724 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8367 diff={max=06.7568, min=00.0086, mean=01.1143} policy_loss=-9.4078 policy updated! \n",
      "train step 05725 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.6717 diff={max=06.3060, min=00.0093, mean=00.7950} policy_loss=-8.2548 policy updated! \n",
      "train step 05726 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7649 diff={max=07.2465, min=00.0270, mean=00.9955} policy_loss=-7.8795 policy updated! \n",
      "train step 05727 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0326 diff={max=10.0410, min=00.0311, mean=00.8817} policy_loss=-9.7031 policy updated! \n",
      "train step 05728 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8601 diff={max=07.8315, min=00.0205, mean=00.7590} policy_loss=-8.4970 policy updated! \n",
      "train step 05729 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5765 diff={max=05.3070, min=00.0154, mean=01.1020} policy_loss=-9.5684 policy updated! \n",
      "train step 05730 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.5282 diff={max=10.7934, min=00.0114, mean=01.0963} policy_loss=-7.9982 policy updated! \n",
      "train step 05731 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4926 diff={max=04.7880, min=00.0815, mean=00.8104} policy_loss=-7.9088 policy updated! \n",
      "train step 05732 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8211 diff={max=07.3919, min=00.0054, mean=01.1652} policy_loss=-8.4494 policy updated! \n",
      "train step 05733 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7228 diff={max=05.4672, min=00.0281, mean=01.0843} policy_loss=-10.2225 policy updated! \n",
      "train step 05734 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2992 diff={max=06.3670, min=00.0189, mean=00.8622} policy_loss=-9.0896 policy updated! \n",
      "train step 05735 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.7870 diff={max=05.2346, min=00.0228, mean=01.1709} policy_loss=-9.5864 policy updated! \n",
      "train step 05736 reward={max=09.0000, min=09.0000, mean=09.0000} optimizing loss=03.6781 diff={max=07.3902, min=00.0181, mean=01.2061} policy_loss=-7.8673 policy updated! \n",
      "train step 05737 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6986 diff={max=04.6097, min=00.0008, mean=01.1772} policy_loss=-8.9819 policy updated! \n",
      "train step 05738 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6495 diff={max=03.7891, min=00.0384, mean=00.8839} policy_loss=-8.9168 policy updated! \n",
      "train step 05739 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3210 diff={max=08.8570, min=00.0404, mean=01.3560} policy_loss=-8.4242 policy updated! \n",
      "train step 05740 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.2324 diff={max=04.9443, min=00.0036, mean=00.9480} policy_loss=-7.4220 policy updated! \n",
      "train step 05741 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=03.7712 diff={max=06.3441, min=00.0153, mean=01.1919} policy_loss=-8.6478 policy updated! \n",
      "train step 05742 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9812 diff={max=09.1108, min=00.0009, mean=01.4806} policy_loss=-6.7074 policy updated! \n",
      "train step 05743 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1194 diff={max=10.4595, min=00.0001, mean=01.8136} policy_loss=-9.1253 policy updated! \n",
      "train step 05744 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2938 diff={max=07.5761, min=00.0190, mean=01.1648} policy_loss=-8.1747 policy updated! \n",
      "train step 05745 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.1909 diff={max=04.0577, min=00.0608, mean=00.8051} policy_loss=-8.0227 policy updated! \n",
      "train step 05746 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9833 diff={max=08.3601, min=00.0020, mean=01.2209} policy_loss=-10.2970 policy updated! \n",
      "train step 05747 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4629 diff={max=05.9435, min=00.0023, mean=01.0706} policy_loss=-9.9592 policy updated! \n",
      "train step 05748 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.9206 diff={max=11.4540, min=00.0072, mean=01.5563} policy_loss=-9.3526 policy updated! \n",
      "train step 05749 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8448 diff={max=09.2834, min=00.0080, mean=01.0727} policy_loss=-8.9253 policy updated! \n",
      "train step 05750 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.2519 diff={max=07.2608, min=00.0099, mean=01.1960} policy_loss=-8.0687 policy updated! \n",
      "train step 05751 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4814 diff={max=03.6895, min=00.0162, mean=00.8740} policy_loss=-8.5836 policy updated! \n",
      "train step 05752 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7662 diff={max=09.1704, min=00.0216, mean=01.6337} policy_loss=-8.3462 policy updated! \n",
      "train step 05753 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6300 diff={max=06.8449, min=00.0223, mean=01.4167} policy_loss=-8.5182 policy updated! \n",
      "train step 05754 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6137 diff={max=04.4576, min=00.0088, mean=00.8580} policy_loss=-7.7486 policy updated! \n",
      "train step 05755 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.8038 diff={max=05.2704, min=00.0254, mean=01.1908} policy_loss=-8.1796 policy updated! \n",
      "train step 05756 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0533 diff={max=05.5363, min=00.0191, mean=00.9569} policy_loss=-9.4822 policy updated! \n",
      "train step 05757 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0013 diff={max=08.5228, min=00.0201, mean=01.2089} policy_loss=-10.0640 policy updated! \n",
      "train step 05758 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4683 diff={max=07.5917, min=00.0263, mean=01.2053} policy_loss=-9.3490 policy updated! \n",
      "train step 05759 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=04.8301 diff={max=06.6657, min=00.0024, mean=01.4198} policy_loss=-8.8040 policy updated! \n",
      "train step 05760 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.7065 diff={max=08.1275, min=00.0064, mean=01.3732} policy_loss=-8.8530 policy updated! \n",
      "train step 05761 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7867 diff={max=10.4684, min=00.0100, mean=01.3831} policy_loss=-8.2939 policy updated! \n",
      "train step 05762 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6934 diff={max=07.0306, min=00.0182, mean=01.4028} policy_loss=-8.7483 policy updated! \n",
      "train step 05763 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=06.9266 diff={max=08.7616, min=00.0273, mean=01.7194} policy_loss=-8.4255 policy updated! \n",
      "train step 05764 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5480 diff={max=05.5661, min=00.0004, mean=01.0281} policy_loss=-9.7368 policy updated! \n",
      "train step 05765 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.7258 diff={max=03.9891, min=00.0170, mean=00.8955} policy_loss=-10.1419 policy updated! \n",
      "train step 05766 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3028 diff={max=05.0071, min=00.0040, mean=01.0221} policy_loss=-9.5292 policy updated! \n",
      "train step 05767 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7844 diff={max=10.3264, min=00.0377, mean=01.2875} policy_loss=-10.1933 policy updated! \n",
      "train step 05768 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6287 diff={max=08.7351, min=00.0141, mean=01.5911} policy_loss=-8.5478 policy updated! \n",
      "train step 05769 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6453 diff={max=07.9521, min=00.0343, mean=01.3977} policy_loss=-8.8145 policy updated! \n",
      "train step 05770 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.7367 diff={max=10.5142, min=00.0197, mean=01.1588} policy_loss=-8.6596 policy updated! \n",
      "train step 05771 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9975 diff={max=06.9901, min=00.0152, mean=01.4327} policy_loss=-9.2432 policy updated! \n",
      "train step 05772 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0434 diff={max=05.4676, min=00.0616, mean=00.9777} policy_loss=-8.9417 policy updated! \n",
      "train step 05773 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8360 diff={max=05.1267, min=00.0688, mean=00.9073} policy_loss=-9.0691 policy updated! \n",
      "train step 05774 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6554 diff={max=05.0963, min=00.0007, mean=01.3392} policy_loss=-8.8224 policy updated! \n",
      "train step 05775 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.9037 diff={max=09.4134, min=00.0088, mean=01.2179} policy_loss=-7.6642 policy updated! \n",
      "train step 05776 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8985 diff={max=07.2425, min=00.0330, mean=01.1156} policy_loss=-10.1520 policy updated! \n",
      "train step 05777 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1641 diff={max=06.2094, min=00.0009, mean=01.1355} policy_loss=-9.5898 policy updated! \n",
      "train step 05778 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7722 diff={max=07.0136, min=00.0608, mean=01.2588} policy_loss=-8.7959 policy updated! \n",
      "train step 05779 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5591 diff={max=05.5971, min=00.0188, mean=01.3012} policy_loss=-9.3859 policy updated! \n",
      "train step 05780 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.4001 diff={max=07.0778, min=00.0060, mean=01.0746} policy_loss=-9.3305 policy updated! \n",
      "train step 05781 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6541 diff={max=05.4581, min=00.0262, mean=01.0371} policy_loss=-8.0579 policy updated! \n",
      "train step 05782 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1976 diff={max=02.9393, min=00.0159, mean=00.7997} policy_loss=-7.7138 policy updated! \n",
      "train step 05783 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1913 diff={max=05.5316, min=00.0062, mean=01.0131} policy_loss=-8.4374 policy updated! \n",
      "train step 05784 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6737 diff={max=07.8743, min=00.0010, mean=00.9967} policy_loss=-9.6323 policy updated! \n",
      "train step 05785 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.6177 diff={max=09.1404, min=00.0014, mean=01.0426} policy_loss=-9.0841 policy updated! \n",
      "train step 05786 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4186 diff={max=08.2517, min=00.0139, mean=01.1484} policy_loss=-9.0492 policy updated! \n",
      "train step 05787 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6714 diff={max=04.4830, min=00.0022, mean=00.9034} policy_loss=-8.7205 policy updated! \n",
      "train step 05788 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=01.7120 diff={max=04.8760, min=00.0257, mean=00.9463} policy_loss=-9.3011 policy updated! \n",
      "train step 05789 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7758 diff={max=05.1464, min=00.0086, mean=00.8435} policy_loss=-9.2046 policy updated! \n",
      "train step 05790 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.2462 diff={max=05.8229, min=00.0082, mean=01.2033} policy_loss=-9.6826 policy updated! \n",
      "train step 05791 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2926 diff={max=06.5774, min=00.0006, mean=01.1049} policy_loss=-8.5652 policy updated! \n",
      "train step 05792 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2409 diff={max=04.7072, min=00.0142, mean=01.0432} policy_loss=-8.3516 policy updated! \n",
      "train step 05793 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.2866 diff={max=08.3674, min=00.0348, mean=01.6821} policy_loss=-9.1143 policy updated! \n",
      "train step 05794 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9924 diff={max=07.0846, min=00.0160, mean=01.4604} policy_loss=-8.4472 policy updated! \n",
      "train step 05795 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.5650 diff={max=03.5432, min=00.0581, mean=00.9252} policy_loss=-7.5171 policy updated! \n",
      "train step 05796 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1077 diff={max=05.5042, min=00.0062, mean=00.9261} policy_loss=-7.4016 policy updated! \n",
      "train step 05797 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8771 diff={max=09.7247, min=00.0507, mean=01.2144} policy_loss=-8.5204 policy updated! \n",
      "train step 05798 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6479 diff={max=06.8825, min=00.0300, mean=00.9721} policy_loss=-9.0513 policy updated! \n",
      "train step 05799 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2124 diff={max=03.4420, min=00.0070, mean=00.7780} policy_loss=-8.9484 policy updated! \n",
      "train step 05800 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=05.2054 diff={max=06.8980, min=00.0461, mean=01.4928} policy_loss=-9.7122 policy updated! \n",
      "train step 05801 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8256 diff={max=07.0299, min=00.0015, mean=01.1497} policy_loss=-9.2898 policy updated! \n",
      "train step 05802 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4462 diff={max=04.4717, min=00.1099, mean=00.9144} policy_loss=-9.7256 policy updated! \n",
      "train step 05803 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=02.6990 diff={max=06.0483, min=00.0292, mean=01.1142} policy_loss=-8.4125 policy updated! \n",
      "train step 05804 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1085 diff={max=04.5137, min=00.0185, mean=00.9634} policy_loss=-9.6242 policy updated! \n",
      "train step 05805 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.2127 diff={max=08.9180, min=00.0044, mean=01.1159} policy_loss=-9.1276 policy updated! \n",
      "train step 05806 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4754 diff={max=06.8769, min=00.0218, mean=01.1049} policy_loss=-10.7187 policy updated! \n",
      "train step 05807 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2286 diff={max=06.1967, min=00.0087, mean=01.1940} policy_loss=-9.8191 policy updated! \n",
      "train step 05808 reward={max=08.0000, min=00.0000, mean=03.2000} optimizing loss=02.2756 diff={max=04.6705, min=00.0111, mean=00.9833} policy_loss=-8.6501 policy updated! \n",
      "train step 05809 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6090 diff={max=05.7360, min=00.0193, mean=01.0917} policy_loss=-10.2858 policy updated! \n",
      "train step 05810 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.0922 diff={max=08.5617, min=00.0067, mean=01.0742} policy_loss=-11.3843 policy updated! \n",
      "train step 05811 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9875 diff={max=05.7266, min=00.0013, mean=01.1495} policy_loss=-8.3892 policy updated! \n",
      "train step 05812 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7361 diff={max=08.0252, min=00.0129, mean=01.1318} policy_loss=-9.2125 policy updated! \n",
      "train step 05813 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6029 diff={max=12.0666, min=00.0091, mean=01.2740} policy_loss=-7.8390 policy updated! \n",
      "train step 05814 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5124 diff={max=04.5739, min=00.0004, mean=00.7882} policy_loss=-9.1876 policy updated! \n",
      "train step 05815 reward={max=09.0000, min=09.0000, mean=09.0000} optimizing loss=03.3906 diff={max=05.6729, min=00.0058, mean=01.1793} policy_loss=-8.8217 policy updated! \n",
      "train step 05816 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0849 diff={max=04.6172, min=00.0037, mean=00.9684} policy_loss=-8.6356 policy updated! \n",
      "train step 05817 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0603 diff={max=06.3297, min=00.0390, mean=00.8308} policy_loss=-9.4841 policy updated! \n",
      "train step 05818 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.8644 diff={max=05.3743, min=00.0245, mean=00.8331} policy_loss=-9.7280 policy updated! \n",
      "train step 05819 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3447 diff={max=08.8104, min=00.0532, mean=01.1744} policy_loss=-8.1359 policy updated! \n",
      "train step 05820 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.9808 diff={max=07.7842, min=00.0174, mean=01.3185} policy_loss=-9.4293 policy updated! \n",
      "train step 05821 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3996 diff={max=03.7063, min=00.0361, mean=00.8219} policy_loss=-10.0163 policy updated! \n",
      "train step 05822 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7541 diff={max=09.8457, min=00.0122, mean=00.9106} policy_loss=-8.3300 policy updated! \n",
      "train step 05823 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8642 diff={max=07.0048, min=00.0003, mean=00.8141} policy_loss=-9.0506 policy updated! \n",
      "train step 05824 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7746 diff={max=05.0279, min=00.0021, mean=01.1084} policy_loss=-9.1355 policy updated! \n",
      "train step 05825 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=05.6497 diff={max=07.8144, min=00.0105, mean=01.3891} policy_loss=-8.7759 policy updated! \n",
      "train step 05826 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3024 diff={max=06.5351, min=00.0587, mean=01.2158} policy_loss=-8.8465 policy updated! \n",
      "train step 05827 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8325 diff={max=06.3263, min=00.0268, mean=01.1971} policy_loss=-7.9654 policy updated! \n",
      "train step 05828 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9115 diff={max=06.4125, min=00.0116, mean=01.0075} policy_loss=-8.1818 policy updated! \n",
      "train step 05829 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4985 diff={max=07.7943, min=00.0106, mean=00.9490} policy_loss=-8.1962 policy updated! \n",
      "train step 05830 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.1007 diff={max=06.4256, min=00.0026, mean=01.2047} policy_loss=-8.7762 policy updated! \n",
      "train step 05831 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0073 diff={max=05.8396, min=00.0604, mean=00.9512} policy_loss=-9.1870 policy updated! \n",
      "train step 05832 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8057 diff={max=05.2826, min=00.0068, mean=01.3743} policy_loss=-9.9157 policy updated! \n",
      "train step 05833 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3297 diff={max=10.8838, min=00.0566, mean=00.9886} policy_loss=-9.6989 policy updated! \n",
      "train step 05834 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6347 diff={max=07.4993, min=00.0403, mean=01.0147} policy_loss=-8.0841 policy updated! \n",
      "train step 05835 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.0204 diff={max=06.1882, min=00.0057, mean=01.1368} policy_loss=-9.3942 policy updated! \n",
      "train step 05836 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1789 diff={max=09.0796, min=00.0017, mean=01.1786} policy_loss=-8.0560 policy updated! \n",
      "train step 05837 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=04.4278 diff={max=08.5022, min=00.0393, mean=01.3692} policy_loss=-9.3219 policy updated! \n",
      "train step 05838 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8555 diff={max=07.4261, min=00.0102, mean=01.0243} policy_loss=-7.5819 policy updated! \n",
      "train step 05839 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3815 diff={max=08.3715, min=00.0059, mean=01.0810} policy_loss=-8.6993 policy updated! \n",
      "train step 05840 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.7218 diff={max=09.4223, min=00.0231, mean=01.1483} policy_loss=-9.0040 policy updated! \n",
      "train step 05841 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6105 diff={max=05.2675, min=00.0168, mean=01.2119} policy_loss=-9.2582 policy updated! \n",
      "train step 05842 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1138 diff={max=05.5794, min=00.0043, mean=01.1564} policy_loss=-7.9924 policy updated! \n",
      "train step 05843 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2477 diff={max=08.8887, min=00.0022, mean=00.7429} policy_loss=-9.2769 policy updated! \n",
      "train step 05844 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1890 diff={max=05.5208, min=00.0040, mean=00.9765} policy_loss=-9.1038 policy updated! \n",
      "train step 05845 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=05.0077 diff={max=08.0903, min=00.0361, mean=01.3055} policy_loss=-9.0380 policy updated! \n",
      "train step 05846 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1557 diff={max=07.5774, min=00.0288, mean=00.8963} policy_loss=-9.2907 policy updated! \n",
      "train step 05847 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3636 diff={max=05.8634, min=00.0343, mean=00.9701} policy_loss=-7.9627 policy updated! \n",
      "train step 05848 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9325 diff={max=04.0631, min=00.0350, mean=00.9877} policy_loss=-8.6964 policy updated! \n",
      "train step 05849 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9477 diff={max=09.3531, min=00.0011, mean=01.2440} policy_loss=-9.3109 policy updated! \n",
      "train step 05850 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=05.5301 diff={max=07.5402, min=00.0115, mean=01.4105} policy_loss=-10.2834 policy updated! \n",
      "train step 05851 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1984 diff={max=08.3091, min=00.0553, mean=01.2619} policy_loss=-9.9791 policy updated! \n",
      "train step 05852 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4000 diff={max=08.5950, min=00.0117, mean=00.8949} policy_loss=-8.7368 policy updated! \n",
      "train step 05853 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9390 diff={max=05.8845, min=00.0396, mean=01.0939} policy_loss=-8.4192 policy updated! \n",
      "train step 05854 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0396 diff={max=04.7364, min=00.0008, mean=00.9381} policy_loss=-7.9107 policy updated! \n",
      "train step 05855 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.4079 diff={max=04.4480, min=00.0104, mean=00.7745} policy_loss=-8.2424 policy updated! \n",
      "train step 05856 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1162 diff={max=07.7380, min=00.0041, mean=00.8341} policy_loss=-7.7081 policy updated! \n",
      "train step 05857 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9261 diff={max=07.3152, min=00.0584, mean=01.0253} policy_loss=-8.0122 policy updated! \n",
      "train step 05858 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4135 diff={max=05.4176, min=00.0137, mean=01.0096} policy_loss=-9.5428 policy updated! \n",
      "train step 05859 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1715 diff={max=08.5538, min=00.0108, mean=01.3041} policy_loss=-10.0812 policy updated! \n",
      "train step 05860 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.0284 diff={max=05.5271, min=00.0123, mean=00.8898} policy_loss=-8.0197 policy updated! \n",
      "train step 05861 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1947 diff={max=06.2226, min=00.0403, mean=01.2182} policy_loss=-10.0654 policy updated! \n",
      "train step 05862 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4479 diff={max=04.7045, min=00.0439, mean=01.1531} policy_loss=-10.0749 policy updated! \n",
      "train step 05863 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=03.1918 diff={max=07.4932, min=00.0015, mean=01.0239} policy_loss=-7.8187 policy updated! \n",
      "train step 05864 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4884 diff={max=13.1452, min=00.0004, mean=01.2342} policy_loss=-7.9100 policy updated! \n",
      "train step 05865 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=05.8820 diff={max=08.2084, min=00.0002, mean=01.4062} policy_loss=-7.7495 policy updated! \n",
      "train step 05866 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1546 diff={max=04.9686, min=00.0298, mean=01.2676} policy_loss=-10.3787 policy updated! \n",
      "train step 05867 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8957 diff={max=05.8069, min=00.0205, mean=01.0788} policy_loss=-7.3319 policy updated! \n",
      "train step 05868 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3261 diff={max=06.3158, min=00.0075, mean=01.2106} policy_loss=-7.8184 policy updated! \n",
      "train step 05869 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.0807 diff={max=10.4095, min=00.0559, mean=01.7280} policy_loss=-11.2395 policy updated! \n",
      "train step 05870 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.0103 diff={max=08.9394, min=00.0179, mean=01.0885} policy_loss=-9.5074 policy updated! \n",
      "train step 05871 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6431 diff={max=05.9169, min=00.0203, mean=01.1184} policy_loss=-9.1356 policy updated! \n",
      "train step 05872 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9219 diff={max=05.9235, min=00.0002, mean=01.1090} policy_loss=-7.4191 policy updated! \n",
      "train step 05873 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4606 diff={max=04.3343, min=00.0038, mean=00.8659} policy_loss=-8.4804 policy updated! \n",
      "train step 05874 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5771 diff={max=07.8935, min=00.0212, mean=01.0623} policy_loss=-8.4424 policy updated! \n",
      "train step 05875 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.4674 diff={max=08.5802, min=00.0068, mean=00.9615} policy_loss=-8.5277 policy updated! \n",
      "train step 05876 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5733 diff={max=06.5971, min=00.0051, mean=01.1473} policy_loss=-9.3840 policy updated! \n",
      "train step 05877 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4472 diff={max=04.1594, min=00.0170, mean=00.8728} policy_loss=-9.1882 policy updated! \n",
      "train step 05878 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.8868 diff={max=08.7034, min=00.0283, mean=01.2011} policy_loss=-8.1763 policy updated! \n",
      "train step 05879 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7545 diff={max=02.2227, min=00.0349, mean=00.6713} policy_loss=-8.5403 policy updated! \n",
      "train step 05880 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=03.2930 diff={max=06.0051, min=00.0003, mean=01.0975} policy_loss=-8.1305 policy updated! \n",
      "train step 05881 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8248 diff={max=05.3185, min=00.0410, mean=01.1484} policy_loss=-8.8765 policy updated! \n",
      "train step 05882 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8441 diff={max=05.8444, min=00.0023, mean=01.1808} policy_loss=-9.3775 policy updated! \n",
      "train step 05883 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0708 diff={max=07.7178, min=00.0315, mean=01.5553} policy_loss=-8.7267 policy updated! \n",
      "train step 05884 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6015 diff={max=05.9712, min=00.0130, mean=01.1872} policy_loss=-9.1295 policy updated! \n",
      "train step 05885 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=05.4385 diff={max=11.3021, min=00.0187, mean=01.3107} policy_loss=-7.3973 policy updated! \n",
      "train step 05886 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6345 diff={max=12.1337, min=00.0253, mean=01.2105} policy_loss=-7.7859 policy updated! \n",
      "train step 05887 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0084 diff={max=07.5897, min=00.0244, mean=01.4790} policy_loss=-9.6220 policy updated! \n",
      "train step 05888 reward={max=09.0000, min=00.0000, mean=03.4000} optimizing loss=02.0390 diff={max=03.6559, min=00.0234, mean=01.0492} policy_loss=-9.9808 policy updated! \n",
      "train step 05889 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6350 diff={max=08.9252, min=00.0179, mean=01.4141} policy_loss=-8.4884 policy updated! \n",
      "train step 05890 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=04.9568 diff={max=09.8448, min=00.0161, mean=01.2238} policy_loss=-7.2385 policy updated! \n",
      "train step 05891 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8775 diff={max=08.3326, min=00.0034, mean=01.0142} policy_loss=-7.0950 policy updated! \n",
      "train step 05892 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5852 diff={max=05.6103, min=00.0070, mean=01.1972} policy_loss=-8.6839 policy updated! \n",
      "train step 05893 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9257 diff={max=05.6151, min=00.0190, mean=01.1423} policy_loss=-10.1866 policy updated! \n",
      "train step 05894 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7733 diff={max=08.3986, min=00.0116, mean=01.5276} policy_loss=-9.8834 policy updated! \n",
      "train step 05895 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=05.0130 diff={max=10.1773, min=00.0409, mean=01.3935} policy_loss=-10.2816 policy updated! \n",
      "train step 05896 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=04.5565 diff={max=05.7644, min=00.0008, mean=01.3902} policy_loss=-8.0689 policy updated! \n",
      "train step 05897 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9001 diff={max=08.9457, min=00.0135, mean=01.2559} policy_loss=-9.2251 policy updated! \n",
      "train step 05898 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7405 diff={max=08.5802, min=00.0184, mean=01.1998} policy_loss=-8.1579 policy updated! \n",
      "train step 05899 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=02.3694 diff={max=06.2559, min=00.0041, mean=01.0232} policy_loss=-10.0716 policy updated! \n",
      "train step 05900 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.9790 diff={max=04.9582, min=00.0359, mean=00.9221} policy_loss=-7.7275 policy updated! \n",
      "train step 05901 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8328 diff={max=06.7245, min=00.0026, mean=01.4040} policy_loss=-7.3683 policy updated! \n",
      "train step 05902 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3341 diff={max=04.6503, min=00.0219, mean=01.0423} policy_loss=-8.5955 policy updated! \n",
      "train step 05903 reward={max=08.0000, min=00.0000, mean=03.2000} optimizing loss=04.8689 diff={max=09.4482, min=00.0226, mean=01.2714} policy_loss=-8.2971 policy updated! \n",
      "train step 05904 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2754 diff={max=09.3195, min=00.0197, mean=01.2306} policy_loss=-7.9762 policy updated! \n",
      "train step 05905 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.3396 diff={max=07.1454, min=00.0222, mean=01.1536} policy_loss=-8.5074 policy updated! \n",
      "train step 05906 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=04.2734 diff={max=10.4673, min=00.0047, mean=01.0658} policy_loss=-8.6238 policy updated! \n",
      "train step 05907 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5718 diff={max=08.5049, min=00.0037, mean=01.2162} policy_loss=-10.6167 policy updated! \n",
      "train step 05908 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3103 diff={max=06.9707, min=00.0491, mean=01.2635} policy_loss=-9.9817 policy updated! \n",
      "train step 05909 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8709 diff={max=07.4085, min=00.0011, mean=01.2088} policy_loss=-8.6584 policy updated! \n",
      "train step 05910 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=02.8107 diff={max=05.4490, min=00.0289, mean=01.1593} policy_loss=-8.6519 policy updated! \n",
      "train step 05911 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=01.8320 diff={max=04.9266, min=00.0001, mean=00.8069} policy_loss=-8.7532 policy updated! \n",
      "train step 05912 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1106 diff={max=09.2435, min=00.0026, mean=01.4885} policy_loss=-7.3163 policy updated! \n",
      "train step 05913 reward={max=08.0000, min=00.0000, mean=03.2000} optimizing loss=02.7192 diff={max=03.8838, min=00.0484, mean=01.1600} policy_loss=-10.3480 policy updated! \n",
      "train step 05914 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9521 diff={max=06.1377, min=00.0162, mean=01.5563} policy_loss=-8.0610 policy updated! \n",
      "train step 05915 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=06.5585 diff={max=09.7991, min=00.0209, mean=01.4805} policy_loss=-9.2971 policy updated! \n",
      "train step 05916 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5525 diff={max=07.7273, min=00.0326, mean=00.9408} policy_loss=-9.5837 policy updated! \n",
      "train step 05917 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9576 diff={max=07.7981, min=00.0663, mean=01.3572} policy_loss=-9.5818 policy updated! \n",
      "train step 05918 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5491 diff={max=06.0821, min=00.0021, mean=01.1758} policy_loss=-9.9287 policy updated! \n",
      "train step 05919 reward={max=09.0000, min=00.0000, mean=06.6000} optimizing loss=04.2563 diff={max=09.6719, min=00.0020, mean=01.1834} policy_loss=-9.0120 policy updated! \n",
      "train step 05920 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.3929 diff={max=05.5792, min=00.0234, mean=01.2255} policy_loss=-9.9951 policy updated! \n",
      "train step 05921 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0622 diff={max=05.0516, min=00.0214, mean=01.4539} policy_loss=-9.5626 policy updated! \n",
      "train step 05922 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4360 diff={max=08.8525, min=00.0027, mean=01.0405} policy_loss=-9.1505 policy updated! \n",
      "train step 05923 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=02.9367 diff={max=07.1094, min=00.0009, mean=01.1108} policy_loss=-8.3102 policy updated! \n",
      "train step 05924 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5672 diff={max=04.8932, min=00.0130, mean=01.1970} policy_loss=-8.7980 policy updated! \n",
      "train step 05925 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=05.3450 diff={max=11.5070, min=00.0052, mean=01.3148} policy_loss=-7.4393 policy updated! \n",
      "train step 05926 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6315 diff={max=09.2026, min=00.0402, mean=01.1857} policy_loss=-8.8673 policy updated! \n",
      "train step 05927 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6034 diff={max=06.5712, min=00.0198, mean=01.2788} policy_loss=-8.7422 policy updated! \n",
      "train step 05928 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7352 diff={max=08.2871, min=00.0125, mean=01.1110} policy_loss=-8.6912 policy updated! \n",
      "train step 05929 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9250 diff={max=04.4555, min=00.0028, mean=01.0029} policy_loss=-9.0758 policy updated! \n",
      "train step 05930 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.4664 diff={max=07.6936, min=00.0035, mean=01.1034} policy_loss=-8.6204 policy updated! \n",
      "train step 05931 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8202 diff={max=05.2914, min=00.0086, mean=01.2853} policy_loss=-10.4477 policy updated! \n",
      "train step 05932 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7047 diff={max=06.0729, min=00.0144, mean=01.4622} policy_loss=-9.9137 policy updated! \n",
      "train step 05933 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=04.6503 diff={max=09.4843, min=00.0450, mean=01.2200} policy_loss=-8.6641 policy updated! \n",
      "train step 05934 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1671 diff={max=08.7699, min=00.0005, mean=01.2139} policy_loss=-7.7875 policy updated! \n",
      "train step 05935 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.8532 diff={max=09.9453, min=00.0055, mean=01.1639} policy_loss=-8.4770 policy updated! \n",
      "train step 05936 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1262 diff={max=09.3450, min=00.0109, mean=01.1200} policy_loss=-7.3316 policy updated! \n",
      "train step 05937 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7530 diff={max=03.3788, min=00.0058, mean=00.9785} policy_loss=-7.9849 policy updated! \n",
      "train step 05938 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5771 diff={max=07.3045, min=00.0023, mean=01.6362} policy_loss=-9.1518 policy updated! \n",
      "train step 05939 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3906 diff={max=04.9677, min=00.0199, mean=01.0785} policy_loss=-9.7120 policy updated! \n",
      "train step 05940 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.1947 diff={max=06.6013, min=00.0205, mean=01.0391} policy_loss=-9.5393 policy updated! \n",
      "train step 05941 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7557 diff={max=10.1063, min=00.0050, mean=01.2873} policy_loss=-9.9021 policy updated! \n",
      "train step 05942 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3431 diff={max=09.4338, min=00.0116, mean=01.3074} policy_loss=-9.6257 policy updated! \n",
      "train step 05943 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6011 diff={max=04.5960, min=00.0019, mean=00.8834} policy_loss=-8.7929 policy updated! \n",
      "train step 05944 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7461 diff={max=05.7451, min=00.0012, mean=01.1958} policy_loss=-7.9729 policy updated! \n",
      "train step 05945 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=05.0930 diff={max=11.9207, min=00.0059, mean=01.1883} policy_loss=-7.8038 policy updated! \n",
      "train step 05946 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=03.0362 diff={max=07.7380, min=00.0039, mean=00.9918} policy_loss=-7.4658 policy updated! \n",
      "train step 05947 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2894 diff={max=05.7371, min=00.0491, mean=01.0483} policy_loss=-9.6216 policy updated! \n",
      "train step 05948 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2292 diff={max=06.0373, min=00.0164, mean=01.0026} policy_loss=-7.6911 policy updated! \n",
      "train step 05949 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6272 diff={max=03.5928, min=00.0242, mean=00.9072} policy_loss=-7.3318 policy updated! \n",
      "train step 05950 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.8882 diff={max=06.5794, min=00.0744, mean=01.1021} policy_loss=-10.5972 policy updated! \n",
      "train step 05951 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9652 diff={max=05.2636, min=00.0454, mean=00.9846} policy_loss=-9.9450 policy updated! \n",
      "train step 05952 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7279 diff={max=08.2493, min=00.0349, mean=01.1385} policy_loss=-8.3052 policy updated! \n",
      "train step 05953 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7853 diff={max=06.6889, min=00.0220, mean=01.2600} policy_loss=-9.8941 policy updated! \n",
      "train step 05954 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2461 diff={max=04.9775, min=00.0425, mean=01.0285} policy_loss=-9.0435 policy updated! \n",
      "train step 05955 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.7064 diff={max=05.0551, min=00.0042, mean=01.0960} policy_loss=-8.1538 policy updated! \n",
      "train step 05956 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0478 diff={max=07.7676, min=00.0081, mean=01.0653} policy_loss=-10.0602 policy updated! \n",
      "train step 05957 reward={max=09.0000, min=08.0000, mean=08.4000} optimizing loss=04.6228 diff={max=08.9569, min=00.0173, mean=01.1992} policy_loss=-9.3223 policy updated! \n",
      "train step 05958 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=02.7403 diff={max=05.6842, min=00.0135, mean=01.1040} policy_loss=-8.2593 policy updated! \n",
      "train step 05959 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6646 diff={max=08.8206, min=00.0401, mean=01.5031} policy_loss=-8.5035 policy updated! \n",
      "train step 05960 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.9790 diff={max=08.3206, min=00.0426, mean=01.3129} policy_loss=-10.0858 policy updated! \n",
      "train step 05961 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4433 diff={max=12.1731, min=00.0169, mean=01.3319} policy_loss=-6.8774 policy updated! \n",
      "train step 05962 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3603 diff={max=11.2256, min=00.0019, mean=00.8867} policy_loss=-7.7913 policy updated! \n",
      "train step 05963 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7351 diff={max=04.9637, min=00.0092, mean=00.8461} policy_loss=-9.6597 policy updated! \n",
      "train step 05964 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1223 diff={max=06.6802, min=00.0348, mean=00.8631} policy_loss=-10.5185 policy updated! \n",
      "train step 05965 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=04.7332 diff={max=08.1813, min=00.0625, mean=01.2824} policy_loss=-9.5105 policy updated! \n",
      "train step 05966 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4952 diff={max=05.5932, min=00.0618, mean=01.0132} policy_loss=-8.8060 policy updated! \n",
      "train step 05967 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0492 diff={max=08.1787, min=00.0212, mean=01.0738} policy_loss=-9.5779 policy updated! \n",
      "train step 05968 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4403 diff={max=04.1217, min=00.0722, mean=01.1094} policy_loss=-8.6499 policy updated! \n",
      "train step 05969 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5994 diff={max=05.6353, min=00.0008, mean=00.9935} policy_loss=-9.1796 policy updated! \n",
      "train step 05970 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.3752 diff={max=05.5981, min=00.0042, mean=01.2571} policy_loss=-8.7075 policy updated! \n",
      "train step 05971 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=04.4044 diff={max=07.1125, min=00.0046, mean=01.1979} policy_loss=-8.6000 policy updated! \n",
      "train step 05972 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5500 diff={max=03.5793, min=00.0005, mean=00.9528} policy_loss=-9.2759 policy updated! \n",
      "train step 05973 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0605 diff={max=10.3554, min=00.0277, mean=01.1218} policy_loss=-7.6373 policy updated! \n",
      "train step 05974 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7361 diff={max=10.4838, min=00.0178, mean=01.1100} policy_loss=-10.0911 policy updated! \n",
      "train step 05975 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=05.6544 diff={max=09.0349, min=00.0232, mean=01.3115} policy_loss=-9.6928 policy updated! \n",
      "train step 05976 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5025 diff={max=08.9597, min=00.0054, mean=01.2161} policy_loss=-8.4642 policy updated! \n",
      "train step 05977 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8518 diff={max=04.0983, min=00.0074, mean=00.9551} policy_loss=-8.4431 policy updated! \n",
      "train step 05978 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8610 diff={max=08.2153, min=00.0003, mean=01.1993} policy_loss=-10.0168 policy updated! \n",
      "train step 05979 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7670 diff={max=04.5946, min=00.0101, mean=00.8765} policy_loss=-8.6478 policy updated! \n",
      "train step 05980 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.1607 diff={max=07.0365, min=00.0138, mean=00.8886} policy_loss=-8.4938 policy updated! \n",
      "train step 05981 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8781 diff={max=06.8481, min=00.0464, mean=01.0897} policy_loss=-9.6027 policy updated! \n",
      "train step 05982 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8786 diff={max=05.6415, min=00.0128, mean=01.0746} policy_loss=-9.3087 policy updated! \n",
      "train step 05983 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=02.7413 diff={max=04.8586, min=00.0093, mean=01.0735} policy_loss=-9.8823 policy updated! \n",
      "train step 05984 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8419 diff={max=07.3677, min=00.0402, mean=01.0510} policy_loss=-9.5177 policy updated! \n",
      "train step 05985 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.8715 diff={max=05.6140, min=00.0493, mean=00.8613} policy_loss=-8.5012 policy updated! \n",
      "train step 05986 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=03.1884 diff={max=08.5088, min=00.0599, mean=01.0462} policy_loss=-10.3640 policy updated! \n",
      "train step 05987 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2498 diff={max=11.0960, min=00.0078, mean=01.1857} policy_loss=-8.7664 policy updated! \n",
      "train step 05988 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=05.1769 diff={max=09.2064, min=00.0038, mean=01.2428} policy_loss=-7.6578 policy updated! \n",
      "train step 05989 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8290 diff={max=05.8898, min=00.0105, mean=01.0135} policy_loss=-8.8616 policy updated! \n",
      "train step 05990 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.1314 diff={max=07.6147, min=00.0670, mean=01.0747} policy_loss=-10.6550 policy updated! \n",
      "train step 05991 reward={max=09.0000, min=00.0000, mean=05.4000} optimizing loss=03.3148 diff={max=08.2726, min=00.0142, mean=01.0422} policy_loss=-7.5669 policy updated! \n",
      "train step 05992 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1381 diff={max=09.3301, min=00.0055, mean=01.2252} policy_loss=-8.9069 policy updated! \n",
      "train step 05993 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4265 diff={max=03.4306, min=00.0130, mean=00.8888} policy_loss=-9.7141 policy updated! \n",
      "train step 05994 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6116 diff={max=07.5499, min=00.0057, mean=01.1514} policy_loss=-10.7882 policy updated! \n",
      "train step 05995 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=06.9123 diff={max=10.5784, min=00.0250, mean=01.4830} policy_loss=-10.3446 policy updated! \n",
      "train step 05996 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9233 diff={max=04.7106, min=00.0107, mean=00.9564} policy_loss=-7.5603 policy updated! \n",
      "train step 05997 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7480 diff={max=05.2861, min=00.0409, mean=00.8752} policy_loss=-9.7030 policy updated! \n",
      "train step 05998 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2786 diff={max=06.0904, min=00.0010, mean=01.2327} policy_loss=-9.9538 policy updated! \n",
      "train step 05999 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7946 diff={max=09.7322, min=00.0247, mean=01.3188} policy_loss=-9.8645 policy updated! \n",
      "train step 06000 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.7460 diff={max=08.3339, min=00.0085, mean=01.1104} policy_loss=-10.1192 policy updated! \n",
      "train step 06001 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3763 diff={max=05.8723, min=00.0004, mean=00.9446} policy_loss=-8.7341 policy updated! \n",
      "train step 06002 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5081 diff={max=07.3399, min=00.0335, mean=01.2907} policy_loss=-9.8479 policy updated! \n",
      "train step 06003 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=04.1987 diff={max=07.1744, min=00.0087, mean=01.3614} policy_loss=-10.8830 policy updated! \n",
      "train step 06004 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6087 diff={max=03.3927, min=00.0107, mean=00.9830} policy_loss=-8.8922 policy updated! \n",
      "train step 06005 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=04.5023 diff={max=07.3822, min=00.0026, mean=01.2515} policy_loss=-9.5242 policy updated! \n",
      "train step 06006 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9880 diff={max=05.4308, min=00.0094, mean=01.1508} policy_loss=-9.7522 policy updated! \n",
      "train step 06007 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4674 diff={max=04.4609, min=00.0100, mean=01.1242} policy_loss=-8.6723 policy updated! \n",
      "train step 06008 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4428 diff={max=06.3968, min=00.0276, mean=01.0410} policy_loss=-8.6469 policy updated! \n",
      "train step 06009 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6263 diff={max=03.6581, min=00.0047, mean=00.8806} policy_loss=-9.2672 policy updated! \n",
      "train step 06010 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.0014 diff={max=06.9720, min=00.0006, mean=01.0653} policy_loss=-9.1539 policy updated! \n",
      "train step 06011 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5384 diff={max=09.8889, min=00.0310, mean=01.1927} policy_loss=-10.0288 policy updated! \n",
      "train step 06012 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8553 diff={max=05.5452, min=00.0107, mean=01.0369} policy_loss=-9.7194 policy updated! \n",
      "train step 06013 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4543 diff={max=06.3341, min=00.0018, mean=01.0506} policy_loss=-10.2617 policy updated! \n",
      "train step 06014 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9666 diff={max=05.5067, min=00.0459, mean=00.9102} policy_loss=-9.5892 policy updated! \n",
      "train step 06015 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.1909 diff={max=09.7407, min=00.0784, mean=00.9913} policy_loss=-10.1522 policy updated! \n",
      "train step 06016 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2644 diff={max=08.3282, min=00.0143, mean=01.0402} policy_loss=-8.9989 policy updated! \n",
      "train step 06017 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5777 diff={max=04.8916, min=00.0086, mean=01.1902} policy_loss=-10.6524 policy updated! \n",
      "train step 06018 reward={max=08.0000, min=00.0000, mean=03.2000} optimizing loss=03.5735 diff={max=06.8498, min=00.0384, mean=01.1099} policy_loss=-8.3831 policy updated! \n",
      "train step 06019 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9642 diff={max=09.9758, min=00.0368, mean=01.4775} policy_loss=-9.4476 policy updated! \n",
      "train step 06020 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.6455 diff={max=07.3941, min=00.0016, mean=01.3669} policy_loss=-10.1628 policy updated! \n",
      "train step 06021 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3830 diff={max=08.9938, min=00.0475, mean=00.7713} policy_loss=-7.4711 policy updated! \n",
      "train step 06022 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0264 diff={max=07.1317, min=00.0146, mean=01.0370} policy_loss=-9.4030 policy updated! \n",
      "train step 06023 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6098 diff={max=07.4237, min=00.0171, mean=01.1519} policy_loss=-7.2444 policy updated! \n",
      "train step 06024 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8315 diff={max=04.8032, min=00.0138, mean=01.2665} policy_loss=-8.9926 policy updated! \n",
      "train step 06025 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.7401 diff={max=07.1617, min=00.0088, mean=01.1937} policy_loss=-9.9939 policy updated! \n",
      "train step 06026 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8355 diff={max=04.3326, min=00.0497, mean=00.9531} policy_loss=-8.3334 policy updated! \n",
      "train step 06027 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3995 diff={max=07.6087, min=00.0285, mean=01.0854} policy_loss=-8.8619 policy updated! \n",
      "train step 06028 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3705 diff={max=05.2118, min=00.0408, mean=01.3188} policy_loss=-8.9287 policy updated! \n",
      "train step 06029 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=07.2146 diff={max=09.6140, min=00.0085, mean=01.5094} policy_loss=-8.4629 policy updated! \n",
      "train step 06030 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=05.4345 diff={max=07.0591, min=00.0005, mean=01.3740} policy_loss=-10.8870 policy updated! \n",
      "train step 06031 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=03.7626 diff={max=07.0572, min=00.0710, mean=01.2213} policy_loss=-8.2606 policy updated! \n",
      "train step 06032 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0971 diff={max=02.9633, min=00.0109, mean=00.7405} policy_loss=-8.9114 policy updated! \n",
      "train step 06033 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9885 diff={max=05.2732, min=00.0133, mean=00.9817} policy_loss=-9.1325 policy updated! \n",
      "train step 06034 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3691 diff={max=08.4838, min=00.0391, mean=00.9890} policy_loss=-7.9414 policy updated! \n",
      "train step 06035 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.8990 diff={max=08.1826, min=00.0152, mean=01.2141} policy_loss=-8.8565 policy updated! \n",
      "train step 06036 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.2541 diff={max=09.6426, min=00.0303, mean=01.6518} policy_loss=-8.6705 policy updated! \n",
      "train step 06037 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9768 diff={max=06.6861, min=00.0025, mean=00.7881} policy_loss=-9.9999 policy updated! \n",
      "train step 06038 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9785 diff={max=09.0630, min=00.0216, mean=00.9759} policy_loss=-9.1406 policy updated! \n",
      "train step 06039 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.5263 diff={max=11.5429, min=00.0058, mean=01.4218} policy_loss=-10.1608 policy updated! \n",
      "train step 06040 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.6947 diff={max=04.4099, min=00.0437, mean=00.9355} policy_loss=-8.3543 policy updated! \n",
      "train step 06041 reward={max=09.0000, min=09.0000, mean=09.0000} optimizing loss=03.8198 diff={max=06.3947, min=00.0242, mean=01.1629} policy_loss=-8.4479 policy updated! \n",
      "train step 06042 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0402 diff={max=07.2807, min=00.0151, mean=00.8422} policy_loss=-7.9641 policy updated! \n",
      "train step 06043 reward={max=08.0000, min=00.0000, mean=03.2000} optimizing loss=02.5153 diff={max=05.8427, min=00.0205, mean=01.0291} policy_loss=-9.2990 policy updated! \n",
      "train step 06044 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1251 diff={max=05.9608, min=00.0063, mean=01.3287} policy_loss=-10.5063 policy updated! \n",
      "train step 06045 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.7831 diff={max=06.3648, min=00.0133, mean=01.0709} policy_loss=-8.8640 policy updated! \n",
      "train step 06046 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5039 diff={max=07.3123, min=00.0086, mean=01.3132} policy_loss=-8.1762 policy updated! \n",
      "train step 06047 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2749 diff={max=07.6282, min=00.0082, mean=01.2275} policy_loss=-8.6990 policy updated! \n",
      "train step 06048 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5858 diff={max=07.3340, min=00.0112, mean=01.0204} policy_loss=-8.5418 policy updated! \n",
      "train step 06049 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2512 diff={max=06.6691, min=00.0412, mean=01.2413} policy_loss=-8.6292 policy updated! \n",
      "train step 06050 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.9350 diff={max=12.1046, min=00.0072, mean=01.1782} policy_loss=-9.1254 policy updated! \n",
      "train step 06051 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7251 diff={max=03.3509, min=00.0115, mean=00.9486} policy_loss=-7.6120 policy updated! \n",
      "train step 06052 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4452 diff={max=06.2415, min=00.0623, mean=01.1917} policy_loss=-9.5061 policy updated! \n",
      "train step 06053 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5115 diff={max=11.4870, min=00.0045, mean=01.3002} policy_loss=-10.0151 policy updated! \n",
      "train step 06054 reward={max=09.0000, min=00.0000, mean=07.2000} optimizing loss=01.4088 diff={max=04.0963, min=00.0056, mean=00.8511} policy_loss=-8.3963 policy updated! \n",
      "train step 06055 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=01.9172 diff={max=05.3181, min=00.0411, mean=00.8783} policy_loss=-7.6170 policy updated! \n",
      "train step 06056 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7693 diff={max=07.9526, min=00.0021, mean=01.4228} policy_loss=-8.7348 policy updated! \n",
      "train step 06057 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5674 diff={max=06.1421, min=00.0030, mean=01.0012} policy_loss=-8.9869 policy updated! \n",
      "train step 06058 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2558 diff={max=06.7736, min=00.0324, mean=01.0127} policy_loss=-9.5322 policy updated! \n",
      "train step 06059 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.2224 diff={max=15.8056, min=00.0044, mean=01.2892} policy_loss=-8.9893 policy updated! \n",
      "train step 06060 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.5563 diff={max=07.5293, min=00.0091, mean=00.9843} policy_loss=-7.2785 policy updated! \n",
      "train step 06061 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7162 diff={max=05.5409, min=00.0120, mean=00.8773} policy_loss=-9.1513 policy updated! \n",
      "train step 06062 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3430 diff={max=08.1718, min=00.0119, mean=01.0936} policy_loss=-10.1499 policy updated! \n",
      "train step 06063 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0420 diff={max=08.3219, min=00.0227, mean=01.3766} policy_loss=-9.0767 policy updated! \n",
      "train step 06064 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3058 diff={max=08.7777, min=00.0228, mean=01.3949} policy_loss=-9.3738 policy updated! \n",
      "train step 06065 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=08.3224 diff={max=12.3802, min=00.0545, mean=01.5930} policy_loss=-9.4192 policy updated! \n",
      "train step 06066 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1994 diff={max=06.0321, min=00.0311, mean=01.2000} policy_loss=-8.0779 policy updated! \n",
      "train step 06067 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7688 diff={max=03.5845, min=00.0031, mean=00.9709} policy_loss=-9.4809 policy updated! \n",
      "train step 06068 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6050 diff={max=05.4616, min=00.0381, mean=01.0478} policy_loss=-9.0826 policy updated! \n",
      "train step 06069 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5482 diff={max=05.8384, min=00.0025, mean=01.2297} policy_loss=-9.4218 policy updated! \n",
      "train step 06070 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.9123 diff={max=06.2981, min=00.0008, mean=01.2319} policy_loss=-8.4179 policy updated! \n",
      "train step 06071 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5526 diff={max=05.0382, min=00.0190, mean=01.2379} policy_loss=-9.0184 policy updated! \n",
      "train step 06072 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5785 diff={max=05.2783, min=00.0411, mean=01.1418} policy_loss=-8.7649 policy updated! \n",
      "train step 06073 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5607 diff={max=04.3239, min=00.0038, mean=00.8820} policy_loss=-8.5704 policy updated! \n",
      "train step 06074 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2696 diff={max=07.4938, min=00.0050, mean=01.2013} policy_loss=-7.6183 policy updated! \n",
      "train step 06075 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.1073 diff={max=09.3667, min=00.0396, mean=00.9953} policy_loss=-8.5009 policy updated! \n",
      "train step 06076 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5424 diff={max=06.4852, min=00.0081, mean=01.3117} policy_loss=-7.4535 policy updated! \n",
      "train step 06077 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1040 diff={max=06.9011, min=00.0076, mean=01.1534} policy_loss=-10.2309 policy updated! \n",
      "train step 06078 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.9977 diff={max=09.0454, min=00.0290, mean=01.7486} policy_loss=-9.3283 policy updated! \n",
      "train step 06079 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7054 diff={max=07.8259, min=00.0082, mean=01.2992} policy_loss=-9.0210 policy updated! \n",
      "train step 06080 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.9007 diff={max=09.7258, min=00.0432, mean=01.2229} policy_loss=-8.8068 policy updated! \n",
      "train step 06081 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6470 diff={max=04.8836, min=00.0125, mean=01.1614} policy_loss=-9.5810 policy updated! \n",
      "train step 06082 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3406 diff={max=07.9024, min=00.0246, mean=01.3851} policy_loss=-9.0985 policy updated! \n",
      "train step 06083 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8221 diff={max=09.6508, min=00.0116, mean=01.4579} policy_loss=-8.7632 policy updated! \n",
      "train step 06084 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9339 diff={max=04.8772, min=00.0183, mean=00.9985} policy_loss=-8.6550 policy updated! \n",
      "train step 06085 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.8827 diff={max=09.1232, min=00.0026, mean=01.1873} policy_loss=-8.1166 policy updated! \n",
      "train step 06086 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7422 diff={max=04.0412, min=00.0410, mean=00.9942} policy_loss=-7.4989 policy updated! \n",
      "train step 06087 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6535 diff={max=09.0044, min=00.0201, mean=01.7277} policy_loss=-9.6929 policy updated! \n",
      "train step 06088 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8519 diff={max=06.2821, min=00.0022, mean=01.3833} policy_loss=-9.7595 policy updated! \n",
      "train step 06089 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4905 diff={max=10.2078, min=00.0184, mean=01.1328} policy_loss=-8.4143 policy updated! \n",
      "train step 06090 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.7414 diff={max=07.1963, min=00.0356, mean=01.2858} policy_loss=-8.9935 policy updated! \n",
      "train step 06091 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5694 diff={max=11.3069, min=00.0470, mean=01.3374} policy_loss=-10.1856 policy updated! \n",
      "train step 06092 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4121 diff={max=07.2905, min=00.0285, mean=01.5458} policy_loss=-7.5165 policy updated! \n",
      "train step 06093 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1858 diff={max=06.5327, min=00.0343, mean=01.1954} policy_loss=-8.4989 policy updated! \n",
      "train step 06094 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0417 diff={max=10.3361, min=00.0044, mean=01.4007} policy_loss=-10.5995 policy updated! \n",
      "train step 06095 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.1353 diff={max=08.7427, min=00.0070, mean=00.9138} policy_loss=-7.5133 policy updated! \n",
      "train step 06096 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1522 diff={max=06.9247, min=00.0061, mean=01.2383} policy_loss=-8.2366 policy updated! \n",
      "train step 06097 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5522 diff={max=07.9588, min=00.0134, mean=01.1838} policy_loss=-9.2446 policy updated! \n",
      "train step 06098 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8860 diff={max=07.8952, min=00.0116, mean=01.4070} policy_loss=-8.3502 policy updated! \n",
      "train step 06099 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0352 diff={max=08.3323, min=00.0017, mean=01.4225} policy_loss=-9.0250 policy updated! \n",
      "train step 06100 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=08.2282 diff={max=12.1551, min=00.0001, mean=01.7175} policy_loss=-8.3170 policy updated! \n",
      "train step 06101 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6178 diff={max=06.5356, min=00.0131, mean=01.7936} policy_loss=-9.5800 policy updated! \n",
      "train step 06102 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.7236 diff={max=12.2543, min=00.0048, mean=01.5705} policy_loss=-8.9822 policy updated! \n",
      "train step 06103 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.2625 diff={max=14.4566, min=00.0091, mean=01.3671} policy_loss=-8.8152 policy updated! \n",
      "train step 06104 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8568 diff={max=07.0602, min=00.0140, mean=01.1361} policy_loss=-9.7684 policy updated! \n",
      "train step 06105 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.9396 diff={max=08.6380, min=00.0005, mean=01.3195} policy_loss=-10.0899 policy updated! \n",
      "train step 06106 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2514 diff={max=04.8308, min=00.0124, mean=01.1767} policy_loss=-9.1286 policy updated! \n",
      "train step 06107 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2099 diff={max=07.3944, min=00.0050, mean=01.3250} policy_loss=-8.7466 policy updated! \n",
      "train step 06108 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8518 diff={max=06.6273, min=00.0024, mean=00.9120} policy_loss=-8.8053 policy updated! \n",
      "train step 06109 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2474 diff={max=05.4699, min=00.0160, mean=01.0275} policy_loss=-7.7802 policy updated! \n",
      "train step 06110 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.4576 diff={max=08.9225, min=00.0123, mean=01.1554} policy_loss=-7.2103 policy updated! \n",
      "train step 06111 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9335 diff={max=06.8845, min=00.0090, mean=01.3396} policy_loss=-8.0651 policy updated! \n",
      "train step 06112 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0527 diff={max=07.5363, min=00.0420, mean=01.2129} policy_loss=-8.0295 policy updated! \n",
      "train step 06113 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3019 diff={max=09.2212, min=00.0083, mean=01.3048} policy_loss=-8.9915 policy updated! \n",
      "train step 06114 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1931 diff={max=03.6417, min=00.0006, mean=00.7724} policy_loss=-8.9629 policy updated! \n",
      "train step 06115 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.1771 diff={max=06.9519, min=00.0091, mean=01.0893} policy_loss=-7.9283 policy updated! \n",
      "train step 06116 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6395 diff={max=06.2208, min=00.0235, mean=01.3270} policy_loss=-10.6375 policy updated! \n",
      "train step 06117 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9453 diff={max=06.3056, min=00.0159, mean=01.5998} policy_loss=-9.9117 policy updated! \n",
      "train step 06118 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3073 diff={max=09.0706, min=00.0074, mean=01.0122} policy_loss=-8.4768 policy updated! \n",
      "train step 06119 reward={max=09.0000, min=09.0000, mean=09.0000} optimizing loss=01.4301 diff={max=04.5133, min=00.0170, mean=00.7393} policy_loss=-8.0940 policy updated! \n",
      "train step 06120 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.4257 diff={max=05.7538, min=00.0392, mean=00.9633} policy_loss=-8.0129 policy updated! \n",
      "train step 06121 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9625 diff={max=04.2810, min=00.0593, mean=00.9397} policy_loss=-8.6447 policy updated! \n",
      "train step 06122 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8569 diff={max=04.8444, min=00.0248, mean=00.8882} policy_loss=-8.2322 policy updated! \n",
      "train step 06123 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9743 diff={max=07.2988, min=00.0016, mean=01.1551} policy_loss=-8.8009 policy updated! \n",
      "train step 06124 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0076 diff={max=10.2970, min=00.0063, mean=01.3875} policy_loss=-10.0514 policy updated! \n",
      "train step 06125 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.0644 diff={max=06.4017, min=00.0092, mean=01.1585} policy_loss=-9.6889 policy updated! \n",
      "train step 06126 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7719 diff={max=08.7400, min=00.0184, mean=01.4457} policy_loss=-8.8744 policy updated! \n",
      "train step 06127 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9557 diff={max=05.4989, min=00.0212, mean=00.9108} policy_loss=-10.0031 policy updated! \n",
      "train step 06128 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9994 diff={max=08.1353, min=00.0234, mean=01.0931} policy_loss=-10.3093 policy updated! \n",
      "train step 06129 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6632 diff={max=08.5654, min=00.0062, mean=01.3315} policy_loss=-10.3815 policy updated! \n",
      "train step 06130 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.9107 diff={max=08.6891, min=00.0054, mean=00.9645} policy_loss=-8.1256 policy updated! \n",
      "train step 06131 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0471 diff={max=04.4148, min=00.0684, mean=01.0444} policy_loss=-9.7131 policy updated! \n",
      "train step 06132 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2023 diff={max=07.3790, min=00.0174, mean=01.0944} policy_loss=-9.4515 policy updated! \n",
      "train step 06133 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2753 diff={max=07.2077, min=00.0007, mean=01.2816} policy_loss=-10.0741 policy updated! \n",
      "train step 06134 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7806 diff={max=07.8045, min=00.0268, mean=01.2815} policy_loss=-9.5074 policy updated! \n",
      "train step 06135 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.5804 diff={max=07.1329, min=00.0059, mean=01.3402} policy_loss=-10.3569 policy updated! \n",
      "train step 06136 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0129 diff={max=08.5766, min=00.0043, mean=01.0084} policy_loss=-8.8641 policy updated! \n",
      "train step 06137 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2630 diff={max=05.8976, min=00.0493, mean=01.3366} policy_loss=-10.7832 policy updated! \n",
      "train step 06138 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9568 diff={max=07.7906, min=00.0101, mean=01.2182} policy_loss=-9.1861 policy updated! \n",
      "train step 06139 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4989 diff={max=06.9149, min=00.0566, mean=01.1407} policy_loss=-8.8151 policy updated! \n",
      "train step 06140 reward={max=09.0000, min=09.0000, mean=09.0000} optimizing loss=05.9647 diff={max=08.7638, min=00.0037, mean=01.4337} policy_loss=-10.4539 policy updated! \n",
      "train step 06141 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6999 diff={max=10.1915, min=00.0182, mean=01.5307} policy_loss=-9.5216 policy updated! \n",
      "train step 06142 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1087 diff={max=06.0031, min=00.0092, mean=01.1550} policy_loss=-9.3168 policy updated! \n",
      "train step 06143 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5809 diff={max=03.0005, min=00.0373, mean=00.9827} policy_loss=-11.2414 policy updated! \n",
      "train step 06144 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4417 diff={max=08.2671, min=00.0178, mean=01.0943} policy_loss=-8.5679 policy updated! \n",
      "train step 06145 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.4085 diff={max=09.0096, min=00.0355, mean=01.0837} policy_loss=-8.4883 policy updated! \n",
      "train step 06146 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6830 diff={max=06.8438, min=00.0031, mean=01.0096} policy_loss=-8.8198 policy updated! \n",
      "train step 06147 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6487 diff={max=04.3824, min=00.0023, mean=00.8616} policy_loss=-8.3300 policy updated! \n",
      "train step 06148 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0186 diff={max=06.4661, min=00.0205, mean=01.3232} policy_loss=-8.5019 policy updated! \n",
      "train step 06149 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9924 diff={max=04.9939, min=00.0011, mean=01.0011} policy_loss=-9.8160 policy updated! \n",
      "train step 06150 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=04.3731 diff={max=07.6438, min=00.0043, mean=01.3812} policy_loss=-10.7615 policy updated! \n",
      "train step 06151 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0255 diff={max=05.9911, min=00.0262, mean=01.1479} policy_loss=-8.8059 policy updated! \n",
      "train step 06152 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0396 diff={max=08.1715, min=00.0206, mean=01.1523} policy_loss=-9.3013 policy updated! \n",
      "train step 06153 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=02.6715 diff={max=06.3837, min=00.0072, mean=00.9288} policy_loss=-8.7806 policy updated! \n",
      "train step 06154 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7795 diff={max=05.4404, min=00.0255, mean=00.9287} policy_loss=-9.3396 policy updated! \n",
      "train step 06155 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=05.7242 diff={max=12.6670, min=00.0065, mean=01.2805} policy_loss=-8.8980 policy updated! \n",
      "train step 06156 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4633 diff={max=04.8054, min=00.0051, mean=01.0492} policy_loss=-9.4450 policy updated! \n",
      "train step 06157 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8995 diff={max=02.6791, min=00.0197, mean=00.6849} policy_loss=-7.2613 policy updated! \n",
      "train step 06158 reward={max=09.0000, min=00.0000, mean=03.6000} optimizing loss=02.7694 diff={max=08.6112, min=00.0206, mean=00.8923} policy_loss=-8.5357 policy updated! \n",
      "train step 06159 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2523 diff={max=10.9048, min=00.0377, mean=01.3714} policy_loss=-9.2427 policy updated! \n",
      "train step 06160 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=03.2477 diff={max=06.0775, min=00.0129, mean=01.1704} policy_loss=-7.5959 policy updated! \n",
      "train step 06161 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9479 diff={max=06.5289, min=00.0171, mean=01.3574} policy_loss=-9.6919 policy updated! \n",
      "train step 06162 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9944 diff={max=07.7004, min=00.0265, mean=01.4418} policy_loss=-8.0053 policy updated! \n",
      "train step 06163 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8007 diff={max=09.0528, min=00.0416, mean=01.2611} policy_loss=-7.8005 policy updated! \n",
      "train step 06164 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2042 diff={max=03.9036, min=00.0261, mean=01.1566} policy_loss=-8.2138 policy updated! \n",
      "train step 06165 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=10.3126 diff={max=09.5746, min=00.0327, mean=02.0106} policy_loss=-9.9453 policy updated! \n",
      "train step 06166 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=02.4879 diff={max=05.5039, min=00.0316, mean=01.0600} policy_loss=-8.9560 policy updated! \n",
      "train step 06167 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2671 diff={max=12.6783, min=00.0503, mean=01.2623} policy_loss=-9.8384 policy updated! \n",
      "train step 06168 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6094 diff={max=05.7236, min=00.0030, mean=01.1052} policy_loss=-9.4016 policy updated! \n",
      "train step 06169 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9833 diff={max=02.7087, min=00.0075, mean=00.8131} policy_loss=-9.6350 policy updated! \n",
      "train step 06170 reward={max=09.0000, min=00.0000, mean=01.8000} optimizing loss=02.9314 diff={max=07.0298, min=00.0227, mean=01.0906} policy_loss=-7.5625 policy updated! \n",
      "train step 06171 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5709 diff={max=08.1094, min=00.0222, mean=01.3814} policy_loss=-8.9355 policy updated! \n",
      "train step 06172 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7775 diff={max=05.7433, min=00.0323, mean=01.1514} policy_loss=-10.5939 policy updated! \n",
      "train step 06173 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7549 diff={max=03.9559, min=00.0384, mean=00.9622} policy_loss=-9.3390 policy updated! \n",
      "train step 06174 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5784 diff={max=08.1175, min=00.0015, mean=00.9872} policy_loss=-8.2849 policy updated! \n",
      "train step 06175 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.9515 diff={max=06.1409, min=00.0196, mean=00.8720} policy_loss=-8.4490 policy updated! \n",
      "train step 06176 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3959 diff={max=05.7620, min=00.0152, mean=00.9795} policy_loss=-8.8934 policy updated! \n",
      "train step 06177 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1061 diff={max=03.1996, min=00.0335, mean=00.7942} policy_loss=-8.6844 policy updated! \n",
      "train step 06178 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2886 diff={max=11.0111, min=00.0138, mean=01.1774} policy_loss=-8.8657 policy updated! \n",
      "train step 06179 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=03.2316 diff={max=08.9287, min=00.0099, mean=01.0523} policy_loss=-9.8963 policy updated! \n",
      "train step 06180 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=02.1955 diff={max=04.3522, min=00.0195, mean=00.9496} policy_loss=-9.1681 policy updated! \n",
      "train step 06181 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4949 diff={max=05.5406, min=00.1166, mean=01.2299} policy_loss=-9.3326 policy updated! \n",
      "train step 06182 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4122 diff={max=10.3602, min=00.0385, mean=01.0229} policy_loss=-8.0365 policy updated! \n",
      "train step 06183 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5142 diff={max=06.8417, min=00.0188, mean=01.0946} policy_loss=-10.3906 policy updated! \n",
      "train step 06184 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=03.0531 diff={max=06.5658, min=00.0095, mean=01.1342} policy_loss=-10.2074 policy updated! \n",
      "train step 06185 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.6653 diff={max=10.0863, min=00.0140, mean=01.0457} policy_loss=-9.2900 policy updated! \n",
      "train step 06186 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1060 diff={max=08.2958, min=00.0123, mean=01.0924} policy_loss=-8.8983 policy updated! \n",
      "train step 06187 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7614 diff={max=06.2084, min=00.0374, mean=00.7974} policy_loss=-8.0696 policy updated! \n",
      "train step 06188 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2545 diff={max=06.5496, min=00.0221, mean=00.9480} policy_loss=-8.7807 policy updated! \n",
      "train step 06189 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3289 diff={max=07.9960, min=00.0018, mean=01.3242} policy_loss=-8.7880 policy updated! \n",
      "train step 06190 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.8978 diff={max=04.9772, min=00.0381, mean=00.9324} policy_loss=-9.8404 policy updated! \n",
      "train step 06191 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5802 diff={max=10.4301, min=00.0078, mean=01.1886} policy_loss=-9.1809 policy updated! \n",
      "train step 06192 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1523 diff={max=14.0383, min=00.0486, mean=01.0413} policy_loss=-9.1852 policy updated! \n",
      "train step 06193 reward={max=08.0000, min=00.0000, mean=03.2000} optimizing loss=04.0044 diff={max=07.8306, min=00.0114, mean=01.1564} policy_loss=-9.4692 policy updated! \n",
      "train step 06194 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8765 diff={max=06.1479, min=00.0011, mean=00.7611} policy_loss=-7.6539 policy updated! \n",
      "train step 06195 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=04.2912 diff={max=11.8472, min=00.0138, mean=01.0521} policy_loss=-9.6416 policy updated! \n",
      "train step 06196 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4158 diff={max=07.8785, min=00.0044, mean=01.4312} policy_loss=-9.3113 policy updated! \n",
      "train step 06197 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8072 diff={max=07.6596, min=00.0626, mean=01.3770} policy_loss=-10.7445 policy updated! \n",
      "train step 06198 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7490 diff={max=05.1041, min=00.0014, mean=00.8235} policy_loss=-8.5244 policy updated! \n",
      "train step 06199 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3612 diff={max=09.8114, min=00.0117, mean=01.2464} policy_loss=-6.6171 policy updated! \n",
      "train step 06200 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=06.0709 diff={max=10.8465, min=00.0002, mean=01.4324} policy_loss=-10.1228 policy updated! \n",
      "train step 06201 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6985 diff={max=04.8722, min=00.0019, mean=00.8748} policy_loss=-8.3614 policy updated! \n",
      "train step 06202 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9698 diff={max=10.8639, min=00.0236, mean=01.5034} policy_loss=-9.2864 policy updated! \n",
      "train step 06203 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3625 diff={max=08.6170, min=00.0230, mean=01.2718} policy_loss=-9.1791 policy updated! \n",
      "train step 06204 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8677 diff={max=04.6904, min=00.0303, mean=01.0034} policy_loss=-9.2417 policy updated! \n",
      "train step 06205 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.7538 diff={max=08.8577, min=00.0322, mean=01.2040} policy_loss=-9.0139 policy updated! \n",
      "train step 06206 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2291 diff={max=04.8610, min=00.0014, mean=00.9101} policy_loss=-7.6796 policy updated! \n",
      "train step 06207 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6241 diff={max=12.7175, min=00.0040, mean=01.2315} policy_loss=-9.8165 policy updated! \n",
      "train step 06208 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4162 diff={max=06.4346, min=00.0601, mean=01.0418} policy_loss=-9.9596 policy updated! \n",
      "train step 06209 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7633 diff={max=09.6859, min=00.0415, mean=01.0804} policy_loss=-9.7497 policy updated! \n",
      "train step 06210 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=02.5516 diff={max=06.0149, min=00.0453, mean=01.0312} policy_loss=-8.6129 policy updated! \n",
      "train step 06211 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7556 diff={max=08.9999, min=00.0002, mean=01.3035} policy_loss=-9.6359 policy updated! \n",
      "train step 06212 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1499 diff={max=09.8348, min=00.0736, mean=01.6139} policy_loss=-9.0104 policy updated! \n",
      "train step 06213 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3565 diff={max=08.0468, min=00.0158, mean=01.0353} policy_loss=-9.1732 policy updated! \n",
      "train step 06214 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5899 diff={max=07.1216, min=00.0109, mean=01.2883} policy_loss=-11.4513 policy updated! \n",
      "train step 06215 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=04.0747 diff={max=09.1407, min=00.0101, mean=01.1945} policy_loss=-9.7943 policy updated! \n",
      "train step 06216 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.8480 diff={max=13.8522, min=00.0051, mean=01.3538} policy_loss=-8.6296 policy updated! \n",
      "train step 06217 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3746 diff={max=06.8502, min=00.0363, mean=01.0176} policy_loss=-9.3062 policy updated! \n",
      "train step 06218 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0496 diff={max=09.2137, min=00.0045, mean=01.4485} policy_loss=-8.4825 policy updated! \n",
      "train step 06219 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9035 diff={max=09.2536, min=00.0322, mean=01.2516} policy_loss=-7.9789 policy updated! \n",
      "train step 06220 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=06.0263 diff={max=08.4433, min=00.0240, mean=01.4689} policy_loss=-8.6404 policy updated! \n",
      "train step 06221 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2920 diff={max=06.8116, min=00.0180, mean=01.3272} policy_loss=-8.9311 policy updated! \n",
      "train step 06222 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0751 diff={max=05.9580, min=00.0842, mean=01.0191} policy_loss=-10.6514 policy updated! \n",
      "train step 06223 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8588 diff={max=08.6605, min=00.0109, mean=01.5936} policy_loss=-10.6880 policy updated! \n",
      "train step 06224 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3289 diff={max=08.2167, min=00.0575, mean=01.3112} policy_loss=-10.1920 policy updated! \n",
      "train step 06225 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=04.7073 diff={max=07.3185, min=00.0091, mean=01.5411} policy_loss=-9.5523 policy updated! \n",
      "train step 06226 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5540 diff={max=07.5859, min=00.0271, mean=01.4476} policy_loss=-10.2469 policy updated! \n",
      "train step 06227 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6709 diff={max=05.6591, min=00.0121, mean=01.2021} policy_loss=-10.0496 policy updated! \n",
      "train step 06228 reward={max=08.0000, min=00.0000, mean=03.2000} optimizing loss=04.3160 diff={max=06.6260, min=00.0039, mean=01.4357} policy_loss=-10.7240 policy updated! \n",
      "train step 06229 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1519 diff={max=04.0818, min=00.0239, mean=01.0166} policy_loss=-10.5560 policy updated! \n",
      "train step 06230 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.0320 diff={max=07.3856, min=00.0168, mean=01.1225} policy_loss=-9.4102 policy updated! \n",
      "train step 06231 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1911 diff={max=07.3894, min=00.0033, mean=01.4323} policy_loss=-9.0575 policy updated! \n",
      "train step 06232 reward={max=08.0000, min=08.0000, mean=08.0000} optimizing loss=03.0546 diff={max=06.6026, min=00.0099, mean=01.0314} policy_loss=-8.7488 policy updated! \n",
      "train step 06233 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8834 diff={max=06.0110, min=00.0085, mean=01.1379} policy_loss=-8.6803 policy updated! \n",
      "train step 06234 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=03.3089 diff={max=06.7859, min=00.0267, mean=01.1196} policy_loss=-8.7244 policy updated! \n",
      "train step 06235 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=02.5236 diff={max=07.0043, min=00.0167, mean=00.9843} policy_loss=-8.3144 policy updated! \n",
      "train step 06236 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0906 diff={max=05.9530, min=00.0031, mean=00.9693} policy_loss=-8.8447 policy updated! \n",
      "train step 06237 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7372 diff={max=07.3464, min=00.0431, mean=00.9613} policy_loss=-7.5216 policy updated! \n",
      "train step 06238 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4764 diff={max=06.8638, min=00.0104, mean=00.9663} policy_loss=-9.7919 policy updated! \n",
      "train step 06239 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=06.6358 diff={max=12.5846, min=00.0048, mean=01.3963} policy_loss=-10.2115 policy updated! \n",
      "train step 06240 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=05.3176 diff={max=07.2559, min=00.0289, mean=01.4750} policy_loss=-8.6565 policy updated! \n",
      "train step 06241 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6824 diff={max=06.1963, min=00.0019, mean=01.3387} policy_loss=-9.8783 policy updated! \n",
      "train step 06242 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6042 diff={max=08.6191, min=00.0191, mean=01.4041} policy_loss=-8.2856 policy updated! \n",
      "train step 06243 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4604 diff={max=05.9174, min=00.0217, mean=01.0785} policy_loss=-8.1943 policy updated! \n",
      "train step 06244 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=03.6382 diff={max=07.9417, min=00.0038, mean=01.2919} policy_loss=-8.4433 policy updated! \n",
      "train step 06245 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=02.9518 diff={max=05.2433, min=00.0762, mean=01.2022} policy_loss=-9.1081 policy updated! \n",
      "train step 06246 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8016 diff={max=06.5874, min=00.0158, mean=01.0763} policy_loss=-9.0729 policy updated! \n",
      "train step 06247 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2783 diff={max=04.4468, min=00.0216, mean=01.0797} policy_loss=-10.1805 policy updated! \n",
      "train step 06248 reward={max=08.0000, min=00.0000, mean=03.2000} optimizing loss=05.0221 diff={max=09.4475, min=00.0001, mean=01.3171} policy_loss=-9.0422 policy updated! \n",
      "train step 06249 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7578 diff={max=12.5502, min=00.0417, mean=01.2691} policy_loss=-10.6188 policy updated! \n",
      "train step 06250 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.4186 diff={max=04.8478, min=00.0451, mean=00.7958} policy_loss=-7.7605 policy updated! \n",
      "train step 06251 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7198 diff={max=06.9387, min=00.0201, mean=01.0543} policy_loss=-7.6483 policy updated! \n",
      "train step 06252 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2547 diff={max=06.7767, min=00.0231, mean=00.9789} policy_loss=-8.8945 policy updated! \n",
      "train step 06253 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2742 diff={max=11.8002, min=00.0100, mean=01.5274} policy_loss=-10.2006 policy updated! \n",
      "train step 06254 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1452 diff={max=10.1269, min=00.0045, mean=01.3659} policy_loss=-10.3518 policy updated! \n",
      "train step 06255 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=04.5720 diff={max=08.8816, min=00.0089, mean=01.2425} policy_loss=-8.6776 policy updated! \n",
      "train step 06256 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8433 diff={max=07.0068, min=00.0547, mean=01.4585} policy_loss=-9.2869 policy updated! \n",
      "train step 06257 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0821 diff={max=05.9750, min=00.0075, mean=01.0703} policy_loss=-8.0776 policy updated! \n",
      "train step 06258 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7075 diff={max=07.7870, min=00.0333, mean=01.6260} policy_loss=-8.2914 policy updated! \n",
      "train step 06259 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7690 diff={max=09.5980, min=00.0257, mean=01.0152} policy_loss=-9.0053 policy updated! \n",
      "train step 06260 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=06.3772 diff={max=09.2413, min=00.0775, mean=01.5324} policy_loss=-8.0216 policy updated! \n",
      "train step 06261 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1010 diff={max=10.5305, min=00.0031, mean=01.3208} policy_loss=-9.1426 policy updated! \n",
      "train step 06262 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4323 diff={max=05.4551, min=00.0170, mean=01.3353} policy_loss=-8.5500 policy updated! \n",
      "train step 06263 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1715 diff={max=05.5305, min=00.0048, mean=00.9584} policy_loss=-7.3013 policy updated! \n",
      "train step 06264 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9057 diff={max=06.3128, min=00.0292, mean=01.1856} policy_loss=-9.6921 policy updated! \n",
      "train step 06265 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.8460 diff={max=08.5121, min=00.0295, mean=01.1567} policy_loss=-9.8656 policy updated! \n",
      "train step 06266 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0241 diff={max=07.3953, min=00.0978, mean=01.0943} policy_loss=-10.8355 policy updated! \n",
      "train step 06267 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8442 diff={max=08.6560, min=00.1057, mean=01.7398} policy_loss=-9.8408 policy updated! \n",
      "train step 06268 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2213 diff={max=10.4252, min=00.0687, mean=01.4304} policy_loss=-10.9474 policy updated! \n",
      "train step 06269 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3234 diff={max=10.7771, min=00.0208, mean=01.5647} policy_loss=-9.9998 policy updated! \n",
      "train step 06270 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.3140 diff={max=07.7232, min=00.0073, mean=01.1447} policy_loss=-9.4363 policy updated! \n",
      "train step 06271 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5023 diff={max=07.4410, min=00.0049, mean=01.0724} policy_loss=-8.1448 policy updated! \n",
      "train step 06272 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8797 diff={max=11.7162, min=00.0126, mean=01.3972} policy_loss=-8.3868 policy updated! \n",
      "train step 06273 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7950 diff={max=05.8560, min=00.0089, mean=00.8352} policy_loss=-9.8250 policy updated! \n",
      "train step 06274 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1269 diff={max=08.4604, min=00.0110, mean=01.4333} policy_loss=-8.6656 policy updated! \n",
      "train step 06275 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=04.4103 diff={max=11.1807, min=00.0063, mean=01.2080} policy_loss=-7.5312 policy updated! \n",
      "train step 06276 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=09.6051 diff={max=09.9111, min=00.0172, mean=01.9562} policy_loss=-10.0092 policy updated! \n",
      "train step 06277 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6419 diff={max=05.9615, min=00.0197, mean=01.0716} policy_loss=-9.1714 policy updated! \n",
      "train step 06278 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3191 diff={max=06.3363, min=00.0383, mean=01.2747} policy_loss=-11.2144 policy updated! \n",
      "train step 06279 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0467 diff={max=06.4210, min=00.0044, mean=01.1332} policy_loss=-10.2452 policy updated! \n",
      "train step 06280 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=05.1476 diff={max=08.3527, min=00.0076, mean=01.4194} policy_loss=-6.7946 policy updated! \n",
      "train step 06281 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=04.9486 diff={max=06.1748, min=00.0196, mean=01.5788} policy_loss=-10.1286 policy updated! \n",
      "train step 06282 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9914 diff={max=09.4549, min=00.0300, mean=01.1734} policy_loss=-9.9346 policy updated! \n",
      "train step 06283 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6845 diff={max=06.8634, min=00.0174, mean=00.7357} policy_loss=-9.8758 policy updated! \n",
      "train step 06284 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=02.7894 diff={max=06.5821, min=00.0152, mean=01.0357} policy_loss=-7.5970 policy updated! \n",
      "train step 06285 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.8327 diff={max=08.6254, min=00.0295, mean=01.2118} policy_loss=-9.4609 policy updated! \n",
      "train step 06286 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=03.0758 diff={max=06.4956, min=00.0540, mean=01.1267} policy_loss=-9.7499 policy updated! \n",
      "train step 06287 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1158 diff={max=06.9757, min=00.0301, mean=01.2775} policy_loss=-9.4417 policy updated! \n",
      "train step 06288 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2066 diff={max=04.8131, min=00.0009, mean=00.9901} policy_loss=-8.1830 policy updated! \n",
      "train step 06289 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2826 diff={max=06.8238, min=00.0045, mean=01.3081} policy_loss=-11.3027 policy updated! \n",
      "train step 06290 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=06.0225 diff={max=12.8886, min=00.0374, mean=01.3355} policy_loss=-9.7344 policy updated! \n",
      "train step 06291 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9241 diff={max=05.0764, min=00.0161, mean=00.9030} policy_loss=-8.2832 policy updated! \n",
      "train step 06292 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5183 diff={max=05.8464, min=00.0138, mean=01.0589} policy_loss=-8.0370 policy updated! \n",
      "train step 06293 reward={max=08.0000, min=00.0000, mean=03.2000} optimizing loss=04.0247 diff={max=07.5316, min=00.0200, mean=01.2143} policy_loss=-9.1249 policy updated! \n",
      "train step 06294 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3756 diff={max=05.6965, min=00.0364, mean=01.0036} policy_loss=-7.6019 policy updated! \n",
      "train step 06295 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=06.4390 diff={max=09.3037, min=00.0050, mean=01.5001} policy_loss=-9.0543 policy updated! \n",
      "train step 06296 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0111 diff={max=08.1982, min=00.0513, mean=01.4728} policy_loss=-10.1708 policy updated! \n",
      "train step 06297 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2963 diff={max=06.7076, min=00.0345, mean=01.1113} policy_loss=-8.9779 policy updated! \n",
      "train step 06298 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6994 diff={max=07.3371, min=00.0167, mean=01.0701} policy_loss=-9.4810 policy updated! \n",
      "train step 06299 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=03.5324 diff={max=06.6320, min=00.0051, mean=01.2774} policy_loss=-10.8886 policy updated! \n",
      "train step 06300 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=07.4605 diff={max=10.5850, min=00.0111, mean=01.5818} policy_loss=-8.3920 policy updated! \n",
      "train step 06301 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3113 diff={max=09.4265, min=00.0080, mean=01.1934} policy_loss=-8.3880 policy updated! \n",
      "train step 06302 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5101 diff={max=08.9658, min=00.0000, mean=01.2163} policy_loss=-8.7745 policy updated! \n",
      "train step 06303 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4837 diff={max=06.6258, min=00.0121, mean=01.0498} policy_loss=-7.8538 policy updated! \n",
      "train step 06304 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=04.8620 diff={max=09.6451, min=00.0060, mean=01.2473} policy_loss=-9.7950 policy updated! \n",
      "train step 06305 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=02.5802 diff={max=06.1640, min=00.0035, mean=01.0185} policy_loss=-7.6276 policy updated! \n",
      "train step 06306 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8406 diff={max=09.3789, min=00.0041, mean=01.1741} policy_loss=-6.8983 policy updated! \n",
      "train step 06307 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7122 diff={max=07.3284, min=00.0208, mean=01.1791} policy_loss=-8.4684 policy updated! \n",
      "train step 06308 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4315 diff={max=06.2397, min=00.0070, mean=00.9832} policy_loss=-9.1007 policy updated! \n",
      "train step 06309 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=04.1937 diff={max=07.7902, min=00.0075, mean=01.2363} policy_loss=-8.4456 policy updated! \n",
      "train step 06310 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=02.0230 diff={max=04.5645, min=00.0007, mean=00.9569} policy_loss=-9.3316 policy updated! \n",
      "train step 06311 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2760 diff={max=04.5488, min=00.0099, mean=01.0766} policy_loss=-9.9483 policy updated! \n",
      "train step 06312 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7545 diff={max=05.2230, min=00.0325, mean=01.2489} policy_loss=-10.3740 policy updated! \n",
      "train step 06313 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3246 diff={max=06.7171, min=00.0376, mean=01.0772} policy_loss=-9.4328 policy updated! \n",
      "train step 06314 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9220 diff={max=06.1144, min=00.0531, mean=01.4164} policy_loss=-9.8598 policy updated! \n",
      "train step 06315 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=04.6513 diff={max=09.2971, min=00.0277, mean=01.3780} policy_loss=-9.0929 policy updated! \n",
      "train step 06316 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5139 diff={max=11.1658, min=00.0149, mean=01.4788} policy_loss=-9.8497 policy updated! \n",
      "train step 06317 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3211 diff={max=05.3708, min=00.0031, mean=00.7918} policy_loss=-7.6438 policy updated! \n",
      "train step 06318 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4789 diff={max=08.0373, min=00.0106, mean=01.4593} policy_loss=-8.9782 policy updated! \n",
      "train step 06319 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=08.5318 diff={max=14.4035, min=00.0042, mean=01.4818} policy_loss=-9.5776 policy updated! \n",
      "train step 06320 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=00.8311 diff={max=02.7796, min=00.0088, mean=00.7164} policy_loss=-8.0809 policy updated! \n",
      "train step 06321 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8337 diff={max=06.1501, min=00.0603, mean=01.1025} policy_loss=-9.9454 policy updated! \n",
      "train step 06322 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7092 diff={max=02.9079, min=00.0169, mean=00.6059} policy_loss=-8.2767 policy updated! \n",
      "train step 06323 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8340 diff={max=06.1379, min=00.0045, mean=01.3260} policy_loss=-10.4464 policy updated! \n",
      "train step 06324 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5526 diff={max=05.2873, min=00.0017, mean=00.9853} policy_loss=-8.4107 policy updated! \n",
      "train step 06325 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.8288 diff={max=04.7658, min=00.0551, mean=00.8919} policy_loss=-10.1510 policy updated! \n",
      "train step 06326 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0284 diff={max=11.2263, min=00.0013, mean=01.2436} policy_loss=-9.5457 policy updated! \n",
      "train step 06327 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6446 diff={max=09.1662, min=00.0106, mean=01.1102} policy_loss=-9.8525 policy updated! \n",
      "train step 06328 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4817 diff={max=07.3707, min=00.0043, mean=01.0899} policy_loss=-8.0676 policy updated! \n",
      "train step 06329 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6948 diff={max=09.8960, min=00.0062, mean=01.0909} policy_loss=-10.3009 policy updated! \n",
      "train step 06330 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.5357 diff={max=06.6654, min=00.0145, mean=01.1084} policy_loss=-9.6882 policy updated! \n",
      "train step 06331 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5931 diff={max=08.4636, min=00.0163, mean=00.8680} policy_loss=-8.2732 policy updated! \n",
      "train step 06332 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8104 diff={max=05.4648, min=00.0034, mean=00.8746} policy_loss=-11.0250 policy updated! \n",
      "train step 06333 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7680 diff={max=05.9780, min=00.0149, mean=01.0427} policy_loss=-9.0523 policy updated! \n",
      "train step 06334 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=01.1148 diff={max=04.5010, min=00.0037, mean=00.7023} policy_loss=-9.8214 policy updated! \n",
      "train step 06335 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.3560 diff={max=05.7632, min=00.0313, mean=01.1338} policy_loss=-10.6151 policy updated! \n",
      "train step 06336 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0869 diff={max=11.5702, min=00.0200, mean=01.5463} policy_loss=-9.2567 policy updated! \n",
      "train step 06337 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3722 diff={max=05.8921, min=00.0169, mean=00.9340} policy_loss=-8.4994 policy updated! \n",
      "train step 06338 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4621 diff={max=06.2377, min=00.0126, mean=00.9507} policy_loss=-8.0327 policy updated! \n",
      "train step 06339 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7294 diff={max=07.3946, min=00.0055, mean=01.2470} policy_loss=-9.2237 policy updated! \n",
      "train step 06340 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=04.4573 diff={max=07.0830, min=00.0214, mean=01.2279} policy_loss=-7.5806 policy updated! \n",
      "train step 06341 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0187 diff={max=07.1942, min=00.0272, mean=01.1782} policy_loss=-8.3384 policy updated! \n",
      "train step 06342 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2885 diff={max=05.4821, min=00.0301, mean=00.9966} policy_loss=-9.4236 policy updated! \n",
      "train step 06343 reward={max=08.0000, min=00.0000, mean=03.2000} optimizing loss=02.4407 diff={max=06.9674, min=00.0032, mean=01.0058} policy_loss=-10.1240 policy updated! \n",
      "train step 06344 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8954 diff={max=08.7808, min=00.0083, mean=01.2030} policy_loss=-10.1568 policy updated! \n",
      "train step 06345 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=13.2680 diff={max=14.4680, min=00.0575, mean=01.9352} policy_loss=-9.4217 policy updated! \n",
      "train step 06346 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5677 diff={max=05.2047, min=00.0122, mean=01.1680} policy_loss=-9.5014 policy updated! \n",
      "train step 06347 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5969 diff={max=07.5198, min=00.0046, mean=01.2553} policy_loss=-8.0015 policy updated! \n",
      "train step 06348 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4802 diff={max=05.2651, min=00.0056, mean=01.3742} policy_loss=-9.8668 policy updated! \n",
      "train step 06349 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1810 diff={max=12.9999, min=00.0103, mean=01.2625} policy_loss=-9.1587 policy updated! \n",
      "train step 06350 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=06.0169 diff={max=12.9684, min=00.0122, mean=01.3029} policy_loss=-9.3639 policy updated! \n",
      "train step 06351 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=08.1011 diff={max=10.4645, min=00.0142, mean=01.5592} policy_loss=-8.6574 policy updated! \n",
      "train step 06352 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9093 diff={max=06.8424, min=00.0226, mean=00.7911} policy_loss=-7.3538 policy updated! \n",
      "train step 06353 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2800 diff={max=04.3926, min=00.0093, mean=00.7351} policy_loss=-7.9561 policy updated! \n",
      "train step 06354 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9990 diff={max=09.3538, min=00.0071, mean=01.3573} policy_loss=-8.4153 policy updated! \n",
      "train step 06355 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=04.8544 diff={max=08.2957, min=00.0289, mean=01.3356} policy_loss=-8.3609 policy updated! \n",
      "train step 06356 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0682 diff={max=14.1416, min=00.0194, mean=01.4146} policy_loss=-10.8748 policy updated! \n",
      "train step 06357 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1511 diff={max=06.6561, min=00.0199, mean=01.4675} policy_loss=-8.8250 policy updated! \n",
      "train step 06358 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5162 diff={max=06.5491, min=00.0035, mean=00.9818} policy_loss=-9.1787 policy updated! \n",
      "train step 06359 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4620 diff={max=07.9023, min=00.0131, mean=00.9117} policy_loss=-9.3882 policy updated! \n",
      "train step 06360 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.5266 diff={max=03.2080, min=00.0354, mean=00.9455} policy_loss=-9.1023 policy updated! \n",
      "train step 06361 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0505 diff={max=06.9900, min=00.0515, mean=01.3723} policy_loss=-7.7505 policy updated! \n",
      "train step 06362 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4562 diff={max=07.7371, min=00.0228, mean=01.3124} policy_loss=-9.5016 policy updated! \n",
      "train step 06363 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6460 diff={max=03.7984, min=00.0391, mean=00.9718} policy_loss=-8.6791 policy updated! \n",
      "train step 06364 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5116 diff={max=09.1852, min=00.0364, mean=01.2575} policy_loss=-7.8967 policy updated! \n",
      "train step 06365 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.9926 diff={max=07.3257, min=00.0789, mean=01.2463} policy_loss=-9.5075 policy updated! \n",
      "train step 06366 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=02.8535 diff={max=07.4438, min=00.0275, mean=01.0707} policy_loss=-7.5993 policy updated! \n",
      "train step 06367 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5172 diff={max=06.6882, min=00.0092, mean=00.9827} policy_loss=-8.4815 policy updated! \n",
      "train step 06368 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0184 diff={max=04.6898, min=00.0009, mean=00.9613} policy_loss=-10.5829 policy updated! \n",
      "train step 06369 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=03.6459 diff={max=04.9309, min=00.0100, mean=01.3136} policy_loss=-9.1608 policy updated! \n",
      "train step 06370 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=04.0568 diff={max=07.6966, min=00.0050, mean=01.3723} policy_loss=-9.7595 policy updated! \n",
      "train step 06371 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4816 diff={max=09.0501, min=00.0168, mean=01.2785} policy_loss=-9.8044 policy updated! \n",
      "train step 06372 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9974 diff={max=07.4801, min=00.0384, mean=00.8544} policy_loss=-10.3284 policy updated! \n",
      "train step 06373 reward={max=08.0000, min=00.0000, mean=03.2000} optimizing loss=09.3969 diff={max=12.5527, min=00.0022, mean=01.7397} policy_loss=-9.5760 policy updated! \n",
      "train step 06374 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9774 diff={max=07.6370, min=00.0614, mean=01.2616} policy_loss=-8.9764 policy updated! \n",
      "train step 06375 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.7249 diff={max=08.1520, min=00.0009, mean=01.1172} policy_loss=-8.9275 policy updated! \n",
      "train step 06376 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5455 diff={max=11.1220, min=00.0000, mean=01.1542} policy_loss=-9.5769 policy updated! \n",
      "train step 06377 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1681 diff={max=07.8208, min=00.0043, mean=01.1443} policy_loss=-8.6655 policy updated! \n",
      "train step 06378 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9732 diff={max=06.9316, min=00.0575, mean=01.1016} policy_loss=-10.3727 policy updated! \n",
      "train step 06379 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2147 diff={max=09.8462, min=00.0211, mean=01.2118} policy_loss=-7.6880 policy updated! \n",
      "train step 06380 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=02.2922 diff={max=06.1884, min=00.0072, mean=00.9821} policy_loss=-9.6493 policy updated! \n",
      "train step 06381 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.9363 diff={max=09.3906, min=00.0157, mean=01.3286} policy_loss=-7.6769 policy updated! \n",
      "train step 06382 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5099 diff={max=05.3216, min=00.0149, mean=00.8095} policy_loss=-7.9300 policy updated! \n",
      "train step 06383 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4647 diff={max=05.6525, min=00.0148, mean=01.1942} policy_loss=-9.1948 policy updated! \n",
      "train step 06384 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=02.5729 diff={max=05.6906, min=00.0125, mean=00.9885} policy_loss=-9.2959 policy updated! \n",
      "train step 06385 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.7624 diff={max=08.7454, min=00.0027, mean=01.1452} policy_loss=-10.1882 policy updated! \n",
      "train step 06386 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9572 diff={max=05.2475, min=00.0263, mean=01.2637} policy_loss=-10.8076 policy updated! \n",
      "train step 06387 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6160 diff={max=09.5787, min=00.0022, mean=01.6187} policy_loss=-9.5994 policy updated! \n",
      "train step 06388 reward={max=08.0000, min=08.0000, mean=08.0000} optimizing loss=05.6857 diff={max=13.7308, min=00.0634, mean=01.2908} policy_loss=-9.4993 policy updated! \n",
      "train step 06389 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5884 diff={max=07.8519, min=00.0095, mean=01.4114} policy_loss=-9.2548 policy updated! \n",
      "train step 06390 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=04.5462 diff={max=09.5768, min=00.0478, mean=01.3126} policy_loss=-9.1047 policy updated! \n",
      "train step 06391 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=03.8153 diff={max=08.1273, min=00.0225, mean=01.1939} policy_loss=-8.8393 policy updated! \n",
      "train step 06392 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6130 diff={max=09.8634, min=00.0065, mean=01.5211} policy_loss=-9.8620 policy updated! \n",
      "train step 06393 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5148 diff={max=05.8085, min=00.0257, mean=01.0231} policy_loss=-9.1315 policy updated! \n",
      "train step 06394 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1083 diff={max=09.0851, min=00.0232, mean=01.0178} policy_loss=-8.6052 policy updated! \n",
      "train step 06395 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.4906 diff={max=07.7546, min=00.0075, mean=01.1880} policy_loss=-9.6921 policy updated! \n",
      "train step 06396 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5120 diff={max=05.3281, min=00.0024, mean=01.0316} policy_loss=-8.8597 policy updated! \n",
      "train step 06397 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.1325 diff={max=06.2607, min=00.0557, mean=01.5534} policy_loss=-11.9972 policy updated! \n",
      "train step 06398 reward={max=08.0000, min=00.0000, mean=03.2000} optimizing loss=01.4665 diff={max=04.2755, min=00.0179, mean=00.7989} policy_loss=-8.8489 policy updated! \n",
      "train step 06399 reward={max=08.0000, min=00.0000, mean=06.4000} optimizing loss=02.2682 diff={max=06.3721, min=00.0049, mean=00.9884} policy_loss=-9.2983 policy updated! \n",
      "train step 06400 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.9219 diff={max=04.7906, min=00.0197, mean=00.9299} policy_loss=-7.8494 policy updated! \n",
      "train step 06401 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7325 diff={max=07.6529, min=00.0137, mean=00.9716} policy_loss=-7.7517 policy updated! \n",
      "train step 06402 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0617 diff={max=03.9512, min=00.0088, mean=00.6766} policy_loss=-9.4917 policy updated! \n",
      "train step 06403 reward={max=08.0000, min=00.0000, mean=03.2000} optimizing loss=05.4665 diff={max=08.6905, min=00.0086, mean=01.4513} policy_loss=-10.7196 policy updated! \n",
      "train step 06404 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1832 diff={max=08.9482, min=00.0382, mean=01.4050} policy_loss=-9.3150 policy updated! \n",
      "train step 06405 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=04.2952 diff={max=07.4238, min=00.0105, mean=01.3304} policy_loss=-10.3687 policy updated! \n",
      "train step 06406 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6764 diff={max=04.0355, min=00.0059, mean=00.8485} policy_loss=-9.1276 policy updated! \n",
      "train step 06407 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9769 diff={max=04.3671, min=00.0004, mean=00.9369} policy_loss=-10.6507 policy updated! \n",
      "train step 06408 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5264 diff={max=04.9621, min=00.0154, mean=01.0519} policy_loss=-7.3517 policy updated! \n",
      "train step 06409 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3533 diff={max=07.9534, min=00.0141, mean=00.7956} policy_loss=-9.8483 policy updated! \n",
      "train step 06410 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.8548 diff={max=10.5134, min=00.0041, mean=01.0304} policy_loss=-7.3219 policy updated! \n",
      "train step 06411 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1463 diff={max=09.3060, min=00.0622, mean=01.2287} policy_loss=-9.2650 policy updated! \n",
      "train step 06412 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9457 diff={max=06.3587, min=00.0332, mean=00.9056} policy_loss=-7.4074 policy updated! \n",
      "train step 06413 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2268 diff={max=06.0738, min=00.0204, mean=00.9510} policy_loss=-9.3141 policy updated! \n",
      "train step 06414 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0693 diff={max=07.5660, min=00.0300, mean=01.1495} policy_loss=-10.4192 policy updated! \n",
      "train step 06415 reward={max=08.0000, min=08.0000, mean=08.0000} optimizing loss=03.9505 diff={max=06.8997, min=00.0172, mean=01.1895} policy_loss=-8.8572 policy updated! \n",
      "train step 06416 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2106 diff={max=08.2889, min=00.0016, mean=01.3856} policy_loss=-11.2124 policy updated! \n",
      "train step 06417 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6005 diff={max=04.7357, min=00.0052, mean=00.8241} policy_loss=-9.2857 policy updated! \n",
      "train step 06418 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6441 diff={max=08.8604, min=00.0115, mean=01.2601} policy_loss=-8.8960 policy updated! \n",
      "train step 06419 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.8922 diff={max=06.0498, min=00.0185, mean=01.0664} policy_loss=-8.1468 policy updated! \n",
      "train step 06420 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=01.9391 diff={max=04.4290, min=00.0086, mean=00.9779} policy_loss=-9.7153 policy updated! \n",
      "train step 06421 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.4352 diff={max=04.2643, min=00.0279, mean=00.7704} policy_loss=-9.3165 policy updated! \n",
      "train step 06422 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8766 diff={max=07.9517, min=00.0199, mean=01.0065} policy_loss=-8.6218 policy updated! \n",
      "train step 06423 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9437 diff={max=09.4951, min=00.0260, mean=00.9166} policy_loss=-9.2254 policy updated! \n",
      "train step 06424 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4663 diff={max=04.7277, min=00.0096, mean=00.8098} policy_loss=-8.5722 policy updated! \n",
      "train step 06425 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.7069 diff={max=11.9830, min=00.0140, mean=01.4071} policy_loss=-9.2373 policy updated! \n",
      "train step 06426 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8034 diff={max=08.1201, min=00.0080, mean=01.5899} policy_loss=-9.4776 policy updated! \n",
      "train step 06427 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8428 diff={max=05.4909, min=00.0041, mean=01.1417} policy_loss=-9.3605 policy updated! \n",
      "train step 06428 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.7998 diff={max=07.1257, min=00.0047, mean=01.0663} policy_loss=-8.4469 policy updated! \n",
      "train step 06429 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.3992 diff={max=05.7748, min=00.0111, mean=01.0361} policy_loss=-8.0908 policy updated! \n",
      "train step 06430 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.8778 diff={max=04.9493, min=00.0425, mean=00.8995} policy_loss=-8.4508 policy updated! \n",
      "train step 06431 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9817 diff={max=05.3856, min=00.0081, mean=01.1073} policy_loss=-9.9085 policy updated! \n",
      "train step 06432 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9003 diff={max=07.9269, min=00.0015, mean=01.1316} policy_loss=-10.0527 policy updated! \n",
      "train step 06433 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8152 diff={max=07.6654, min=00.0500, mean=00.9428} policy_loss=-8.0468 policy updated! \n",
      "train step 06434 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0749 diff={max=07.3213, min=00.0178, mean=01.2290} policy_loss=-8.3597 policy updated! \n",
      "train step 06435 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.9062 diff={max=06.1857, min=00.0075, mean=00.8031} policy_loss=-8.2492 policy updated! \n",
      "train step 06436 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.1978 diff={max=04.7215, min=00.0615, mean=00.7634} policy_loss=-9.1439 policy updated! \n",
      "train step 06437 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0419 diff={max=04.7960, min=00.0859, mean=00.9785} policy_loss=-8.0074 policy updated! \n",
      "train step 06438 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.2149 diff={max=06.2915, min=00.0149, mean=01.1426} policy_loss=-9.5304 policy updated! \n",
      "train step 06439 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.4922 diff={max=06.1452, min=00.0093, mean=01.0227} policy_loss=-9.5690 policy updated! \n",
      "train step 06440 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.3004 diff={max=09.8886, min=00.0347, mean=01.5864} policy_loss=-10.0504 policy updated! \n",
      "train step 06441 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8210 diff={max=10.6563, min=00.0229, mean=01.3397} policy_loss=-9.4899 policy updated! \n",
      "train step 06442 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0594 diff={max=06.8421, min=00.0178, mean=01.1636} policy_loss=-9.9538 policy updated! \n",
      "train step 06443 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9807 diff={max=08.1956, min=00.0081, mean=01.2419} policy_loss=-10.2781 policy updated! \n",
      "train step 06444 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.4152 diff={max=06.1404, min=00.0058, mean=01.2818} policy_loss=-8.7883 policy updated! \n",
      "train step 06445 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.8216 diff={max=03.9232, min=00.0169, mean=00.9441} policy_loss=-9.1442 policy updated! \n",
      "train step 06446 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4092 diff={max=08.6627, min=00.0076, mean=01.2958} policy_loss=-9.7069 policy updated! \n",
      "train step 06447 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0139 diff={max=04.5925, min=00.0030, mean=01.3071} policy_loss=-9.8992 policy updated! \n",
      "train step 06448 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9408 diff={max=06.0702, min=00.0584, mean=01.2719} policy_loss=-10.8624 policy updated! \n",
      "train step 06449 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2966 diff={max=05.5782, min=00.0021, mean=00.9426} policy_loss=-8.4702 policy updated! \n",
      "train step 06450 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.5212 diff={max=06.0339, min=00.0199, mean=01.1364} policy_loss=-8.1334 policy updated! \n",
      "train step 06451 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4476 diff={max=06.7626, min=00.0124, mean=00.9781} policy_loss=-9.3366 policy updated! \n",
      "train step 06452 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1050 diff={max=03.1690, min=00.0183, mean=00.7633} policy_loss=-7.4416 policy updated! \n",
      "train step 06453 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4675 diff={max=04.6800, min=00.0011, mean=00.7813} policy_loss=-8.0058 policy updated! \n",
      "train step 06454 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9295 diff={max=06.2768, min=00.0255, mean=00.9581} policy_loss=-9.3894 policy updated! \n",
      "train step 06455 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.5194 diff={max=07.3461, min=00.0139, mean=01.1939} policy_loss=-7.8768 policy updated! \n",
      "train step 06456 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=03.6763 diff={max=09.6789, min=00.0025, mean=01.1352} policy_loss=-10.0971 policy updated! \n",
      "train step 06457 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7297 diff={max=06.7494, min=00.0180, mean=01.3352} policy_loss=-10.6264 policy updated! \n",
      "train step 06458 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6859 diff={max=10.4438, min=00.0172, mean=01.2494} policy_loss=-9.1661 policy updated! \n",
      "train step 06459 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.5620 diff={max=18.9498, min=00.0421, mean=01.7866} policy_loss=-10.5629 policy updated! \n",
      "train step 06460 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=09.7229 diff={max=10.2924, min=00.0347, mean=01.9628} policy_loss=-10.7902 policy updated! \n",
      "train step 06461 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4819 diff={max=04.7847, min=00.0144, mean=00.8233} policy_loss=-9.2099 policy updated! \n",
      "train step 06462 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4069 diff={max=07.1190, min=00.0008, mean=01.0259} policy_loss=-8.4697 policy updated! \n",
      "train step 06463 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0197 diff={max=07.4263, min=00.0218, mean=01.3042} policy_loss=-8.5884 policy updated! \n",
      "train step 06464 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5969 diff={max=07.1808, min=00.0182, mean=01.2917} policy_loss=-9.9449 policy updated! \n",
      "train step 06465 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.2260 diff={max=07.0793, min=00.0425, mean=01.1270} policy_loss=-9.4635 policy updated! \n",
      "train step 06466 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4184 diff={max=06.0453, min=00.0050, mean=01.0016} policy_loss=-10.7702 policy updated! \n",
      "train step 06467 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4828 diff={max=08.3498, min=00.0100, mean=00.9267} policy_loss=-7.0824 policy updated! \n",
      "train step 06468 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5211 diff={max=09.1749, min=00.0374, mean=01.1466} policy_loss=-11.3995 policy updated! \n",
      "train step 06469 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9306 diff={max=04.2065, min=00.0161, mean=00.9652} policy_loss=-10.1246 policy updated! \n",
      "train step 06470 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.8761 diff={max=06.9311, min=00.0059, mean=01.0501} policy_loss=-11.4975 policy updated! \n",
      "train step 06471 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.9657 diff={max=04.5966, min=00.0320, mean=01.0701} policy_loss=-9.0686 policy updated! \n",
      "train step 06472 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1434 diff={max=06.1328, min=00.0022, mean=01.1084} policy_loss=-8.0533 policy updated! \n",
      "train step 06473 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2223 diff={max=06.3416, min=00.0107, mean=00.9035} policy_loss=-9.9819 policy updated! \n",
      "train step 06474 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.2698 diff={max=08.9082, min=00.0124, mean=01.0006} policy_loss=-9.9239 policy updated! \n",
      "train step 06475 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.6798 diff={max=08.4469, min=00.0022, mean=01.1733} policy_loss=-7.6327 policy updated! \n",
      "train step 06476 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.4242 diff={max=07.3390, min=00.0150, mean=00.9738} policy_loss=-9.6436 policy updated! \n",
      "train step 06477 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1524 diff={max=05.3503, min=00.0043, mean=01.0031} policy_loss=-9.0936 policy updated! \n",
      "train step 06478 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4822 diff={max=05.3533, min=00.0894, mean=01.2898} policy_loss=-8.6739 policy updated! \n",
      "train step 06479 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1143 diff={max=06.6320, min=00.0198, mean=00.9564} policy_loss=-10.1230 policy updated! \n",
      "train step 06480 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.4730 diff={max=06.6713, min=00.0341, mean=01.0919} policy_loss=-9.5065 policy updated! \n",
      "train step 06481 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.5648 diff={max=15.3027, min=00.0523, mean=01.3973} policy_loss=-9.1088 policy updated! \n",
      "train step 06482 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3843 diff={max=09.2124, min=00.0083, mean=01.1339} policy_loss=-9.4159 policy updated! \n",
      "train step 06483 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.0221 diff={max=08.5314, min=00.0063, mean=01.1348} policy_loss=-10.1558 policy updated! \n",
      "train step 06484 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6065 diff={max=08.5997, min=00.0147, mean=01.2880} policy_loss=-9.7746 policy updated! \n",
      "train step 06485 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=00.8205 diff={max=02.3829, min=00.0248, mean=00.6911} policy_loss=-9.0858 policy updated! \n",
      "train step 06486 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4369 diff={max=08.6361, min=00.0286, mean=00.7796} policy_loss=-8.4620 policy updated! \n",
      "train step 06487 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2028 diff={max=07.2004, min=00.0347, mean=01.0892} policy_loss=-9.5434 policy updated! \n",
      "train step 06488 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6985 diff={max=12.1471, min=00.0081, mean=01.1977} policy_loss=-9.2065 policy updated! \n",
      "train step 06489 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5737 diff={max=08.7901, min=00.0146, mean=01.0680} policy_loss=-10.3385 policy updated! \n",
      "train step 06490 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.7594 diff={max=08.7642, min=00.0121, mean=01.3376} policy_loss=-8.1736 policy updated! \n",
      "train step 06491 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8632 diff={max=05.4678, min=00.0193, mean=01.0704} policy_loss=-10.2406 policy updated! \n",
      "train step 06492 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7586 diff={max=05.6273, min=00.0305, mean=01.1067} policy_loss=-9.1163 policy updated! \n",
      "train step 06493 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7168 diff={max=05.0875, min=00.0197, mean=00.8831} policy_loss=-8.1058 policy updated! \n",
      "train step 06494 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4437 diff={max=04.9419, min=00.0026, mean=00.7994} policy_loss=-8.0746 policy updated! \n",
      "train step 06495 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.5439 diff={max=07.5298, min=00.0269, mean=01.3381} policy_loss=-8.2490 policy updated! \n",
      "train step 06496 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5357 diff={max=09.0983, min=00.0090, mean=01.4175} policy_loss=-10.2236 policy updated! \n",
      "train step 06497 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9281 diff={max=07.9887, min=00.0003, mean=01.1574} policy_loss=-9.1126 policy updated! \n",
      "train step 06498 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9916 diff={max=05.0718, min=00.0473, mean=01.2230} policy_loss=-9.8730 policy updated! \n",
      "train step 06499 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7165 diff={max=07.3524, min=00.0113, mean=01.4658} policy_loss=-7.8145 policy updated! \n",
      "train step 06500 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.4306 diff={max=06.7608, min=00.0085, mean=01.2641} policy_loss=-9.9383 policy updated! \n",
      "train step 06501 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4261 diff={max=09.1222, min=00.0099, mean=01.3487} policy_loss=-7.7627 policy updated! \n",
      "train step 06502 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7694 diff={max=08.5852, min=00.0367, mean=01.3889} policy_loss=-7.8385 policy updated! \n",
      "train step 06503 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0972 diff={max=08.7434, min=00.0255, mean=01.3585} policy_loss=-10.7976 policy updated! \n",
      "train step 06504 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6846 diff={max=10.8608, min=00.0271, mean=01.4161} policy_loss=-8.3624 policy updated! \n",
      "train step 06505 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.4671 diff={max=02.7970, min=00.0182, mean=00.9201} policy_loss=-9.6177 policy updated! \n",
      "train step 06506 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7106 diff={max=08.2993, min=00.0191, mean=01.2343} policy_loss=-9.2957 policy updated! \n",
      "train step 06507 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5588 diff={max=07.0420, min=00.0131, mean=00.7234} policy_loss=-8.9729 policy updated! \n",
      "train step 06508 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=01.8326 diff={max=04.1298, min=00.0185, mean=00.9176} policy_loss=-9.0811 policy updated! \n",
      "train step 06509 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.9340 diff={max=08.1891, min=00.0011, mean=01.3279} policy_loss=-8.3337 policy updated! \n",
      "train step 06510 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.8066 diff={max=04.8664, min=00.0070, mean=00.8990} policy_loss=-7.6659 policy updated! \n",
      "train step 06511 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5636 diff={max=08.8797, min=00.0056, mean=01.2397} policy_loss=-8.7897 policy updated! \n",
      "train step 06512 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9869 diff={max=04.4041, min=00.0058, mean=00.9771} policy_loss=-8.9737 policy updated! \n",
      "train step 06513 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7371 diff={max=07.1037, min=00.0171, mean=01.0013} policy_loss=-9.7103 policy updated! \n",
      "train step 06514 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.8133 diff={max=09.7389, min=00.0060, mean=01.4968} policy_loss=-10.9904 policy updated! \n",
      "train step 06515 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.4562 diff={max=05.9998, min=00.0595, mean=01.2124} policy_loss=-9.4342 policy updated! \n",
      "train step 06516 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.6600 diff={max=07.3504, min=00.0156, mean=01.0551} policy_loss=-8.7198 policy updated! \n",
      "train step 06517 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9621 diff={max=07.1439, min=00.0129, mean=01.1610} policy_loss=-9.2744 policy updated! \n",
      "train step 06518 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.9001 diff={max=05.9909, min=00.0047, mean=00.8365} policy_loss=-7.3664 policy updated! \n",
      "train step 06519 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7698 diff={max=02.8697, min=00.0063, mean=00.6382} policy_loss=-8.9413 policy updated! \n",
      "train step 06520 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.2853 diff={max=04.6523, min=00.0247, mean=00.9577} policy_loss=-8.5280 policy updated! \n",
      "train step 06521 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5068 diff={max=09.5599, min=00.0040, mean=01.1747} policy_loss=-9.9674 policy updated! \n",
      "train step 06522 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.8616 diff={max=10.5861, min=00.0372, mean=01.5628} policy_loss=-9.7346 policy updated! \n",
      "train step 06523 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2457 diff={max=06.1708, min=00.0007, mean=00.9968} policy_loss=-8.0665 policy updated! \n",
      "train step 06524 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5777 diff={max=07.0389, min=00.0306, mean=01.1023} policy_loss=-9.7807 policy updated! \n",
      "train step 06525 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.4878 diff={max=07.4420, min=00.0366, mean=00.9348} policy_loss=-10.3736 policy updated! \n",
      "train step 06526 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6034 diff={max=08.5007, min=00.0289, mean=01.3096} policy_loss=-9.1384 policy updated! \n",
      "train step 06527 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3081 diff={max=03.3108, min=00.0168, mean=00.7645} policy_loss=-8.7913 policy updated! \n",
      "train step 06528 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9110 diff={max=07.6203, min=00.0106, mean=01.2258} policy_loss=-9.0712 policy updated! \n",
      "train step 06529 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8969 diff={max=10.0464, min=00.0497, mean=01.1542} policy_loss=-9.9935 policy updated! \n",
      "train step 06530 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.1633 diff={max=06.1995, min=00.0262, mean=01.2167} policy_loss=-8.4331 policy updated! \n",
      "train step 06531 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.9191 diff={max=08.6727, min=00.0046, mean=01.1838} policy_loss=-9.3321 policy updated! \n",
      "train step 06532 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.5062 diff={max=11.0276, min=00.0153, mean=01.7471} policy_loss=-10.2508 policy updated! \n",
      "train step 06533 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=00.9423 diff={max=03.2256, min=00.0563, mean=00.6998} policy_loss=-8.8654 policy updated! \n",
      "train step 06534 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7336 diff={max=07.0423, min=00.0332, mean=01.0240} policy_loss=-10.4131 policy updated! \n",
      "train step 06535 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.2715 diff={max=07.9611, min=00.1051, mean=01.4974} policy_loss=-10.1360 policy updated! \n",
      "train step 06536 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8198 diff={max=07.0123, min=00.0038, mean=01.1068} policy_loss=-8.6568 policy updated! \n",
      "train step 06537 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0738 diff={max=09.8075, min=00.0581, mean=01.1875} policy_loss=-10.2411 policy updated! \n",
      "train step 06538 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4931 diff={max=06.4046, min=00.0203, mean=01.1813} policy_loss=-8.7626 policy updated! \n",
      "train step 06539 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.4987 diff={max=04.0604, min=00.0273, mean=00.8832} policy_loss=-8.8942 policy updated! \n",
      "train step 06540 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.8192 diff={max=03.2403, min=00.0100, mean=00.9851} policy_loss=-9.4648 policy updated! \n",
      "train step 06541 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0940 diff={max=06.9444, min=00.0098, mean=00.8948} policy_loss=-7.7810 policy updated! \n",
      "train step 06542 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3633 diff={max=09.1285, min=00.0055, mean=00.9959} policy_loss=-10.6013 policy updated! \n",
      "train step 06543 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6972 diff={max=07.9201, min=00.0496, mean=01.3638} policy_loss=-9.1176 policy updated! \n",
      "train step 06544 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4100 diff={max=07.5024, min=00.0117, mean=01.1074} policy_loss=-7.0304 policy updated! \n",
      "train step 06545 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.4306 diff={max=07.3525, min=00.0101, mean=00.9634} policy_loss=-9.6286 policy updated! \n",
      "train step 06546 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.7371 diff={max=06.8359, min=00.0037, mean=01.1620} policy_loss=-9.5121 policy updated! \n",
      "train step 06547 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.8449 diff={max=08.3746, min=00.0120, mean=01.5636} policy_loss=-9.3794 policy updated! \n",
      "train step 06548 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8630 diff={max=06.2950, min=00.0038, mean=00.8035} policy_loss=-9.8518 policy updated! \n",
      "train step 06549 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9264 diff={max=06.4609, min=00.0166, mean=01.1653} policy_loss=-8.7691 policy updated! \n",
      "train step 06550 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.8484 diff={max=10.7567, min=00.0128, mean=01.0106} policy_loss=-8.8942 policy updated! \n",
      "train step 06551 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.7388 diff={max=05.7167, min=00.0090, mean=01.1805} policy_loss=-10.2160 policy updated! \n",
      "train step 06552 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0461 diff={max=04.8884, min=00.0019, mean=00.9363} policy_loss=-9.6191 policy updated! \n",
      "train step 06553 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5023 diff={max=05.5694, min=00.0659, mean=01.0273} policy_loss=-10.8514 policy updated! \n",
      "train step 06554 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.5140 diff={max=08.6991, min=00.0030, mean=00.8307} policy_loss=-9.4969 policy updated! \n",
      "train step 06555 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.0517 diff={max=07.4500, min=00.0437, mean=00.8530} policy_loss=-8.3559 policy updated! \n",
      "train step 06556 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.9014 diff={max=06.5658, min=00.0200, mean=01.0238} policy_loss=-8.4044 policy updated! \n",
      "train step 06557 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4587 diff={max=08.3030, min=00.0075, mean=01.2034} policy_loss=-8.3939 policy updated! \n",
      "train step 06558 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4079 diff={max=09.1635, min=00.0035, mean=01.4398} policy_loss=-9.2807 policy updated! \n",
      "train step 06559 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1425 diff={max=07.3651, min=00.0086, mean=01.3663} policy_loss=-8.3762 policy updated! \n",
      "train step 06560 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.4106 diff={max=07.3429, min=00.0384, mean=01.3368} policy_loss=-10.0401 policy updated! \n",
      "train step 06561 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.8663 diff={max=06.0989, min=00.0451, mean=00.8915} policy_loss=-7.9317 policy updated! \n",
      "train step 06562 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.1191 diff={max=10.6922, min=00.1028, mean=02.1260} policy_loss=-11.6500 policy updated! \n",
      "train step 06563 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7846 diff={max=06.4084, min=00.0057, mean=00.8540} policy_loss=-8.9472 policy updated! \n",
      "train step 06564 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=09.0435 diff={max=10.4142, min=00.0201, mean=01.7520} policy_loss=-10.7271 policy updated! \n",
      "train step 06565 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.8840 diff={max=09.9542, min=00.0175, mean=01.4125} policy_loss=-9.0097 policy updated! \n",
      "train step 06566 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2818 diff={max=05.9570, min=00.0029, mean=01.2387} policy_loss=-9.7471 policy updated! \n",
      "train step 06567 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7942 diff={max=05.7822, min=00.0023, mean=01.0134} policy_loss=-8.7509 policy updated! \n",
      "train step 06568 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.5174 diff={max=04.9296, min=00.0278, mean=01.0263} policy_loss=-8.5187 policy updated! \n",
      "train step 06569 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6228 diff={max=08.7955, min=00.0552, mean=01.0735} policy_loss=-9.3375 policy updated! \n",
      "train step 06570 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.2794 diff={max=07.6635, min=00.0106, mean=01.3122} policy_loss=-10.7453 policy updated! \n",
      "train step 06571 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6444 diff={max=08.8350, min=00.0072, mean=01.4455} policy_loss=-8.2769 policy updated! \n",
      "train step 06572 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9803 diff={max=08.1115, min=00.0075, mean=01.2545} policy_loss=-7.8801 policy updated! \n",
      "train step 06573 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3442 diff={max=09.0194, min=00.0953, mean=01.5730} policy_loss=-10.0126 policy updated! \n",
      "train step 06574 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8818 diff={max=07.7059, min=00.0058, mean=01.0502} policy_loss=-7.7913 policy updated! \n",
      "train step 06575 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.6744 diff={max=04.1528, min=00.0453, mean=00.9131} policy_loss=-9.6892 policy updated! \n",
      "train step 06576 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6683 diff={max=04.8304, min=00.0318, mean=01.1356} policy_loss=-9.2415 policy updated! \n",
      "train step 06577 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2557 diff={max=08.6206, min=00.0249, mean=01.4495} policy_loss=-7.3985 policy updated! \n",
      "train step 06578 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.3527 diff={max=05.3171, min=00.0073, mean=00.9948} policy_loss=-8.2859 policy updated! \n",
      "train step 06579 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3601 diff={max=05.4851, min=00.0029, mean=01.2216} policy_loss=-10.3436 policy updated! \n",
      "train step 06580 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.7474 diff={max=05.6720, min=00.0192, mean=00.8338} policy_loss=-7.3246 policy updated! \n",
      "train step 06581 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2917 diff={max=09.2924, min=00.0061, mean=01.3076} policy_loss=-7.3370 policy updated! \n",
      "train step 06582 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1013 diff={max=08.8994, min=00.0107, mean=01.2290} policy_loss=-10.0001 policy updated! \n",
      "train step 06583 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.3200 diff={max=04.6148, min=00.0011, mean=01.0801} policy_loss=-9.5200 policy updated! \n",
      "train step 06584 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.7271 diff={max=13.1418, min=00.0078, mean=01.0160} policy_loss=-8.4257 policy updated! \n",
      "train step 06585 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.9854 diff={max=07.9840, min=00.0253, mean=01.1736} policy_loss=-10.5137 policy updated! \n",
      "train step 06586 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8649 diff={max=05.7950, min=00.0325, mean=01.2603} policy_loss=-10.6923 policy updated! \n",
      "train step 06587 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8883 diff={max=06.2102, min=00.0015, mean=00.8391} policy_loss=-8.6903 policy updated! \n",
      "train step 06588 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.2861 diff={max=05.4948, min=00.0133, mean=01.2878} policy_loss=-10.1959 policy updated! \n",
      "train step 06589 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=08.4118 diff={max=11.7541, min=00.0146, mean=01.4986} policy_loss=-9.8838 policy updated! \n",
      "train step 06590 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.4058 diff={max=05.6473, min=00.0353, mean=01.0238} policy_loss=-8.2420 policy updated! \n",
      "train step 06591 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1024 diff={max=07.6649, min=00.0149, mean=00.8397} policy_loss=-9.2394 policy updated! \n",
      "train step 06592 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5566 diff={max=06.8942, min=00.0056, mean=01.5137} policy_loss=-9.6269 policy updated! \n",
      "train step 06593 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7131 diff={max=07.3968, min=00.0260, mean=01.1730} policy_loss=-9.4924 policy updated! \n",
      "train step 06594 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7251 diff={max=09.0421, min=00.0299, mean=01.0978} policy_loss=-9.9031 policy updated! \n",
      "train step 06595 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.7639 diff={max=11.0552, min=00.0021, mean=01.3439} policy_loss=-8.9659 policy updated! \n",
      "train step 06596 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.3096 diff={max=10.2907, min=00.0080, mean=01.2627} policy_loss=-9.0718 policy updated! \n",
      "train step 06597 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3613 diff={max=13.5988, min=00.0077, mean=01.1074} policy_loss=-7.2227 policy updated! \n",
      "train step 06598 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6278 diff={max=08.0795, min=00.0057, mean=01.2250} policy_loss=-9.6882 policy updated! \n",
      "train step 06599 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7890 diff={max=10.5808, min=00.0005, mean=01.4568} policy_loss=-8.6717 policy updated! \n",
      "train step 06600 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.6802 diff={max=07.0079, min=00.0031, mean=01.5282} policy_loss=-10.4394 policy updated! \n",
      "train step 06601 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8144 diff={max=08.1119, min=00.0322, mean=01.0958} policy_loss=-10.4561 policy updated! \n",
      "train step 06602 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6528 diff={max=06.7665, min=00.0251, mean=01.1627} policy_loss=-10.3634 policy updated! \n",
      "train step 06603 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.1595 diff={max=10.0488, min=00.0049, mean=00.9906} policy_loss=-9.1054 policy updated! \n",
      "train step 06604 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=07.1538 diff={max=14.1360, min=00.0111, mean=01.4273} policy_loss=-9.4535 policy updated! \n",
      "train step 06605 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.9877 diff={max=12.3311, min=00.0287, mean=01.0821} policy_loss=-9.6767 policy updated! \n",
      "train step 06606 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2999 diff={max=06.6449, min=00.0027, mean=01.0648} policy_loss=-9.6426 policy updated! \n",
      "train step 06607 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0902 diff={max=09.6028, min=00.0435, mean=01.4466} policy_loss=-9.2629 policy updated! \n",
      "train step 06608 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1076 diff={max=08.8885, min=00.0062, mean=01.5927} policy_loss=-8.0942 policy updated! \n",
      "train step 06609 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0515 diff={max=05.8928, min=00.0243, mean=00.9022} policy_loss=-9.9398 policy updated! \n",
      "train step 06610 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.8280 diff={max=08.5540, min=00.0578, mean=01.6030} policy_loss=-11.4447 policy updated! \n",
      "train step 06611 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7718 diff={max=05.9910, min=00.0804, mean=00.8264} policy_loss=-10.2047 policy updated! \n",
      "train step 06612 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8412 diff={max=06.5869, min=00.0053, mean=01.2442} policy_loss=-10.5586 policy updated! \n",
      "train step 06613 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3850 diff={max=04.8795, min=00.0416, mean=01.0286} policy_loss=-10.1737 policy updated! \n",
      "train step 06614 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.5024 diff={max=06.5876, min=00.0013, mean=01.1410} policy_loss=-8.7271 policy updated! \n",
      "train step 06615 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.4555 diff={max=06.0300, min=00.0099, mean=01.2291} policy_loss=-9.3927 policy updated! \n",
      "train step 06616 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.3292 diff={max=06.2294, min=00.0443, mean=01.2280} policy_loss=-9.0218 policy updated! \n",
      "train step 06617 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9770 diff={max=04.5769, min=00.0051, mean=01.2252} policy_loss=-7.7954 policy updated! \n",
      "train step 06618 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8514 diff={max=04.7418, min=00.0049, mean=00.9503} policy_loss=-9.5850 policy updated! \n",
      "train step 06619 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8378 diff={max=06.5965, min=00.0031, mean=01.2870} policy_loss=-10.5928 policy updated! \n",
      "train step 06620 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.4453 diff={max=06.2118, min=00.0439, mean=01.0148} policy_loss=-10.5536 policy updated! \n",
      "train step 06621 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3698 diff={max=04.6259, min=00.0106, mean=01.1046} policy_loss=-10.1183 policy updated! \n",
      "train step 06622 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1906 diff={max=11.6956, min=00.0104, mean=01.6435} policy_loss=-11.5905 policy updated! \n",
      "train step 06623 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4936 diff={max=07.4995, min=00.0090, mean=00.9159} policy_loss=-8.4783 policy updated! \n",
      "train step 06624 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4292 diff={max=09.1550, min=00.0024, mean=01.3291} policy_loss=-9.2077 policy updated! \n",
      "train step 06625 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.8516 diff={max=07.6104, min=00.0738, mean=01.2656} policy_loss=-10.3409 policy updated! \n",
      "train step 06626 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.3531 diff={max=09.2805, min=00.0201, mean=01.3956} policy_loss=-9.2343 policy updated! \n",
      "train step 06627 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0844 diff={max=06.6536, min=00.0007, mean=01.0330} policy_loss=-9.0519 policy updated! \n",
      "train step 06628 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3847 diff={max=05.3541, min=00.0294, mean=00.9978} policy_loss=-8.7966 policy updated! \n",
      "train step 06629 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.0237 diff={max=07.5544, min=00.0224, mean=01.0799} policy_loss=-9.6516 policy updated! \n",
      "train step 06630 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.3812 diff={max=05.3158, min=00.0224, mean=01.0244} policy_loss=-8.5539 policy updated! \n",
      "train step 06631 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3768 diff={max=06.8148, min=00.0321, mean=01.2230} policy_loss=-8.7497 policy updated! \n",
      "train step 06632 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1257 diff={max=08.6049, min=00.0333, mean=01.2885} policy_loss=-9.2393 policy updated! \n",
      "train step 06633 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5162 diff={max=03.4740, min=00.0190, mean=00.9237} policy_loss=-8.9254 policy updated! \n",
      "train step 06634 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.9551 diff={max=07.4930, min=00.0064, mean=01.3922} policy_loss=-11.9931 policy updated! \n",
      "train step 06635 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.5664 diff={max=07.3285, min=00.0038, mean=00.9056} policy_loss=-8.4540 policy updated! \n",
      "train step 06636 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2548 diff={max=05.4967, min=00.0169, mean=01.0394} policy_loss=-9.5717 policy updated! \n",
      "train step 06637 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7155 diff={max=07.9156, min=00.0068, mean=01.0043} policy_loss=-8.3065 policy updated! \n",
      "train step 06638 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5266 diff={max=09.1754, min=00.0079, mean=01.2379} policy_loss=-8.3262 policy updated! \n",
      "train step 06639 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.3808 diff={max=08.0355, min=00.0058, mean=01.2056} policy_loss=-8.6685 policy updated! \n",
      "train step 06640 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.1922 diff={max=06.3968, min=00.0519, mean=01.2089} policy_loss=-7.9380 policy updated! \n",
      "train step 06641 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0032 diff={max=08.8798, min=00.0322, mean=01.2560} policy_loss=-10.1552 policy updated! \n",
      "train step 06642 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5377 diff={max=04.5438, min=00.0163, mean=00.8720} policy_loss=-9.7962 policy updated! \n",
      "train step 06643 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3814 diff={max=07.8772, min=00.0006, mean=01.0717} policy_loss=-8.4313 policy updated! \n",
      "train step 06644 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0338 diff={max=07.8158, min=00.0020, mean=01.0410} policy_loss=-9.4916 policy updated! \n",
      "train step 06645 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.2530 diff={max=05.7556, min=00.0118, mean=00.9384} policy_loss=-8.9448 policy updated! \n",
      "train step 06646 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.5566 diff={max=06.5486, min=00.0020, mean=01.1139} policy_loss=-7.9583 policy updated! \n",
      "train step 06647 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7759 diff={max=07.5774, min=00.0040, mean=01.5045} policy_loss=-10.3045 policy updated! \n",
      "train step 06648 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7050 diff={max=06.7317, min=00.0458, mean=01.1997} policy_loss=-9.0185 policy updated! \n",
      "train step 06649 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1024 diff={max=05.7901, min=00.0136, mean=00.8365} policy_loss=-8.7497 policy updated! \n",
      "train step 06650 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.5956 diff={max=04.3029, min=00.0264, mean=00.8453} policy_loss=-8.4787 policy updated! \n",
      "train step 06651 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.8792 diff={max=07.1329, min=00.0430, mean=01.0868} policy_loss=-8.4327 policy updated! \n",
      "train step 06652 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1558 diff={max=07.1372, min=00.0004, mean=00.9062} policy_loss=-7.8343 policy updated! \n",
      "train step 06653 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.8371 diff={max=07.9129, min=00.0188, mean=01.1485} policy_loss=-10.3958 policy updated! \n",
      "train step 06654 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.1230 diff={max=10.8132, min=00.0091, mean=01.6518} policy_loss=-10.2218 policy updated! \n",
      "train step 06655 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.9746 diff={max=07.1402, min=00.0123, mean=01.3806} policy_loss=-10.0729 policy updated! \n",
      "train step 06656 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3820 diff={max=07.4545, min=00.0144, mean=01.6088} policy_loss=-10.5707 policy updated! \n",
      "train step 06657 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.5432 diff={max=14.2411, min=00.0313, mean=01.5656} policy_loss=-10.2581 policy updated! \n",
      "train step 06658 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.5545 diff={max=08.1302, min=00.0500, mean=01.4538} policy_loss=-8.9574 policy updated! \n",
      "train step 06659 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5315 diff={max=12.5866, min=00.0143, mean=01.4789} policy_loss=-9.1609 policy updated! \n",
      "train step 06660 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.6261 diff={max=06.3153, min=00.0200, mean=01.3766} policy_loss=-9.4914 policy updated! \n",
      "train step 06661 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6695 diff={max=08.1598, min=00.0151, mean=01.0532} policy_loss=-7.1600 policy updated! \n",
      "train step 06662 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0508 diff={max=07.0668, min=00.0145, mean=01.1902} policy_loss=-8.3190 policy updated! \n",
      "train step 06663 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.9296 diff={max=04.4672, min=00.0060, mean=01.0097} policy_loss=-8.0845 policy updated! \n",
      "train step 06664 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.3921 diff={max=10.3185, min=00.0555, mean=01.7370} policy_loss=-10.0330 policy updated! \n",
      "train step 06665 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.8934 diff={max=05.5493, min=00.0395, mean=00.9569} policy_loss=-10.1285 policy updated! \n",
      "train step 06666 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.3593 diff={max=05.2533, min=00.0322, mean=01.0061} policy_loss=-10.8775 policy updated! \n",
      "train step 06667 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4389 diff={max=09.6368, min=00.0272, mean=00.9571} policy_loss=-10.1076 policy updated! \n",
      "train step 06668 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1792 diff={max=06.9463, min=00.0399, mean=01.4821} policy_loss=-8.3951 policy updated! \n",
      "train step 06669 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0675 diff={max=05.2518, min=00.0071, mean=00.9492} policy_loss=-8.6741 policy updated! \n",
      "train step 06670 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.3860 diff={max=07.7707, min=00.0053, mean=01.3523} policy_loss=-10.7119 policy updated! \n",
      "train step 06671 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1786 diff={max=04.1426, min=00.0027, mean=00.9917} policy_loss=-10.0910 policy updated! \n",
      "train step 06672 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4883 diff={max=06.8328, min=00.0035, mean=01.1820} policy_loss=-9.1586 policy updated! \n",
      "train step 06673 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.1217 diff={max=03.8888, min=00.0189, mean=00.7577} policy_loss=-9.0772 policy updated! \n",
      "train step 06674 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.3910 diff={max=05.2147, min=00.0298, mean=01.3712} policy_loss=-8.2623 policy updated! \n",
      "train step 06675 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.2864 diff={max=07.1340, min=00.0613, mean=01.3702} policy_loss=-9.2939 policy updated! \n",
      "train step 06676 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.3324 diff={max=07.5903, min=00.0070, mean=01.5156} policy_loss=-8.9140 policy updated! \n",
      "train step 06677 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7147 diff={max=12.2997, min=00.0099, mean=01.2112} policy_loss=-9.3650 policy updated! \n",
      "train step 06678 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7089 diff={max=05.3880, min=00.0162, mean=00.8833} policy_loss=-9.9080 policy updated! \n",
      "train step 06679 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.5667 diff={max=07.7145, min=00.0131, mean=01.6065} policy_loss=-10.9846 policy updated! \n",
      "train step 06680 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.9424 diff={max=12.7438, min=00.0077, mean=01.3794} policy_loss=-9.8777 policy updated! \n",
      "train step 06681 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6018 diff={max=12.6190, min=00.0162, mean=01.6637} policy_loss=-11.6926 policy updated! \n",
      "train step 06682 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4430 diff={max=06.7759, min=00.0356, mean=01.3113} policy_loss=-11.9191 policy updated! \n",
      "train step 06683 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2989 diff={max=03.6634, min=00.0410, mean=00.8389} policy_loss=-8.9188 policy updated! \n",
      "train step 06684 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3861 diff={max=08.4440, min=00.0201, mean=01.2820} policy_loss=-8.7699 policy updated! \n",
      "train step 06685 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.3176 diff={max=06.3871, min=00.0749, mean=01.2267} policy_loss=-10.0119 policy updated! \n",
      "train step 06686 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.5632 diff={max=04.2668, min=00.0098, mean=00.8932} policy_loss=-10.5086 policy updated! \n",
      "train step 06687 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6175 diff={max=08.7342, min=00.0001, mean=01.1933} policy_loss=-9.2095 policy updated! \n",
      "train step 06688 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.1352 diff={max=14.3010, min=00.0162, mean=01.6088} policy_loss=-9.8079 policy updated! \n",
      "train step 06689 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.8953 diff={max=06.6422, min=00.0075, mean=01.0728} policy_loss=-8.9701 policy updated! \n",
      "train step 06690 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.3847 diff={max=13.1931, min=00.0147, mean=01.5832} policy_loss=-10.3705 policy updated! \n",
      "train step 06691 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3982 diff={max=02.9324, min=00.0146, mean=00.9569} policy_loss=-9.9845 policy updated! \n",
      "train step 06692 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2056 diff={max=06.7008, min=00.0099, mean=01.4736} policy_loss=-10.3953 policy updated! \n",
      "train step 06693 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=03.1694 diff={max=06.4586, min=00.0121, mean=01.1187} policy_loss=-9.0462 policy updated! \n",
      "train step 06694 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6181 diff={max=06.5864, min=00.0102, mean=01.2048} policy_loss=-8.0371 policy updated! \n",
      "train step 06695 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.3965 diff={max=06.9890, min=00.0250, mean=01.2485} policy_loss=-9.0749 policy updated! \n",
      "train step 06696 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.9266 diff={max=11.2795, min=00.0041, mean=01.4139} policy_loss=-8.7943 policy updated! \n",
      "train step 06697 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4896 diff={max=05.6418, min=00.0033, mean=00.7700} policy_loss=-7.9168 policy updated! \n",
      "train step 06698 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.6685 diff={max=04.2945, min=00.1384, mean=01.0231} policy_loss=-11.4558 policy updated! \n",
      "train step 06699 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.8129 diff={max=08.5834, min=00.0029, mean=01.7546} policy_loss=-9.2628 policy updated! \n",
      "train step 06700 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.6495 diff={max=07.5351, min=00.0021, mean=01.2164} policy_loss=-8.5329 policy updated! \n",
      "train step 06701 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.7895 diff={max=05.7404, min=00.0375, mean=01.1036} policy_loss=-10.6376 policy updated! \n",
      "train step 06702 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9873 diff={max=06.2760, min=00.0329, mean=01.0065} policy_loss=-7.9461 policy updated! \n",
      "train step 06703 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7789 diff={max=05.4832, min=00.0354, mean=01.0385} policy_loss=-9.5099 policy updated! \n",
      "train step 06704 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8432 diff={max=06.9428, min=00.0271, mean=01.1283} policy_loss=-9.9379 policy updated! \n",
      "train step 06705 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.8854 diff={max=09.9252, min=00.0032, mean=01.5200} policy_loss=-10.7711 policy updated! \n",
      "train step 06706 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2993 diff={max=09.0871, min=00.0125, mean=01.3975} policy_loss=-7.5982 policy updated! \n",
      "train step 06707 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.7656 diff={max=09.1083, min=00.0552, mean=01.6646} policy_loss=-9.4880 policy updated! \n",
      "train step 06708 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.0818 diff={max=05.1083, min=00.0128, mean=00.9743} policy_loss=-7.4516 policy updated! \n",
      "train step 06709 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.8332 diff={max=06.9188, min=00.0145, mean=01.2073} policy_loss=-10.2117 policy updated! \n",
      "train step 06710 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.0496 diff={max=10.2462, min=00.0124, mean=01.2633} policy_loss=-10.4218 policy updated! \n",
      "train step 06711 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.0550 diff={max=10.0022, min=00.0118, mean=01.7043} policy_loss=-10.3687 policy updated! \n",
      "train step 06712 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=02.5168 diff={max=04.0444, min=00.0012, mean=01.1558} policy_loss=-8.8018 policy updated! \n",
      "train step 06713 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8765 diff={max=04.0098, min=00.0193, mean=00.9863} policy_loss=-10.4788 policy updated! \n",
      "train step 06714 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3999 diff={max=10.3627, min=00.0224, mean=01.5294} policy_loss=-8.7683 policy updated! \n",
      "train step 06715 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=11.4909 diff={max=15.1163, min=00.0285, mean=01.8524} policy_loss=-10.3241 policy updated! \n",
      "train step 06716 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4757 diff={max=09.3474, min=00.0055, mean=01.4017} policy_loss=-9.1308 policy updated! \n",
      "train step 06717 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0360 diff={max=06.8195, min=00.0379, mean=01.4628} policy_loss=-10.4769 policy updated! \n",
      "train step 06718 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.1052 diff={max=10.7671, min=00.0268, mean=01.7018} policy_loss=-10.2096 policy updated! \n",
      "train step 06719 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9522 diff={max=07.3860, min=00.0100, mean=01.1007} policy_loss=-9.3883 policy updated! \n",
      "train step 06720 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.3345 diff={max=06.1744, min=00.0196, mean=01.1166} policy_loss=-9.8493 policy updated! \n",
      "train step 06721 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8187 diff={max=10.1637, min=00.0410, mean=01.1925} policy_loss=-9.8981 policy updated! \n",
      "train step 06722 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=05.4683 diff={max=07.6574, min=00.0041, mean=01.3429} policy_loss=-9.7675 policy updated! \n",
      "train step 06723 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1781 diff={max=08.6966, min=00.0099, mean=01.3354} policy_loss=-9.1343 policy updated! \n",
      "train step 06724 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9393 diff={max=05.9215, min=00.0046, mean=01.3293} policy_loss=-10.8071 policy updated! \n",
      "train step 06725 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.3757 diff={max=06.4222, min=00.0081, mean=00.9115} policy_loss=-9.9834 policy updated! \n",
      "train step 06726 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.1406 diff={max=10.7307, min=00.0429, mean=01.3725} policy_loss=-10.1508 policy updated! \n",
      "train step 06727 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0533 diff={max=10.1485, min=00.0118, mean=01.4124} policy_loss=-9.1484 policy updated! \n",
      "train step 06728 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1460 diff={max=03.3108, min=00.0263, mean=00.7724} policy_loss=-10.2117 policy updated! \n",
      "train step 06729 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3067 diff={max=05.3241, min=00.0164, mean=00.9467} policy_loss=-9.5251 policy updated! \n",
      "train step 06730 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.3730 diff={max=11.1085, min=00.0002, mean=01.1832} policy_loss=-9.3276 policy updated! \n",
      "train step 06731 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3757 diff={max=07.4608, min=00.0213, mean=01.2424} policy_loss=-10.7839 policy updated! \n",
      "train step 06732 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5866 diff={max=05.4022, min=00.0458, mean=01.1456} policy_loss=-8.5941 policy updated! \n",
      "train step 06733 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0102 diff={max=07.4115, min=00.0237, mean=01.2269} policy_loss=-8.6233 policy updated! \n",
      "train step 06734 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1999 diff={max=06.2735, min=00.0208, mean=01.3424} policy_loss=-7.8798 policy updated! \n",
      "train step 06735 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.6531 diff={max=06.4006, min=00.0274, mean=00.9618} policy_loss=-7.5822 policy updated! \n",
      "train step 06736 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6325 diff={max=09.2557, min=00.0129, mean=01.3913} policy_loss=-11.0499 policy updated! \n",
      "train step 06737 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8131 diff={max=06.7251, min=00.0232, mean=01.4195} policy_loss=-8.9217 policy updated! \n",
      "train step 06738 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=03.6231 diff={max=07.0087, min=00.0060, mean=01.1029} policy_loss=-8.9395 policy updated! \n",
      "train step 06739 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3147 diff={max=04.1200, min=00.0117, mean=01.1260} policy_loss=-9.0191 policy updated! \n",
      "train step 06740 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.7976 diff={max=10.2519, min=00.0190, mean=01.1661} policy_loss=-9.1504 policy updated! \n",
      "train step 06741 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3786 diff={max=08.3383, min=00.0548, mean=01.0493} policy_loss=-7.4106 policy updated! \n",
      "train step 06742 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2116 diff={max=04.9315, min=00.0273, mean=00.9446} policy_loss=-8.3798 policy updated! \n",
      "train step 06743 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6547 diff={max=06.8825, min=00.0038, mean=00.7484} policy_loss=-8.5871 policy updated! \n",
      "train step 06744 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5597 diff={max=06.6430, min=00.0719, mean=01.0741} policy_loss=-9.3638 policy updated! \n",
      "train step 06745 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.0410 diff={max=06.7756, min=00.0098, mean=00.9232} policy_loss=-9.0158 policy updated! \n",
      "train step 06746 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=08.6155 diff={max=16.1090, min=00.0096, mean=01.4132} policy_loss=-9.4149 policy updated! \n",
      "train step 06747 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9203 diff={max=06.3844, min=00.0480, mean=01.0490} policy_loss=-9.5247 policy updated! \n",
      "train step 06748 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3665 diff={max=09.0127, min=00.0162, mean=01.0538} policy_loss=-9.7778 policy updated! \n",
      "train step 06749 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9262 diff={max=08.2916, min=00.0243, mean=01.4988} policy_loss=-10.6936 policy updated! \n",
      "train step 06750 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.1034 diff={max=07.2728, min=00.0251, mean=00.8658} policy_loss=-9.1385 policy updated! \n",
      "train step 06751 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.3481 diff={max=05.7960, min=00.0164, mean=00.9945} policy_loss=-8.3239 policy updated! \n",
      "train step 06752 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3212 diff={max=04.4929, min=00.0129, mean=01.0403} policy_loss=-10.2322 policy updated! \n",
      "train step 06753 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2660 diff={max=09.6107, min=00.0697, mean=01.0972} policy_loss=-7.4530 policy updated! \n",
      "train step 06754 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1775 diff={max=08.1482, min=00.0090, mean=01.2135} policy_loss=-8.4652 policy updated! \n",
      "train step 06755 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.6746 diff={max=08.0469, min=00.0127, mean=01.4892} policy_loss=-9.2196 policy updated! \n",
      "train step 06756 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.7967 diff={max=06.0930, min=00.0361, mean=01.1094} policy_loss=-9.4480 policy updated! \n",
      "train step 06757 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5760 diff={max=04.6933, min=00.0003, mean=00.8891} policy_loss=-8.8366 policy updated! \n",
      "train step 06758 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1466 diff={max=06.3454, min=00.0012, mean=00.9676} policy_loss=-8.6687 policy updated! \n",
      "train step 06759 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4270 diff={max=08.2600, min=00.0074, mean=01.5412} policy_loss=-10.6038 policy updated! \n",
      "train step 06760 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.1423 diff={max=09.2414, min=00.0325, mean=01.2821} policy_loss=-8.6166 policy updated! \n",
      "train step 06761 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.7797 diff={max=11.1277, min=00.0176, mean=01.6066} policy_loss=-9.2603 policy updated! \n",
      "train step 06762 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=02.2368 diff={max=05.3596, min=00.0061, mean=00.9910} policy_loss=-8.9924 policy updated! \n",
      "train step 06763 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.6298 diff={max=08.1517, min=00.0398, mean=01.7818} policy_loss=-9.9838 policy updated! \n",
      "train step 06764 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.9530 diff={max=07.3154, min=00.0613, mean=01.3517} policy_loss=-9.9041 policy updated! \n",
      "train step 06765 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.4667 diff={max=08.6768, min=00.0233, mean=01.1453} policy_loss=-9.1606 policy updated! \n",
      "train step 06766 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9573 diff={max=07.0259, min=00.0308, mean=01.6035} policy_loss=-11.1550 policy updated! \n",
      "train step 06767 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6552 diff={max=06.5179, min=00.0044, mean=01.3137} policy_loss=-10.0426 policy updated! \n",
      "train step 06768 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.8359 diff={max=08.9654, min=00.0074, mean=01.2026} policy_loss=-9.8171 policy updated! \n",
      "train step 06769 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2184 diff={max=05.7802, min=00.0224, mean=01.1959} policy_loss=-9.4000 policy updated! \n",
      "train step 06770 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.9738 diff={max=06.1374, min=00.0151, mean=00.8090} policy_loss=-10.0295 policy updated! \n",
      "train step 06771 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=07.3841 diff={max=09.8608, min=00.0161, mean=01.5796} policy_loss=-8.2888 policy updated! \n",
      "train step 06772 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0259 diff={max=06.3005, min=00.0125, mean=01.3001} policy_loss=-8.7703 policy updated! \n",
      "train step 06773 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.7457 diff={max=07.3181, min=00.0213, mean=01.0716} policy_loss=-9.1918 policy updated! \n",
      "train step 06774 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0185 diff={max=03.5783, min=00.0588, mean=01.0265} policy_loss=-9.7889 policy updated! \n",
      "train step 06775 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.2559 diff={max=05.7270, min=00.0317, mean=00.8919} policy_loss=-9.0283 policy updated! \n",
      "train step 06776 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6550 diff={max=05.7720, min=00.0576, mean=00.8617} policy_loss=-8.6849 policy updated! \n",
      "train step 06777 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6047 diff={max=04.2326, min=00.0189, mean=00.8376} policy_loss=-9.1453 policy updated! \n",
      "train step 06778 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3571 diff={max=09.8302, min=00.0108, mean=01.6624} policy_loss=-10.2486 policy updated! \n",
      "train step 06779 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1034 diff={max=07.2511, min=00.0050, mean=01.3623} policy_loss=-7.6169 policy updated! \n",
      "train step 06780 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.8033 diff={max=06.6287, min=00.0150, mean=01.0373} policy_loss=-8.1875 policy updated! \n",
      "train step 06781 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3036 diff={max=03.8868, min=00.0001, mean=00.8022} policy_loss=-9.2865 policy updated! \n",
      "train step 06782 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5290 diff={max=08.6101, min=00.0069, mean=01.1005} policy_loss=-10.2190 policy updated! \n",
      "train step 06783 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6562 diff={max=08.0572, min=00.0532, mean=01.5743} policy_loss=-10.1376 policy updated! \n",
      "train step 06784 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6865 diff={max=06.6443, min=00.0474, mean=01.0317} policy_loss=-8.2096 policy updated! \n",
      "train step 06785 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.3902 diff={max=04.4797, min=00.0109, mean=00.8076} policy_loss=-9.5710 policy updated! \n",
      "train step 06786 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5213 diff={max=08.5901, min=00.1109, mean=01.1824} policy_loss=-8.8140 policy updated! \n",
      "train step 06787 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4842 diff={max=07.5555, min=00.0363, mean=01.0630} policy_loss=-9.1731 policy updated! \n",
      "train step 06788 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1364 diff={max=04.3046, min=00.0019, mean=00.7175} policy_loss=-9.3268 policy updated! \n",
      "train step 06789 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7556 diff={max=08.1757, min=00.0027, mean=01.0692} policy_loss=-8.7389 policy updated! \n",
      "train step 06790 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.3564 diff={max=05.2564, min=00.0068, mean=01.0788} policy_loss=-11.0865 policy updated! \n",
      "train step 06791 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=09.1401 diff={max=09.9436, min=00.0609, mean=01.8332} policy_loss=-10.2542 policy updated! \n",
      "train step 06792 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9322 diff={max=08.4367, min=00.0069, mean=01.0013} policy_loss=-9.2587 policy updated! \n",
      "train step 06793 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.1392 diff={max=06.2973, min=00.0219, mean=01.1022} policy_loss=-10.0331 policy updated! \n",
      "train step 06794 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0649 diff={max=03.7857, min=00.0067, mean=01.0234} policy_loss=-9.8523 policy updated! \n",
      "train step 06795 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.7950 diff={max=08.3318, min=00.0073, mean=01.3283} policy_loss=-8.2863 policy updated! \n",
      "train step 06796 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9013 diff={max=08.5559, min=00.0046, mean=01.2422} policy_loss=-9.3086 policy updated! \n",
      "train step 06797 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.0747 diff={max=10.4107, min=00.0387, mean=01.6516} policy_loss=-9.3284 policy updated! \n",
      "train step 06798 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1754 diff={max=05.4092, min=00.0025, mean=00.9654} policy_loss=-10.2588 policy updated! \n",
      "train step 06799 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.2281 diff={max=05.0100, min=00.0065, mean=01.1987} policy_loss=-8.5975 policy updated! \n",
      "train step 06800 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.4985 diff={max=05.6696, min=00.0143, mean=00.7661} policy_loss=-10.4756 policy updated! \n",
      "train step 06801 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2351 diff={max=05.6682, min=00.0138, mean=00.9405} policy_loss=-8.3542 policy updated! \n",
      "train step 06802 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.4535 diff={max=09.5535, min=00.0000, mean=01.7049} policy_loss=-10.4715 policy updated! \n",
      "train step 06803 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7131 diff={max=03.1096, min=00.0626, mean=01.0147} policy_loss=-9.2727 policy updated! \n",
      "train step 06804 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4765 diff={max=05.6382, min=00.0012, mean=00.9445} policy_loss=-9.4219 policy updated! \n",
      "train step 06805 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.1363 diff={max=06.7172, min=00.0015, mean=01.1756} policy_loss=-9.6910 policy updated! \n",
      "train step 06806 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4442 diff={max=08.6081, min=00.0146, mean=01.3556} policy_loss=-10.3256 policy updated! \n",
      "train step 06807 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0752 diff={max=07.4304, min=00.0280, mean=00.7919} policy_loss=-8.3502 policy updated! \n",
      "train step 06808 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9481 diff={max=05.2335, min=00.0232, mean=00.9500} policy_loss=-10.9342 policy updated! \n",
      "train step 06809 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.2369 diff={max=12.7382, min=00.0346, mean=01.3492} policy_loss=-9.1928 policy updated! \n",
      "train step 06810 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=03.2364 diff={max=06.3244, min=00.0149, mean=01.1127} policy_loss=-8.6658 policy updated! \n",
      "train step 06811 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7398 diff={max=09.7881, min=00.0329, mean=01.4746} policy_loss=-10.0670 policy updated! \n",
      "train step 06812 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4077 diff={max=08.4507, min=00.1296, mean=01.1274} policy_loss=-10.1469 policy updated! \n",
      "train step 06813 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.7948 diff={max=11.4472, min=00.0158, mean=01.1977} policy_loss=-10.0099 policy updated! \n",
      "train step 06814 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2506 diff={max=05.8907, min=00.0635, mean=01.0846} policy_loss=-10.6755 policy updated! \n",
      "train step 06815 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.0864 diff={max=03.2894, min=00.0111, mean=00.7626} policy_loss=-8.8635 policy updated! \n",
      "train step 06816 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.4343 diff={max=03.6287, min=00.0123, mean=00.8668} policy_loss=-10.3457 policy updated! \n",
      "train step 06817 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6867 diff={max=06.7458, min=00.0085, mean=01.1510} policy_loss=-9.3957 policy updated! \n",
      "train step 06818 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2185 diff={max=05.4509, min=00.0259, mean=00.9928} policy_loss=-6.8088 policy updated! \n",
      "train step 06819 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4506 diff={max=05.2537, min=00.0221, mean=01.0686} policy_loss=-9.6404 policy updated! \n",
      "train step 06820 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.7385 diff={max=06.7974, min=00.0330, mean=01.0082} policy_loss=-8.9452 policy updated! \n",
      "train step 06821 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6214 diff={max=08.6431, min=00.0187, mean=01.4856} policy_loss=-10.1657 policy updated! \n",
      "train step 06822 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5140 diff={max=04.7286, min=00.0157, mean=01.0975} policy_loss=-11.6068 policy updated! \n",
      "train step 06823 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.6560 diff={max=06.8057, min=00.0071, mean=00.7919} policy_loss=-8.3369 policy updated! \n",
      "train step 06824 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6107 diff={max=08.0542, min=00.0122, mean=01.1159} policy_loss=-9.8481 policy updated! \n",
      "train step 06825 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.9285 diff={max=05.9027, min=00.0088, mean=01.1476} policy_loss=-9.1441 policy updated! \n",
      "train step 06826 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.9648 diff={max=05.2334, min=00.0129, mean=01.1242} policy_loss=-9.0743 policy updated! \n",
      "train step 06827 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8698 diff={max=04.2460, min=00.0240, mean=00.9074} policy_loss=-9.3080 policy updated! \n",
      "train step 06828 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9154 diff={max=09.1409, min=00.0158, mean=01.2640} policy_loss=-8.5922 policy updated! \n",
      "train step 06829 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2782 diff={max=07.3871, min=00.0283, mean=01.0207} policy_loss=-7.4863 policy updated! \n",
      "train step 06830 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.4601 diff={max=09.4952, min=00.0007, mean=00.9815} policy_loss=-10.8234 policy updated! \n",
      "train step 06831 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3113 diff={max=08.0945, min=00.0116, mean=01.1194} policy_loss=-10.5612 policy updated! \n",
      "train step 06832 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6590 diff={max=08.9435, min=00.0017, mean=01.1260} policy_loss=-9.9756 policy updated! \n",
      "train step 06833 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9250 diff={max=04.7489, min=00.0032, mean=00.9445} policy_loss=-9.2123 policy updated! \n",
      "train step 06834 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3794 diff={max=10.0430, min=00.0225, mean=01.1233} policy_loss=-10.5586 policy updated! \n",
      "train step 06835 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.8391 diff={max=15.2257, min=00.0174, mean=01.4588} policy_loss=-10.5828 policy updated! \n",
      "train step 06836 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.2913 diff={max=09.4778, min=00.0025, mean=01.5385} policy_loss=-10.2966 policy updated! \n",
      "train step 06837 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4627 diff={max=06.6995, min=00.0079, mean=01.1082} policy_loss=-8.8226 policy updated! \n",
      "train step 06838 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1559 diff={max=09.9654, min=00.0263, mean=01.4263} policy_loss=-11.0314 policy updated! \n",
      "train step 06839 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0981 diff={max=07.4907, min=00.0208, mean=01.1061} policy_loss=-11.4970 policy updated! \n",
      "train step 06840 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.3912 diff={max=08.4005, min=00.0250, mean=01.3063} policy_loss=-8.8429 policy updated! \n",
      "train step 06841 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1852 diff={max=05.5712, min=00.0039, mean=01.0079} policy_loss=-9.1553 policy updated! \n",
      "train step 06842 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7753 diff={max=06.2855, min=00.0114, mean=01.1491} policy_loss=-9.4229 policy updated! \n",
      "train step 06843 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9402 diff={max=11.7972, min=00.0495, mean=01.2875} policy_loss=-8.4111 policy updated! \n",
      "train step 06844 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.1496 diff={max=06.4425, min=00.0342, mean=01.3925} policy_loss=-10.3166 policy updated! \n",
      "train step 06845 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.4546 diff={max=02.7219, min=00.0455, mean=00.9543} policy_loss=-10.2226 policy updated! \n",
      "train step 06846 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.9488 diff={max=10.3841, min=00.0165, mean=01.3258} policy_loss=-9.2231 policy updated! \n",
      "train step 06847 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1310 diff={max=07.5103, min=00.0008, mean=01.2785} policy_loss=-10.6160 policy updated! \n",
      "train step 06848 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0130 diff={max=05.4147, min=00.0250, mean=01.2169} policy_loss=-10.6104 policy updated! \n",
      "train step 06849 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.3360 diff={max=11.0296, min=00.0115, mean=01.3003} policy_loss=-8.6779 policy updated! \n",
      "train step 06850 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.6954 diff={max=13.1990, min=00.0170, mean=01.2044} policy_loss=-10.4042 policy updated! \n",
      "train step 06851 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1454 diff={max=04.6198, min=00.0123, mean=01.0224} policy_loss=-8.0051 policy updated! \n",
      "train step 06852 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3510 diff={max=05.4545, min=00.0364, mean=01.0681} policy_loss=-9.3627 policy updated! \n",
      "train step 06853 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8528 diff={max=07.2807, min=00.0054, mean=01.0814} policy_loss=-9.1450 policy updated! \n",
      "train step 06854 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.4370 diff={max=06.4905, min=00.0413, mean=01.3783} policy_loss=-9.1053 policy updated! \n",
      "train step 06855 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.4291 diff={max=07.3904, min=00.0054, mean=01.1918} policy_loss=-8.8497 policy updated! \n",
      "train step 06856 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.3208 diff={max=07.5464, min=00.0079, mean=01.1258} policy_loss=-10.2616 policy updated! \n",
      "train step 06857 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0457 diff={max=13.1891, min=00.0103, mean=01.1470} policy_loss=-8.5404 policy updated! \n",
      "train step 06858 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8355 diff={max=08.0909, min=00.0096, mean=01.2095} policy_loss=-9.6500 policy updated! \n",
      "train step 06859 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1683 diff={max=04.5671, min=00.0041, mean=01.0607} policy_loss=-9.7196 policy updated! \n",
      "train step 06860 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.8077 diff={max=11.4252, min=00.0038, mean=01.1710} policy_loss=-9.3104 policy updated! \n",
      "train step 06861 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2611 diff={max=02.7692, min=00.0102, mean=00.8752} policy_loss=-9.1131 policy updated! \n",
      "train step 06862 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1632 diff={max=12.1477, min=00.0189, mean=01.1809} policy_loss=-9.2923 policy updated! \n",
      "train step 06863 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.4733 diff={max=03.9757, min=00.0219, mean=00.8413} policy_loss=-10.5508 policy updated! \n",
      "train step 06864 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8526 diff={max=07.7296, min=00.0378, mean=01.2772} policy_loss=-9.9360 policy updated! \n",
      "train step 06865 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.7846 diff={max=06.4919, min=00.0127, mean=01.0367} policy_loss=-8.3855 policy updated! \n",
      "train step 06866 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7728 diff={max=07.3979, min=00.0194, mean=01.1161} policy_loss=-10.5553 policy updated! \n",
      "train step 06867 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0787 diff={max=06.5954, min=00.0018, mean=01.0837} policy_loss=-10.0238 policy updated! \n",
      "train step 06868 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.2406 diff={max=08.1550, min=00.0074, mean=01.0023} policy_loss=-9.3209 policy updated! \n",
      "train step 06869 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7042 diff={max=06.6594, min=00.0072, mean=00.8013} policy_loss=-10.0479 policy updated! \n",
      "train step 06870 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.5242 diff={max=04.8036, min=00.0279, mean=00.8409} policy_loss=-8.8480 policy updated! \n",
      "train step 06871 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5107 diff={max=04.9598, min=00.0402, mean=00.8149} policy_loss=-10.4327 policy updated! \n",
      "train step 06872 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2454 diff={max=06.4278, min=00.0302, mean=01.0435} policy_loss=-9.2981 policy updated! \n",
      "train step 06873 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3247 diff={max=08.0129, min=00.0240, mean=01.5379} policy_loss=-9.4936 policy updated! \n",
      "train step 06874 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.7358 diff={max=10.7336, min=00.0006, mean=01.3714} policy_loss=-9.8359 policy updated! \n",
      "train step 06875 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.8365 diff={max=09.2508, min=00.0099, mean=01.0912} policy_loss=-11.8269 policy updated! \n",
      "train step 06876 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5849 diff={max=07.3615, min=00.0163, mean=01.6647} policy_loss=-10.6835 policy updated! \n",
      "train step 06877 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7349 diff={max=07.7873, min=00.0024, mean=01.3856} policy_loss=-11.4587 policy updated! \n",
      "train step 06878 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6129 diff={max=06.1421, min=00.0086, mean=00.9976} policy_loss=-7.7120 policy updated! \n",
      "train step 06879 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.1383 diff={max=07.4692, min=00.0351, mean=01.1808} policy_loss=-9.0845 policy updated! \n",
      "train step 06880 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.2202 diff={max=09.5880, min=00.0049, mean=01.0913} policy_loss=-10.9304 policy updated! \n",
      "train step 06881 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4014 diff={max=07.3140, min=00.0627, mean=01.4775} policy_loss=-11.1374 policy updated! \n",
      "train step 06882 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9736 diff={max=06.3807, min=00.0230, mean=00.9370} policy_loss=-10.5696 policy updated! \n",
      "train step 06883 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2245 diff={max=06.2143, min=00.0091, mean=01.3523} policy_loss=-9.6684 policy updated! \n",
      "train step 06884 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1264 diff={max=08.2372, min=00.0031, mean=01.5632} policy_loss=-9.7686 policy updated! \n",
      "train step 06885 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.5217 diff={max=06.1153, min=00.0054, mean=00.7452} policy_loss=-10.3336 policy updated! \n",
      "train step 06886 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.1800 diff={max=05.6581, min=00.0147, mean=00.8837} policy_loss=-7.2154 policy updated! \n",
      "train step 06887 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.6862 diff={max=11.7891, min=00.0151, mean=01.7090} policy_loss=-8.3800 policy updated! \n",
      "train step 06888 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9003 diff={max=07.6498, min=00.0123, mean=01.1409} policy_loss=-12.0083 policy updated! \n",
      "train step 06889 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1530 diff={max=08.1287, min=00.0134, mean=01.1234} policy_loss=-9.8426 policy updated! \n",
      "train step 06890 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.1268 diff={max=04.7780, min=00.0594, mean=01.0157} policy_loss=-9.6440 policy updated! \n",
      "train step 06891 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.2750 diff={max=10.4324, min=00.0902, mean=01.2162} policy_loss=-10.9149 policy updated! \n",
      "train step 06892 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3176 diff={max=05.8453, min=00.0272, mean=00.9139} policy_loss=-10.2693 policy updated! \n",
      "train step 06893 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8950 diff={max=10.0419, min=00.0213, mean=01.3220} policy_loss=-8.7703 policy updated! \n",
      "train step 06894 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.5803 diff={max=04.9927, min=00.0465, mean=00.9067} policy_loss=-8.4488 policy updated! \n",
      "train step 06895 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.8797 diff={max=05.7670, min=00.0123, mean=00.8059} policy_loss=-9.0582 policy updated! \n",
      "train step 06896 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5989 diff={max=06.4909, min=00.0087, mean=01.1658} policy_loss=-9.0552 policy updated! \n",
      "train step 06897 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5545 diff={max=07.7282, min=00.0055, mean=01.2159} policy_loss=-8.9000 policy updated! \n",
      "train step 06898 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.6242 diff={max=04.1506, min=00.0015, mean=00.8890} policy_loss=-8.7333 policy updated! \n",
      "train step 06899 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6653 diff={max=06.4520, min=00.0103, mean=01.0008} policy_loss=-7.9338 policy updated! \n",
      "train step 06900 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.0148 diff={max=07.5577, min=00.0195, mean=00.9929} policy_loss=-10.2239 policy updated! \n",
      "train step 06901 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.3168 diff={max=06.2906, min=00.0059, mean=00.9560} policy_loss=-8.4146 policy updated! \n",
      "train step 06902 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6157 diff={max=06.5095, min=00.0171, mean=00.9468} policy_loss=-10.4398 policy updated! \n",
      "train step 06903 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=06.3786 diff={max=12.3222, min=00.0498, mean=01.3952} policy_loss=-10.2465 policy updated! \n",
      "train step 06904 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6183 diff={max=04.7689, min=00.0913, mean=00.8564} policy_loss=-8.9644 policy updated! \n",
      "train step 06905 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.5101 diff={max=08.8392, min=00.0229, mean=01.1534} policy_loss=-9.6326 policy updated! \n",
      "train step 06906 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1036 diff={max=12.3391, min=00.0104, mean=00.9910} policy_loss=-7.8395 policy updated! \n",
      "train step 06907 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5238 diff={max=06.3050, min=00.0035, mean=01.3288} policy_loss=-8.8475 policy updated! \n",
      "train step 06908 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1322 diff={max=09.1964, min=00.0179, mean=01.2635} policy_loss=-10.1119 policy updated! \n",
      "train step 06909 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.9353 diff={max=08.0789, min=00.0050, mean=01.1692} policy_loss=-12.5812 policy updated! \n",
      "train step 06910 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.6758 diff={max=10.0406, min=00.0308, mean=00.9922} policy_loss=-7.5354 policy updated! \n",
      "train step 06911 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.1010 diff={max=03.9762, min=00.0097, mean=00.9839} policy_loss=-9.6587 policy updated! \n",
      "train step 06912 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4599 diff={max=07.2008, min=00.0234, mean=00.9363} policy_loss=-8.4575 policy updated! \n",
      "train step 06913 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.6748 diff={max=08.4923, min=00.0024, mean=01.0591} policy_loss=-10.7194 policy updated! \n",
      "train step 06914 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.1991 diff={max=05.9553, min=00.0205, mean=00.9683} policy_loss=-9.8347 policy updated! \n",
      "train step 06915 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.4221 diff={max=12.0848, min=00.0428, mean=01.0720} policy_loss=-11.3341 policy updated! \n",
      "train step 06916 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0262 diff={max=03.6526, min=00.0008, mean=00.6605} policy_loss=-9.7036 policy updated! \n",
      "train step 06917 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0763 diff={max=09.3080, min=00.0042, mean=01.1831} policy_loss=-10.1049 policy updated! \n",
      "train step 06918 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9966 diff={max=07.9123, min=00.0482, mean=01.1331} policy_loss=-9.8743 policy updated! \n",
      "train step 06919 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3950 diff={max=07.1548, min=00.0214, mean=01.3070} policy_loss=-9.2009 policy updated! \n",
      "train step 06920 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.0843 diff={max=08.1380, min=00.0069, mean=01.1231} policy_loss=-7.9969 policy updated! \n",
      "train step 06921 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7442 diff={max=09.6589, min=00.0172, mean=01.1695} policy_loss=-8.4982 policy updated! \n",
      "train step 06922 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6666 diff={max=07.1501, min=00.0192, mean=00.9633} policy_loss=-7.9322 policy updated! \n",
      "train step 06923 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7833 diff={max=08.8398, min=00.0155, mean=01.4119} policy_loss=-10.6346 policy updated! \n",
      "train step 06924 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4318 diff={max=04.5631, min=00.0583, mean=00.8290} policy_loss=-10.2982 policy updated! \n",
      "train step 06925 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.0542 diff={max=03.6661, min=00.0012, mean=01.0185} policy_loss=-11.0338 policy updated! \n",
      "train step 06926 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2108 diff={max=09.3724, min=00.0325, mean=01.4407} policy_loss=-10.0337 policy updated! \n",
      "train step 06927 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1207 diff={max=05.1134, min=00.0532, mean=01.2139} policy_loss=-8.9343 policy updated! \n",
      "train step 06928 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6218 diff={max=05.6424, min=00.0038, mean=01.1464} policy_loss=-10.3214 policy updated! \n",
      "train step 06929 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4965 diff={max=09.2134, min=00.0182, mean=01.4544} policy_loss=-9.0649 policy updated! \n",
      "train step 06930 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.5273 diff={max=12.5514, min=00.0038, mean=01.5358} policy_loss=-10.1478 policy updated! \n",
      "train step 06931 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1161 diff={max=07.2983, min=00.0376, mean=00.9026} policy_loss=-8.4315 policy updated! \n",
      "train step 06932 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1427 diff={max=02.6550, min=00.0063, mean=00.8237} policy_loss=-8.3234 policy updated! \n",
      "train step 06933 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9160 diff={max=10.5462, min=00.0202, mean=01.6225} policy_loss=-8.5178 policy updated! \n",
      "train step 06934 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8092 diff={max=08.7648, min=00.0196, mean=01.6706} policy_loss=-9.9582 policy updated! \n",
      "train step 06935 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.5138 diff={max=10.3945, min=00.0362, mean=01.2519} policy_loss=-8.2513 policy updated! \n",
      "train step 06936 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.2954 diff={max=07.1748, min=00.0456, mean=01.5888} policy_loss=-11.3377 policy updated! \n",
      "train step 06937 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4138 diff={max=06.3648, min=00.0070, mean=01.0073} policy_loss=-9.0406 policy updated! \n",
      "train step 06938 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3281 diff={max=09.6259, min=00.0999, mean=01.2884} policy_loss=-9.1677 policy updated! \n",
      "train step 06939 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.2688 diff={max=03.3034, min=00.0256, mean=01.1469} policy_loss=-10.9597 policy updated! \n",
      "train step 06940 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.2041 diff={max=06.9818, min=00.0078, mean=01.1810} policy_loss=-9.2129 policy updated! \n",
      "train step 06941 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7444 diff={max=09.1921, min=00.0012, mean=01.6571} policy_loss=-11.6872 policy updated! \n",
      "train step 06942 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5354 diff={max=08.1944, min=00.0347, mean=01.5653} policy_loss=-9.4982 policy updated! \n",
      "train step 06943 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.2104 diff={max=05.4910, min=00.0029, mean=00.9839} policy_loss=-8.3489 policy updated! \n",
      "train step 06944 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1775 diff={max=05.4604, min=00.0091, mean=01.1494} policy_loss=-8.2975 policy updated! \n",
      "train step 06945 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.9590 diff={max=07.3045, min=00.0054, mean=01.0947} policy_loss=-8.5065 policy updated! \n",
      "train step 06946 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0101 diff={max=09.0414, min=00.0185, mean=01.4239} policy_loss=-8.3214 policy updated! \n",
      "train step 06947 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1180 diff={max=09.1988, min=00.0191, mean=01.2831} policy_loss=-10.0996 policy updated! \n",
      "train step 06948 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1878 diff={max=07.9627, min=00.0028, mean=01.1139} policy_loss=-8.2465 policy updated! \n",
      "train step 06949 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1327 diff={max=08.8645, min=00.0458, mean=01.1910} policy_loss=-9.4016 policy updated! \n",
      "train step 06950 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.5607 diff={max=04.9350, min=00.0115, mean=01.0672} policy_loss=-9.1597 policy updated! \n",
      "train step 06951 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3002 diff={max=07.8697, min=00.0351, mean=01.0676} policy_loss=-10.1190 policy updated! \n",
      "train step 06952 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.6481 diff={max=12.7105, min=00.0154, mean=01.4363} policy_loss=-9.6532 policy updated! \n",
      "train step 06953 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4634 diff={max=05.2619, min=00.0025, mean=01.0199} policy_loss=-8.6079 policy updated! \n",
      "train step 06954 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=04.6635 diff={max=07.5072, min=00.0337, mean=01.3278} policy_loss=-10.1916 policy updated! \n",
      "train step 06955 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.6492 diff={max=04.4517, min=00.0015, mean=00.8055} policy_loss=-8.1858 policy updated! \n",
      "train step 06956 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.9615 diff={max=09.6630, min=00.0053, mean=01.5379} policy_loss=-11.3109 policy updated! \n",
      "train step 06957 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9681 diff={max=07.1999, min=00.0000, mean=01.2967} policy_loss=-8.9141 policy updated! \n",
      "train step 06958 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8450 diff={max=09.6777, min=00.0178, mean=01.2866} policy_loss=-10.4345 policy updated! \n",
      "train step 06959 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0925 diff={max=07.3633, min=00.0101, mean=01.0237} policy_loss=-9.1509 policy updated! \n",
      "train step 06960 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.4761 diff={max=09.5109, min=00.0508, mean=01.7235} policy_loss=-8.9008 policy updated! \n",
      "train step 06961 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2446 diff={max=09.1597, min=00.0514, mean=01.1993} policy_loss=-9.0745 policy updated! \n",
      "train step 06962 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0843 diff={max=10.4252, min=00.0158, mean=01.2762} policy_loss=-9.8106 policy updated! \n",
      "train step 06963 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9376 diff={max=07.2978, min=00.0253, mean=01.3165} policy_loss=-7.7302 policy updated! \n",
      "train step 06964 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.3007 diff={max=07.7019, min=00.0362, mean=01.0773} policy_loss=-9.4851 policy updated! \n",
      "train step 06965 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.1674 diff={max=05.5940, min=00.0071, mean=01.1483} policy_loss=-9.5104 policy updated! \n",
      "train step 06966 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4522 diff={max=09.6963, min=00.0190, mean=01.3907} policy_loss=-9.1047 policy updated! \n",
      "train step 06967 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3394 diff={max=06.1761, min=00.0164, mean=01.0117} policy_loss=-8.3744 policy updated! \n",
      "train step 06968 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7460 diff={max=04.1532, min=00.0097, mean=00.9533} policy_loss=-10.9983 policy updated! \n",
      "train step 06969 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.7238 diff={max=06.6857, min=00.0053, mean=01.0434} policy_loss=-10.7206 policy updated! \n",
      "train step 06970 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.9522 diff={max=05.4861, min=00.0082, mean=00.9635} policy_loss=-10.8551 policy updated! \n",
      "train step 06971 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.9009 diff={max=10.7621, min=00.0232, mean=00.9851} policy_loss=-9.4614 policy updated! \n",
      "train step 06972 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8879 diff={max=07.6025, min=00.0057, mean=01.2938} policy_loss=-9.0782 policy updated! \n",
      "train step 06973 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9225 diff={max=07.8446, min=00.0097, mean=01.1808} policy_loss=-12.3592 policy updated! \n",
      "train step 06974 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9525 diff={max=05.8260, min=00.0491, mean=00.8934} policy_loss=-8.7093 policy updated! \n",
      "train step 06975 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.7006 diff={max=05.7742, min=00.0056, mean=01.0813} policy_loss=-10.0443 policy updated! \n",
      "train step 06976 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.1575 diff={max=12.4657, min=00.0141, mean=01.6005} policy_loss=-9.0682 policy updated! \n",
      "train step 06977 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.6476 diff={max=08.5777, min=00.0040, mean=02.0022} policy_loss=-9.6251 policy updated! \n",
      "train step 06978 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8782 diff={max=06.9631, min=00.0003, mean=01.1538} policy_loss=-10.2236 policy updated! \n",
      "train step 06979 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.4701 diff={max=07.9586, min=00.0190, mean=01.1553} policy_loss=-9.0728 policy updated! \n",
      "train step 06980 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.9437 diff={max=10.4239, min=00.0389, mean=01.3577} policy_loss=-9.2900 policy updated! \n",
      "train step 06981 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.0562 diff={max=08.6174, min=00.0191, mean=01.4985} policy_loss=-12.5811 policy updated! \n",
      "train step 06982 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6559 diff={max=05.0029, min=00.0004, mean=00.8288} policy_loss=-9.9435 policy updated! \n",
      "train step 06983 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.3102 diff={max=05.7025, min=00.0253, mean=01.1175} policy_loss=-9.6516 policy updated! \n",
      "train step 06984 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7797 diff={max=08.4953, min=00.0674, mean=01.4214} policy_loss=-10.5006 policy updated! \n",
      "train step 06985 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.5297 diff={max=04.8012, min=00.0398, mean=01.2306} policy_loss=-11.1483 policy updated! \n",
      "train step 06986 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.3650 diff={max=04.6586, min=00.0199, mean=01.0080} policy_loss=-8.9928 policy updated! \n",
      "train step 06987 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6137 diff={max=09.4195, min=00.0014, mean=01.4424} policy_loss=-9.7792 policy updated! \n",
      "train step 06988 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5366 diff={max=05.2240, min=00.0215, mean=01.0659} policy_loss=-9.5821 policy updated! \n",
      "train step 06989 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.8854 diff={max=06.8309, min=00.0038, mean=00.7971} policy_loss=-11.5119 policy updated! \n",
      "train step 06990 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.8041 diff={max=04.2844, min=00.0133, mean=00.9239} policy_loss=-8.9183 policy updated! \n",
      "train step 06991 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.5920 diff={max=07.5884, min=00.0052, mean=01.6090} policy_loss=-10.3423 policy updated! \n",
      "train step 06992 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8600 diff={max=10.9120, min=00.0625, mean=01.4880} policy_loss=-10.1080 policy updated! \n",
      "train step 06993 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1887 diff={max=07.7270, min=00.0041, mean=01.3252} policy_loss=-8.7951 policy updated! \n",
      "train step 06994 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.8800 diff={max=04.7685, min=00.0166, mean=00.9188} policy_loss=-9.7335 policy updated! \n",
      "train step 06995 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.8403 diff={max=07.8700, min=00.0026, mean=01.3190} policy_loss=-9.5452 policy updated! \n",
      "train step 06996 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8929 diff={max=05.4709, min=00.0018, mean=01.2789} policy_loss=-10.1790 policy updated! \n",
      "train step 06997 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8265 diff={max=06.6518, min=00.0626, mean=01.1257} policy_loss=-10.4743 policy updated! \n",
      "train step 06998 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8387 diff={max=06.1198, min=00.0202, mean=00.8072} policy_loss=-8.3529 policy updated! \n",
      "train step 06999 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2597 diff={max=09.2499, min=00.0053, mean=01.2661} policy_loss=-10.4583 policy updated! \n",
      "train step 07000 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.6971 diff={max=08.2281, min=00.0084, mean=01.0088} policy_loss=-8.5055 policy updated! \n",
      "train step 07001 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.7225 diff={max=04.7574, min=00.0055, mean=01.1327} policy_loss=-9.5045 policy updated! \n",
      "train step 07002 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1510 diff={max=04.1707, min=00.0643, mean=01.0280} policy_loss=-9.5596 policy updated! \n",
      "train step 07003 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.4494 diff={max=06.2357, min=00.0097, mean=01.2771} policy_loss=-10.2795 policy updated! \n",
      "train step 07004 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1058 diff={max=11.8651, min=00.0109, mean=01.0228} policy_loss=-10.7184 policy updated! \n",
      "train step 07005 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=03.3133 diff={max=08.9674, min=00.0017, mean=01.0891} policy_loss=-8.7935 policy updated! \n",
      "train step 07006 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7240 diff={max=10.2720, min=00.0020, mean=01.0778} policy_loss=-10.3989 policy updated! \n",
      "train step 07007 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9592 diff={max=08.9212, min=00.0478, mean=01.1849} policy_loss=-10.4347 policy updated! \n",
      "train step 07008 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.8302 diff={max=04.2237, min=00.0275, mean=00.9946} policy_loss=-9.6879 policy updated! \n",
      "train step 07009 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7498 diff={max=08.0777, min=00.0065, mean=01.1949} policy_loss=-8.8334 policy updated! \n",
      "train step 07010 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=02.8781 diff={max=07.6796, min=00.0164, mean=01.0847} policy_loss=-10.7949 policy updated! \n",
      "train step 07011 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3332 diff={max=07.2263, min=00.0259, mean=01.2560} policy_loss=-10.3224 policy updated! \n",
      "train step 07012 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1680 diff={max=05.9176, min=00.0281, mean=01.2251} policy_loss=-9.8551 policy updated! \n",
      "train step 07013 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.0886 diff={max=06.8568, min=00.0109, mean=01.0635} policy_loss=-10.6398 policy updated! \n",
      "train step 07014 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6934 diff={max=05.5207, min=00.0121, mean=01.0611} policy_loss=-9.9600 policy updated! \n",
      "train step 07015 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.8285 diff={max=06.9819, min=00.0217, mean=01.3107} policy_loss=-10.2206 policy updated! \n",
      "train step 07016 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6898 diff={max=03.7911, min=00.0006, mean=00.8770} policy_loss=-10.8810 policy updated! \n",
      "train step 07017 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0925 diff={max=04.8476, min=00.0333, mean=01.0900} policy_loss=-10.6222 policy updated! \n",
      "train step 07018 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=07.6515 diff={max=11.3042, min=00.0116, mean=01.5634} policy_loss=-9.2835 policy updated! \n",
      "train step 07019 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5795 diff={max=06.3678, min=00.0515, mean=01.0941} policy_loss=-9.1739 policy updated! \n",
      "train step 07020 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.5686 diff={max=07.5772, min=00.0216, mean=01.3015} policy_loss=-9.4728 policy updated! \n",
      "train step 07021 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.6958 diff={max=08.9531, min=00.0227, mean=01.5328} policy_loss=-10.0677 policy updated! \n",
      "train step 07022 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1057 diff={max=08.9724, min=00.0298, mean=00.9965} policy_loss=-8.3340 policy updated! \n",
      "train step 07023 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6711 diff={max=10.1584, min=00.0354, mean=01.1585} policy_loss=-8.9305 policy updated! \n",
      "train step 07024 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.8109 diff={max=04.2072, min=00.0132, mean=01.2033} policy_loss=-10.2062 policy updated! \n",
      "train step 07025 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.5337 diff={max=08.1854, min=00.0269, mean=01.2365} policy_loss=-11.1768 policy updated! \n",
      "train step 07026 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5515 diff={max=07.3273, min=00.0090, mean=01.4083} policy_loss=-11.0228 policy updated! \n",
      "train step 07027 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9983 diff={max=06.5276, min=00.0065, mean=01.1096} policy_loss=-9.7221 policy updated! \n",
      "train step 07028 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5323 diff={max=10.7196, min=00.0218, mean=01.3242} policy_loss=-9.7643 policy updated! \n",
      "train step 07029 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4314 diff={max=04.3542, min=00.0132, mean=01.1415} policy_loss=-9.0616 policy updated! \n",
      "train step 07030 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.6534 diff={max=05.5619, min=00.0577, mean=01.3009} policy_loss=-10.5934 policy updated! \n",
      "train step 07031 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.0934 diff={max=13.5121, min=00.0013, mean=01.6513} policy_loss=-9.9541 policy updated! \n",
      "train step 07032 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0813 diff={max=10.7819, min=00.0019, mean=01.1164} policy_loss=-8.7371 policy updated! \n",
      "train step 07033 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6235 diff={max=07.5088, min=00.0081, mean=01.3261} policy_loss=-8.2273 policy updated! \n",
      "train step 07034 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5207 diff={max=07.7689, min=00.0127, mean=01.3475} policy_loss=-8.6423 policy updated! \n",
      "train step 07035 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=08.5943 diff={max=12.5916, min=00.0054, mean=01.6888} policy_loss=-8.6460 policy updated! \n",
      "train step 07036 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3637 diff={max=07.5243, min=00.0373, mean=01.0296} policy_loss=-10.5557 policy updated! \n",
      "train step 07037 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2341 diff={max=09.5163, min=00.0457, mean=01.2113} policy_loss=-9.1257 policy updated! \n",
      "train step 07038 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9744 diff={max=08.4544, min=00.0029, mean=00.9894} policy_loss=-9.0588 policy updated! \n",
      "train step 07039 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=10.7142 diff={max=14.5994, min=00.0032, mean=01.7619} policy_loss=-11.6141 policy updated! \n",
      "train step 07040 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.4172 diff={max=10.9791, min=00.0088, mean=01.3636} policy_loss=-10.3571 policy updated! \n",
      "train step 07041 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.1555 diff={max=06.6001, min=00.0643, mean=01.0613} policy_loss=-8.3667 policy updated! \n",
      "train step 07042 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1329 diff={max=03.8647, min=00.0127, mean=01.0405} policy_loss=-11.3322 policy updated! \n",
      "train step 07043 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3163 diff={max=08.8660, min=00.0040, mean=00.8019} policy_loss=-8.5925 policy updated! \n",
      "train step 07044 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8037 diff={max=10.7960, min=00.0095, mean=01.3311} policy_loss=-9.4017 policy updated! \n",
      "train step 07045 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.7873 diff={max=06.3918, min=00.0187, mean=01.0310} policy_loss=-7.5544 policy updated! \n",
      "train step 07046 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3295 diff={max=07.3208, min=00.0417, mean=01.1132} policy_loss=-9.4630 policy updated! \n",
      "train step 07047 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6353 diff={max=07.5959, min=00.0104, mean=01.1652} policy_loss=-8.4033 policy updated! \n",
      "train step 07048 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1813 diff={max=06.4866, min=00.0080, mean=01.2432} policy_loss=-9.9697 policy updated! \n",
      "train step 07049 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7657 diff={max=05.6932, min=00.0586, mean=01.2670} policy_loss=-10.8054 policy updated! \n",
      "train step 07050 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.1245 diff={max=07.9467, min=00.0087, mean=01.1334} policy_loss=-11.5158 policy updated! \n",
      "train step 07051 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8885 diff={max=07.5004, min=00.0424, mean=01.2284} policy_loss=-9.8886 policy updated! \n",
      "train step 07052 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1301 diff={max=06.6945, min=00.0081, mean=01.1356} policy_loss=-8.8752 policy updated! \n",
      "train step 07053 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9473 diff={max=09.3406, min=00.0039, mean=01.3801} policy_loss=-10.9298 policy updated! \n",
      "train step 07054 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.8165 diff={max=04.7073, min=00.0281, mean=01.2146} policy_loss=-9.3597 policy updated! \n",
      "train step 07055 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.4257 diff={max=11.7552, min=00.0008, mean=01.5148} policy_loss=-11.2057 policy updated! \n",
      "train step 07056 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.3053 diff={max=07.9325, min=00.0155, mean=00.9685} policy_loss=-9.4201 policy updated! \n",
      "train step 07057 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2022 diff={max=09.6920, min=00.0001, mean=01.0599} policy_loss=-9.3811 policy updated! \n",
      "train step 07058 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8669 diff={max=10.8115, min=00.0510, mean=01.2004} policy_loss=-8.8534 policy updated! \n",
      "train step 07059 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.1747 diff={max=08.7365, min=00.0413, mean=01.0742} policy_loss=-9.1565 policy updated! \n",
      "train step 07060 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.2916 diff={max=10.8998, min=00.0065, mean=01.4696} policy_loss=-10.5862 policy updated! \n",
      "train step 07061 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.2699 diff={max=07.3464, min=00.0025, mean=01.2349} policy_loss=-9.7200 policy updated! \n",
      "train step 07062 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6902 diff={max=07.9639, min=00.0507, mean=01.2950} policy_loss=-10.0655 policy updated! \n",
      "train step 07063 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=02.9535 diff={max=05.0540, min=00.0011, mean=01.1989} policy_loss=-10.7907 policy updated! \n",
      "train step 07064 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8676 diff={max=07.3678, min=00.0156, mean=01.0499} policy_loss=-9.9298 policy updated! \n",
      "train step 07065 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.1435 diff={max=04.9109, min=00.0163, mean=00.9947} policy_loss=-7.6212 policy updated! \n",
      "train step 07066 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.7622 diff={max=06.8376, min=00.0177, mean=00.7304} policy_loss=-8.6040 policy updated! \n",
      "train step 07067 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2508 diff={max=05.4280, min=00.0039, mean=00.7099} policy_loss=-8.3341 policy updated! \n",
      "train step 07068 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6027 diff={max=05.4077, min=00.0080, mean=01.0512} policy_loss=-9.4549 policy updated! \n",
      "train step 07069 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5359 diff={max=10.0846, min=00.0638, mean=01.5537} policy_loss=-9.8816 policy updated! \n",
      "train step 07070 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.7134 diff={max=05.8413, min=00.0262, mean=01.1090} policy_loss=-9.5233 policy updated! \n",
      "train step 07071 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.1345 diff={max=07.6351, min=00.0357, mean=00.8669} policy_loss=-8.8928 policy updated! \n",
      "train step 07072 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6159 diff={max=08.9528, min=00.0139, mean=01.5969} policy_loss=-10.0036 policy updated! \n",
      "train step 07073 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9501 diff={max=09.1725, min=00.0107, mean=01.3298} policy_loss=-11.1836 policy updated! \n",
      "train step 07074 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7644 diff={max=04.5947, min=00.0364, mean=01.0049} policy_loss=-8.8733 policy updated! \n",
      "train step 07075 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.2248 diff={max=03.1965, min=00.0156, mean=00.7959} policy_loss=-11.1227 policy updated! \n",
      "train step 07076 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.0878 diff={max=03.0195, min=00.0062, mean=00.8186} policy_loss=-10.0415 policy updated! \n",
      "train step 07077 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4822 diff={max=06.1483, min=00.0497, mean=01.1462} policy_loss=-9.7675 policy updated! \n",
      "train step 07078 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4993 diff={max=09.1210, min=00.0045, mean=01.1361} policy_loss=-9.1815 policy updated! \n",
      "train step 07079 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.2844 diff={max=09.8829, min=00.0277, mean=01.2316} policy_loss=-9.2799 policy updated! \n",
      "train step 07080 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.9268 diff={max=06.8671, min=00.0292, mean=01.4644} policy_loss=-10.2709 policy updated! \n",
      "train step 07081 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.5980 diff={max=07.8317, min=00.0079, mean=01.2040} policy_loss=-10.1494 policy updated! \n",
      "train step 07082 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7421 diff={max=08.0185, min=00.0671, mean=01.4861} policy_loss=-11.0167 policy updated! \n",
      "train step 07083 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.5922 diff={max=05.7150, min=00.0013, mean=01.0669} policy_loss=-10.1422 policy updated! \n",
      "train step 07084 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.7138 diff={max=06.3894, min=00.0010, mean=00.9508} policy_loss=-8.9428 policy updated! \n",
      "train step 07085 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.2925 diff={max=06.5314, min=00.0236, mean=01.1478} policy_loss=-10.8731 policy updated! \n",
      "train step 07086 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4295 diff={max=08.0040, min=00.0456, mean=01.4052} policy_loss=-10.5142 policy updated! \n",
      "train step 07087 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0744 diff={max=06.4675, min=00.0200, mean=01.0772} policy_loss=-8.4937 policy updated! \n",
      "train step 07088 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.5553 diff={max=05.1798, min=00.0349, mean=00.9906} policy_loss=-10.3435 policy updated! \n",
      "train step 07089 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.2637 diff={max=09.3168, min=00.0048, mean=01.2899} policy_loss=-9.0620 policy updated! \n",
      "train step 07090 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.1302 diff={max=08.3313, min=00.0178, mean=01.0401} policy_loss=-9.9228 policy updated! \n",
      "train step 07091 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.7119 diff={max=03.9179, min=00.0487, mean=00.9223} policy_loss=-9.1965 policy updated! \n",
      "train step 07092 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6122 diff={max=10.6911, min=00.0731, mean=01.2854} policy_loss=-11.5585 policy updated! \n",
      "train step 07093 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5088 diff={max=11.5253, min=00.0029, mean=01.1874} policy_loss=-9.7643 policy updated! \n",
      "train step 07094 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.5914 diff={max=10.9913, min=00.0107, mean=00.9990} policy_loss=-8.9239 policy updated! \n",
      "train step 07095 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.9450 diff={max=06.6895, min=00.0131, mean=01.4362} policy_loss=-10.3944 policy updated! \n",
      "train step 07096 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0639 diff={max=04.8451, min=00.0195, mean=00.9702} policy_loss=-9.3865 policy updated! \n",
      "train step 07097 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5807 diff={max=08.5864, min=00.0151, mean=01.5243} policy_loss=-9.0835 policy updated! \n",
      "train step 07098 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5347 diff={max=07.8761, min=00.0211, mean=01.2345} policy_loss=-9.3832 policy updated! \n",
      "train step 07099 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0834 diff={max=08.9483, min=00.0380, mean=01.0592} policy_loss=-10.6091 policy updated! \n",
      "train step 07100 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.0756 diff={max=13.7643, min=00.0064, mean=01.0151} policy_loss=-9.4810 policy updated! \n",
      "train step 07101 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0510 diff={max=09.9099, min=00.0305, mean=01.1887} policy_loss=-9.0337 policy updated! \n",
      "train step 07102 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=02.5330 diff={max=08.2431, min=00.0010, mean=00.9264} policy_loss=-9.2450 policy updated! \n",
      "train step 07103 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3857 diff={max=07.0083, min=00.0236, mean=01.1278} policy_loss=-8.0715 policy updated! \n",
      "train step 07104 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.8313 diff={max=08.9295, min=00.0101, mean=01.2870} policy_loss=-7.9563 policy updated! \n",
      "train step 07105 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.9501 diff={max=06.2619, min=00.0300, mean=00.9078} policy_loss=-11.1058 policy updated! \n",
      "train step 07106 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4346 diff={max=05.3223, min=00.0036, mean=00.9924} policy_loss=-8.8085 policy updated! \n",
      "train step 07107 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8584 diff={max=06.2308, min=00.0225, mean=01.0823} policy_loss=-10.5626 policy updated! \n",
      "train step 07108 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0164 diff={max=10.7961, min=00.0310, mean=01.3286} policy_loss=-9.0000 policy updated! \n",
      "train step 07109 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0619 diff={max=08.0082, min=00.0103, mean=01.2122} policy_loss=-10.1630 policy updated! \n",
      "train step 07110 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.9876 diff={max=09.1477, min=00.0060, mean=01.0918} policy_loss=-9.7077 policy updated! \n",
      "train step 07111 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8402 diff={max=07.0420, min=00.0367, mean=01.1512} policy_loss=-9.3106 policy updated! \n",
      "train step 07112 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8427 diff={max=07.2929, min=00.0011, mean=01.1958} policy_loss=-11.1103 policy updated! \n",
      "train step 07113 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5882 diff={max=06.9312, min=00.0029, mean=01.4815} policy_loss=-10.0832 policy updated! \n",
      "train step 07114 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4988 diff={max=08.2791, min=00.0015, mean=00.8444} policy_loss=-10.1960 policy updated! \n",
      "train step 07115 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.3629 diff={max=12.8391, min=00.0119, mean=01.7690} policy_loss=-11.4084 policy updated! \n",
      "train step 07116 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8042 diff={max=07.8142, min=00.0197, mean=01.0843} policy_loss=-8.4355 policy updated! \n",
      "train step 07117 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5196 diff={max=07.4648, min=00.0015, mean=01.1469} policy_loss=-8.5628 policy updated! \n",
      "train step 07118 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7283 diff={max=06.0784, min=00.0102, mean=01.0053} policy_loss=-8.4597 policy updated! \n",
      "train step 07119 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7891 diff={max=07.0393, min=00.0033, mean=01.0396} policy_loss=-10.0807 policy updated! \n",
      "train step 07120 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.3713 diff={max=11.1157, min=00.0581, mean=01.7535} policy_loss=-11.3038 policy updated! \n",
      "train step 07121 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9513 diff={max=08.7333, min=00.0093, mean=01.3434} policy_loss=-10.6831 policy updated! \n",
      "train step 07122 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4816 diff={max=08.4091, min=00.0009, mean=01.2848} policy_loss=-9.6738 policy updated! \n",
      "train step 07123 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.7097 diff={max=08.2115, min=00.0165, mean=01.3398} policy_loss=-11.1321 policy updated! \n",
      "train step 07124 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.7415 diff={max=07.1629, min=00.0012, mean=01.1465} policy_loss=-9.2392 policy updated! \n",
      "train step 07125 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.1140 diff={max=03.1664, min=00.0127, mean=00.7850} policy_loss=-8.9184 policy updated! \n",
      "train step 07126 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.8020 diff={max=07.6030, min=00.0021, mean=01.1910} policy_loss=-8.1665 policy updated! \n",
      "train step 07127 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8220 diff={max=08.5841, min=00.0318, mean=01.4807} policy_loss=-8.1454 policy updated! \n",
      "train step 07128 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8122 diff={max=04.7198, min=00.0068, mean=00.9372} policy_loss=-10.1255 policy updated! \n",
      "train step 07129 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6964 diff={max=06.0283, min=00.0126, mean=01.0336} policy_loss=-10.5722 policy updated! \n",
      "train step 07130 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.0827 diff={max=04.9233, min=00.0664, mean=01.1081} policy_loss=-10.5072 policy updated! \n",
      "train step 07131 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.7513 diff={max=12.5187, min=00.0165, mean=01.4471} policy_loss=-9.5053 policy updated! \n",
      "train step 07132 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3752 diff={max=06.6324, min=00.0028, mean=01.0300} policy_loss=-11.7794 policy updated! \n",
      "train step 07133 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6617 diff={max=06.2734, min=00.0206, mean=01.0060} policy_loss=-9.6233 policy updated! \n",
      "train step 07134 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9658 diff={max=07.9427, min=00.0363, mean=01.6335} policy_loss=-10.6187 policy updated! \n",
      "train step 07135 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.1433 diff={max=04.3284, min=00.0097, mean=01.0443} policy_loss=-11.8556 policy updated! \n",
      "train step 07136 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7895 diff={max=09.3752, min=00.0223, mean=01.2362} policy_loss=-10.6767 policy updated! \n",
      "train step 07137 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2425 diff={max=09.8415, min=00.0258, mean=01.3505} policy_loss=-10.0914 policy updated! \n",
      "train step 07138 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0995 diff={max=06.9654, min=00.0135, mean=01.2294} policy_loss=-8.3590 policy updated! \n",
      "train step 07139 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.7841 diff={max=10.9619, min=00.0108, mean=01.6844} policy_loss=-10.9121 policy updated! \n",
      "train step 07140 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.5294 diff={max=06.3554, min=00.0159, mean=01.2978} policy_loss=-10.6982 policy updated! \n",
      "train step 07141 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0162 diff={max=05.7955, min=00.0628, mean=01.2266} policy_loss=-9.4701 policy updated! \n",
      "train step 07142 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8674 diff={max=09.3172, min=00.0257, mean=01.3985} policy_loss=-10.2731 policy updated! \n",
      "train step 07143 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9970 diff={max=08.4950, min=00.0119, mean=00.8415} policy_loss=-7.9526 policy updated! \n",
      "train step 07144 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0853 diff={max=08.4141, min=00.0021, mean=01.2687} policy_loss=-9.6733 policy updated! \n",
      "train step 07145 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.4311 diff={max=08.8319, min=00.0123, mean=01.3556} policy_loss=-8.4814 policy updated! \n",
      "train step 07146 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8081 diff={max=07.7394, min=00.0194, mean=01.1530} policy_loss=-10.4636 policy updated! \n",
      "train step 07147 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4083 diff={max=09.4959, min=00.0026, mean=01.1312} policy_loss=-9.0576 policy updated! \n",
      "train step 07148 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9404 diff={max=06.8128, min=00.0249, mean=01.0303} policy_loss=-8.9315 policy updated! \n",
      "train step 07149 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.5613 diff={max=11.1823, min=00.0447, mean=01.7635} policy_loss=-9.9376 policy updated! \n",
      "train step 07150 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.7909 diff={max=09.2916, min=00.0056, mean=01.7081} policy_loss=-9.7685 policy updated! \n",
      "train step 07151 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.2946 diff={max=08.7970, min=00.0440, mean=01.0456} policy_loss=-9.6904 policy updated! \n",
      "train step 07152 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9580 diff={max=07.5006, min=00.0345, mean=01.2226} policy_loss=-10.1696 policy updated! \n",
      "train step 07153 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9984 diff={max=09.1616, min=00.0195, mean=01.3207} policy_loss=-8.8067 policy updated! \n",
      "train step 07154 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5407 diff={max=07.5517, min=00.0402, mean=01.2683} policy_loss=-10.9014 policy updated! \n",
      "train step 07155 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.5504 diff={max=08.4975, min=00.0146, mean=01.5980} policy_loss=-9.9601 policy updated! \n",
      "train step 07156 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.5097 diff={max=07.3487, min=00.0690, mean=01.2457} policy_loss=-8.6929 policy updated! \n",
      "train step 07157 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1946 diff={max=04.0300, min=00.0456, mean=01.0808} policy_loss=-9.8859 policy updated! \n",
      "train step 07158 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0229 diff={max=07.2463, min=00.0558, mean=01.0974} policy_loss=-8.6339 policy updated! \n",
      "train step 07159 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.4298 diff={max=09.8888, min=00.0090, mean=01.1560} policy_loss=-11.4342 policy updated! \n",
      "train step 07160 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.6452 diff={max=07.6095, min=00.0068, mean=00.9429} policy_loss=-10.2296 policy updated! \n",
      "train step 07161 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6390 diff={max=10.4946, min=00.0600, mean=01.2093} policy_loss=-10.5706 policy updated! \n",
      "train step 07162 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.7857 diff={max=10.8371, min=00.0546, mean=01.8969} policy_loss=-10.6367 policy updated! \n",
      "train step 07163 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3012 diff={max=07.0223, min=00.0537, mean=01.1379} policy_loss=-10.3966 policy updated! \n",
      "train step 07164 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3718 diff={max=07.3555, min=00.0318, mean=01.1471} policy_loss=-11.2873 policy updated! \n",
      "train step 07165 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.1247 diff={max=10.2523, min=00.0127, mean=01.7480} policy_loss=-9.9092 policy updated! \n",
      "train step 07166 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2060 diff={max=05.3407, min=00.0435, mean=01.1099} policy_loss=-8.3732 policy updated! \n",
      "train step 07167 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1006 diff={max=08.1932, min=00.0664, mean=01.1085} policy_loss=-8.2716 policy updated! \n",
      "train step 07168 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9507 diff={max=09.2271, min=00.0330, mean=01.2172} policy_loss=-8.6416 policy updated! \n",
      "train step 07169 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8865 diff={max=06.5562, min=00.0078, mean=01.3181} policy_loss=-9.6060 policy updated! \n",
      "train step 07170 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.8828 diff={max=09.3399, min=00.0236, mean=00.9677} policy_loss=-7.8356 policy updated! \n",
      "train step 07171 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2918 diff={max=08.5237, min=00.0717, mean=01.4421} policy_loss=-11.6002 policy updated! \n",
      "train step 07172 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3660 diff={max=06.2619, min=00.0121, mean=00.9704} policy_loss=-10.7903 policy updated! \n",
      "train step 07173 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4777 diff={max=06.7389, min=00.0065, mean=00.9416} policy_loss=-8.9125 policy updated! \n",
      "train step 07174 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3240 diff={max=07.8117, min=00.0045, mean=01.3211} policy_loss=-9.8053 policy updated! \n",
      "train step 07175 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.6852 diff={max=08.1952, min=00.0021, mean=01.2118} policy_loss=-9.3656 policy updated! \n",
      "train step 07176 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.1167 diff={max=07.8607, min=00.0068, mean=01.1983} policy_loss=-9.4421 policy updated! \n",
      "train step 07177 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8036 diff={max=10.8144, min=00.0015, mean=01.2761} policy_loss=-9.7047 policy updated! \n",
      "train step 07178 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.5918 diff={max=08.5826, min=00.0035, mean=01.4069} policy_loss=-12.1142 policy updated! \n",
      "train step 07179 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3490 diff={max=04.5016, min=00.0075, mean=01.1248} policy_loss=-9.9139 policy updated! \n",
      "train step 07180 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.6723 diff={max=04.0225, min=00.0220, mean=00.9266} policy_loss=-8.0953 policy updated! \n",
      "train step 07181 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9931 diff={max=06.9731, min=00.0290, mean=01.0791} policy_loss=-9.1166 policy updated! \n",
      "train step 07182 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6804 diff={max=08.0002, min=00.0061, mean=01.1828} policy_loss=-8.4602 policy updated! \n",
      "train step 07183 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.0563 diff={max=08.6910, min=00.0034, mean=00.9878} policy_loss=-9.8643 policy updated! \n",
      "train step 07184 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.7784 diff={max=07.0895, min=00.0347, mean=01.3148} policy_loss=-11.1778 policy updated! \n",
      "train step 07185 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.5794 diff={max=03.9588, min=00.0005, mean=00.8961} policy_loss=-9.0297 policy updated! \n",
      "train step 07186 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6478 diff={max=11.8158, min=00.0073, mean=01.2252} policy_loss=-8.5106 policy updated! \n",
      "train step 07187 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4517 diff={max=03.3623, min=00.0049, mean=00.8358} policy_loss=-8.1479 policy updated! \n",
      "train step 07188 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3413 diff={max=07.2359, min=00.0133, mean=00.9709} policy_loss=-10.4459 policy updated! \n",
      "train step 07189 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4329 diff={max=12.1686, min=00.0046, mean=01.1620} policy_loss=-11.2580 policy updated! \n",
      "train step 07190 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.4887 diff={max=09.4734, min=00.0112, mean=01.2604} policy_loss=-10.9230 policy updated! \n",
      "train step 07191 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9435 diff={max=07.2698, min=00.0166, mean=01.1784} policy_loss=-9.3674 policy updated! \n",
      "train step 07192 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8169 diff={max=06.4539, min=00.0099, mean=01.2284} policy_loss=-9.5789 policy updated! \n",
      "train step 07193 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.2758 diff={max=06.1059, min=00.0248, mean=01.1816} policy_loss=-10.1358 policy updated! \n",
      "train step 07194 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8135 diff={max=06.2178, min=00.0193, mean=01.1059} policy_loss=-10.0086 policy updated! \n",
      "train step 07195 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.7238 diff={max=07.9882, min=00.0337, mean=01.0217} policy_loss=-9.1814 policy updated! \n",
      "train step 07196 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9217 diff={max=06.3076, min=00.0465, mean=01.2309} policy_loss=-9.2268 policy updated! \n",
      "train step 07197 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4808 diff={max=06.9140, min=00.0010, mean=01.1393} policy_loss=-9.8141 policy updated! \n",
      "train step 07198 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5781 diff={max=04.2601, min=00.0013, mean=01.0904} policy_loss=-8.2520 policy updated! \n",
      "train step 07199 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9755 diff={max=09.7384, min=00.0094, mean=01.1718} policy_loss=-8.9870 policy updated! \n",
      "train step 07200 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.6561 diff={max=07.0840, min=00.0058, mean=01.3281} policy_loss=-11.1649 policy updated! \n",
      "train step 07201 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6123 diff={max=07.5256, min=00.0460, mean=01.1187} policy_loss=-10.1493 policy updated! \n",
      "train step 07202 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6203 diff={max=05.4315, min=00.0342, mean=01.0546} policy_loss=-11.0271 policy updated! \n",
      "train step 07203 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2809 diff={max=04.7005, min=00.0366, mean=01.0292} policy_loss=-10.3561 policy updated! \n",
      "train step 07204 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5871 diff={max=10.2576, min=00.0066, mean=01.3711} policy_loss=-9.9699 policy updated! \n",
      "train step 07205 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.5187 diff={max=06.7055, min=00.0101, mean=01.3018} policy_loss=-9.3483 policy updated! \n",
      "train step 07206 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8116 diff={max=05.1001, min=00.0471, mean=01.1595} policy_loss=-10.0294 policy updated! \n",
      "train step 07207 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7608 diff={max=06.5152, min=00.0174, mean=01.1484} policy_loss=-9.8787 policy updated! \n",
      "train step 07208 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0064 diff={max=07.2076, min=00.0058, mean=01.1725} policy_loss=-8.8153 policy updated! \n",
      "train step 07209 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.5746 diff={max=10.5466, min=00.0670, mean=01.6995} policy_loss=-8.9866 policy updated! \n",
      "train step 07210 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.6392 diff={max=06.8639, min=00.0369, mean=01.4056} policy_loss=-9.2546 policy updated! \n",
      "train step 07211 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1297 diff={max=07.2220, min=00.0168, mean=00.8451} policy_loss=-11.0469 policy updated! \n",
      "train step 07212 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7504 diff={max=04.3582, min=00.0055, mean=01.0116} policy_loss=-9.1161 policy updated! \n",
      "train step 07213 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.6194 diff={max=05.4305, min=00.0187, mean=01.0921} policy_loss=-12.1952 policy updated! \n",
      "train step 07214 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2872 diff={max=03.4942, min=00.0852, mean=01.2306} policy_loss=-10.2587 policy updated! \n",
      "train step 07215 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.6796 diff={max=06.1062, min=00.0199, mean=01.1111} policy_loss=-11.4267 policy updated! \n",
      "train step 07216 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.3392 diff={max=04.9751, min=00.0142, mean=01.2992} policy_loss=-11.9209 policy updated! \n",
      "train step 07217 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5683 diff={max=03.7007, min=00.0457, mean=00.8767} policy_loss=-11.0164 policy updated! \n",
      "train step 07218 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3329 diff={max=06.2739, min=00.0195, mean=00.9168} policy_loss=-9.3234 policy updated! \n",
      "train step 07219 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1544 diff={max=08.2077, min=00.0304, mean=01.2576} policy_loss=-9.2898 policy updated! \n",
      "train step 07220 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.1909 diff={max=06.9758, min=00.0430, mean=01.1713} policy_loss=-8.9190 policy updated! \n",
      "train step 07221 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1869 diff={max=07.1737, min=00.0253, mean=01.1162} policy_loss=-11.5933 policy updated! \n",
      "train step 07222 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9347 diff={max=13.2535, min=00.0050, mean=01.0484} policy_loss=-8.3924 policy updated! \n",
      "train step 07223 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.8654 diff={max=06.9376, min=00.0005, mean=01.0286} policy_loss=-10.9108 policy updated! \n",
      "train step 07224 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1937 diff={max=07.2936, min=00.0201, mean=00.9978} policy_loss=-10.5508 policy updated! \n",
      "train step 07225 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.6550 diff={max=08.3931, min=00.0025, mean=01.3722} policy_loss=-9.4069 policy updated! \n",
      "train step 07226 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5666 diff={max=04.6109, min=00.0273, mean=00.8361} policy_loss=-9.7834 policy updated! \n",
      "train step 07227 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2421 diff={max=05.8213, min=00.0117, mean=01.1010} policy_loss=-11.1578 policy updated! \n",
      "train step 07228 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9961 diff={max=07.6121, min=00.0232, mean=01.5929} policy_loss=-9.9963 policy updated! \n",
      "train step 07229 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=07.6962 diff={max=13.3840, min=00.0087, mean=01.5385} policy_loss=-11.4048 policy updated! \n",
      "train step 07230 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.1975 diff={max=06.6954, min=00.0372, mean=00.9627} policy_loss=-8.9246 policy updated! \n",
      "train step 07231 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3079 diff={max=06.9155, min=00.0735, mean=01.1476} policy_loss=-9.3117 policy updated! \n",
      "train step 07232 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2325 diff={max=05.9955, min=00.0078, mean=01.1781} policy_loss=-9.8992 policy updated! \n",
      "train step 07233 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1917 diff={max=04.8895, min=00.0004, mean=00.9843} policy_loss=-9.8457 policy updated! \n",
      "train step 07234 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.2727 diff={max=08.4562, min=00.0256, mean=01.3621} policy_loss=-10.1346 policy updated! \n",
      "train step 07235 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.7515 diff={max=07.4280, min=00.0422, mean=01.4597} policy_loss=-9.7596 policy updated! \n",
      "train step 07236 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7137 diff={max=07.5930, min=00.0062, mean=01.3906} policy_loss=-10.3933 policy updated! \n",
      "train step 07237 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.3768 diff={max=19.8593, min=00.0045, mean=01.6935} policy_loss=-9.8078 policy updated! \n",
      "train step 07238 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9513 diff={max=05.2566, min=00.0029, mean=00.9074} policy_loss=-9.3646 policy updated! \n",
      "train step 07239 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.3325 diff={max=04.6195, min=00.0033, mean=00.7820} policy_loss=-9.3500 policy updated! \n",
      "train step 07240 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.4390 diff={max=08.1393, min=00.0204, mean=01.2908} policy_loss=-10.4887 policy updated! \n",
      "train step 07241 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9462 diff={max=10.0803, min=00.0215, mean=01.3025} policy_loss=-10.3144 policy updated! \n",
      "train step 07242 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8630 diff={max=02.6130, min=00.0000, mean=00.7013} policy_loss=-8.5344 policy updated! \n",
      "train step 07243 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2098 diff={max=14.5193, min=00.0014, mean=01.1795} policy_loss=-10.8561 policy updated! \n",
      "train step 07244 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0256 diff={max=07.3870, min=00.0009, mean=01.2838} policy_loss=-10.3984 policy updated! \n",
      "train step 07245 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.4540 diff={max=09.2485, min=00.0083, mean=01.3050} policy_loss=-9.3974 policy updated! \n",
      "train step 07246 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.9360 diff={max=04.2834, min=00.0272, mean=00.9339} policy_loss=-10.2527 policy updated! \n",
      "train step 07247 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1686 diff={max=08.6549, min=00.0050, mean=01.2492} policy_loss=-10.3853 policy updated! \n",
      "train step 07248 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.2114 diff={max=05.6631, min=00.0355, mean=01.1427} policy_loss=-11.9696 policy updated! \n",
      "train step 07249 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.4303 diff={max=08.1032, min=00.0144, mean=01.3437} policy_loss=-8.6256 policy updated! \n",
      "train step 07250 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.3925 diff={max=07.1110, min=00.0185, mean=00.9620} policy_loss=-11.0449 policy updated! \n",
      "train step 07251 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.0358 diff={max=10.3987, min=00.0217, mean=01.5818} policy_loss=-9.0434 policy updated! \n",
      "train step 07252 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5171 diff={max=05.8795, min=00.0188, mean=00.9908} policy_loss=-9.5545 policy updated! \n",
      "train step 07253 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.0987 diff={max=09.5171, min=00.0139, mean=01.1639} policy_loss=-10.9191 policy updated! \n",
      "train step 07254 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7509 diff={max=05.5343, min=00.0029, mean=01.1081} policy_loss=-9.4744 policy updated! \n",
      "train step 07255 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.2193 diff={max=05.8908, min=00.0145, mean=01.1710} policy_loss=-9.3246 policy updated! \n",
      "train step 07256 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6064 diff={max=15.6662, min=00.0123, mean=01.1701} policy_loss=-11.4948 policy updated! \n",
      "train step 07257 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5627 diff={max=08.1645, min=00.0231, mean=01.1517} policy_loss=-10.5473 policy updated! \n",
      "train step 07258 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9455 diff={max=07.0121, min=00.0106, mean=01.0324} policy_loss=-8.9193 policy updated! \n",
      "train step 07259 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8309 diff={max=08.7884, min=00.0052, mean=01.2329} policy_loss=-8.7144 policy updated! \n",
      "train step 07260 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.3976 diff={max=11.3462, min=00.0234, mean=01.7469} policy_loss=-9.7624 policy updated! \n",
      "train step 07261 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3149 diff={max=08.2190, min=00.0397, mean=01.5493} policy_loss=-9.4983 policy updated! \n",
      "train step 07262 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9461 diff={max=06.1851, min=00.0623, mean=01.0856} policy_loss=-9.3994 policy updated! \n",
      "train step 07263 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8586 diff={max=07.6983, min=00.0087, mean=01.0336} policy_loss=-10.6868 policy updated! \n",
      "train step 07264 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0802 diff={max=07.6977, min=00.0311, mean=01.4328} policy_loss=-12.1624 policy updated! \n",
      "train step 07265 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.5548 diff={max=07.9304, min=00.0058, mean=01.2531} policy_loss=-10.1390 policy updated! \n",
      "train step 07266 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2422 diff={max=05.9030, min=00.0169, mean=00.8365} policy_loss=-10.2866 policy updated! \n",
      "train step 07267 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2963 diff={max=03.3177, min=00.0199, mean=00.8651} policy_loss=-9.9280 policy updated! \n",
      "train step 07268 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0489 diff={max=06.5720, min=00.0028, mean=01.0789} policy_loss=-8.9384 policy updated! \n",
      "train step 07269 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9950 diff={max=09.5302, min=00.0015, mean=01.2843} policy_loss=-9.0081 policy updated! \n",
      "train step 07270 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.5247 diff={max=04.2239, min=00.0140, mean=00.8452} policy_loss=-9.1010 policy updated! \n",
      "train step 07271 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8376 diff={max=07.2826, min=00.0397, mean=01.1382} policy_loss=-9.3470 policy updated! \n",
      "train step 07272 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0307 diff={max=06.9910, min=00.0108, mean=01.2430} policy_loss=-7.8797 policy updated! \n",
      "train step 07273 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.2733 diff={max=06.1540, min=00.0297, mean=01.5559} policy_loss=-9.9005 policy updated! \n",
      "train step 07274 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.9044 diff={max=08.5022, min=00.0001, mean=01.5371} policy_loss=-9.9608 policy updated! \n",
      "train step 07275 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.9570 diff={max=09.1071, min=00.0144, mean=01.4496} policy_loss=-9.3925 policy updated! \n",
      "train step 07276 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3524 diff={max=05.9007, min=00.0122, mean=01.2336} policy_loss=-10.5456 policy updated! \n",
      "train step 07277 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0505 diff={max=04.8014, min=00.0038, mean=01.0664} policy_loss=-10.7451 policy updated! \n",
      "train step 07278 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2314 diff={max=04.6383, min=00.0013, mean=01.0267} policy_loss=-11.6859 policy updated! \n",
      "train step 07279 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0127 diff={max=05.2265, min=00.0002, mean=00.9036} policy_loss=-9.4716 policy updated! \n",
      "train step 07280 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.4372 diff={max=08.5255, min=00.0495, mean=01.1051} policy_loss=-9.8422 policy updated! \n",
      "train step 07281 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3519 diff={max=04.2280, min=00.0231, mean=00.7537} policy_loss=-9.3478 policy updated! \n",
      "train step 07282 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9655 diff={max=08.6475, min=00.0152, mean=01.3027} policy_loss=-9.6872 policy updated! \n",
      "train step 07283 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.0422 diff={max=05.6305, min=00.0088, mean=00.8809} policy_loss=-10.2950 policy updated! \n",
      "train step 07284 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8557 diff={max=16.1208, min=00.0006, mean=01.0805} policy_loss=-9.1982 policy updated! \n",
      "train step 07285 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.5048 diff={max=06.1742, min=00.0052, mean=01.2197} policy_loss=-9.3223 policy updated! \n",
      "train step 07286 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.8804 diff={max=06.4825, min=00.0154, mean=00.8016} policy_loss=-9.0814 policy updated! \n",
      "train step 07287 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.2309 diff={max=12.8612, min=00.0403, mean=01.6760} policy_loss=-10.6387 policy updated! \n",
      "train step 07288 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.6018 diff={max=13.0890, min=00.0213, mean=01.4841} policy_loss=-10.5751 policy updated! \n",
      "train step 07289 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1339 diff={max=04.4078, min=00.0178, mean=01.0234} policy_loss=-11.2172 policy updated! \n",
      "train step 07290 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.8092 diff={max=10.4841, min=00.0011, mean=01.0952} policy_loss=-10.7986 policy updated! \n",
      "train step 07291 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=17.2867 diff={max=24.4853, min=00.0266, mean=01.6393} policy_loss=-10.5487 policy updated! \n",
      "train step 07292 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7229 diff={max=06.7098, min=00.0018, mean=01.4882} policy_loss=-9.2833 policy updated! \n",
      "train step 07293 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.1304 diff={max=09.4237, min=00.0239, mean=01.3072} policy_loss=-9.5759 policy updated! \n",
      "train step 07294 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5779 diff={max=03.9986, min=00.0044, mean=00.8163} policy_loss=-9.7109 policy updated! \n",
      "train step 07295 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.1364 diff={max=11.3550, min=00.0434, mean=01.7473} policy_loss=-10.3107 policy updated! \n",
      "train step 07296 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1260 diff={max=05.8443, min=00.0258, mean=00.9304} policy_loss=-9.9293 policy updated! \n",
      "train step 07297 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3993 diff={max=09.8109, min=00.0031, mean=01.3824} policy_loss=-9.6229 policy updated! \n",
      "train step 07298 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.4630 diff={max=07.5911, min=00.0492, mean=01.1405} policy_loss=-10.9552 policy updated! \n",
      "train step 07299 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.1072 diff={max=13.9770, min=00.0236, mean=01.2259} policy_loss=-8.4362 policy updated! \n",
      "train step 07300 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.5902 diff={max=08.7106, min=00.0269, mean=01.1161} policy_loss=-11.1850 policy updated! \n",
      "train step 07301 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9299 diff={max=09.0137, min=00.0032, mean=01.2445} policy_loss=-11.5086 policy updated! \n",
      "train step 07302 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3028 diff={max=07.0815, min=00.0050, mean=01.0767} policy_loss=-10.6994 policy updated! \n",
      "train step 07303 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3198 diff={max=07.2142, min=00.0137, mean=01.1284} policy_loss=-10.1769 policy updated! \n",
      "train step 07304 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.5103 diff={max=08.0638, min=00.0243, mean=01.5052} policy_loss=-11.8604 policy updated! \n",
      "train step 07305 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.0748 diff={max=06.8070, min=00.0007, mean=01.1370} policy_loss=-10.2387 policy updated! \n",
      "train step 07306 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.5304 diff={max=06.5718, min=00.0268, mean=01.1874} policy_loss=-9.5654 policy updated! \n",
      "train step 07307 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4622 diff={max=05.6606, min=00.0275, mean=01.1729} policy_loss=-8.0528 policy updated! \n",
      "train step 07308 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=06.7747 diff={max=08.8920, min=00.0078, mean=01.4699} policy_loss=-10.6114 policy updated! \n",
      "train step 07309 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0933 diff={max=09.2571, min=00.0034, mean=01.2460} policy_loss=-8.6172 policy updated! \n",
      "train step 07310 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.5864 diff={max=04.6842, min=00.0059, mean=00.8620} policy_loss=-10.0566 policy updated! \n",
      "train step 07311 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.5887 diff={max=09.2948, min=00.0222, mean=01.5515} policy_loss=-9.8289 policy updated! \n",
      "train step 07312 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7678 diff={max=06.8685, min=00.0051, mean=01.2675} policy_loss=-9.2219 policy updated! \n",
      "train step 07313 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3651 diff={max=06.3530, min=00.0131, mean=01.1648} policy_loss=-9.9664 policy updated! \n",
      "train step 07314 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.0041 diff={max=06.8024, min=00.0070, mean=01.3504} policy_loss=-8.6469 policy updated! \n",
      "train step 07315 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.7685 diff={max=03.2138, min=00.0050, mean=01.0018} policy_loss=-10.1571 policy updated! \n",
      "train step 07316 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6588 diff={max=06.5482, min=00.0193, mean=01.2295} policy_loss=-9.0770 policy updated! \n",
      "train step 07317 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5261 diff={max=07.8695, min=00.0163, mean=01.3119} policy_loss=-8.6330 policy updated! \n",
      "train step 07318 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6146 diff={max=09.0012, min=00.0321, mean=01.2849} policy_loss=-7.6752 policy updated! \n",
      "train step 07319 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8537 diff={max=08.7082, min=00.0182, mean=01.6169} policy_loss=-10.8204 policy updated! \n",
      "train step 07320 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.8873 diff={max=06.1286, min=00.0008, mean=00.8074} policy_loss=-9.5093 policy updated! \n",
      "train step 07321 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7970 diff={max=07.1789, min=00.0084, mean=00.9879} policy_loss=-10.1212 policy updated! \n",
      "train step 07322 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8884 diff={max=02.9515, min=00.0031, mean=00.6706} policy_loss=-8.3738 policy updated! \n",
      "train step 07323 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5704 diff={max=03.5718, min=00.0184, mean=00.9199} policy_loss=-10.6098 policy updated! \n",
      "train step 07324 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.5801 diff={max=08.6359, min=00.0062, mean=01.2828} policy_loss=-10.3869 policy updated! \n",
      "train step 07325 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=15.9885 diff={max=22.7679, min=00.0327, mean=01.7444} policy_loss=-10.0002 policy updated! \n",
      "train step 07326 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2834 diff={max=05.0120, min=00.0217, mean=01.2660} policy_loss=-10.3156 policy updated! \n",
      "train step 07327 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7995 diff={max=07.2977, min=00.0004, mean=00.9711} policy_loss=-10.1054 policy updated! \n",
      "train step 07328 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8399 diff={max=10.8565, min=00.0078, mean=01.2876} policy_loss=-9.3624 policy updated! \n",
      "train step 07329 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=03.1002 diff={max=07.3895, min=00.0049, mean=01.0257} policy_loss=-9.3685 policy updated! \n",
      "train step 07330 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.2513 diff={max=08.6318, min=00.0160, mean=01.1828} policy_loss=-8.7747 policy updated! \n",
      "train step 07331 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.9858 diff={max=09.0484, min=00.0185, mean=01.1814} policy_loss=-9.0560 policy updated! \n",
      "train step 07332 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8199 diff={max=07.5475, min=00.0054, mean=01.1935} policy_loss=-8.5134 policy updated! \n",
      "train step 07333 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4868 diff={max=05.8732, min=00.0601, mean=01.1647} policy_loss=-11.2451 policy updated! \n",
      "train step 07334 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=04.9088 diff={max=08.8349, min=00.0228, mean=01.2942} policy_loss=-10.5656 policy updated! \n",
      "train step 07335 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.3144 diff={max=05.8156, min=00.0024, mean=01.2116} policy_loss=-10.2199 policy updated! \n",
      "train step 07336 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.7676 diff={max=14.4498, min=00.0195, mean=01.3594} policy_loss=-10.2028 policy updated! \n",
      "train step 07337 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.8237 diff={max=10.5214, min=00.0077, mean=01.7933} policy_loss=-10.3635 policy updated! \n",
      "train step 07338 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.6939 diff={max=03.8087, min=00.0144, mean=00.9214} policy_loss=-9.1282 policy updated! \n",
      "train step 07339 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5756 diff={max=10.9137, min=00.0020, mean=01.2565} policy_loss=-10.7313 policy updated! \n",
      "train step 07340 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.7272 diff={max=06.0855, min=00.0100, mean=01.0283} policy_loss=-9.9829 policy updated! \n",
      "train step 07341 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9175 diff={max=07.8750, min=00.0094, mean=01.4412} policy_loss=-11.6670 policy updated! \n",
      "train step 07342 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.5139 diff={max=10.8776, min=00.0067, mean=02.0370} policy_loss=-11.6564 policy updated! \n",
      "train step 07343 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.3504 diff={max=07.2920, min=00.0088, mean=01.1487} policy_loss=-10.0598 policy updated! \n",
      "train step 07344 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5307 diff={max=05.5571, min=00.0030, mean=00.9446} policy_loss=-9.5675 policy updated! \n",
      "train step 07345 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.0285 diff={max=13.7518, min=00.0487, mean=01.4349} policy_loss=-9.9142 policy updated! \n",
      "train step 07346 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5060 diff={max=10.0626, min=00.0171, mean=00.9538} policy_loss=-8.5353 policy updated! \n",
      "train step 07347 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9033 diff={max=08.8685, min=00.0381, mean=01.3467} policy_loss=-9.7951 policy updated! \n",
      "train step 07348 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.8263 diff={max=04.0199, min=00.0049, mean=00.9073} policy_loss=-8.7323 policy updated! \n",
      "train step 07349 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9911 diff={max=11.7842, min=00.0089, mean=01.3961} policy_loss=-8.2687 policy updated! \n",
      "train step 07350 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.8149 diff={max=05.9460, min=00.0222, mean=01.3086} policy_loss=-9.3408 policy updated! \n",
      "train step 07351 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0290 diff={max=06.2572, min=00.0005, mean=00.8987} policy_loss=-8.2741 policy updated! \n",
      "train step 07352 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9804 diff={max=04.6790, min=00.0211, mean=01.0111} policy_loss=-8.5506 policy updated! \n",
      "train step 07353 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9554 diff={max=07.3052, min=00.0046, mean=01.4537} policy_loss=-9.8087 policy updated! \n",
      "train step 07354 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.6100 diff={max=10.0033, min=00.0103, mean=01.7448} policy_loss=-9.7735 policy updated! \n",
      "train step 07355 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.9889 diff={max=14.5408, min=00.0028, mean=01.4388} policy_loss=-9.4969 policy updated! \n",
      "train step 07356 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2878 diff={max=08.1448, min=00.0472, mean=01.0598} policy_loss=-9.4119 policy updated! \n",
      "train step 07357 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0247 diff={max=10.3825, min=00.0019, mean=01.1392} policy_loss=-9.3543 policy updated! \n",
      "train step 07358 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.9277 diff={max=07.2298, min=00.1005, mean=01.3093} policy_loss=-11.9125 policy updated! \n",
      "train step 07359 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1185 diff={max=04.6763, min=00.0403, mean=00.9951} policy_loss=-11.7542 policy updated! \n",
      "train step 07360 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.8829 diff={max=08.5006, min=00.0068, mean=01.0966} policy_loss=-10.3700 policy updated! \n",
      "train step 07361 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5059 diff={max=11.0494, min=00.0060, mean=01.2517} policy_loss=-11.0078 policy updated! \n",
      "train step 07362 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2156 diff={max=06.3943, min=00.0118, mean=01.2264} policy_loss=-9.7844 policy updated! \n",
      "train step 07363 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4472 diff={max=07.6239, min=00.0138, mean=01.1234} policy_loss=-10.6156 policy updated! \n",
      "train step 07364 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2465 diff={max=02.7681, min=00.0073, mean=00.8347} policy_loss=-10.0692 policy updated! \n",
      "train step 07365 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.7150 diff={max=06.1476, min=00.0051, mean=00.8258} policy_loss=-9.1549 policy updated! \n",
      "train step 07366 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4478 diff={max=07.9475, min=00.0043, mean=01.3692} policy_loss=-11.7954 policy updated! \n",
      "train step 07367 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1665 diff={max=07.9023, min=00.0034, mean=01.1629} policy_loss=-9.2171 policy updated! \n",
      "train step 07368 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3969 diff={max=06.9480, min=00.0481, mean=01.1085} policy_loss=-8.9492 policy updated! \n",
      "train step 07369 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8127 diff={max=09.1565, min=00.0657, mean=00.9235} policy_loss=-9.7367 policy updated! \n",
      "train step 07370 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.0027 diff={max=06.6257, min=00.0069, mean=01.2529} policy_loss=-9.9090 policy updated! \n",
      "train step 07371 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9875 diff={max=05.1439, min=00.0537, mean=01.0023} policy_loss=-10.3519 policy updated! \n",
      "train step 07372 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4070 diff={max=05.6707, min=00.0023, mean=00.9857} policy_loss=-10.2541 policy updated! \n",
      "train step 07373 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3524 diff={max=08.5537, min=00.0116, mean=01.1013} policy_loss=-8.0821 policy updated! \n",
      "train step 07374 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3635 diff={max=09.7953, min=00.0122, mean=01.2377} policy_loss=-11.5795 policy updated! \n",
      "train step 07375 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.0480 diff={max=05.9082, min=00.0085, mean=01.1763} policy_loss=-10.0145 policy updated! \n",
      "train step 07376 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0997 diff={max=05.6770, min=00.0121, mean=00.9309} policy_loss=-9.6902 policy updated! \n",
      "train step 07377 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4483 diff={max=05.8966, min=00.0005, mean=01.0625} policy_loss=-11.0805 policy updated! \n",
      "train step 07378 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8379 diff={max=08.9981, min=00.0306, mean=01.3530} policy_loss=-9.8655 policy updated! \n",
      "train step 07379 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4951 diff={max=09.1227, min=00.0304, mean=01.6108} policy_loss=-10.5630 policy updated! \n",
      "train step 07380 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.4049 diff={max=06.9443, min=00.0027, mean=00.9789} policy_loss=-12.1506 policy updated! \n",
      "train step 07381 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1136 diff={max=04.7430, min=00.0189, mean=01.0733} policy_loss=-12.2640 policy updated! \n",
      "train step 07382 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6898 diff={max=08.0058, min=00.0267, mean=01.0577} policy_loss=-10.9688 policy updated! \n",
      "train step 07383 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.9048 diff={max=07.0249, min=00.0226, mean=00.8829} policy_loss=-10.4791 policy updated! \n",
      "train step 07384 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4476 diff={max=08.9081, min=00.0548, mean=01.3111} policy_loss=-9.6225 policy updated! \n",
      "train step 07385 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.7436 diff={max=05.4762, min=00.0097, mean=00.8058} policy_loss=-9.9650 policy updated! \n",
      "train step 07386 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0015 diff={max=09.3654, min=00.0269, mean=00.9923} policy_loss=-9.9405 policy updated! \n",
      "train step 07387 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6016 diff={max=05.5412, min=00.0039, mean=00.9620} policy_loss=-9.4328 policy updated! \n",
      "train step 07388 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.4291 diff={max=08.1807, min=00.0338, mean=01.3369} policy_loss=-9.3915 policy updated! \n",
      "train step 07389 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6608 diff={max=08.7971, min=00.0005, mean=01.1293} policy_loss=-10.8764 policy updated! \n",
      "train step 07390 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.9837 diff={max=07.3567, min=00.0069, mean=00.8318} policy_loss=-9.6750 policy updated! \n",
      "train step 07391 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2479 diff={max=07.9106, min=00.0026, mean=01.0461} policy_loss=-9.8947 policy updated! \n",
      "train step 07392 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0143 diff={max=08.0767, min=00.0206, mean=01.4935} policy_loss=-9.8345 policy updated! \n",
      "train step 07393 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5769 diff={max=10.2827, min=00.0201, mean=01.0894} policy_loss=-9.3914 policy updated! \n",
      "train step 07394 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.7075 diff={max=07.5384, min=00.0214, mean=00.9708} policy_loss=-9.3685 policy updated! \n",
      "train step 07395 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.2454 diff={max=06.6813, min=00.0038, mean=01.0364} policy_loss=-10.0070 policy updated! \n",
      "train step 07396 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.9273 diff={max=04.5436, min=00.0027, mean=00.9182} policy_loss=-9.1535 policy updated! \n",
      "train step 07397 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3969 diff={max=09.0197, min=00.0222, mean=01.2671} policy_loss=-11.0034 policy updated! \n",
      "train step 07398 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3332 diff={max=08.9740, min=00.0224, mean=01.1513} policy_loss=-10.4311 policy updated! \n",
      "train step 07399 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.6235 diff={max=04.7592, min=00.0212, mean=01.1661} policy_loss=-11.4584 policy updated! \n",
      "train step 07400 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=00.9343 diff={max=03.2071, min=00.0037, mean=00.6957} policy_loss=-10.6838 policy updated! \n",
      "train step 07401 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.1553 diff={max=08.2505, min=00.0175, mean=00.9622} policy_loss=-9.4047 policy updated! \n",
      "train step 07402 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5566 diff={max=06.4382, min=00.0420, mean=01.3471} policy_loss=-10.0041 policy updated! \n",
      "train step 07403 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.5180 diff={max=07.3163, min=00.0220, mean=00.9997} policy_loss=-10.1424 policy updated! \n",
      "train step 07404 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8704 diff={max=13.6983, min=00.0029, mean=01.3899} policy_loss=-10.4300 policy updated! \n",
      "train step 07405 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.1210 diff={max=08.4864, min=00.0467, mean=01.1071} policy_loss=-9.8245 policy updated! \n",
      "train step 07406 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1067 diff={max=12.7476, min=00.0041, mean=01.3040} policy_loss=-11.8315 policy updated! \n",
      "train step 07407 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2444 diff={max=13.7207, min=00.0499, mean=01.1205} policy_loss=-9.3291 policy updated! \n",
      "train step 07408 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.7359 diff={max=07.1207, min=00.0219, mean=01.0760} policy_loss=-8.7605 policy updated! \n",
      "train step 07409 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1324 diff={max=08.4351, min=00.0046, mean=01.4010} policy_loss=-10.7856 policy updated! \n",
      "train step 07410 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.1525 diff={max=03.8191, min=00.0168, mean=00.7534} policy_loss=-9.3949 policy updated! \n",
      "train step 07411 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.7785 diff={max=11.2044, min=00.0022, mean=01.6345} policy_loss=-9.9196 policy updated! \n",
      "train step 07412 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4267 diff={max=07.5726, min=00.0034, mean=01.1166} policy_loss=-10.4171 policy updated! \n",
      "train step 07413 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3315 diff={max=04.5800, min=00.0007, mean=01.0076} policy_loss=-10.8509 policy updated! \n",
      "train step 07414 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1157 diff={max=11.9065, min=00.0333, mean=00.9997} policy_loss=-10.5041 policy updated! \n",
      "train step 07415 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.5857 diff={max=09.5581, min=00.0294, mean=01.1613} policy_loss=-9.7218 policy updated! \n",
      "train step 07416 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5512 diff={max=04.1450, min=00.0023, mean=00.8149} policy_loss=-8.0160 policy updated! \n",
      "train step 07417 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9445 diff={max=06.9489, min=00.0038, mean=01.0153} policy_loss=-10.9387 policy updated! \n",
      "train step 07418 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2765 diff={max=11.3175, min=00.0178, mean=01.2807} policy_loss=-9.4567 policy updated! \n",
      "train step 07419 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6410 diff={max=03.9525, min=00.0165, mean=00.8923} policy_loss=-10.8274 policy updated! \n",
      "train step 07420 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.8074 diff={max=07.2865, min=00.0183, mean=01.0933} policy_loss=-9.8528 policy updated! \n",
      "train step 07421 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2405 diff={max=04.2501, min=00.0007, mean=00.6767} policy_loss=-8.7939 policy updated! \n",
      "train step 07422 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1963 diff={max=08.0763, min=00.0413, mean=01.6318} policy_loss=-11.5157 policy updated! \n",
      "train step 07423 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6400 diff={max=11.1969, min=00.0121, mean=01.3004} policy_loss=-11.6772 policy updated! \n",
      "train step 07424 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4476 diff={max=09.8671, min=00.0191, mean=01.0379} policy_loss=-9.7538 policy updated! \n",
      "train step 07425 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.9300 diff={max=09.2539, min=00.0330, mean=01.5995} policy_loss=-8.2061 policy updated! \n",
      "train step 07426 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9312 diff={max=09.2365, min=00.0203, mean=01.0865} policy_loss=-10.3811 policy updated! \n",
      "train step 07427 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1196 diff={max=07.5587, min=00.0375, mean=01.2963} policy_loss=-8.5934 policy updated! \n",
      "train step 07428 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.4698 diff={max=06.5523, min=00.0080, mean=01.0926} policy_loss=-8.7362 policy updated! \n",
      "train step 07429 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.6897 diff={max=06.3558, min=00.0193, mean=01.2480} policy_loss=-10.8090 policy updated! \n",
      "train step 07430 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.0355 diff={max=07.5266, min=00.0193, mean=01.2483} policy_loss=-10.7564 policy updated! \n",
      "train step 07431 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6890 diff={max=06.6741, min=00.0016, mean=00.9581} policy_loss=-9.7507 policy updated! \n",
      "train step 07432 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0450 diff={max=09.0672, min=00.0068, mean=01.4514} policy_loss=-8.8977 policy updated! \n",
      "train step 07433 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9668 diff={max=05.2364, min=00.0131, mean=00.9373} policy_loss=-12.4638 policy updated! \n",
      "train step 07434 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0954 diff={max=06.9306, min=00.0571, mean=01.2473} policy_loss=-9.9386 policy updated! \n",
      "train step 07435 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.0648 diff={max=06.6898, min=00.0006, mean=01.2537} policy_loss=-10.7717 policy updated! \n",
      "train step 07436 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2453 diff={max=14.1235, min=00.0540, mean=01.3130} policy_loss=-10.9100 policy updated! \n",
      "train step 07437 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7053 diff={max=07.8319, min=00.0000, mean=01.0367} policy_loss=-10.3006 policy updated! \n",
      "train step 07438 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8352 diff={max=07.2215, min=00.0535, mean=01.2623} policy_loss=-10.5605 policy updated! \n",
      "train step 07439 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.5539 diff={max=10.5801, min=00.0054, mean=01.0367} policy_loss=-8.7852 policy updated! \n",
      "train step 07440 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=00.8825 diff={max=02.7122, min=00.0179, mean=00.6623} policy_loss=-10.0268 policy updated! \n",
      "train step 07441 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.9378 diff={max=07.7718, min=00.0591, mean=01.0542} policy_loss=-8.5825 policy updated! \n",
      "train step 07442 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3788 diff={max=04.1429, min=00.0020, mean=00.8896} policy_loss=-10.4767 policy updated! \n",
      "train step 07443 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9658 diff={max=08.2415, min=00.0191, mean=00.9940} policy_loss=-8.4836 policy updated! \n",
      "train step 07444 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4890 diff={max=05.4898, min=00.0164, mean=01.2594} policy_loss=-9.8939 policy updated! \n",
      "train step 07445 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.3532 diff={max=06.0022, min=00.0220, mean=01.1995} policy_loss=-12.6901 policy updated! \n",
      "train step 07446 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3139 diff={max=04.0839, min=00.0069, mean=00.7871} policy_loss=-9.5933 policy updated! \n",
      "train step 07447 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3783 diff={max=07.7362, min=00.0458, mean=01.3960} policy_loss=-10.8390 policy updated! \n",
      "train step 07448 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.0081 diff={max=04.1491, min=00.0280, mean=01.0001} policy_loss=-9.6019 policy updated! \n",
      "train step 07449 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7672 diff={max=11.8762, min=00.0085, mean=01.3593} policy_loss=-10.3790 policy updated! \n",
      "train step 07450 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.1588 diff={max=08.1971, min=00.0106, mean=01.1949} policy_loss=-8.2454 policy updated! \n",
      "train step 07451 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=08.5884 diff={max=09.9741, min=00.0731, mean=01.7506} policy_loss=-10.9848 policy updated! \n",
      "train step 07452 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2916 diff={max=09.0566, min=00.0078, mean=01.1759} policy_loss=-9.1423 policy updated! \n",
      "train step 07453 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5723 diff={max=07.6479, min=00.0049, mean=01.0610} policy_loss=-10.4362 policy updated! \n",
      "train step 07454 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8074 diff={max=05.9568, min=00.0311, mean=01.1246} policy_loss=-9.1558 policy updated! \n",
      "train step 07455 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.8558 diff={max=08.4459, min=00.0130, mean=01.2572} policy_loss=-8.9525 policy updated! \n",
      "train step 07456 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.7035 diff={max=09.4752, min=00.0102, mean=01.2077} policy_loss=-10.3583 policy updated! \n",
      "train step 07457 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1179 diff={max=10.8187, min=00.0768, mean=01.3022} policy_loss=-8.5780 policy updated! \n",
      "train step 07458 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=06.0020 diff={max=09.7502, min=00.0149, mean=01.5877} policy_loss=-10.7532 policy updated! \n",
      "train step 07459 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8577 diff={max=10.9947, min=00.0105, mean=01.4057} policy_loss=-10.0267 policy updated! \n",
      "train step 07460 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.6334 diff={max=06.9057, min=00.0184, mean=01.0468} policy_loss=-10.8175 policy updated! \n",
      "train step 07461 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4524 diff={max=09.9686, min=00.0039, mean=01.2691} policy_loss=-9.9476 policy updated! \n",
      "train step 07462 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8327 diff={max=05.6547, min=00.0198, mean=01.1210} policy_loss=-8.7732 policy updated! \n",
      "train step 07463 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5056 diff={max=07.9533, min=00.0240, mean=01.2210} policy_loss=-8.3672 policy updated! \n",
      "train step 07464 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.1660 diff={max=12.3583, min=00.0380, mean=01.6627} policy_loss=-8.7815 policy updated! \n",
      "train step 07465 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.9046 diff={max=07.6053, min=00.0062, mean=00.9397} policy_loss=-9.2209 policy updated! \n",
      "train step 07466 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4659 diff={max=08.5569, min=00.0108, mean=01.3632} policy_loss=-12.0969 policy updated! \n",
      "train step 07467 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=04.7026 diff={max=07.3860, min=00.0651, mean=01.3517} policy_loss=-11.2457 policy updated! \n",
      "train step 07468 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2690 diff={max=13.6529, min=00.0150, mean=01.4130} policy_loss=-10.4947 policy updated! \n",
      "train step 07469 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6428 diff={max=08.1428, min=00.0016, mean=01.6901} policy_loss=-11.8312 policy updated! \n",
      "train step 07470 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.6444 diff={max=05.7682, min=00.0145, mean=01.3296} policy_loss=-11.1555 policy updated! \n",
      "train step 07471 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.5137 diff={max=12.2511, min=00.0179, mean=01.5415} policy_loss=-10.4357 policy updated! \n",
      "train step 07472 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7462 diff={max=03.6485, min=00.0078, mean=00.9161} policy_loss=-9.4484 policy updated! \n",
      "train step 07473 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=07.9018 diff={max=09.1267, min=00.0030, mean=01.8435} policy_loss=-10.4890 policy updated! \n",
      "train step 07474 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2043 diff={max=12.2848, min=00.0233, mean=01.2283} policy_loss=-9.2756 policy updated! \n",
      "train step 07475 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.6580 diff={max=05.6811, min=00.0201, mean=00.7654} policy_loss=-8.2789 policy updated! \n",
      "train step 07476 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7479 diff={max=08.1795, min=00.0288, mean=01.6534} policy_loss=-9.3150 policy updated! \n",
      "train step 07477 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5847 diff={max=09.1899, min=00.0035, mean=01.2596} policy_loss=-10.0364 policy updated! \n",
      "train step 07478 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9771 diff={max=09.5874, min=00.0280, mean=01.2058} policy_loss=-9.8862 policy updated! \n",
      "train step 07479 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6099 diff={max=06.8249, min=00.0091, mean=01.0887} policy_loss=-9.5121 policy updated! \n",
      "train step 07480 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.9440 diff={max=08.2552, min=00.0032, mean=01.2195} policy_loss=-7.4124 policy updated! \n",
      "train step 07481 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3593 diff={max=10.4661, min=00.0236, mean=01.4864} policy_loss=-10.0043 policy updated! \n",
      "train step 07482 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5385 diff={max=10.1002, min=00.0043, mean=01.3685} policy_loss=-10.2502 policy updated! \n",
      "train step 07483 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.0050 diff={max=05.4686, min=00.0056, mean=00.9934} policy_loss=-10.2931 policy updated! \n",
      "train step 07484 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.9646 diff={max=07.0431, min=00.0127, mean=01.4885} policy_loss=-10.7441 policy updated! \n",
      "train step 07485 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.4968 diff={max=05.7691, min=00.0599, mean=01.2591} policy_loss=-9.1784 policy updated! \n",
      "train step 07486 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6651 diff={max=09.0438, min=00.0170, mean=01.2849} policy_loss=-10.1077 policy updated! \n",
      "train step 07487 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.8462 diff={max=13.1709, min=00.0519, mean=01.5867} policy_loss=-9.9334 policy updated! \n",
      "train step 07488 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5713 diff={max=09.4401, min=00.0186, mean=01.1869} policy_loss=-10.2426 policy updated! \n",
      "train step 07489 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.6375 diff={max=06.5729, min=00.0137, mean=01.3372} policy_loss=-10.7870 policy updated! \n",
      "train step 07490 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.9760 diff={max=10.3817, min=00.0019, mean=01.3629} policy_loss=-8.2150 policy updated! \n",
      "train step 07491 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.4814 diff={max=06.0383, min=00.0034, mean=00.9971} policy_loss=-8.8078 policy updated! \n",
      "train step 07492 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7454 diff={max=04.2845, min=00.0060, mean=01.2839} policy_loss=-11.2846 policy updated! \n",
      "train step 07493 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0393 diff={max=10.3842, min=00.0303, mean=01.3938} policy_loss=-9.4332 policy updated! \n",
      "train step 07494 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7241 diff={max=06.0618, min=00.0075, mean=01.0543} policy_loss=-9.7032 policy updated! \n",
      "train step 07495 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.8922 diff={max=07.2992, min=00.0058, mean=01.1992} policy_loss=-9.4481 policy updated! \n",
      "train step 07496 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=00.9712 diff={max=03.4697, min=00.0259, mean=00.6287} policy_loss=-8.6760 policy updated! \n",
      "train step 07497 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4187 diff={max=08.5887, min=00.0012, mean=00.9504} policy_loss=-9.5391 policy updated! \n",
      "train step 07498 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8063 diff={max=05.6043, min=00.0052, mean=01.1131} policy_loss=-9.9869 policy updated! \n",
      "train step 07499 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.9354 diff={max=11.2004, min=00.0569, mean=01.3607} policy_loss=-10.4210 policy updated! \n",
      "train step 07500 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.8297 diff={max=09.9211, min=00.0306, mean=01.1559} policy_loss=-9.6307 policy updated! \n",
      "train step 07501 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.2318 diff={max=07.4489, min=00.0169, mean=00.8913} policy_loss=-8.6552 policy updated! \n",
      "train step 07502 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2726 diff={max=07.6208, min=00.0419, mean=01.0456} policy_loss=-9.6435 policy updated! \n",
      "train step 07503 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3202 diff={max=04.8879, min=00.0185, mean=01.0367} policy_loss=-10.5512 policy updated! \n",
      "train step 07504 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=13.6567 diff={max=13.0116, min=00.0234, mean=01.9604} policy_loss=-9.6391 policy updated! \n",
      "train step 07505 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.1865 diff={max=10.4772, min=00.0478, mean=01.4737} policy_loss=-8.6502 policy updated! \n",
      "train step 07506 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.0405 diff={max=07.5405, min=00.0057, mean=01.0170} policy_loss=-9.5432 policy updated! \n",
      "train step 07507 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8141 diff={max=06.8145, min=00.0146, mean=01.2244} policy_loss=-9.6567 policy updated! \n",
      "train step 07508 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6053 diff={max=11.9007, min=00.0008, mean=01.1393} policy_loss=-11.1986 policy updated! \n",
      "train step 07509 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6791 diff={max=07.0002, min=00.0142, mean=01.1134} policy_loss=-10.0213 policy updated! \n",
      "train step 07510 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.1435 diff={max=08.0956, min=00.0270, mean=01.3336} policy_loss=-12.9999 policy updated! \n",
      "train step 07511 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0444 diff={max=09.8461, min=00.0066, mean=01.5905} policy_loss=-11.3130 policy updated! \n",
      "train step 07512 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5662 diff={max=07.1974, min=00.0170, mean=00.9878} policy_loss=-9.4178 policy updated! \n",
      "train step 07513 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9327 diff={max=08.6671, min=00.0359, mean=01.3933} policy_loss=-9.7658 policy updated! \n",
      "train step 07514 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4961 diff={max=08.9874, min=00.0433, mean=01.3238} policy_loss=-11.4505 policy updated! \n",
      "train step 07515 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.5351 diff={max=09.8121, min=00.0567, mean=01.3945} policy_loss=-10.5239 policy updated! \n",
      "train step 07516 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1612 diff={max=06.5144, min=00.0215, mean=00.9090} policy_loss=-7.9897 policy updated! \n",
      "train step 07517 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7556 diff={max=09.3698, min=00.0357, mean=01.3713} policy_loss=-9.3624 policy updated! \n",
      "train step 07518 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1023 diff={max=07.4932, min=00.0401, mean=01.1693} policy_loss=-8.4589 policy updated! \n",
      "train step 07519 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3693 diff={max=11.7277, min=00.0003, mean=01.0640} policy_loss=-8.8623 policy updated! \n",
      "train step 07520 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.8636 diff={max=04.6645, min=00.0200, mean=01.2313} policy_loss=-10.6048 policy updated! \n",
      "train step 07521 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4635 diff={max=09.2614, min=00.0099, mean=01.4191} policy_loss=-10.2235 policy updated! \n",
      "train step 07522 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.1876 diff={max=10.6101, min=00.0161, mean=02.0029} policy_loss=-9.9334 policy updated! \n",
      "train step 07523 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2252 diff={max=06.6809, min=00.0116, mean=01.1414} policy_loss=-10.3789 policy updated! \n",
      "train step 07524 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.5828 diff={max=07.8858, min=00.0263, mean=01.1019} policy_loss=-10.6867 policy updated! \n",
      "train step 07525 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.1813 diff={max=15.0577, min=00.0221, mean=01.4744} policy_loss=-9.8916 policy updated! \n",
      "train step 07526 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3844 diff={max=07.8052, min=00.0312, mean=00.9900} policy_loss=-9.3802 policy updated! \n",
      "train step 07527 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4116 diff={max=08.9059, min=00.0224, mean=01.3306} policy_loss=-10.3967 policy updated! \n",
      "train step 07528 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.9961 diff={max=06.0359, min=00.0126, mean=01.1974} policy_loss=-9.3268 policy updated! \n",
      "train step 07529 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=08.8697 diff={max=10.8694, min=00.0246, mean=01.7604} policy_loss=-11.0759 policy updated! \n",
      "train step 07530 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.0164 diff={max=06.0997, min=00.0448, mean=01.2273} policy_loss=-9.5097 policy updated! \n",
      "train step 07531 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=09.7499 diff={max=11.0033, min=00.0135, mean=01.7951} policy_loss=-10.1281 policy updated! \n",
      "train step 07532 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.2561 diff={max=08.4017, min=00.0031, mean=01.6256} policy_loss=-9.4795 policy updated! \n",
      "train step 07533 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.0810 diff={max=07.0875, min=00.0153, mean=01.0262} policy_loss=-11.7386 policy updated! \n",
      "train step 07534 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1752 diff={max=08.2372, min=00.0117, mean=01.4461} policy_loss=-10.7481 policy updated! \n",
      "train step 07535 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.5259 diff={max=18.6422, min=00.0743, mean=01.5529} policy_loss=-10.0136 policy updated! \n",
      "train step 07536 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1854 diff={max=08.6943, min=00.0480, mean=01.2669} policy_loss=-8.6590 policy updated! \n",
      "train step 07537 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8113 diff={max=05.4985, min=00.0268, mean=01.1757} policy_loss=-9.8688 policy updated! \n",
      "train step 07538 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.5132 diff={max=06.3804, min=00.0239, mean=01.1887} policy_loss=-10.5969 policy updated! \n",
      "train step 07539 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7519 diff={max=03.8129, min=00.0292, mean=00.8470} policy_loss=-10.8613 policy updated! \n",
      "train step 07540 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.0229 diff={max=05.4445, min=00.0180, mean=01.2225} policy_loss=-9.5376 policy updated! \n",
      "train step 07541 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8821 diff={max=08.3353, min=00.0348, mean=01.6771} policy_loss=-12.5050 policy updated! \n",
      "train step 07542 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4366 diff={max=04.7512, min=00.0278, mean=00.8434} policy_loss=-10.9706 policy updated! \n",
      "train step 07543 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4610 diff={max=07.4822, min=00.0139, mean=01.0170} policy_loss=-10.0480 policy updated! \n",
      "train step 07544 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.2314 diff={max=06.5179, min=00.0258, mean=00.9598} policy_loss=-9.0643 policy updated! \n",
      "train step 07545 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.1232 diff={max=06.6342, min=00.0008, mean=01.1744} policy_loss=-9.5521 policy updated! \n",
      "train step 07546 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4502 diff={max=07.5960, min=00.0183, mean=01.3210} policy_loss=-9.3214 policy updated! \n",
      "train step 07547 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3666 diff={max=06.8153, min=00.0111, mean=01.3840} policy_loss=-9.9929 policy updated! \n",
      "train step 07548 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3108 diff={max=07.5069, min=00.0074, mean=01.3208} policy_loss=-11.3991 policy updated! \n",
      "train step 07549 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6937 diff={max=05.0095, min=00.0058, mean=00.8851} policy_loss=-8.0063 policy updated! \n",
      "train step 07550 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=05.5350 diff={max=06.6205, min=00.0019, mean=01.6896} policy_loss=-8.7867 policy updated! \n",
      "train step 07551 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0930 diff={max=05.0390, min=00.0394, mean=01.0704} policy_loss=-10.0032 policy updated! \n",
      "train step 07552 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9766 diff={max=04.6394, min=00.0068, mean=01.0571} policy_loss=-9.5938 policy updated! \n",
      "train step 07553 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0342 diff={max=08.1919, min=00.0085, mean=01.5621} policy_loss=-10.6676 policy updated! \n",
      "train step 07554 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1150 diff={max=06.4936, min=00.0288, mean=00.9942} policy_loss=-10.7602 policy updated! \n",
      "train step 07555 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=04.8023 diff={max=08.9847, min=00.0046, mean=01.2313} policy_loss=-8.6545 policy updated! \n",
      "train step 07556 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6463 diff={max=09.4964, min=00.0021, mean=01.2817} policy_loss=-9.2435 policy updated! \n",
      "train step 07557 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0120 diff={max=07.9024, min=00.0031, mean=01.0353} policy_loss=-9.3448 policy updated! \n",
      "train step 07558 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4094 diff={max=06.4924, min=00.0025, mean=00.9848} policy_loss=-8.8918 policy updated! \n",
      "train step 07559 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5338 diff={max=10.5948, min=00.0070, mean=01.4777} policy_loss=-9.2625 policy updated! \n",
      "train step 07560 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.0064 diff={max=06.1796, min=00.0264, mean=01.1348} policy_loss=-11.2367 policy updated! \n",
      "train step 07561 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0643 diff={max=08.2951, min=00.0037, mean=01.2171} policy_loss=-8.9598 policy updated! \n",
      "train step 07562 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8830 diff={max=06.1749, min=00.0534, mean=01.1694} policy_loss=-9.9836 policy updated! \n",
      "train step 07563 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1615 diff={max=05.7239, min=00.0136, mean=01.4387} policy_loss=-11.7505 policy updated! \n",
      "train step 07564 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8514 diff={max=10.4553, min=00.0085, mean=01.2714} policy_loss=-11.2067 policy updated! \n",
      "train step 07565 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.7290 diff={max=04.2363, min=00.0211, mean=00.9212} policy_loss=-11.0563 policy updated! \n",
      "train step 07566 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3465 diff={max=04.8579, min=00.0154, mean=01.0045} policy_loss=-10.1018 policy updated! \n",
      "train step 07567 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1985 diff={max=06.2955, min=00.0324, mean=01.1161} policy_loss=-9.7120 policy updated! \n",
      "train step 07568 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.4378 diff={max=06.8571, min=00.0700, mean=01.1543} policy_loss=-8.6523 policy updated! \n",
      "train step 07569 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2803 diff={max=05.6201, min=00.0242, mean=00.7220} policy_loss=-8.3748 policy updated! \n",
      "train step 07570 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.7641 diff={max=05.9303, min=00.0237, mean=01.1741} policy_loss=-9.5272 policy updated! \n",
      "train step 07571 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.2485 diff={max=10.0510, min=00.0055, mean=01.0379} policy_loss=-9.7921 policy updated! \n",
      "train step 07572 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7039 diff={max=07.1141, min=00.0053, mean=01.0579} policy_loss=-9.9786 policy updated! \n",
      "train step 07573 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.5586 diff={max=15.3215, min=00.0269, mean=01.6542} policy_loss=-10.4793 policy updated! \n",
      "train step 07574 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2400 diff={max=09.7838, min=00.0080, mean=01.5809} policy_loss=-10.5089 policy updated! \n",
      "train step 07575 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.1184 diff={max=05.6693, min=00.0203, mean=01.2117} policy_loss=-8.5527 policy updated! \n",
      "train step 07576 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.3509 diff={max=07.8235, min=00.0115, mean=01.3524} policy_loss=-10.4016 policy updated! \n",
      "train step 07577 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5415 diff={max=05.5603, min=00.0287, mean=01.0929} policy_loss=-12.3203 policy updated! \n",
      "train step 07578 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.5457 diff={max=04.9729, min=00.0252, mean=00.8162} policy_loss=-10.1324 policy updated! \n",
      "train step 07579 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0932 diff={max=10.0257, min=00.0213, mean=01.3490} policy_loss=-11.0068 policy updated! \n",
      "train step 07580 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.2787 diff={max=08.3764, min=00.0037, mean=01.3860} policy_loss=-10.5413 policy updated! \n",
      "train step 07581 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5293 diff={max=04.5714, min=00.0245, mean=00.8510} policy_loss=-8.4457 policy updated! \n",
      "train step 07582 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9586 diff={max=08.5936, min=00.0410, mean=01.3774} policy_loss=-10.3139 policy updated! \n",
      "train step 07583 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.2199 diff={max=05.3972, min=00.0206, mean=01.3226} policy_loss=-9.7468 policy updated! \n",
      "train step 07584 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.3878 diff={max=10.4151, min=00.0026, mean=01.0794} policy_loss=-8.7385 policy updated! \n",
      "train step 07585 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.0171 diff={max=07.7153, min=00.0103, mean=01.2556} policy_loss=-9.9742 policy updated! \n",
      "train step 07586 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.8929 diff={max=05.2846, min=00.0062, mean=00.8101} policy_loss=-7.6639 policy updated! \n",
      "train step 07587 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=02.3945 diff={max=07.1445, min=00.0118, mean=00.9496} policy_loss=-9.9718 policy updated! \n",
      "train step 07588 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4844 diff={max=13.0493, min=00.0291, mean=01.5193} policy_loss=-12.5084 policy updated! \n",
      "train step 07589 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6533 diff={max=06.5952, min=00.0392, mean=01.2197} policy_loss=-9.9085 policy updated! \n",
      "train step 07590 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.5550 diff={max=07.2741, min=00.0493, mean=01.2550} policy_loss=-9.4778 policy updated! \n",
      "train step 07591 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7095 diff={max=08.2627, min=00.0126, mean=01.4057} policy_loss=-9.0141 policy updated! \n",
      "train step 07592 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2714 diff={max=04.5214, min=00.0120, mean=01.0477} policy_loss=-9.8235 policy updated! \n",
      "train step 07593 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.8813 diff={max=04.8967, min=00.0059, mean=00.9736} policy_loss=-8.5703 policy updated! \n",
      "train step 07594 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.6612 diff={max=07.3382, min=00.0129, mean=01.0016} policy_loss=-10.7819 policy updated! \n",
      "train step 07595 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.5843 diff={max=11.4171, min=00.0316, mean=01.8628} policy_loss=-9.9514 policy updated! \n",
      "train step 07596 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8021 diff={max=06.8205, min=00.0164, mean=01.1747} policy_loss=-10.3147 policy updated! \n",
      "train step 07597 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7902 diff={max=06.9922, min=00.0255, mean=01.1753} policy_loss=-11.0207 policy updated! \n",
      "train step 07598 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9093 diff={max=07.6544, min=00.0203, mean=01.5012} policy_loss=-10.3891 policy updated! \n",
      "train step 07599 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.4755 diff={max=06.1787, min=00.0207, mean=01.3530} policy_loss=-11.7274 policy updated! \n",
      "train step 07600 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.2701 diff={max=08.5925, min=00.0132, mean=01.3043} policy_loss=-11.7974 policy updated! \n",
      "train step 07601 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.8326 diff={max=06.0088, min=00.0030, mean=00.8755} policy_loss=-8.1056 policy updated! \n",
      "train step 07602 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1072 diff={max=07.6624, min=00.0166, mean=00.8033} policy_loss=-9.8811 policy updated! \n",
      "train step 07603 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5209 diff={max=11.3610, min=00.0096, mean=01.1914} policy_loss=-8.1895 policy updated! \n",
      "train step 07604 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7248 diff={max=09.1878, min=00.0156, mean=01.2896} policy_loss=-9.1401 policy updated! \n",
      "train step 07605 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.0426 diff={max=04.4340, min=00.0030, mean=00.9509} policy_loss=-8.8877 policy updated! \n",
      "train step 07606 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.4460 diff={max=09.8220, min=00.0126, mean=01.2895} policy_loss=-11.2453 policy updated! \n",
      "train step 07607 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.3781 diff={max=10.9491, min=00.0335, mean=01.9285} policy_loss=-9.5969 policy updated! \n",
      "train step 07608 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9549 diff={max=05.9476, min=00.0146, mean=01.3369} policy_loss=-10.0445 policy updated! \n",
      "train step 07609 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0641 diff={max=04.9036, min=00.0428, mean=00.9030} policy_loss=-9.6941 policy updated! \n",
      "train step 07610 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.3800 diff={max=13.8539, min=00.0298, mean=01.5184} policy_loss=-10.8645 policy updated! \n",
      "train step 07611 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1565 diff={max=08.8784, min=00.0088, mean=01.1007} policy_loss=-10.8163 policy updated! \n",
      "train step 07612 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5990 diff={max=10.2538, min=00.0150, mean=01.1988} policy_loss=-9.6658 policy updated! \n",
      "train step 07613 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8101 diff={max=06.3700, min=00.0055, mean=01.3205} policy_loss=-9.2612 policy updated! \n",
      "train step 07614 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.3126 diff={max=07.0453, min=00.0244, mean=01.6320} policy_loss=-11.5595 policy updated! \n",
      "train step 07615 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.9547 diff={max=08.1068, min=00.0060, mean=01.4346} policy_loss=-9.6778 policy updated! \n",
      "train step 07616 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6926 diff={max=07.9964, min=00.0222, mean=01.3227} policy_loss=-8.7781 policy updated! \n",
      "train step 07617 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6528 diff={max=07.0300, min=00.0034, mean=00.9535} policy_loss=-8.6052 policy updated! \n",
      "train step 07618 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.8426 diff={max=04.2723, min=00.0495, mean=00.9145} policy_loss=-9.1687 policy updated! \n",
      "train step 07619 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5897 diff={max=08.0034, min=00.0086, mean=01.1965} policy_loss=-9.4379 policy updated! \n",
      "train step 07620 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.8531 diff={max=06.2858, min=00.0183, mean=01.2929} policy_loss=-8.7750 policy updated! \n",
      "train step 07621 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5446 diff={max=06.9047, min=00.0192, mean=01.2492} policy_loss=-8.6525 policy updated! \n",
      "train step 07622 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1786 diff={max=08.4493, min=00.0199, mean=01.5399} policy_loss=-10.5527 policy updated! \n",
      "train step 07623 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3934 diff={max=06.7032, min=00.0092, mean=01.1573} policy_loss=-8.1433 policy updated! \n",
      "train step 07624 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.8919 diff={max=10.2785, min=00.0173, mean=01.0897} policy_loss=-9.1566 policy updated! \n",
      "train step 07625 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.5016 diff={max=06.9911, min=00.0681, mean=01.1157} policy_loss=-10.4175 policy updated! \n",
      "train step 07626 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6084 diff={max=10.3793, min=00.0073, mean=01.5731} policy_loss=-10.1264 policy updated! \n",
      "train step 07627 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3040 diff={max=05.8294, min=00.0009, mean=01.4585} policy_loss=-11.8925 policy updated! \n",
      "train step 07628 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5807 diff={max=07.1774, min=00.0137, mean=01.6822} policy_loss=-11.1630 policy updated! \n",
      "train step 07629 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7422 diff={max=08.1483, min=00.0159, mean=01.4445} policy_loss=-8.6872 policy updated! \n",
      "train step 07630 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.1578 diff={max=03.6027, min=00.0108, mean=00.7767} policy_loss=-10.1824 policy updated! \n",
      "train step 07631 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7754 diff={max=07.5207, min=00.0109, mean=01.2676} policy_loss=-10.7919 policy updated! \n",
      "train step 07632 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0284 diff={max=06.0419, min=00.0344, mean=00.9406} policy_loss=-10.0589 policy updated! \n",
      "train step 07633 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=06.2268 diff={max=12.9839, min=00.0802, mean=01.4454} policy_loss=-8.5323 policy updated! \n",
      "train step 07634 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0657 diff={max=10.4569, min=00.0169, mean=01.3693} policy_loss=-7.9157 policy updated! \n",
      "train step 07635 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.5595 diff={max=05.0720, min=00.0260, mean=01.2032} policy_loss=-9.2338 policy updated! \n",
      "train step 07636 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2234 diff={max=07.6266, min=00.0168, mean=01.1789} policy_loss=-8.8553 policy updated! \n",
      "train step 07637 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5726 diff={max=08.4236, min=00.0441, mean=01.2186} policy_loss=-11.2843 policy updated! \n",
      "train step 07638 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.0609 diff={max=04.8586, min=00.0037, mean=00.9796} policy_loss=-10.9064 policy updated! \n",
      "train step 07639 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1138 diff={max=07.3353, min=00.0011, mean=01.0922} policy_loss=-10.7500 policy updated! \n",
      "train step 07640 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.3716 diff={max=04.1709, min=00.0201, mean=01.0373} policy_loss=-10.9536 policy updated! \n",
      "train step 07641 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4534 diff={max=07.6229, min=00.0056, mean=01.5987} policy_loss=-11.9077 policy updated! \n",
      "train step 07642 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.9019 diff={max=14.7109, min=00.0269, mean=01.5516} policy_loss=-12.2532 policy updated! \n",
      "train step 07643 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7334 diff={max=08.7124, min=00.0339, mean=01.5325} policy_loss=-9.1467 policy updated! \n",
      "train step 07644 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4115 diff={max=07.3896, min=00.0162, mean=01.1051} policy_loss=-10.2769 policy updated! \n",
      "train step 07645 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.8236 diff={max=07.2901, min=00.0679, mean=01.0986} policy_loss=-9.9706 policy updated! \n",
      "train step 07646 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0071 diff={max=07.4040, min=00.0004, mean=01.3477} policy_loss=-11.0522 policy updated! \n",
      "train step 07647 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.6774 diff={max=15.5496, min=00.0147, mean=01.6705} policy_loss=-9.1492 policy updated! \n",
      "train step 07648 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.4517 diff={max=03.8719, min=00.0013, mean=00.8340} policy_loss=-9.7788 policy updated! \n",
      "train step 07649 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8914 diff={max=04.8242, min=00.0207, mean=00.9334} policy_loss=-9.9115 policy updated! \n",
      "train step 07650 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.8359 diff={max=08.5991, min=00.0197, mean=01.3825} policy_loss=-10.2218 policy updated! \n",
      "train step 07651 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.3008 diff={max=09.6012, min=00.0027, mean=01.6994} policy_loss=-9.6114 policy updated! \n",
      "train step 07652 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7368 diff={max=06.8091, min=00.0590, mean=01.3159} policy_loss=-11.8701 policy updated! \n",
      "train step 07653 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.0999 diff={max=13.6573, min=00.0303, mean=01.7723} policy_loss=-11.5828 policy updated! \n",
      "train step 07654 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6722 diff={max=11.7762, min=00.0162, mean=01.2366} policy_loss=-9.1309 policy updated! \n",
      "train step 07655 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.8484 diff={max=04.7506, min=00.0097, mean=01.2054} policy_loss=-9.7148 policy updated! \n",
      "train step 07656 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6218 diff={max=08.9437, min=00.0060, mean=01.2871} policy_loss=-9.2426 policy updated! \n",
      "train step 07657 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.0329 diff={max=14.0392, min=00.0023, mean=01.7689} policy_loss=-10.7410 policy updated! \n",
      "train step 07658 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0636 diff={max=05.8211, min=00.0090, mean=00.9057} policy_loss=-9.8193 policy updated! \n",
      "train step 07659 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.9077 diff={max=08.3702, min=00.0281, mean=01.2192} policy_loss=-8.8844 policy updated! \n",
      "train step 07660 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=11.0596 diff={max=15.9986, min=00.0501, mean=01.8744} policy_loss=-11.6990 policy updated! \n",
      "train step 07661 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5766 diff={max=09.9651, min=00.0082, mean=01.4235} policy_loss=-11.6478 policy updated! \n",
      "train step 07662 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4634 diff={max=05.4613, min=00.0326, mean=00.9669} policy_loss=-10.0755 policy updated! \n",
      "train step 07663 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.3546 diff={max=15.4367, min=00.0013, mean=01.5436} policy_loss=-10.3962 policy updated! \n",
      "train step 07664 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.8204 diff={max=08.5223, min=00.0380, mean=01.1875} policy_loss=-10.1822 policy updated! \n",
      "train step 07665 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.1920 diff={max=07.0862, min=00.1355, mean=01.2033} policy_loss=-7.8525 policy updated! \n",
      "train step 07666 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6181 diff={max=12.4524, min=00.0099, mean=01.3986} policy_loss=-9.9824 policy updated! \n",
      "train step 07667 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9169 diff={max=04.6034, min=00.0179, mean=01.2824} policy_loss=-11.7779 policy updated! \n",
      "train step 07668 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2812 diff={max=06.5188, min=00.0016, mean=01.1475} policy_loss=-9.7304 policy updated! \n",
      "train step 07669 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0598 diff={max=08.7176, min=00.0490, mean=01.2445} policy_loss=-10.0757 policy updated! \n",
      "train step 07670 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.8141 diff={max=05.7833, min=00.0096, mean=01.0433} policy_loss=-10.3859 policy updated! \n",
      "train step 07671 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=02.2206 diff={max=05.4136, min=00.0289, mean=00.9950} policy_loss=-10.6439 policy updated! \n",
      "train step 07672 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8031 diff={max=10.1459, min=00.0197, mean=01.4742} policy_loss=-10.4736 policy updated! \n",
      "train step 07673 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.4434 diff={max=07.2170, min=00.0052, mean=00.9116} policy_loss=-8.1443 policy updated! \n",
      "train step 07674 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4613 diff={max=05.1267, min=00.0081, mean=01.2765} policy_loss=-12.1053 policy updated! \n",
      "train step 07675 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.7961 diff={max=07.4357, min=00.1025, mean=01.0869} policy_loss=-9.7367 policy updated! \n",
      "train step 07676 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.7348 diff={max=07.5266, min=00.0412, mean=01.2499} policy_loss=-11.4038 policy updated! \n",
      "train step 07677 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8016 diff={max=06.2059, min=00.0238, mean=00.9964} policy_loss=-8.1474 policy updated! \n",
      "train step 07678 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.6714 diff={max=05.2163, min=00.0267, mean=01.0404} policy_loss=-9.5380 policy updated! \n",
      "train step 07679 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8650 diff={max=07.5346, min=00.0012, mean=01.1898} policy_loss=-7.6718 policy updated! \n",
      "train step 07680 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.7968 diff={max=08.7553, min=00.0170, mean=01.1832} policy_loss=-10.1391 policy updated! \n",
      "train step 07681 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6692 diff={max=08.4887, min=00.0381, mean=01.0584} policy_loss=-8.8703 policy updated! \n",
      "train step 07682 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.4483 diff={max=12.6159, min=00.0377, mean=01.5888} policy_loss=-11.2537 policy updated! \n",
      "train step 07683 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0577 diff={max=05.5079, min=00.1201, mean=01.0185} policy_loss=-10.9914 policy updated! \n",
      "train step 07684 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3587 diff={max=08.8279, min=00.0250, mean=01.2580} policy_loss=-10.2950 policy updated! \n",
      "train step 07685 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.6063 diff={max=09.3662, min=00.0194, mean=01.1305} policy_loss=-9.5340 policy updated! \n",
      "train step 07686 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.1655 diff={max=06.0333, min=00.0492, mean=01.3179} policy_loss=-9.2599 policy updated! \n",
      "train step 07687 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3420 diff={max=11.9606, min=00.0260, mean=01.4193} policy_loss=-9.1753 policy updated! \n",
      "train step 07688 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7214 diff={max=07.3045, min=00.0401, mean=01.4351} policy_loss=-12.0305 policy updated! \n",
      "train step 07689 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1334 diff={max=03.5285, min=00.0258, mean=00.8008} policy_loss=-9.8601 policy updated! \n",
      "train step 07690 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.8194 diff={max=09.5998, min=00.0005, mean=01.3871} policy_loss=-9.2566 policy updated! \n",
      "train step 07691 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4336 diff={max=06.3386, min=00.0026, mean=01.2346} policy_loss=-10.2826 policy updated! \n",
      "train step 07692 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1073 diff={max=08.1042, min=00.0011, mean=00.9916} policy_loss=-10.2552 policy updated! \n",
      "train step 07693 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.6580 diff={max=08.0556, min=00.0209, mean=01.0019} policy_loss=-9.2383 policy updated! \n",
      "train step 07694 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8900 diff={max=08.4320, min=00.0028, mean=01.5526} policy_loss=-11.9138 policy updated! \n",
      "train step 07695 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.3821 diff={max=07.2991, min=00.0417, mean=01.1027} policy_loss=-9.3617 policy updated! \n",
      "train step 07696 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7291 diff={max=04.0949, min=00.0463, mean=01.3697} policy_loss=-9.4376 policy updated! \n",
      "train step 07697 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6495 diff={max=11.5290, min=00.0852, mean=01.2215} policy_loss=-12.3527 policy updated! \n",
      "train step 07698 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=04.6243 diff={max=06.8058, min=00.0070, mean=01.3838} policy_loss=-11.4589 policy updated! \n",
      "train step 07699 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5846 diff={max=05.7775, min=00.0194, mean=01.1148} policy_loss=-11.8713 policy updated! \n",
      "train step 07700 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.5279 diff={max=04.8731, min=00.0406, mean=01.0131} policy_loss=-9.5781 policy updated! \n",
      "train step 07701 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4176 diff={max=08.1050, min=00.0103, mean=01.6629} policy_loss=-10.5818 policy updated! \n",
      "train step 07702 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9346 diff={max=08.0098, min=00.0053, mean=01.4005} policy_loss=-11.4767 policy updated! \n",
      "train step 07703 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2633 diff={max=06.7345, min=00.0039, mean=01.2181} policy_loss=-11.4764 policy updated! \n",
      "train step 07704 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0482 diff={max=06.1291, min=00.0005, mean=01.1447} policy_loss=-9.6754 policy updated! \n",
      "train step 07705 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.1864 diff={max=08.5117, min=00.0725, mean=01.2876} policy_loss=-9.3895 policy updated! \n",
      "train step 07706 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6211 diff={max=05.5064, min=00.0010, mean=01.0861} policy_loss=-9.7696 policy updated! \n",
      "train step 07707 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0342 diff={max=08.1355, min=00.0207, mean=01.4764} policy_loss=-10.8918 policy updated! \n",
      "train step 07708 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2529 diff={max=07.5858, min=00.0113, mean=01.5186} policy_loss=-11.1365 policy updated! \n",
      "train step 07709 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0833 diff={max=08.0570, min=00.0470, mean=01.2231} policy_loss=-9.9540 policy updated! \n",
      "train step 07710 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.9251 diff={max=04.8462, min=00.1193, mean=01.0359} policy_loss=-6.9889 policy updated! \n",
      "train step 07711 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3296 diff={max=05.4686, min=00.0186, mean=01.1972} policy_loss=-10.8761 policy updated! \n",
      "train step 07712 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1309 diff={max=08.0440, min=00.0116, mean=01.7336} policy_loss=-9.8561 policy updated! \n",
      "train step 07713 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5600 diff={max=04.8351, min=00.0234, mean=01.0790} policy_loss=-10.0960 policy updated! \n",
      "train step 07714 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=09.4674 diff={max=11.6413, min=00.0053, mean=01.7619} policy_loss=-12.1895 policy updated! \n",
      "train step 07715 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.8693 diff={max=07.0892, min=00.0421, mean=01.0755} policy_loss=-9.7794 policy updated! \n",
      "train step 07716 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7401 diff={max=06.8154, min=00.0218, mean=00.9465} policy_loss=-10.4359 policy updated! \n",
      "train step 07717 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.5908 diff={max=14.9546, min=00.0112, mean=01.5954} policy_loss=-10.9838 policy updated! \n",
      "train step 07718 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=01.7814 diff={max=05.9526, min=00.0135, mean=00.8600} policy_loss=-10.0431 policy updated! \n",
      "train step 07719 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1001 diff={max=11.1913, min=00.0006, mean=01.0220} policy_loss=-9.8676 policy updated! \n",
      "train step 07720 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.9241 diff={max=06.9898, min=00.0116, mean=01.1196} policy_loss=-11.2303 policy updated! \n",
      "train step 07721 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.5542 diff={max=09.7830, min=00.0773, mean=01.8949} policy_loss=-10.5506 policy updated! \n",
      "train step 07722 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7336 diff={max=05.6777, min=00.0260, mean=01.3565} policy_loss=-10.1036 policy updated! \n",
      "train step 07723 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4477 diff={max=08.2350, min=00.0073, mean=01.3366} policy_loss=-12.1669 policy updated! \n",
      "train step 07724 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.3006 diff={max=07.4817, min=00.0188, mean=01.4265} policy_loss=-10.7348 policy updated! \n",
      "train step 07725 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.6104 diff={max=10.3638, min=00.0263, mean=01.4644} policy_loss=-9.8918 policy updated! \n",
      "train step 07726 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6895 diff={max=09.3257, min=00.0303, mean=01.2196} policy_loss=-9.9443 policy updated! \n",
      "train step 07727 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7161 diff={max=06.8884, min=00.0395, mean=01.3055} policy_loss=-11.6968 policy updated! \n",
      "train step 07728 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.6271 diff={max=07.7769, min=00.0213, mean=01.5201} policy_loss=-11.2902 policy updated! \n",
      "train step 07729 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=05.7499 diff={max=10.7104, min=00.0074, mean=01.4262} policy_loss=-11.5145 policy updated! \n",
      "train step 07730 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.8672 diff={max=09.2432, min=00.0004, mean=01.3622} policy_loss=-10.4224 policy updated! \n",
      "train step 07731 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.5174 diff={max=07.1828, min=00.0015, mean=01.0401} policy_loss=-9.0369 policy updated! \n",
      "train step 07732 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6751 diff={max=06.5895, min=00.0115, mean=00.8979} policy_loss=-8.7425 policy updated! \n",
      "train step 07733 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9599 diff={max=06.6962, min=00.0218, mean=01.0261} policy_loss=-10.9362 policy updated! \n",
      "train step 07734 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1956 diff={max=09.6376, min=00.0004, mean=01.4067} policy_loss=-9.7499 policy updated! \n",
      "train step 07735 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.5262 diff={max=10.9286, min=00.0600, mean=01.4016} policy_loss=-8.2825 policy updated! \n",
      "train step 07736 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1411 diff={max=10.1045, min=00.0043, mean=01.3455} policy_loss=-10.0421 policy updated! \n",
      "train step 07737 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6415 diff={max=05.8640, min=00.0216, mean=01.0237} policy_loss=-9.5735 policy updated! \n",
      "train step 07738 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2624 diff={max=05.2425, min=00.0013, mean=00.9611} policy_loss=-11.3620 policy updated! \n",
      "train step 07739 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3402 diff={max=04.8133, min=00.0327, mean=00.9610} policy_loss=-8.3352 policy updated! \n",
      "train step 07740 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.3764 diff={max=08.4705, min=00.0154, mean=01.2016} policy_loss=-10.3520 policy updated! \n",
      "train step 07741 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1938 diff={max=05.7811, min=00.0186, mean=01.2468} policy_loss=-11.9604 policy updated! \n",
      "train step 07742 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0039 diff={max=12.6064, min=00.0112, mean=01.3609} policy_loss=-11.4978 policy updated! \n",
      "train step 07743 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3385 diff={max=05.9270, min=00.0046, mean=01.3408} policy_loss=-10.4581 policy updated! \n",
      "train step 07744 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1511 diff={max=05.4638, min=00.0304, mean=00.9405} policy_loss=-9.5564 policy updated! \n",
      "train step 07745 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.0024 diff={max=06.4093, min=00.0394, mean=01.1150} policy_loss=-10.3968 policy updated! \n",
      "train step 07746 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8331 diff={max=09.0077, min=00.0084, mean=01.3230} policy_loss=-8.8317 policy updated! \n",
      "train step 07747 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1557 diff={max=08.2710, min=00.0150, mean=00.9519} policy_loss=-11.0738 policy updated! \n",
      "train step 07748 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0343 diff={max=04.5159, min=00.0693, mean=00.9953} policy_loss=-10.9210 policy updated! \n",
      "train step 07749 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.7911 diff={max=09.2358, min=00.0230, mean=01.5161} policy_loss=-11.3281 policy updated! \n",
      "train step 07750 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.3165 diff={max=04.7249, min=00.0299, mean=00.8550} policy_loss=-9.2938 policy updated! \n",
      "train step 07751 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9371 diff={max=07.8839, min=00.0043, mean=01.0861} policy_loss=-10.9485 policy updated! \n",
      "train step 07752 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.8925 diff={max=13.1857, min=00.0216, mean=01.5445} policy_loss=-12.0926 policy updated! \n",
      "train step 07753 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.9354 diff={max=06.8869, min=00.0171, mean=00.9612} policy_loss=-8.8358 policy updated! \n",
      "train step 07754 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=08.9530 diff={max=12.7444, min=00.0011, mean=01.5561} policy_loss=-8.7270 policy updated! \n",
      "train step 07755 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.4819 diff={max=03.1899, min=00.0140, mean=00.8915} policy_loss=-8.6522 policy updated! \n",
      "train step 07756 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7320 diff={max=06.6029, min=00.0452, mean=00.9956} policy_loss=-11.0999 policy updated! \n",
      "train step 07757 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4683 diff={max=08.7154, min=00.0040, mean=01.3170} policy_loss=-11.7346 policy updated! \n",
      "train step 07758 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.0749 diff={max=07.4615, min=00.0042, mean=01.5738} policy_loss=-10.2424 policy updated! \n",
      "train step 07759 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2452 diff={max=07.3838, min=00.0018, mean=01.3383} policy_loss=-7.8282 policy updated! \n",
      "train step 07760 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.7547 diff={max=05.1616, min=00.0083, mean=00.8958} policy_loss=-8.2871 policy updated! \n",
      "train step 07761 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9015 diff={max=08.6498, min=00.0117, mean=01.5508} policy_loss=-11.3865 policy updated! \n",
      "train step 07762 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3466 diff={max=07.0710, min=00.0083, mean=01.3518} policy_loss=-8.6237 policy updated! \n",
      "train step 07763 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8589 diff={max=04.6140, min=00.0026, mean=01.0249} policy_loss=-10.8557 policy updated! \n",
      "train step 07764 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4991 diff={max=06.2555, min=00.0170, mean=01.3181} policy_loss=-10.8637 policy updated! \n",
      "train step 07765 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.8588 diff={max=07.5914, min=00.0576, mean=01.3812} policy_loss=-10.3643 policy updated! \n",
      "train step 07766 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9472 diff={max=12.4331, min=00.0304, mean=01.2302} policy_loss=-9.5649 policy updated! \n",
      "train step 07767 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6849 diff={max=10.6968, min=00.0099, mean=01.5647} policy_loss=-11.5879 policy updated! \n",
      "train step 07768 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9306 diff={max=06.2805, min=00.0172, mean=01.0333} policy_loss=-9.5351 policy updated! \n",
      "train step 07769 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1204 diff={max=06.6839, min=00.0008, mean=01.0885} policy_loss=-9.6795 policy updated! \n",
      "train step 07770 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.5054 diff={max=14.4844, min=00.0113, mean=01.6967} policy_loss=-9.9945 policy updated! \n",
      "train step 07771 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1345 diff={max=13.2565, min=00.0145, mean=01.1501} policy_loss=-11.3662 policy updated! \n",
      "train step 07772 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4417 diff={max=08.4878, min=00.0308, mean=01.4922} policy_loss=-11.6572 policy updated! \n",
      "train step 07773 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.0287 diff={max=06.5688, min=00.0246, mean=01.2861} policy_loss=-9.1727 policy updated! \n",
      "train step 07774 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7195 diff={max=09.7633, min=00.0046, mean=01.4026} policy_loss=-11.3679 policy updated! \n",
      "train step 07775 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.5978 diff={max=05.5125, min=00.0088, mean=00.7343} policy_loss=-9.3106 policy updated! \n",
      "train step 07776 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8865 diff={max=05.8164, min=00.0798, mean=01.3542} policy_loss=-9.5626 policy updated! \n",
      "train step 07777 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0568 diff={max=08.6713, min=00.0155, mean=01.2479} policy_loss=-9.5081 policy updated! \n",
      "train step 07778 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9636 diff={max=08.7395, min=00.0003, mean=01.1681} policy_loss=-10.9903 policy updated! \n",
      "train step 07779 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7562 diff={max=11.3915, min=00.0026, mean=01.0729} policy_loss=-10.2573 policy updated! \n",
      "train step 07780 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.3692 diff={max=06.7808, min=00.0474, mean=00.9097} policy_loss=-9.4826 policy updated! \n",
      "train step 07781 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=07.7155 diff={max=11.9057, min=00.0105, mean=01.6046} policy_loss=-10.4891 policy updated! \n",
      "train step 07782 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1548 diff={max=08.5533, min=00.1563, mean=01.2445} policy_loss=-11.5708 policy updated! \n",
      "train step 07783 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0529 diff={max=05.8608, min=00.0117, mean=01.0003} policy_loss=-11.2834 policy updated! \n",
      "train step 07784 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8235 diff={max=09.9576, min=00.0493, mean=01.3845} policy_loss=-10.5675 policy updated! \n",
      "train step 07785 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.5562 diff={max=07.5731, min=00.0279, mean=01.0472} policy_loss=-10.5469 policy updated! \n",
      "train step 07786 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4939 diff={max=06.4658, min=00.0118, mean=00.9651} policy_loss=-12.0988 policy updated! \n",
      "train step 07787 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2253 diff={max=07.7707, min=00.0039, mean=01.0196} policy_loss=-12.2737 policy updated! \n",
      "train step 07788 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2659 diff={max=07.4628, min=00.0057, mean=01.1515} policy_loss=-12.2548 policy updated! \n",
      "train step 07789 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2789 diff={max=06.6373, min=00.0160, mean=01.1389} policy_loss=-9.5117 policy updated! \n",
      "train step 07790 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.4641 diff={max=11.8747, min=00.0207, mean=01.2538} policy_loss=-10.4402 policy updated! \n",
      "train step 07791 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3751 diff={max=07.1537, min=00.0022, mean=01.2434} policy_loss=-9.1393 policy updated! \n",
      "train step 07792 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.5315 diff={max=13.4254, min=00.0294, mean=01.8100} policy_loss=-10.1288 policy updated! \n",
      "train step 07793 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7396 diff={max=11.8235, min=00.0111, mean=01.4206} policy_loss=-10.0094 policy updated! \n",
      "train step 07794 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.5169 diff={max=10.5314, min=00.0033, mean=01.4295} policy_loss=-10.3368 policy updated! \n",
      "train step 07795 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.9094 diff={max=05.9955, min=00.0062, mean=01.0625} policy_loss=-10.5166 policy updated! \n",
      "train step 07796 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.3609 diff={max=16.9952, min=00.0148, mean=01.3160} policy_loss=-9.3643 policy updated! \n",
      "train step 07797 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8623 diff={max=03.7454, min=00.0095, mean=01.0258} policy_loss=-9.2895 policy updated! \n",
      "train step 07798 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7294 diff={max=04.2042, min=00.0494, mean=01.2049} policy_loss=-9.6016 policy updated! \n",
      "train step 07799 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9795 diff={max=05.4943, min=00.0693, mean=01.2082} policy_loss=-12.1276 policy updated! \n",
      "train step 07800 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.9901 diff={max=09.2807, min=00.0077, mean=01.3280} policy_loss=-10.2208 policy updated! \n",
      "train step 07801 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9790 diff={max=06.6247, min=00.0202, mean=01.3154} policy_loss=-10.0901 policy updated! \n",
      "train step 07802 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5539 diff={max=09.0128, min=00.0034, mean=01.0437} policy_loss=-9.2681 policy updated! \n",
      "train step 07803 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5870 diff={max=06.8720, min=00.0412, mean=01.4454} policy_loss=-9.2543 policy updated! \n",
      "train step 07804 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9913 diff={max=04.6137, min=00.0645, mean=01.2750} policy_loss=-11.7181 policy updated! \n",
      "train step 07805 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.7601 diff={max=08.8058, min=00.0007, mean=01.2190} policy_loss=-9.6679 policy updated! \n",
      "train step 07806 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.6760 diff={max=07.5891, min=00.0289, mean=01.4003} policy_loss=-9.9337 policy updated! \n",
      "train step 07807 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6589 diff={max=05.2104, min=00.0078, mean=01.1790} policy_loss=-10.9079 policy updated! \n",
      "train step 07808 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9357 diff={max=09.1646, min=00.0063, mean=00.9824} policy_loss=-9.6024 policy updated! \n",
      "train step 07809 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2504 diff={max=07.2108, min=00.0003, mean=01.2304} policy_loss=-9.2341 policy updated! \n",
      "train step 07810 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.6287 diff={max=07.7808, min=00.0317, mean=01.1953} policy_loss=-10.5672 policy updated! \n",
      "train step 07811 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1268 diff={max=10.3419, min=00.0375, mean=01.5125} policy_loss=-11.0098 policy updated! \n",
      "train step 07812 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.5317 diff={max=10.1997, min=00.0548, mean=01.5990} policy_loss=-8.9388 policy updated! \n",
      "train step 07813 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.5845 diff={max=07.9317, min=00.0137, mean=01.3727} policy_loss=-8.8767 policy updated! \n",
      "train step 07814 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.6797 diff={max=12.0924, min=00.0462, mean=01.5559} policy_loss=-10.9764 policy updated! \n",
      "train step 07815 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.6070 diff={max=07.6737, min=00.0336, mean=00.9846} policy_loss=-11.4365 policy updated! \n",
      "train step 07816 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6932 diff={max=11.1507, min=00.0308, mean=01.1822} policy_loss=-11.1304 policy updated! \n",
      "train step 07817 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2331 diff={max=05.3937, min=00.0588, mean=01.0594} policy_loss=-8.4233 policy updated! \n",
      "train step 07818 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.1626 diff={max=07.0730, min=00.0040, mean=01.4443} policy_loss=-10.8188 policy updated! \n",
      "train step 07819 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.7972 diff={max=07.5120, min=00.0061, mean=00.9973} policy_loss=-9.1205 policy updated! \n",
      "train step 07820 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.1790 diff={max=06.2607, min=00.0091, mean=01.0749} policy_loss=-11.0290 policy updated! \n",
      "train step 07821 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3127 diff={max=08.3904, min=00.0243, mean=01.3177} policy_loss=-10.3812 policy updated! \n",
      "train step 07822 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5095 diff={max=13.9255, min=00.0025, mean=01.1424} policy_loss=-9.6694 policy updated! \n",
      "train step 07823 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7409 diff={max=10.0149, min=00.0509, mean=01.1425} policy_loss=-8.1215 policy updated! \n",
      "train step 07824 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4010 diff={max=06.8033, min=00.0447, mean=01.3033} policy_loss=-9.4404 policy updated! \n",
      "train step 07825 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.3016 diff={max=10.4260, min=00.0094, mean=01.6280} policy_loss=-9.8120 policy updated! \n",
      "train step 07826 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1537 diff={max=09.2992, min=00.0351, mean=01.0785} policy_loss=-9.9935 policy updated! \n",
      "train step 07827 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.2170 diff={max=13.4489, min=00.0285, mean=01.3556} policy_loss=-9.9086 policy updated! \n",
      "train step 07828 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3873 diff={max=09.2433, min=00.0273, mean=01.4391} policy_loss=-10.7727 policy updated! \n",
      "train step 07829 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2125 diff={max=12.5103, min=00.0034, mean=01.5392} policy_loss=-10.4645 policy updated! \n",
      "train step 07830 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.1916 diff={max=11.1636, min=00.0203, mean=01.5039} policy_loss=-9.8938 policy updated! \n",
      "train step 07831 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0588 diff={max=05.8481, min=00.0199, mean=00.8695} policy_loss=-9.4679 policy updated! \n",
      "train step 07832 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7310 diff={max=07.1037, min=00.0203, mean=00.7433} policy_loss=-7.9058 policy updated! \n",
      "train step 07833 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3202 diff={max=07.8377, min=00.0019, mean=01.4791} policy_loss=-9.4199 policy updated! \n",
      "train step 07834 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2463 diff={max=05.8470, min=00.0048, mean=00.8882} policy_loss=-8.6769 policy updated! \n",
      "train step 07835 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=03.1957 diff={max=06.8182, min=00.0043, mean=01.1494} policy_loss=-10.8129 policy updated! \n",
      "train step 07836 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6373 diff={max=07.5405, min=00.0333, mean=01.3771} policy_loss=-9.7145 policy updated! \n",
      "train step 07837 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2264 diff={max=08.0790, min=00.0061, mean=01.1024} policy_loss=-11.3342 policy updated! \n",
      "train step 07838 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3247 diff={max=07.8351, min=00.0082, mean=01.3122} policy_loss=-9.0613 policy updated! \n",
      "train step 07839 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8361 diff={max=09.3790, min=00.0003, mean=01.6980} policy_loss=-10.2067 policy updated! \n",
      "train step 07840 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.2818 diff={max=07.0525, min=00.0105, mean=01.2002} policy_loss=-10.7144 policy updated! \n",
      "train step 07841 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.5119 diff={max=04.4694, min=00.0033, mean=00.8718} policy_loss=-10.9157 policy updated! \n",
      "train step 07842 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3466 diff={max=06.2572, min=00.0232, mean=01.5684} policy_loss=-10.8491 policy updated! \n",
      "train step 07843 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2888 diff={max=05.1076, min=00.0209, mean=00.9591} policy_loss=-11.6560 policy updated! \n",
      "train step 07844 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3829 diff={max=11.7935, min=00.0057, mean=01.2127} policy_loss=-9.5069 policy updated! \n",
      "train step 07845 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.2229 diff={max=04.3359, min=00.0219, mean=01.0051} policy_loss=-10.7567 policy updated! \n",
      "train step 07846 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2327 diff={max=10.5573, min=00.0197, mean=01.3715} policy_loss=-9.6267 policy updated! \n",
      "train step 07847 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8714 diff={max=06.1820, min=00.0071, mean=01.0894} policy_loss=-8.7454 policy updated! \n",
      "train step 07848 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3993 diff={max=05.4812, min=00.0190, mean=01.3281} policy_loss=-10.1691 policy updated! \n",
      "train step 07849 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=14.6570 diff={max=20.5501, min=00.0100, mean=01.9684} policy_loss=-10.7022 policy updated! \n",
      "train step 07850 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.1301 diff={max=06.3039, min=00.0065, mean=01.0683} policy_loss=-11.4223 policy updated! \n",
      "train step 07851 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4468 diff={max=05.8744, min=00.0066, mean=00.9500} policy_loss=-9.3583 policy updated! \n",
      "train step 07852 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9028 diff={max=06.1662, min=00.0187, mean=00.8436} policy_loss=-9.4962 policy updated! \n",
      "train step 07853 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5232 diff={max=09.2848, min=00.0137, mean=01.3941} policy_loss=-11.1686 policy updated! \n",
      "train step 07854 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=07.1120 diff={max=11.3897, min=00.0202, mean=01.3694} policy_loss=-11.7966 policy updated! \n",
      "train step 07855 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.6142 diff={max=10.4087, min=00.0096, mean=01.2389} policy_loss=-10.3601 policy updated! \n",
      "train step 07856 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.3387 diff={max=08.5091, min=00.0047, mean=01.4163} policy_loss=-9.2789 policy updated! \n",
      "train step 07857 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2992 diff={max=07.2240, min=00.0153, mean=01.3078} policy_loss=-8.9711 policy updated! \n",
      "train step 07858 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2624 diff={max=09.5594, min=00.0268, mean=01.1554} policy_loss=-9.3326 policy updated! \n",
      "train step 07859 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8208 diff={max=07.5853, min=00.0197, mean=01.1283} policy_loss=-9.4733 policy updated! \n",
      "train step 07860 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.6610 diff={max=06.6025, min=00.0657, mean=01.2786} policy_loss=-12.2377 policy updated! \n",
      "train step 07861 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9076 diff={max=07.2868, min=00.0064, mean=01.0469} policy_loss=-7.6612 policy updated! \n",
      "train step 07862 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4517 diff={max=06.3846, min=00.0094, mean=01.0812} policy_loss=-11.2887 policy updated! \n",
      "train step 07863 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.5979 diff={max=07.4247, min=00.0219, mean=00.9865} policy_loss=-9.2829 policy updated! \n",
      "train step 07864 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.4671 diff={max=06.7079, min=00.0091, mean=01.2249} policy_loss=-10.4343 policy updated! \n",
      "train step 07865 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.2815 diff={max=06.5009, min=00.0081, mean=01.3467} policy_loss=-10.6705 policy updated! \n",
      "train step 07866 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.8376 diff={max=09.1653, min=00.0272, mean=01.8445} policy_loss=-11.0389 policy updated! \n",
      "train step 07867 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.9476 diff={max=13.7377, min=00.0039, mean=01.4148} policy_loss=-10.3689 policy updated! \n",
      "train step 07868 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3911 diff={max=05.7921, min=00.0387, mean=00.9491} policy_loss=-9.0137 policy updated! \n",
      "train step 07869 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9473 diff={max=09.6816, min=00.0466, mean=01.4041} policy_loss=-9.5288 policy updated! \n",
      "train step 07870 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.4543 diff={max=10.5732, min=00.0199, mean=01.2238} policy_loss=-9.6818 policy updated! \n",
      "train step 07871 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7394 diff={max=07.2502, min=00.0388, mean=01.4754} policy_loss=-11.3847 policy updated! \n",
      "train step 07872 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0890 diff={max=07.2910, min=00.0119, mean=01.5865} policy_loss=-12.6818 policy updated! \n",
      "train step 07873 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2246 diff={max=07.0455, min=00.0247, mean=01.5537} policy_loss=-10.2096 policy updated! \n",
      "train step 07874 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.1977 diff={max=04.2475, min=00.0058, mean=01.1266} policy_loss=-11.8584 policy updated! \n",
      "train step 07875 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.4077 diff={max=04.5533, min=00.0096, mean=01.1462} policy_loss=-8.7401 policy updated! \n",
      "train step 07876 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0160 diff={max=10.9332, min=00.0184, mean=01.5209} policy_loss=-9.2964 policy updated! \n",
      "train step 07877 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8804 diff={max=07.2635, min=00.0016, mean=01.1868} policy_loss=-8.6865 policy updated! \n",
      "train step 07878 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3316 diff={max=06.6239, min=00.0358, mean=01.2776} policy_loss=-11.9702 policy updated! \n",
      "train step 07879 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9480 diff={max=09.4225, min=00.0036, mean=01.5260} policy_loss=-10.5895 policy updated! \n",
      "train step 07880 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.1397 diff={max=06.8395, min=00.0347, mean=01.1127} policy_loss=-10.6518 policy updated! \n",
      "train step 07881 reward={max=08.0000, min=00.0000, mean=04.4000} optimizing loss=03.9800 diff={max=06.5181, min=00.0268, mean=01.2369} policy_loss=-9.1227 policy updated! \n",
      "train step 07882 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2115 diff={max=08.5891, min=00.0015, mean=01.0253} policy_loss=-9.5526 policy updated! \n",
      "train step 07883 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8945 diff={max=08.8862, min=00.0296, mean=01.3125} policy_loss=-10.1606 policy updated! \n",
      "train step 07884 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6161 diff={max=10.6659, min=00.1731, mean=01.5064} policy_loss=-10.7700 policy updated! \n",
      "train step 07885 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.1243 diff={max=04.2198, min=00.0421, mean=01.1066} policy_loss=-9.9162 policy updated! \n",
      "train step 07886 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.1196 diff={max=09.8266, min=00.0176, mean=01.4757} policy_loss=-11.6977 policy updated! \n",
      "train step 07887 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3544 diff={max=11.7551, min=00.0379, mean=01.4410} policy_loss=-11.2203 policy updated! \n",
      "train step 07888 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7194 diff={max=10.6946, min=00.0187, mean=01.4981} policy_loss=-9.7491 policy updated! \n",
      "train step 07889 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2351 diff={max=07.5106, min=00.0468, mean=01.2927} policy_loss=-10.3062 policy updated! \n",
      "train step 07890 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.0058 diff={max=10.5696, min=00.0162, mean=01.1702} policy_loss=-12.1867 policy updated! \n",
      "train step 07891 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9752 diff={max=05.0388, min=00.0370, mean=01.1558} policy_loss=-9.1144 policy updated! \n",
      "train step 07892 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7286 diff={max=11.7454, min=00.0037, mean=01.3079} policy_loss=-10.6238 policy updated! \n",
      "train step 07893 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.0197 diff={max=14.8749, min=00.0020, mean=01.7317} policy_loss=-11.4243 policy updated! \n",
      "train step 07894 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0213 diff={max=08.7601, min=00.0348, mean=01.4383} policy_loss=-10.0829 policy updated! \n",
      "train step 07895 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.5479 diff={max=08.1567, min=00.0030, mean=01.1052} policy_loss=-9.5057 policy updated! \n",
      "train step 07896 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5042 diff={max=08.3088, min=00.0022, mean=01.3274} policy_loss=-11.2187 policy updated! \n",
      "train step 07897 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4985 diff={max=08.1249, min=00.0027, mean=01.2982} policy_loss=-10.8518 policy updated! \n",
      "train step 07898 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=06.7273 diff={max=10.0505, min=00.0624, mean=01.4983} policy_loss=-10.1965 policy updated! \n",
      "train step 07899 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.0126 diff={max=04.9662, min=00.0263, mean=01.1953} policy_loss=-9.2261 policy updated! \n",
      "train step 07900 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.2563 diff={max=11.0596, min=00.0266, mean=01.7094} policy_loss=-10.2834 policy updated! \n",
      "train step 07901 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0377 diff={max=08.5515, min=00.0154, mean=01.4246} policy_loss=-10.9546 policy updated! \n",
      "train step 07902 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6447 diff={max=07.7769, min=00.0032, mean=01.0984} policy_loss=-10.2462 policy updated! \n",
      "train step 07903 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6036 diff={max=09.2442, min=00.0029, mean=01.0471} policy_loss=-9.9488 policy updated! \n",
      "train step 07904 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.7848 diff={max=08.7244, min=00.0295, mean=01.2593} policy_loss=-11.7204 policy updated! \n",
      "train step 07905 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.9253 diff={max=09.2078, min=00.0141, mean=01.9316} policy_loss=-10.8741 policy updated! \n",
      "train step 07906 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2623 diff={max=11.5184, min=00.0075, mean=01.3888} policy_loss=-11.1558 policy updated! \n",
      "train step 07907 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9784 diff={max=10.1120, min=00.0007, mean=01.3306} policy_loss=-11.6537 policy updated! \n",
      "train step 07908 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8997 diff={max=08.0849, min=00.0071, mean=01.4246} policy_loss=-10.1271 policy updated! \n",
      "train step 07909 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4107 diff={max=03.7751, min=00.0299, mean=01.1664} policy_loss=-12.2661 policy updated! \n",
      "train step 07910 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.2215 diff={max=10.8560, min=00.0070, mean=01.9079} policy_loss=-12.3408 policy updated! \n",
      "train step 07911 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0268 diff={max=08.7621, min=00.0028, mean=01.1531} policy_loss=-9.6508 policy updated! \n",
      "train step 07912 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4457 diff={max=07.3287, min=00.0287, mean=01.1873} policy_loss=-10.8282 policy updated! \n",
      "train step 07913 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.2102 diff={max=06.4378, min=00.0050, mean=01.3004} policy_loss=-9.4366 policy updated! \n",
      "train step 07914 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6893 diff={max=05.2011, min=00.0276, mean=00.8362} policy_loss=-9.7856 policy updated! \n",
      "train step 07915 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.7213 diff={max=06.8690, min=00.0180, mean=01.4865} policy_loss=-9.8432 policy updated! \n",
      "train step 07916 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1331 diff={max=07.8878, min=00.0271, mean=01.3946} policy_loss=-10.4360 policy updated! \n",
      "train step 07917 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3314 diff={max=06.9566, min=00.0079, mean=01.3208} policy_loss=-9.0496 policy updated! \n",
      "train step 07918 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6280 diff={max=11.6959, min=00.0181, mean=01.0902} policy_loss=-9.3854 policy updated! \n",
      "train step 07919 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6730 diff={max=10.1459, min=00.0084, mean=01.3612} policy_loss=-11.5674 policy updated! \n",
      "train step 07920 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.1906 diff={max=10.2023, min=00.0006, mean=01.1298} policy_loss=-9.9834 policy updated! \n",
      "train step 07921 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6638 diff={max=08.1907, min=00.0424, mean=01.3229} policy_loss=-11.6825 policy updated! \n",
      "train step 07922 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2949 diff={max=06.8735, min=00.0094, mean=00.9438} policy_loss=-9.3340 policy updated! \n",
      "train step 07923 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4036 diff={max=06.5386, min=00.0223, mean=01.5043} policy_loss=-9.7638 policy updated! \n",
      "train step 07924 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.5548 diff={max=10.4275, min=00.0174, mean=01.2534} policy_loss=-8.7638 policy updated! \n",
      "train step 07925 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.9855 diff={max=07.7094, min=00.0157, mean=01.3616} policy_loss=-10.3614 policy updated! \n",
      "train step 07926 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9534 diff={max=08.7007, min=00.0236, mean=01.0422} policy_loss=-8.9873 policy updated! \n",
      "train step 07927 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7834 diff={max=04.8296, min=00.0028, mean=01.0889} policy_loss=-8.5462 policy updated! \n",
      "train step 07928 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6618 diff={max=04.2707, min=00.0101, mean=00.8314} policy_loss=-7.2592 policy updated! \n",
      "train step 07929 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0513 diff={max=06.8805, min=00.0023, mean=01.1849} policy_loss=-11.2684 policy updated! \n",
      "train step 07930 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.7618 diff={max=06.2990, min=00.0349, mean=00.7857} policy_loss=-7.2164 policy updated! \n",
      "train step 07931 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2946 diff={max=07.6485, min=00.0237, mean=01.0855} policy_loss=-11.0759 policy updated! \n",
      "train step 07932 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8940 diff={max=11.3586, min=00.0069, mean=01.2129} policy_loss=-9.4431 policy updated! \n",
      "train step 07933 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3171 diff={max=08.2669, min=00.0026, mean=01.0956} policy_loss=-9.7237 policy updated! \n",
      "train step 07934 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3411 diff={max=07.5859, min=00.0317, mean=01.2408} policy_loss=-12.2890 policy updated! \n",
      "train step 07935 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.5057 diff={max=08.6427, min=00.0144, mean=01.4065} policy_loss=-8.6091 policy updated! \n",
      "train step 07936 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5829 diff={max=04.7569, min=00.0099, mean=01.0792} policy_loss=-8.5283 policy updated! \n",
      "train step 07937 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8886 diff={max=04.0163, min=00.0894, mean=01.0851} policy_loss=-11.1212 policy updated! \n",
      "train step 07938 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3377 diff={max=08.4621, min=00.0220, mean=01.3467} policy_loss=-10.4476 policy updated! \n",
      "train step 07939 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9666 diff={max=11.0629, min=00.0397, mean=01.5037} policy_loss=-8.4846 policy updated! \n",
      "train step 07940 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.6109 diff={max=06.5666, min=00.0148, mean=01.0843} policy_loss=-10.0919 policy updated! \n",
      "train step 07941 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.0086 diff={max=09.9076, min=00.0125, mean=01.2808} policy_loss=-10.0236 policy updated! \n",
      "train step 07942 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0575 diff={max=07.6901, min=00.0003, mean=01.0715} policy_loss=-10.6914 policy updated! \n",
      "train step 07943 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=10.5598 diff={max=15.4170, min=00.0575, mean=01.8696} policy_loss=-12.6093 policy updated! \n",
      "train step 07944 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1203 diff={max=02.5315, min=00.0092, mean=00.8163} policy_loss=-7.5146 policy updated! \n",
      "train step 07945 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.4036 diff={max=09.3413, min=00.0004, mean=01.3280} policy_loss=-9.7609 policy updated! \n",
      "train step 07946 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.7404 diff={max=08.0677, min=00.0083, mean=01.2763} policy_loss=-11.3982 policy updated! \n",
      "train step 07947 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=05.4866 diff={max=07.4897, min=00.0499, mean=01.5150} policy_loss=-9.0800 policy updated! \n",
      "train step 07948 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5727 diff={max=07.4120, min=00.0487, mean=01.3271} policy_loss=-10.2849 policy updated! \n",
      "train step 07949 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4500 diff={max=05.3068, min=00.1114, mean=01.2862} policy_loss=-10.5996 policy updated! \n",
      "train step 07950 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.9016 diff={max=05.4416, min=00.0402, mean=01.0944} policy_loss=-9.8012 policy updated! \n",
      "train step 07951 reward={max=07.0000, min=00.0000, mean=03.8000} optimizing loss=03.0166 diff={max=05.0788, min=00.0133, mean=01.1828} policy_loss=-10.7425 policy updated! \n",
      "train step 07952 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9617 diff={max=08.3477, min=00.0049, mean=01.1904} policy_loss=-10.2084 policy updated! \n",
      "train step 07953 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=01.9873 diff={max=06.2318, min=00.0020, mean=00.9600} policy_loss=-12.5555 policy updated! \n",
      "train step 07954 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=04.6242 diff={max=12.8707, min=00.0017, mean=01.0695} policy_loss=-10.9936 policy updated! \n",
      "train step 07955 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.9611 diff={max=04.3041, min=00.0044, mean=00.9735} policy_loss=-11.4847 policy updated! \n",
      "train step 07956 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1011 diff={max=06.1064, min=00.0106, mean=01.3194} policy_loss=-12.2807 policy updated! \n",
      "train step 07957 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9081 diff={max=04.9485, min=00.0012, mean=00.9597} policy_loss=-11.2810 policy updated! \n",
      "train step 07958 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.5037 diff={max=12.3491, min=00.0383, mean=01.2167} policy_loss=-9.6398 policy updated! \n",
      "train step 07959 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4936 diff={max=09.4909, min=00.0044, mean=01.0228} policy_loss=-10.1042 policy updated! \n",
      "train step 07960 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.4991 diff={max=06.6809, min=00.0211, mean=01.0453} policy_loss=-8.5854 policy updated! \n",
      "train step 07961 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.2613 diff={max=04.0220, min=00.0075, mean=00.6892} policy_loss=-9.4950 policy updated! \n",
      "train step 07962 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7232 diff={max=06.7507, min=00.0067, mean=01.0966} policy_loss=-10.2667 policy updated! \n",
      "train step 07963 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=02.9417 diff={max=09.1502, min=00.0612, mean=01.0345} policy_loss=-8.9150 policy updated! \n",
      "train step 07964 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=02.4174 diff={max=05.8193, min=00.0230, mean=01.1144} policy_loss=-10.5798 policy updated! \n",
      "train step 07965 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.3822 diff={max=09.0908, min=00.0035, mean=01.3255} policy_loss=-8.1015 policy updated! \n",
      "train step 07966 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=04.5359 diff={max=09.1019, min=00.0040, mean=01.3232} policy_loss=-9.4993 policy updated! \n",
      "train step 07967 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3141 diff={max=09.6675, min=00.0294, mean=01.5634} policy_loss=-10.7289 policy updated! \n",
      "train step 07968 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3210 diff={max=05.3850, min=00.0132, mean=00.9919} policy_loss=-11.1577 policy updated! \n",
      "train step 07969 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=02.0985 diff={max=04.8583, min=00.0139, mean=00.9591} policy_loss=-9.6706 policy updated! \n",
      "train step 07970 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.3811 diff={max=07.2973, min=00.0069, mean=01.3946} policy_loss=-11.6229 policy updated! \n",
      "train step 07971 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=07.1655 diff={max=10.2817, min=00.0404, mean=01.6109} policy_loss=-9.6284 policy updated! \n",
      "train step 07972 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8733 diff={max=10.0216, min=00.0089, mean=01.5592} policy_loss=-10.0779 policy updated! \n",
      "train step 07973 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0304 diff={max=07.5040, min=00.0005, mean=01.0559} policy_loss=-9.8946 policy updated! \n",
      "train step 07974 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3567 diff={max=08.1364, min=00.0111, mean=01.0776} policy_loss=-10.5074 policy updated! \n",
      "train step 07975 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.9908 diff={max=06.8148, min=00.0011, mean=01.2239} policy_loss=-10.4589 policy updated! \n",
      "train step 07976 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=01.9559 diff={max=04.5440, min=00.0081, mean=00.9481} policy_loss=-9.4565 policy updated! \n",
      "train step 07977 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.5776 diff={max=10.5245, min=00.0143, mean=01.6903} policy_loss=-11.6184 policy updated! \n",
      "train step 07978 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=02.5784 diff={max=04.8312, min=00.0016, mean=01.0772} policy_loss=-8.7780 policy updated! \n",
      "train step 07979 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=04.3422 diff={max=08.9178, min=00.0059, mean=01.2862} policy_loss=-12.1853 policy updated! \n",
      "train step 07980 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.4284 diff={max=06.9457, min=00.0330, mean=01.1002} policy_loss=-10.9227 policy updated! \n",
      "train step 07981 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=05.7944 diff={max=08.2689, min=00.0007, mean=01.5009} policy_loss=-10.2676 policy updated! \n",
      "train step 07982 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.2177 diff={max=11.5884, min=00.0033, mean=01.4867} policy_loss=-11.6177 policy updated! \n",
      "train step 07983 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.9605 diff={max=08.9471, min=00.0110, mean=01.0222} policy_loss=-9.9502 policy updated! \n",
      "train step 07984 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6136 diff={max=07.6827, min=00.0105, mean=01.1285} policy_loss=-8.6301 policy updated! \n",
      "train step 07985 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=07.6872 diff={max=11.7704, min=00.0525, mean=01.6712} policy_loss=-9.2701 policy updated! \n",
      "train step 07986 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=02.9631 diff={max=06.5876, min=00.0428, mean=01.1861} policy_loss=-11.0250 policy updated! \n",
      "train step 07987 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5990 diff={max=07.7913, min=00.0043, mean=00.9111} policy_loss=-8.7119 policy updated! \n",
      "train step 07988 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=04.3209 diff={max=08.4040, min=00.0016, mean=01.1775} policy_loss=-10.8918 policy updated! \n",
      "train step 07989 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7183 diff={max=07.9626, min=00.0334, mean=01.5592} policy_loss=-9.9638 policy updated! \n",
      "train step 07990 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.1341 diff={max=05.3171, min=00.0276, mean=01.2386} policy_loss=-10.4607 policy updated! \n",
      "train step 07991 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6131 diff={max=10.8390, min=00.0043, mean=01.2001} policy_loss=-10.5971 policy updated! \n",
      "train step 07992 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7296 diff={max=07.6635, min=00.0095, mean=00.9695} policy_loss=-10.3888 policy updated! \n",
      "train step 07993 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3621 diff={max=08.5211, min=00.0041, mean=01.1420} policy_loss=-10.3221 policy updated! \n",
      "train step 07994 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5893 diff={max=08.9696, min=00.0052, mean=01.0630} policy_loss=-10.3900 policy updated! \n",
      "train step 07995 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.6155 diff={max=09.2111, min=00.0309, mean=01.0415} policy_loss=-9.1511 policy updated! \n",
      "train step 07996 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=02.7359 diff={max=08.5507, min=00.0408, mean=00.8756} policy_loss=-8.9762 policy updated! \n",
      "train step 07997 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.4769 diff={max=09.9629, min=00.0314, mean=01.9482} policy_loss=-9.8267 policy updated! \n",
      "train step 07998 reward={max=07.0000, min=00.0000, mean=02.6000} optimizing loss=05.0313 diff={max=07.8250, min=00.0341, mean=01.4011} policy_loss=-10.0457 policy updated! \n",
      "train step 07999 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.2625 diff={max=10.5348, min=00.0094, mean=01.6045} policy_loss=-10.9060 policy updated! \n",
      "train step 08000 reward={max=07.0000, min=00.0000, mean=05.0000} optimizing loss=03.5596 diff={max=07.2434, min=00.0047, mean=01.2099} policy_loss=-8.6711 policy updated! \n",
      "train step 08001 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7864 diff={max=07.9858, min=00.0524, mean=01.6040} policy_loss=-11.2501 policy updated! \n",
      "train step 08002 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8455 diff={max=04.6321, min=00.0300, mean=00.9836} policy_loss=-10.9688 policy updated! \n",
      "train step 08003 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9108 diff={max=06.7969, min=00.0069, mean=01.4243} policy_loss=-10.5199 policy updated! \n",
      "train step 08004 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4260 diff={max=05.1843, min=00.0624, mean=01.5656} policy_loss=-9.9700 policy updated! \n",
      "train step 08005 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.9200 diff={max=09.0133, min=00.0284, mean=01.5875} policy_loss=-9.8377 policy updated! \n",
      "train step 08006 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8375 diff={max=06.0607, min=00.0053, mean=01.1283} policy_loss=-8.7411 policy updated! \n",
      "train step 08007 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5818 diff={max=05.2915, min=00.0020, mean=01.1108} policy_loss=-9.7117 policy updated! \n",
      "train step 08008 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9173 diff={max=12.6048, min=00.0104, mean=01.3722} policy_loss=-10.2578 policy updated! \n",
      "train step 08009 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3410 diff={max=11.7999, min=00.0106, mean=01.5037} policy_loss=-11.6152 policy updated! \n",
      "train step 08010 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.1243 diff={max=08.4497, min=00.0124, mean=01.0630} policy_loss=-8.4499 policy updated! \n",
      "train step 08011 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3915 diff={max=06.2211, min=00.0028, mean=01.1251} policy_loss=-11.3381 policy updated! \n",
      "train step 08012 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=04.3621 diff={max=06.8407, min=00.0246, mean=01.3375} policy_loss=-10.5351 policy updated! \n",
      "train step 08013 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=02.4172 diff={max=04.3686, min=00.0191, mean=01.1003} policy_loss=-11.1357 policy updated! \n",
      "train step 08014 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5714 diff={max=08.4749, min=00.0019, mean=01.3017} policy_loss=-8.4287 policy updated! \n",
      "train step 08015 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.3993 diff={max=06.6153, min=00.0401, mean=01.2374} policy_loss=-9.4872 policy updated! \n",
      "train step 08016 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7519 diff={max=04.8100, min=00.0002, mean=00.9278} policy_loss=-7.8434 policy updated! \n",
      "train step 08017 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3267 diff={max=05.8165, min=00.0483, mean=01.0418} policy_loss=-10.6909 policy updated! \n",
      "train step 08018 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3378 diff={max=07.2857, min=00.0052, mean=01.3083} policy_loss=-10.9503 policy updated! \n",
      "train step 08019 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8416 diff={max=06.9280, min=00.0108, mean=00.8328} policy_loss=-9.1702 policy updated! \n",
      "train step 08020 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=06.8475 diff={max=09.5304, min=00.0025, mean=01.4225} policy_loss=-10.0406 policy updated! \n",
      "train step 08021 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3244 diff={max=10.2740, min=00.0095, mean=01.4917} policy_loss=-12.2946 policy updated! \n",
      "train step 08022 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8469 diff={max=13.6429, min=00.0019, mean=01.3794} policy_loss=-10.4389 policy updated! \n",
      "train step 08023 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=02.0242 diff={max=07.1950, min=00.0025, mean=00.8616} policy_loss=-8.8670 policy updated! \n",
      "train step 08024 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8283 diff={max=04.5879, min=00.0138, mean=00.8964} policy_loss=-9.5491 policy updated! \n",
      "train step 08025 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.5388 diff={max=10.2147, min=00.0077, mean=01.2172} policy_loss=-8.8411 policy updated! \n",
      "train step 08026 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7958 diff={max=05.1462, min=00.0093, mean=01.1101} policy_loss=-10.7448 policy updated! \n",
      "train step 08027 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0193 diff={max=08.8617, min=00.0055, mean=01.2003} policy_loss=-12.5113 policy updated! \n",
      "train step 08028 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.2203 diff={max=05.6429, min=00.0251, mean=01.3038} policy_loss=-11.8421 policy updated! \n",
      "train step 08029 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=05.6570 diff={max=11.4812, min=00.1082, mean=01.3929} policy_loss=-8.4137 policy updated! \n",
      "train step 08030 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=05.6691 diff={max=09.1733, min=00.0304, mean=01.5690} policy_loss=-10.8142 policy updated! \n",
      "train step 08031 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2689 diff={max=06.5045, min=00.0590, mean=01.1759} policy_loss=-10.1509 policy updated! \n",
      "train step 08032 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6419 diff={max=08.1184, min=00.0148, mean=01.1839} policy_loss=-10.1914 policy updated! \n",
      "train step 08033 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7322 diff={max=05.7622, min=00.0241, mean=01.1277} policy_loss=-10.9906 policy updated! \n",
      "train step 08034 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2408 diff={max=07.2863, min=00.0071, mean=01.0904} policy_loss=-10.4340 policy updated! \n",
      "train step 08035 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.9183 diff={max=06.6376, min=00.0106, mean=01.4489} policy_loss=-9.3631 policy updated! \n",
      "train step 08036 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.5927 diff={max=06.9687, min=00.0069, mean=01.1860} policy_loss=-9.1594 policy updated! \n",
      "train step 08037 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3580 diff={max=09.1833, min=00.0420, mean=01.6114} policy_loss=-14.2360 policy updated! \n",
      "train step 08038 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8804 diff={max=08.4596, min=00.0041, mean=01.0560} policy_loss=-8.2707 policy updated! \n",
      "train step 08039 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1103 diff={max=09.2221, min=00.0175, mean=01.4755} policy_loss=-10.2836 policy updated! \n",
      "train step 08040 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.8729 diff={max=07.4390, min=00.0089, mean=01.0514} policy_loss=-10.9674 policy updated! \n",
      "train step 08041 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6149 diff={max=05.5532, min=00.0433, mean=01.3048} policy_loss=-9.5907 policy updated! \n",
      "train step 08042 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0371 diff={max=04.4842, min=00.0063, mean=01.0547} policy_loss=-7.7083 policy updated! \n",
      "train step 08043 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=01.9680 diff={max=05.6914, min=00.0574, mean=00.9471} policy_loss=-10.2873 policy updated! \n",
      "train step 08044 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.7102 diff={max=09.0114, min=00.0177, mean=01.5576} policy_loss=-12.1042 policy updated! \n",
      "train step 08045 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.0979 diff={max=06.4668, min=00.0325, mean=00.9674} policy_loss=-11.0951 policy updated! \n",
      "train step 08046 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9115 diff={max=11.4385, min=00.0144, mean=01.5059} policy_loss=-9.5949 policy updated! \n",
      "train step 08047 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.2083 diff={max=09.5663, min=00.0355, mean=02.0582} policy_loss=-12.0434 policy updated! \n",
      "train step 08048 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5062 diff={max=07.0923, min=00.0044, mean=01.2620} policy_loss=-9.6506 policy updated! \n",
      "train step 08049 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0026 diff={max=07.2914, min=00.0037, mean=01.1732} policy_loss=-8.7167 policy updated! \n",
      "train step 08050 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.4400 diff={max=10.4136, min=00.0281, mean=01.2339} policy_loss=-10.1521 policy updated! \n",
      "train step 08051 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=04.0331 diff={max=06.5725, min=00.0031, mean=01.2385} policy_loss=-10.4255 policy updated! \n",
      "train step 08052 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2371 diff={max=08.2586, min=00.0011, mean=01.0473} policy_loss=-9.8831 policy updated! \n",
      "train step 08053 reward={max=08.0000, min=00.0000, mean=03.2000} optimizing loss=02.1028 diff={max=03.5562, min=00.0135, mean=01.0266} policy_loss=-7.6195 policy updated! \n",
      "train step 08054 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2022 diff={max=05.4150, min=00.0280, mean=01.0316} policy_loss=-9.7554 policy updated! \n",
      "train step 08055 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.9760 diff={max=11.3052, min=00.0101, mean=01.2823} policy_loss=-9.9533 policy updated! \n",
      "train step 08056 reward={max=07.0000, min=00.0000, mean=04.0000} optimizing loss=05.1100 diff={max=08.6080, min=00.0009, mean=01.3491} policy_loss=-10.1054 policy updated! \n",
      "train step 08057 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7260 diff={max=06.9806, min=00.0150, mean=01.5965} policy_loss=-12.0275 policy updated! \n",
      "train step 08058 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.9081 diff={max=11.2056, min=00.0195, mean=01.5334} policy_loss=-9.8465 policy updated! \n",
      "train step 08059 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1842 diff={max=07.7373, min=00.0211, mean=01.2781} policy_loss=-11.2118 policy updated! \n",
      "train step 08060 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=06.0229 diff={max=12.6018, min=00.0201, mean=01.2846} policy_loss=-9.7010 policy updated! \n",
      "train step 08061 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8194 diff={max=04.4244, min=00.0031, mean=01.2114} policy_loss=-10.3829 policy updated! \n",
      "train step 08062 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2516 diff={max=05.5980, min=00.0108, mean=00.9683} policy_loss=-10.6479 policy updated! \n",
      "train step 08063 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=03.6559 diff={max=09.1559, min=00.0058, mean=01.1364} policy_loss=-11.1735 policy updated! \n",
      "train step 08064 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3044 diff={max=05.7061, min=00.0054, mean=00.9937} policy_loss=-11.0919 policy updated! \n",
      "train step 08065 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.0157 diff={max=06.3560, min=00.0180, mean=01.3164} policy_loss=-9.7388 policy updated! \n",
      "train step 08066 reward={max=07.0000, min=00.0000, mean=04.0000} optimizing loss=05.1373 diff={max=07.5797, min=00.0035, mean=01.3823} policy_loss=-10.4930 policy updated! \n",
      "train step 08067 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4125 diff={max=08.6383, min=00.0004, mean=01.3238} policy_loss=-11.3477 policy updated! \n",
      "train step 08068 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2204 diff={max=08.7900, min=00.0383, mean=01.4227} policy_loss=-10.8084 policy updated! \n",
      "train step 08069 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9930 diff={max=10.7595, min=00.0131, mean=01.4728} policy_loss=-11.4816 policy updated! \n",
      "train step 08070 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=03.4767 diff={max=06.3915, min=00.0003, mean=01.1762} policy_loss=-10.8518 policy updated! \n",
      "train step 08071 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=04.8950 diff={max=11.8008, min=00.0006, mean=01.2095} policy_loss=-11.8802 policy updated! \n",
      "train step 08072 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6301 diff={max=06.9478, min=00.0141, mean=01.1928} policy_loss=-9.5207 policy updated! \n",
      "train step 08073 reward={max=08.0000, min=00.0000, mean=03.2000} optimizing loss=04.0837 diff={max=08.2405, min=00.0206, mean=01.3204} policy_loss=-10.7119 policy updated! \n",
      "train step 08074 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5154 diff={max=06.5038, min=00.0020, mean=01.2781} policy_loss=-9.3108 policy updated! \n",
      "train step 08075 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.4344 diff={max=06.6260, min=00.0066, mean=01.1694} policy_loss=-10.0232 policy updated! \n",
      "train step 08076 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0623 diff={max=05.5574, min=00.0021, mean=01.1128} policy_loss=-10.9723 policy updated! \n",
      "train step 08077 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0425 diff={max=07.8500, min=00.0067, mean=01.1085} policy_loss=-8.0106 policy updated! \n",
      "train step 08078 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5167 diff={max=10.6280, min=00.0051, mean=01.1164} policy_loss=-10.2323 policy updated! \n",
      "train step 08079 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=03.9723 diff={max=06.8418, min=00.0029, mean=01.2687} policy_loss=-11.1746 policy updated! \n",
      "train step 08080 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.3402 diff={max=05.6469, min=00.0006, mean=00.9817} policy_loss=-9.7120 policy updated! \n",
      "train step 08081 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2708 diff={max=09.2130, min=00.0084, mean=01.2454} policy_loss=-8.5853 policy updated! \n",
      "train step 08082 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7728 diff={max=06.4297, min=00.0313, mean=01.4182} policy_loss=-11.3891 policy updated! \n",
      "train step 08083 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9158 diff={max=09.2755, min=00.0027, mean=01.1689} policy_loss=-10.4904 policy updated! \n",
      "train step 08084 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5522 diff={max=07.6377, min=00.0169, mean=01.0557} policy_loss=-10.0274 policy updated! \n",
      "train step 08085 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.1013 diff={max=09.9417, min=00.0031, mean=01.5371} policy_loss=-10.9664 policy updated! \n",
      "train step 08086 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=09.9795 diff={max=14.0538, min=00.0116, mean=01.7546} policy_loss=-10.8277 policy updated! \n",
      "train step 08087 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9240 diff={max=06.8152, min=00.0006, mean=00.9499} policy_loss=-12.1201 policy updated! \n",
      "train step 08088 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5323 diff={max=06.6251, min=00.0042, mean=01.0618} policy_loss=-8.1058 policy updated! \n",
      "train step 08089 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.1438 diff={max=05.3124, min=00.0012, mean=01.1586} policy_loss=-9.9517 policy updated! \n",
      "train step 08090 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.4204 diff={max=11.5754, min=00.0221, mean=01.0991} policy_loss=-10.7728 policy updated! \n",
      "train step 08091 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=10.0043 diff={max=12.8341, min=00.0479, mean=01.8676} policy_loss=-10.1698 policy updated! \n",
      "train step 08092 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9185 diff={max=04.8136, min=00.0211, mean=00.9199} policy_loss=-9.7579 policy updated! \n",
      "train step 08093 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=03.0609 diff={max=06.9050, min=00.0114, mean=01.1029} policy_loss=-9.8455 policy updated! \n",
      "train step 08094 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0577 diff={max=08.3411, min=00.0760, mean=01.2145} policy_loss=-11.3955 policy updated! \n",
      "train step 08095 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.5787 diff={max=05.8465, min=00.0617, mean=01.5317} policy_loss=-10.1003 policy updated! \n",
      "train step 08096 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=03.1536 diff={max=07.4520, min=00.0890, mean=01.3952} policy_loss=-9.1578 policy updated! \n",
      "train step 08097 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.8931 diff={max=13.0955, min=00.0090, mean=01.8758} policy_loss=-10.9450 policy updated! \n",
      "train step 08098 reward={max=07.0000, min=00.0000, mean=02.6000} optimizing loss=05.1966 diff={max=09.0089, min=00.0142, mean=01.3084} policy_loss=-11.0889 policy updated! \n",
      "train step 08099 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6321 diff={max=07.3417, min=00.0041, mean=01.3528} policy_loss=-10.4824 policy updated! \n",
      "train step 08100 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.4976 diff={max=04.5487, min=00.0009, mean=01.0134} policy_loss=-9.7666 policy updated! \n",
      "train step 08101 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=02.7189 diff={max=06.8520, min=00.0094, mean=01.0718} policy_loss=-8.9946 policy updated! \n",
      "train step 08102 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8471 diff={max=07.7498, min=00.0846, mean=01.4829} policy_loss=-12.2247 policy updated! \n",
      "train step 08103 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.9351 diff={max=11.2805, min=00.0282, mean=01.3396} policy_loss=-10.6929 policy updated! \n",
      "train step 08104 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=08.6542 diff={max=11.1998, min=00.0135, mean=01.6660} policy_loss=-11.1284 policy updated! \n",
      "train step 08105 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.1464 diff={max=05.8005, min=00.0208, mean=00.8785} policy_loss=-12.0260 policy updated! \n",
      "train step 08106 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1248 diff={max=05.4870, min=00.0293, mean=01.0047} policy_loss=-10.6830 policy updated! \n",
      "train step 08107 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9928 diff={max=07.9878, min=00.0104, mean=01.3126} policy_loss=-8.9896 policy updated! \n",
      "train step 08108 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5235 diff={max=10.6199, min=00.0080, mean=01.3650} policy_loss=-10.5912 policy updated! \n",
      "train step 08109 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7411 diff={max=09.4207, min=00.0068, mean=01.3030} policy_loss=-11.8940 policy updated! \n",
      "train step 08110 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=06.6528 diff={max=12.3293, min=00.0083, mean=01.4152} policy_loss=-10.9722 policy updated! \n",
      "train step 08111 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8189 diff={max=06.8192, min=00.0072, mean=01.1062} policy_loss=-10.7453 policy updated! \n",
      "train step 08112 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2844 diff={max=06.1216, min=00.0347, mean=01.2406} policy_loss=-9.2475 policy updated! \n",
      "train step 08113 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8780 diff={max=04.7671, min=00.1039, mean=01.0518} policy_loss=-10.6478 policy updated! \n",
      "train step 08114 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4245 diff={max=07.1125, min=00.0387, mean=01.5601} policy_loss=-8.9498 policy updated! \n",
      "train step 08115 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.3981 diff={max=07.9158, min=00.0009, mean=01.6826} policy_loss=-10.4127 policy updated! \n",
      "train step 08116 reward={max=07.0000, min=00.0000, mean=04.0000} optimizing loss=05.5128 diff={max=12.8004, min=00.0160, mean=01.2530} policy_loss=-11.0432 policy updated! \n",
      "train step 08117 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7848 diff={max=09.0682, min=00.0087, mean=01.4627} policy_loss=-11.5880 policy updated! \n",
      "train step 08118 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=06.0325 diff={max=06.8318, min=00.0347, mean=01.5994} policy_loss=-13.4882 policy updated! \n",
      "train step 08119 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8804 diff={max=10.2076, min=00.0004, mean=01.0975} policy_loss=-7.6345 policy updated! \n",
      "train step 08120 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.8775 diff={max=08.1467, min=00.0729, mean=01.3586} policy_loss=-10.1156 policy updated! \n",
      "train step 08121 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.3113 diff={max=17.7894, min=00.0362, mean=01.6705} policy_loss=-9.8576 policy updated! \n",
      "train step 08122 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5500 diff={max=06.3816, min=00.0577, mean=01.3595} policy_loss=-11.4197 policy updated! \n",
      "train step 08123 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=04.4229 diff={max=07.3935, min=00.0003, mean=01.4385} policy_loss=-10.2993 policy updated! \n",
      "train step 08124 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.6082 diff={max=13.6487, min=00.0042, mean=01.6024} policy_loss=-10.1438 policy updated! \n",
      "train step 08125 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.0807 diff={max=06.1545, min=00.0113, mean=01.2437} policy_loss=-11.8681 policy updated! \n",
      "train step 08126 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5220 diff={max=04.9767, min=00.0124, mean=00.8093} policy_loss=-7.9090 policy updated! \n",
      "train step 08127 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7139 diff={max=05.7441, min=00.0183, mean=01.6265} policy_loss=-12.8519 policy updated! \n",
      "train step 08128 reward={max=07.0000, min=00.0000, mean=02.6000} optimizing loss=06.7297 diff={max=09.0433, min=00.0009, mean=01.5905} policy_loss=-12.9274 policy updated! \n",
      "train step 08129 reward={max=08.0000, min=00.0000, mean=06.2000} optimizing loss=03.0634 diff={max=06.2740, min=00.0020, mean=01.0910} policy_loss=-11.1488 policy updated! \n",
      "train step 08130 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.7020 diff={max=14.3469, min=00.0232, mean=01.4655} policy_loss=-9.7152 policy updated! \n",
      "train step 08131 reward={max=07.0000, min=00.0000, mean=05.0000} optimizing loss=03.1008 diff={max=07.6020, min=00.0047, mean=01.0552} policy_loss=-9.7389 policy updated! \n",
      "train step 08132 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.1563 diff={max=06.4908, min=00.0045, mean=01.1133} policy_loss=-11.5982 policy updated! \n",
      "train step 08133 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.0725 diff={max=14.6325, min=00.0039, mean=01.7372} policy_loss=-11.8151 policy updated! \n",
      "train step 08134 reward={max=07.0000, min=00.0000, mean=05.2000} optimizing loss=07.3449 diff={max=10.0837, min=00.0093, mean=01.7542} policy_loss=-13.0689 policy updated! \n",
      "train step 08135 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=02.7468 diff={max=05.8207, min=00.0015, mean=01.1318} policy_loss=-10.5700 policy updated! \n",
      "train step 08136 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5403 diff={max=08.4963, min=00.0022, mean=01.1649} policy_loss=-9.8622 policy updated! \n",
      "train step 08137 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7953 diff={max=07.8853, min=00.0178, mean=01.0687} policy_loss=-9.1811 policy updated! \n",
      "train step 08138 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5458 diff={max=04.4802, min=00.0298, mean=00.8097} policy_loss=-9.8243 policy updated! \n",
      "train step 08139 reward={max=08.0000, min=00.0000, mean=05.8000} optimizing loss=03.2545 diff={max=08.1751, min=00.0108, mean=00.9371} policy_loss=-8.9720 policy updated! \n",
      "train step 08140 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.3177 diff={max=07.7427, min=00.0077, mean=01.6803} policy_loss=-10.6292 policy updated! \n",
      "train step 08141 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.2629 diff={max=11.2815, min=00.0671, mean=02.1915} policy_loss=-9.9857 policy updated! \n",
      "train step 08142 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5841 diff={max=05.3927, min=00.0011, mean=00.9670} policy_loss=-9.7107 policy updated! \n",
      "train step 08143 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=03.7313 diff={max=06.3844, min=00.0158, mean=01.3282} policy_loss=-13.1385 policy updated! \n",
      "train step 08144 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2829 diff={max=03.3885, min=00.0053, mean=00.8624} policy_loss=-11.8382 policy updated! \n",
      "train step 08145 reward={max=08.0000, min=06.0000, mean=06.8000} optimizing loss=07.0014 diff={max=08.6504, min=00.0071, mean=01.6984} policy_loss=-11.5843 policy updated! \n",
      "train step 08146 reward={max=07.0000, min=00.0000, mean=03.8000} optimizing loss=06.1655 diff={max=09.0327, min=00.0081, mean=01.5062} policy_loss=-10.2719 policy updated! \n",
      "train step 08147 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1642 diff={max=04.7346, min=00.0174, mean=01.0422} policy_loss=-10.3829 policy updated! \n",
      "train step 08148 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=06.1105 diff={max=09.2103, min=00.0015, mean=01.4430} policy_loss=-11.8237 policy updated! \n",
      "train step 08149 reward={max=08.0000, min=00.0000, mean=06.2000} optimizing loss=08.6236 diff={max=10.5213, min=00.0228, mean=01.7886} policy_loss=-10.3864 policy updated! \n",
      "train step 08150 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=04.9692 diff={max=10.5525, min=00.0081, mean=01.2847} policy_loss=-7.2280 policy updated! \n",
      "train step 08151 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1090 diff={max=07.0452, min=00.1210, mean=01.3989} policy_loss=-10.2593 policy updated! \n",
      "train step 08152 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3492 diff={max=07.7432, min=00.0110, mean=01.1340} policy_loss=-9.1118 policy updated! \n",
      "train step 08153 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.3367 diff={max=07.9448, min=00.0280, mean=01.3468} policy_loss=-10.3858 policy updated! \n",
      "train step 08154 reward={max=07.0000, min=00.0000, mean=05.4000} optimizing loss=11.4334 diff={max=17.7530, min=00.0156, mean=01.7824} policy_loss=-9.7014 policy updated! \n",
      "train step 08155 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.1812 diff={max=07.0396, min=00.0120, mean=01.1610} policy_loss=-8.6399 policy updated! \n",
      "train step 08156 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.8280 diff={max=13.7488, min=00.0294, mean=01.6765} policy_loss=-9.9488 policy updated! \n",
      "train step 08157 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.8501 diff={max=10.0787, min=00.1071, mean=01.7516} policy_loss=-10.2810 policy updated! \n",
      "train step 08158 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.2931 diff={max=12.2477, min=00.0218, mean=01.5641} policy_loss=-10.7001 policy updated! \n",
      "train step 08159 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0659 diff={max=09.2809, min=00.0254, mean=01.5588} policy_loss=-11.8970 policy updated! \n",
      "train step 08160 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.9710 diff={max=10.1569, min=00.0138, mean=01.4798} policy_loss=-9.9720 policy updated! \n",
      "train step 08161 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=02.8774 diff={max=05.3917, min=00.0308, mean=01.2195} policy_loss=-12.4338 policy updated! \n",
      "train step 08162 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3451 diff={max=11.8255, min=00.0046, mean=01.1492} policy_loss=-9.7934 policy updated! \n",
      "train step 08163 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.6409 diff={max=18.3431, min=00.0157, mean=01.7468} policy_loss=-10.9362 policy updated! \n",
      "train step 08164 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.1109 diff={max=14.5971, min=00.0015, mean=01.9839} policy_loss=-13.0045 policy updated! \n",
      "train step 08165 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.3875 diff={max=06.8575, min=00.0071, mean=01.3203} policy_loss=-10.0073 policy updated! \n",
      "train step 08166 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6316 diff={max=08.5605, min=00.0050, mean=01.5180} policy_loss=-9.4537 policy updated! \n",
      "train step 08167 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2773 diff={max=08.1176, min=00.0021, mean=01.5744} policy_loss=-10.4702 policy updated! \n",
      "train step 08168 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.7894 diff={max=11.0485, min=00.0154, mean=01.6993} policy_loss=-10.8419 policy updated! \n",
      "train step 08169 reward={max=08.0000, min=00.0000, mean=05.8000} optimizing loss=03.1408 diff={max=05.2409, min=00.0441, mean=01.3127} policy_loss=-10.5807 policy updated! \n",
      "train step 08170 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.5352 diff={max=08.4909, min=00.0266, mean=01.5453} policy_loss=-11.7688 policy updated! \n",
      "train step 08171 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8625 diff={max=07.2455, min=00.0233, mean=01.2607} policy_loss=-12.7665 policy updated! \n",
      "train step 08172 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6456 diff={max=08.9981, min=00.0202, mean=01.4375} policy_loss=-10.1732 policy updated! \n",
      "train step 08173 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8378 diff={max=06.5976, min=00.0278, mean=01.3234} policy_loss=-12.6259 policy updated! \n",
      "train step 08174 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.3567 diff={max=11.8002, min=00.0295, mean=01.7525} policy_loss=-10.4766 policy updated! \n",
      "train step 08175 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.0652 diff={max=05.7110, min=00.0384, mean=01.1230} policy_loss=-9.5972 policy updated! \n",
      "train step 08176 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.7646 diff={max=12.6018, min=00.0110, mean=01.5955} policy_loss=-11.0917 policy updated! \n",
      "train step 08177 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5248 diff={max=08.2924, min=00.0075, mean=01.3659} policy_loss=-9.5140 policy updated! \n",
      "train step 08178 reward={max=07.0000, min=00.0000, mean=02.6000} optimizing loss=05.0877 diff={max=08.1968, min=00.0067, mean=01.3692} policy_loss=-8.6904 policy updated! \n",
      "train step 08179 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=06.8477 diff={max=09.8690, min=00.0327, mean=01.4770} policy_loss=-8.0492 policy updated! \n",
      "train step 08180 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.8838 diff={max=05.6082, min=00.0237, mean=01.1700} policy_loss=-10.2407 policy updated! \n",
      "train step 08181 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=05.0290 diff={max=10.2169, min=00.0019, mean=01.2006} policy_loss=-8.3735 policy updated! \n",
      "train step 08182 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3612 diff={max=08.6639, min=00.0564, mean=01.3414} policy_loss=-12.9323 policy updated! \n",
      "train step 08183 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2026 diff={max=10.1750, min=00.0033, mean=01.4108} policy_loss=-11.7630 policy updated! \n",
      "train step 08184 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=08.1203 diff={max=11.6020, min=00.0238, mean=01.7824} policy_loss=-9.3796 policy updated! \n",
      "train step 08185 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.9944 diff={max=07.8132, min=00.0067, mean=01.4051} policy_loss=-10.2736 policy updated! \n",
      "train step 08186 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8687 diff={max=05.0081, min=00.0268, mean=01.2386} policy_loss=-10.1899 policy updated! \n",
      "train step 08187 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0936 diff={max=08.6010, min=00.0206, mean=01.7633} policy_loss=-10.3648 policy updated! \n",
      "train step 08188 reward={max=07.0000, min=00.0000, mean=02.6000} optimizing loss=03.8145 diff={max=07.0855, min=00.0004, mean=01.2774} policy_loss=-11.3829 policy updated! \n",
      "train step 08189 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=05.9233 diff={max=09.4793, min=00.0013, mean=01.3508} policy_loss=-10.2390 policy updated! \n",
      "train step 08190 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.6623 diff={max=10.6294, min=00.0161, mean=01.6513} policy_loss=-9.5147 policy updated! \n",
      "train step 08191 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=05.3882 diff={max=09.8273, min=00.0079, mean=01.3210} policy_loss=-10.8903 policy updated! \n",
      "train step 08192 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5570 diff={max=05.4366, min=00.0062, mean=01.0662} policy_loss=-9.7014 policy updated! \n",
      "train step 08193 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1291 diff={max=07.8115, min=00.0229, mean=01.1555} policy_loss=-8.7587 policy updated! \n",
      "train step 08194 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5839 diff={max=07.6608, min=00.0026, mean=01.2033} policy_loss=-9.0186 policy updated! \n",
      "train step 08195 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.6876 diff={max=07.6890, min=00.0092, mean=01.3874} policy_loss=-9.4939 policy updated! \n",
      "train step 08196 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6452 diff={max=03.0012, min=00.0114, mean=00.9154} policy_loss=-9.3066 policy updated! \n",
      "train step 08197 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.6248 diff={max=11.7757, min=00.0086, mean=01.5116} policy_loss=-9.8388 policy updated! \n",
      "train step 08198 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=06.7695 diff={max=10.8770, min=00.0125, mean=01.6476} policy_loss=-10.4499 policy updated! \n",
      "train step 08199 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=03.9784 diff={max=07.9220, min=00.0252, mean=01.2931} policy_loss=-10.4816 policy updated! \n",
      "train step 08200 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=08.1541 diff={max=09.8083, min=00.0193, mean=01.7654} policy_loss=-9.6087 policy updated! \n",
      "train step 08201 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=06.2089 diff={max=09.7752, min=00.0191, mean=01.5179} policy_loss=-11.4562 policy updated! \n",
      "train step 08202 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.8226 diff={max=11.9710, min=00.0076, mean=01.7731} policy_loss=-9.7436 policy updated! \n",
      "train step 08203 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=05.1319 diff={max=08.1447, min=00.0287, mean=01.4535} policy_loss=-9.1507 policy updated! \n",
      "train step 08204 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=04.1429 diff={max=07.8304, min=00.0121, mean=01.2549} policy_loss=-10.4719 policy updated! \n",
      "train step 08205 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=05.4587 diff={max=08.5930, min=00.0493, mean=01.4281} policy_loss=-9.8552 policy updated! \n",
      "train step 08206 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=07.2733 diff={max=09.8915, min=00.0044, mean=01.5061} policy_loss=-9.9247 policy updated! \n",
      "train step 08207 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3301 diff={max=08.7014, min=00.0369, mean=01.5550} policy_loss=-10.8084 policy updated! \n",
      "train step 08208 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7506 diff={max=06.2818, min=00.0140, mean=01.2443} policy_loss=-10.2479 policy updated! \n",
      "train step 08209 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1615 diff={max=07.8925, min=00.0134, mean=01.7254} policy_loss=-10.0549 policy updated! \n",
      "train step 08210 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=07.0306 diff={max=10.5734, min=00.0086, mean=01.4981} policy_loss=-10.4342 policy updated! \n",
      "train step 08211 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8341 diff={max=07.8876, min=00.0002, mean=01.5657} policy_loss=-12.9193 policy updated! \n",
      "train step 08212 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2102 diff={max=06.8450, min=00.0004, mean=01.2067} policy_loss=-10.4575 policy updated! \n",
      "train step 08213 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0112 diff={max=05.9227, min=00.0134, mean=01.4268} policy_loss=-10.9389 policy updated! \n",
      "train step 08214 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=05.0473 diff={max=08.9379, min=00.0064, mean=01.4348} policy_loss=-8.2139 policy updated! \n",
      "train step 08215 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.4480 diff={max=10.0604, min=00.0652, mean=01.5370} policy_loss=-10.4647 policy updated! \n",
      "train step 08216 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6838 diff={max=08.2185, min=00.1280, mean=01.7251} policy_loss=-11.2276 policy updated! \n",
      "train step 08217 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6731 diff={max=06.9264, min=00.0215, mean=01.2946} policy_loss=-10.7695 policy updated! \n",
      "train step 08218 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=03.3289 diff={max=05.4842, min=00.0250, mean=01.0920} policy_loss=-8.7524 policy updated! \n",
      "train step 08219 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.9982 diff={max=11.8118, min=00.0094, mean=01.7790} policy_loss=-12.6063 policy updated! \n",
      "train step 08220 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=07.2016 diff={max=11.5562, min=00.0103, mean=01.4042} policy_loss=-8.1223 policy updated! \n",
      "train step 08221 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3358 diff={max=08.0869, min=00.0153, mean=01.3581} policy_loss=-10.6235 policy updated! \n",
      "train step 08222 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5634 diff={max=10.9161, min=00.0017, mean=01.2456} policy_loss=-10.9196 policy updated! \n",
      "train step 08223 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=02.4666 diff={max=04.3872, min=00.0056, mean=01.0986} policy_loss=-10.8644 policy updated! \n",
      "train step 08224 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7493 diff={max=10.0837, min=00.0069, mean=01.3910} policy_loss=-8.6498 policy updated! \n",
      "train step 08225 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.5551 diff={max=08.7874, min=00.0054, mean=01.3286} policy_loss=-10.0866 policy updated! \n",
      "train step 08226 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7092 diff={max=11.3328, min=00.0086, mean=01.4210} policy_loss=-13.3314 policy updated! \n",
      "train step 08227 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1943 diff={max=05.4674, min=00.0147, mean=01.0188} policy_loss=-10.6336 policy updated! \n",
      "train step 08228 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7032 diff={max=06.1728, min=00.0018, mean=01.6116} policy_loss=-10.7155 policy updated! \n",
      "train step 08229 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7345 diff={max=07.4576, min=00.0013, mean=01.2378} policy_loss=-9.7637 policy updated! \n",
      "train step 08230 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=06.2208 diff={max=07.8704, min=00.0100, mean=01.5560} policy_loss=-11.7927 policy updated! \n",
      "train step 08231 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6831 diff={max=09.2926, min=00.0056, mean=01.3760} policy_loss=-10.8085 policy updated! \n",
      "train step 08232 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5033 diff={max=04.5586, min=00.0234, mean=01.1532} policy_loss=-10.3433 policy updated! \n",
      "train step 08233 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=03.3372 diff={max=05.5916, min=00.0604, mean=01.2832} policy_loss=-10.6064 policy updated! \n",
      "train step 08234 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6190 diff={max=07.3653, min=00.0001, mean=01.5225} policy_loss=-11.0544 policy updated! \n",
      "train step 08235 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.7881 diff={max=05.4684, min=00.0558, mean=01.0518} policy_loss=-10.8803 policy updated! \n",
      "train step 08236 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=05.9607 diff={max=12.3656, min=00.0116, mean=01.3046} policy_loss=-11.3690 policy updated! \n",
      "train step 08237 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2622 diff={max=08.7881, min=00.0253, mean=01.5126} policy_loss=-13.5021 policy updated! \n",
      "train step 08238 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2837 diff={max=07.5544, min=00.0086, mean=01.1676} policy_loss=-13.2626 policy updated! \n",
      "train step 08239 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.7273 diff={max=17.1683, min=00.0097, mean=01.4940} policy_loss=-10.6306 policy updated! \n",
      "train step 08240 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=06.3395 diff={max=11.8224, min=00.0126, mean=01.3863} policy_loss=-10.7376 policy updated! \n",
      "train step 08241 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=04.3786 diff={max=10.5581, min=00.0741, mean=01.3374} policy_loss=-9.6294 policy updated! \n",
      "train step 08242 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2280 diff={max=07.9177, min=00.0559, mean=01.5528} policy_loss=-9.3025 policy updated! \n",
      "train step 08243 reward={max=07.0000, min=00.0000, mean=02.6000} optimizing loss=07.2586 diff={max=09.7064, min=00.0252, mean=01.7171} policy_loss=-9.9092 policy updated! \n",
      "train step 08244 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9482 diff={max=07.8203, min=00.0054, mean=01.1562} policy_loss=-8.9730 policy updated! \n",
      "train step 08245 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.9841 diff={max=07.2128, min=00.0203, mean=01.1039} policy_loss=-12.0068 policy updated! \n",
      "train step 08246 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3886 diff={max=07.9333, min=00.0027, mean=01.1543} policy_loss=-10.4599 policy updated! \n",
      "train step 08247 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.2870 diff={max=11.5440, min=00.0313, mean=01.8629} policy_loss=-12.2996 policy updated! \n",
      "train step 08248 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=07.8635 diff={max=11.6416, min=00.0305, mean=01.6579} policy_loss=-11.7153 policy updated! \n",
      "train step 08249 reward={max=07.0000, min=00.0000, mean=05.2000} optimizing loss=06.1488 diff={max=11.5532, min=00.0056, mean=01.3580} policy_loss=-11.4344 policy updated! \n",
      "train step 08250 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=06.2379 diff={max=08.0787, min=00.0049, mean=01.3908} policy_loss=-10.4406 policy updated! \n",
      "train step 08251 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.4263 diff={max=11.8212, min=00.0008, mean=01.6333} policy_loss=-13.0737 policy updated! \n",
      "train step 08252 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.7259 diff={max=11.2101, min=00.0368, mean=01.7044} policy_loss=-11.8747 policy updated! \n",
      "train step 08253 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8091 diff={max=03.5635, min=00.0003, mean=01.0096} policy_loss=-9.2582 policy updated! \n",
      "train step 08254 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9702 diff={max=11.7719, min=00.0010, mean=01.5932} policy_loss=-10.6036 policy updated! \n",
      "train step 08255 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=09.6630 diff={max=15.5719, min=00.0420, mean=01.9005} policy_loss=-9.7793 policy updated! \n",
      "train step 08256 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7323 diff={max=10.0609, min=00.0027, mean=01.6641} policy_loss=-11.7430 policy updated! \n",
      "train step 08257 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=02.2190 diff={max=05.6592, min=00.0007, mean=00.9624} policy_loss=-12.2921 policy updated! \n",
      "train step 08258 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.5247 diff={max=13.6700, min=00.0086, mean=01.6525} policy_loss=-11.2637 policy updated! \n",
      "train step 08259 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=02.6781 diff={max=05.6152, min=00.0077, mean=01.1758} policy_loss=-8.5606 policy updated! \n",
      "train step 08260 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.4285 diff={max=06.3301, min=00.0232, mean=00.9368} policy_loss=-8.7618 policy updated! \n",
      "train step 08261 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.8701 diff={max=10.7670, min=00.0014, mean=01.2784} policy_loss=-9.6467 policy updated! \n",
      "train step 08262 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.9768 diff={max=08.9928, min=00.0044, mean=01.8291} policy_loss=-10.5674 policy updated! \n",
      "train step 08263 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=05.1901 diff={max=07.1100, min=00.0290, mean=01.5769} policy_loss=-10.7554 policy updated! \n",
      "train step 08264 reward={max=07.0000, min=00.0000, mean=05.2000} optimizing loss=03.6024 diff={max=05.9852, min=00.0107, mean=01.3866} policy_loss=-10.6223 policy updated! \n",
      "train step 08265 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.4698 diff={max=07.4869, min=00.0307, mean=01.1524} policy_loss=-10.2460 policy updated! \n",
      "train step 08266 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1404 diff={max=08.8432, min=00.0113, mean=01.1088} policy_loss=-12.7787 policy updated! \n",
      "train step 08267 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.6481 diff={max=13.2991, min=00.0184, mean=01.7272} policy_loss=-12.7922 policy updated! \n",
      "train step 08268 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5221 diff={max=04.3235, min=00.0079, mean=01.1096} policy_loss=-12.3708 policy updated! \n",
      "train step 08269 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3317 diff={max=12.3045, min=00.0032, mean=01.3889} policy_loss=-9.5503 policy updated! \n",
      "train step 08270 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.6918 diff={max=08.8323, min=00.1233, mean=01.3984} policy_loss=-11.6023 policy updated! \n",
      "train step 08271 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=04.2289 diff={max=06.8189, min=00.0029, mean=01.3082} policy_loss=-9.8390 policy updated! \n",
      "train step 08272 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0906 diff={max=07.2007, min=00.0005, mean=01.0170} policy_loss=-9.3390 policy updated! \n",
      "train step 08273 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5258 diff={max=05.5057, min=00.0156, mean=01.1165} policy_loss=-10.9454 policy updated! \n",
      "train step 08274 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7083 diff={max=05.3333, min=00.0007, mean=00.8558} policy_loss=-12.4344 policy updated! \n",
      "train step 08275 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.0410 diff={max=08.3555, min=00.0180, mean=01.4654} policy_loss=-9.8637 policy updated! \n",
      "train step 08276 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8080 diff={max=08.8980, min=00.0227, mean=01.1632} policy_loss=-10.3966 policy updated! \n",
      "train step 08277 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.4375 diff={max=08.6584, min=00.0409, mean=01.5883} policy_loss=-9.8859 policy updated! \n",
      "train step 08278 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.3126 diff={max=08.0134, min=00.0042, mean=01.2414} policy_loss=-8.8404 policy updated! \n",
      "train step 08279 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.5027 diff={max=08.1654, min=00.0195, mean=01.6987} policy_loss=-14.2571 policy updated! \n",
      "train step 08280 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.5479 diff={max=11.2751, min=00.0335, mean=01.5548} policy_loss=-10.3463 policy updated! \n",
      "train step 08281 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.0825 diff={max=12.1948, min=00.0203, mean=02.1239} policy_loss=-11.0047 policy updated! \n",
      "train step 08282 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5158 diff={max=10.5012, min=00.0877, mean=01.0999} policy_loss=-10.1911 policy updated! \n",
      "train step 08283 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1812 diff={max=05.4873, min=00.0176, mean=00.9679} policy_loss=-12.5207 policy updated! \n",
      "train step 08284 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.8446 diff={max=04.7995, min=00.0120, mean=00.9414} policy_loss=-9.9955 policy updated! \n",
      "train step 08285 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.4687 diff={max=05.6686, min=00.0161, mean=01.0228} policy_loss=-9.8295 policy updated! \n",
      "train step 08286 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.9758 diff={max=05.5709, min=00.0124, mean=01.2303} policy_loss=-9.5947 policy updated! \n",
      "train step 08287 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6560 diff={max=09.1452, min=00.0146, mean=01.3085} policy_loss=-8.8834 policy updated! \n",
      "train step 08288 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4184 diff={max=07.2271, min=00.0326, mean=01.2702} policy_loss=-11.7991 policy updated! \n",
      "train step 08289 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.6853 diff={max=13.3873, min=00.0189, mean=01.8060} policy_loss=-9.0988 policy updated! \n",
      "train step 08290 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.0845 diff={max=08.2731, min=00.0158, mean=01.2504} policy_loss=-12.6452 policy updated! \n",
      "train step 08291 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1887 diff={max=11.7225, min=00.0019, mean=01.1699} policy_loss=-10.0143 policy updated! \n",
      "train step 08292 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5408 diff={max=05.0940, min=00.0048, mean=01.0119} policy_loss=-9.6238 policy updated! \n",
      "train step 08293 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0115 diff={max=08.0512, min=00.0207, mean=01.3386} policy_loss=-11.3386 policy updated! \n",
      "train step 08294 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8688 diff={max=06.1335, min=00.0073, mean=01.1390} policy_loss=-12.8608 policy updated! \n",
      "train step 08295 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.7703 diff={max=09.1887, min=00.0031, mean=01.0185} policy_loss=-8.1516 policy updated! \n",
      "train step 08296 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4922 diff={max=04.4660, min=00.0034, mean=00.8243} policy_loss=-8.7765 policy updated! \n",
      "train step 08297 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0685 diff={max=11.9694, min=00.0402, mean=01.1898} policy_loss=-9.7045 policy updated! \n",
      "train step 08298 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8215 diff={max=04.2908, min=00.0168, mean=00.9240} policy_loss=-8.8287 policy updated! \n",
      "train step 08299 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6916 diff={max=06.7504, min=00.0299, mean=01.2648} policy_loss=-9.5331 policy updated! \n",
      "train step 08300 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.6316 diff={max=06.8993, min=00.0178, mean=00.9552} policy_loss=-9.5194 policy updated! \n",
      "train step 08301 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2354 diff={max=09.6377, min=00.0153, mean=01.4287} policy_loss=-7.6243 policy updated! \n",
      "train step 08302 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3942 diff={max=04.3613, min=00.0274, mean=00.8100} policy_loss=-9.4666 policy updated! \n",
      "train step 08303 reward={max=07.0000, min=00.0000, mean=02.6000} optimizing loss=05.3810 diff={max=07.0751, min=00.0222, mean=01.3992} policy_loss=-10.8575 policy updated! \n",
      "train step 08304 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3242 diff={max=08.7801, min=00.0098, mean=01.1642} policy_loss=-11.8349 policy updated! \n",
      "train step 08305 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.7592 diff={max=03.5691, min=00.1176, mean=01.0491} policy_loss=-10.7236 policy updated! \n",
      "train step 08306 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9332 diff={max=10.5714, min=00.0070, mean=01.2995} policy_loss=-10.0049 policy updated! \n",
      "train step 08307 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9518 diff={max=10.2141, min=00.0112, mean=01.4531} policy_loss=-8.8931 policy updated! \n",
      "train step 08308 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.1319 diff={max=10.5028, min=00.0324, mean=01.8104} policy_loss=-9.5127 policy updated! \n",
      "train step 08309 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5782 diff={max=10.6915, min=00.0391, mean=01.0582} policy_loss=-11.5074 policy updated! \n",
      "train step 08310 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.2925 diff={max=07.2946, min=00.0240, mean=01.1356} policy_loss=-8.6621 policy updated! \n",
      "train step 08311 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.9559 diff={max=06.7974, min=00.0034, mean=01.0584} policy_loss=-9.4206 policy updated! \n",
      "train step 08312 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0043 diff={max=07.7129, min=00.0198, mean=01.3513} policy_loss=-9.6027 policy updated! \n",
      "train step 08313 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.3290 diff={max=05.9207, min=00.0020, mean=00.9375} policy_loss=-8.5610 policy updated! \n",
      "train step 08314 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.0435 diff={max=04.9238, min=00.0178, mean=00.8948} policy_loss=-9.3208 policy updated! \n",
      "train step 08315 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.8339 diff={max=08.0336, min=00.0047, mean=01.2983} policy_loss=-9.7806 policy updated! \n",
      "train step 08316 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9922 diff={max=08.7769, min=00.0090, mean=01.5603} policy_loss=-10.4931 policy updated! \n",
      "train step 08317 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.3351 diff={max=10.8138, min=00.0280, mean=01.6504} policy_loss=-10.9075 policy updated! \n",
      "train step 08318 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2584 diff={max=07.9712, min=00.0399, mean=01.2259} policy_loss=-8.7738 policy updated! \n",
      "train step 08319 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=07.8063 diff={max=09.2086, min=00.0029, mean=01.6367} policy_loss=-10.2042 policy updated! \n",
      "train step 08320 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.3692 diff={max=08.0123, min=00.0032, mean=01.7642} policy_loss=-11.7216 policy updated! \n",
      "train step 08321 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.4829 diff={max=05.4630, min=00.0231, mean=01.2593} policy_loss=-9.9272 policy updated! \n",
      "train step 08322 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9028 diff={max=06.6509, min=00.0755, mean=01.3852} policy_loss=-11.8106 policy updated! \n",
      "train step 08323 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.8241 diff={max=08.5558, min=00.0093, mean=01.5315} policy_loss=-11.0358 policy updated! \n",
      "train step 08324 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.0894 diff={max=15.8277, min=00.0050, mean=01.4990} policy_loss=-10.0410 policy updated! \n",
      "train step 08325 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.1282 diff={max=06.3929, min=00.0030, mean=00.9181} policy_loss=-9.5125 policy updated! \n",
      "train step 08326 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0413 diff={max=06.8096, min=00.0083, mean=00.9123} policy_loss=-10.8173 policy updated! \n",
      "train step 08327 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8565 diff={max=06.5295, min=00.0118, mean=01.3507} policy_loss=-10.2139 policy updated! \n",
      "train step 08328 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1701 diff={max=07.1856, min=00.0416, mean=01.1350} policy_loss=-10.5598 policy updated! \n",
      "train step 08329 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0555 diff={max=06.8873, min=00.0291, mean=01.1903} policy_loss=-11.0834 policy updated! \n",
      "train step 08330 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.6591 diff={max=03.8168, min=00.0062, mean=00.9025} policy_loss=-10.6051 policy updated! \n",
      "train step 08331 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3833 diff={max=12.4887, min=00.1012, mean=01.3929} policy_loss=-10.6364 policy updated! \n",
      "train step 08332 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0682 diff={max=09.8075, min=00.0131, mean=01.5182} policy_loss=-9.4946 policy updated! \n",
      "train step 08333 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1438 diff={max=07.2840, min=00.0340, mean=01.5963} policy_loss=-10.2680 policy updated! \n",
      "train step 08334 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9198 diff={max=06.2524, min=00.0013, mean=01.2363} policy_loss=-10.5008 policy updated! \n",
      "train step 08335 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.2363 diff={max=07.8315, min=00.0001, mean=01.2665} policy_loss=-8.0589 policy updated! \n",
      "train step 08336 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.0106 diff={max=09.8486, min=00.0020, mean=01.2350} policy_loss=-10.4042 policy updated! \n",
      "train step 08337 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1494 diff={max=07.4678, min=00.0126, mean=01.2300} policy_loss=-11.7581 policy updated! \n",
      "train step 08338 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4857 diff={max=08.0457, min=00.0415, mean=01.2578} policy_loss=-10.3266 policy updated! \n",
      "train step 08339 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2069 diff={max=08.8108, min=00.0201, mean=01.3480} policy_loss=-11.5806 policy updated! \n",
      "train step 08340 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.1250 diff={max=07.5534, min=00.0754, mean=01.7548} policy_loss=-11.5807 policy updated! \n",
      "train step 08341 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5500 diff={max=06.1619, min=00.0384, mean=01.8537} policy_loss=-9.5151 policy updated! \n",
      "train step 08342 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9235 diff={max=06.5663, min=00.0719, mean=01.5121} policy_loss=-12.9518 policy updated! \n",
      "train step 08343 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.1876 diff={max=11.0014, min=00.0121, mean=01.7732} policy_loss=-10.0927 policy updated! \n",
      "train step 08344 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5464 diff={max=06.9602, min=00.0013, mean=01.6291} policy_loss=-9.1307 policy updated! \n",
      "train step 08345 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.7348 diff={max=04.7921, min=00.0065, mean=01.1859} policy_loss=-9.3345 policy updated! \n",
      "train step 08346 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8267 diff={max=09.1892, min=00.0285, mean=01.5887} policy_loss=-9.5264 policy updated! \n",
      "train step 08347 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1000 diff={max=11.2444, min=00.0968, mean=01.2753} policy_loss=-9.7801 policy updated! \n",
      "train step 08348 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1328 diff={max=06.2155, min=00.0101, mean=01.0798} policy_loss=-10.0498 policy updated! \n",
      "train step 08349 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7943 diff={max=07.9308, min=00.0004, mean=00.9927} policy_loss=-9.0830 policy updated! \n",
      "train step 08350 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.6596 diff={max=04.3648, min=00.0041, mean=00.9263} policy_loss=-10.8241 policy updated! \n",
      "train step 08351 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.4968 diff={max=15.0754, min=00.0379, mean=01.4578} policy_loss=-10.8473 policy updated! \n",
      "train step 08352 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0926 diff={max=05.8774, min=00.0466, mean=01.1337} policy_loss=-8.4051 policy updated! \n",
      "train step 08353 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5174 diff={max=05.7310, min=00.0046, mean=01.1082} policy_loss=-10.6250 policy updated! \n",
      "train step 08354 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.1145 diff={max=09.3398, min=00.0253, mean=01.5872} policy_loss=-13.3670 policy updated! \n",
      "train step 08355 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.2646 diff={max=07.5490, min=00.0075, mean=00.8976} policy_loss=-9.5157 policy updated! \n",
      "train step 08356 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=05.3906 diff={max=07.5905, min=00.0102, mean=01.4596} policy_loss=-9.9328 policy updated! \n",
      "train step 08357 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2856 diff={max=10.1323, min=00.0169, mean=01.2433} policy_loss=-9.5687 policy updated! \n",
      "train step 08358 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=06.9999 diff={max=10.8027, min=00.0279, mean=01.4770} policy_loss=-9.0787 policy updated! \n",
      "train step 08359 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1575 diff={max=03.9750, min=00.0021, mean=01.0221} policy_loss=-9.5905 policy updated! \n",
      "train step 08360 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.1876 diff={max=07.2644, min=00.0055, mean=01.4375} policy_loss=-11.6574 policy updated! \n",
      "train step 08361 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6942 diff={max=06.1140, min=00.0069, mean=01.0288} policy_loss=-10.1616 policy updated! \n",
      "train step 08362 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6295 diff={max=08.1407, min=00.0545, mean=01.0737} policy_loss=-9.9322 policy updated! \n",
      "train step 08363 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8914 diff={max=08.6068, min=00.1147, mean=01.4834} policy_loss=-13.2270 policy updated! \n",
      "train step 08364 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0375 diff={max=11.5087, min=00.0178, mean=01.5394} policy_loss=-12.2040 policy updated! \n",
      "train step 08365 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.2192 diff={max=08.2593, min=00.0051, mean=01.1751} policy_loss=-10.1403 policy updated! \n",
      "train step 08366 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0255 diff={max=06.3942, min=00.0107, mean=01.2862} policy_loss=-11.0312 policy updated! \n",
      "train step 08367 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0352 diff={max=06.2007, min=00.0159, mean=01.1002} policy_loss=-12.2755 policy updated! \n",
      "train step 08368 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=06.6036 diff={max=08.4180, min=00.0270, mean=01.6581} policy_loss=-12.7053 policy updated! \n",
      "train step 08369 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.9190 diff={max=05.2534, min=00.0341, mean=01.2303} policy_loss=-12.3732 policy updated! \n",
      "train step 08370 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.9873 diff={max=07.5636, min=00.0033, mean=01.2827} policy_loss=-11.2493 policy updated! \n",
      "train step 08371 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4201 diff={max=04.7878, min=00.0170, mean=01.0359} policy_loss=-10.4267 policy updated! \n",
      "train step 08372 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6712 diff={max=08.5838, min=00.0319, mean=01.2912} policy_loss=-9.1424 policy updated! \n",
      "train step 08373 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3773 diff={max=06.3428, min=00.0070, mean=00.9761} policy_loss=-9.6372 policy updated! \n",
      "train step 08374 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.6789 diff={max=06.1242, min=00.0029, mean=00.9498} policy_loss=-10.3476 policy updated! \n",
      "train step 08375 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.6395 diff={max=06.3065, min=00.0152, mean=01.1856} policy_loss=-13.4407 policy updated! \n",
      "train step 08376 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3081 diff={max=11.0798, min=00.0417, mean=01.4863} policy_loss=-8.4051 policy updated! \n",
      "train step 08377 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9388 diff={max=05.8396, min=00.0344, mean=01.0743} policy_loss=-10.7977 policy updated! \n",
      "train step 08378 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7364 diff={max=07.8085, min=00.0119, mean=01.4117} policy_loss=-10.1674 policy updated! \n",
      "train step 08379 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.5456 diff={max=07.0467, min=00.0417, mean=01.3806} policy_loss=-11.1410 policy updated! \n",
      "train step 08380 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.1216 diff={max=05.1122, min=00.0643, mean=01.0244} policy_loss=-11.0864 policy updated! \n",
      "train step 08381 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3085 diff={max=06.8514, min=00.0831, mean=01.1638} policy_loss=-10.4658 policy updated! \n",
      "train step 08382 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3008 diff={max=04.8343, min=00.0009, mean=00.9733} policy_loss=-10.2631 policy updated! \n",
      "train step 08383 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1611 diff={max=05.8948, min=00.0021, mean=01.1623} policy_loss=-10.7801 policy updated! \n",
      "train step 08384 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.5036 diff={max=06.8970, min=00.0114, mean=01.3532} policy_loss=-10.7160 policy updated! \n",
      "train step 08385 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.6987 diff={max=06.9358, min=00.0125, mean=01.3778} policy_loss=-8.2888 policy updated! \n",
      "train step 08386 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9351 diff={max=08.6073, min=00.0038, mean=01.2946} policy_loss=-9.7423 policy updated! \n",
      "train step 08387 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4890 diff={max=13.7088, min=00.0104, mean=01.3580} policy_loss=-10.3229 policy updated! \n",
      "train step 08388 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7734 diff={max=09.6628, min=00.0023, mean=01.1631} policy_loss=-8.3440 policy updated! \n",
      "train step 08389 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6291 diff={max=09.3237, min=00.0142, mean=01.2365} policy_loss=-9.6335 policy updated! \n",
      "train step 08390 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.4124 diff={max=06.9967, min=00.0114, mean=01.3808} policy_loss=-9.4152 policy updated! \n",
      "train step 08391 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4758 diff={max=07.5247, min=00.0382, mean=01.2157} policy_loss=-11.5184 policy updated! \n",
      "train step 08392 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6022 diff={max=07.5390, min=00.0166, mean=01.2378} policy_loss=-12.4859 policy updated! \n",
      "train step 08393 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2529 diff={max=06.6682, min=00.0166, mean=01.0073} policy_loss=-11.6701 policy updated! \n",
      "train step 08394 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9686 diff={max=06.3733, min=00.0100, mean=01.1075} policy_loss=-9.2773 policy updated! \n",
      "train step 08395 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.3634 diff={max=07.8222, min=00.0067, mean=01.1807} policy_loss=-11.6538 policy updated! \n",
      "train step 08396 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0397 diff={max=08.8623, min=00.0150, mean=01.4359} policy_loss=-10.9113 policy updated! \n",
      "train step 08397 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.5502 diff={max=20.0281, min=00.0106, mean=01.7214} policy_loss=-11.9251 policy updated! \n",
      "train step 08398 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7475 diff={max=07.3394, min=00.0068, mean=01.6047} policy_loss=-9.9834 policy updated! \n",
      "train step 08399 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9425 diff={max=09.5365, min=00.0056, mean=01.5315} policy_loss=-9.2366 policy updated! \n",
      "train step 08400 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.5812 diff={max=08.2742, min=00.0062, mean=01.2660} policy_loss=-9.4229 policy updated! \n",
      "train step 08401 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.4705 diff={max=09.0872, min=00.0469, mean=01.5405} policy_loss=-9.4869 policy updated! \n",
      "train step 08402 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3346 diff={max=08.9165, min=00.0237, mean=01.8182} policy_loss=-9.5394 policy updated! \n",
      "train step 08403 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3431 diff={max=07.5159, min=00.0025, mean=01.3909} policy_loss=-10.4366 policy updated! \n",
      "train step 08404 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8284 diff={max=08.1646, min=00.0770, mean=01.3288} policy_loss=-9.4260 policy updated! \n",
      "train step 08405 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.7975 diff={max=04.7971, min=00.0281, mean=01.3155} policy_loss=-9.9804 policy updated! \n",
      "train step 08406 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8946 diff={max=08.6359, min=00.0009, mean=01.1732} policy_loss=-9.2986 policy updated! \n",
      "train step 08407 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1058 diff={max=05.4222, min=00.0253, mean=01.1858} policy_loss=-10.7937 policy updated! \n",
      "train step 08408 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.8038 diff={max=06.4793, min=00.0149, mean=01.2776} policy_loss=-10.8226 policy updated! \n",
      "train step 08409 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.4982 diff={max=12.7921, min=00.0622, mean=01.2809} policy_loss=-9.4682 policy updated! \n",
      "train step 08410 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.3881 diff={max=06.3686, min=00.0102, mean=01.2343} policy_loss=-9.9350 policy updated! \n",
      "train step 08411 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3537 diff={max=12.4164, min=00.0238, mean=01.7197} policy_loss=-9.9224 policy updated! \n",
      "train step 08412 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.2528 diff={max=04.2062, min=00.0291, mean=01.1085} policy_loss=-11.0044 policy updated! \n",
      "train step 08413 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2563 diff={max=04.7718, min=00.0037, mean=01.0110} policy_loss=-11.3540 policy updated! \n",
      "train step 08414 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=18.8504 diff={max=22.1081, min=00.0307, mean=02.0039} policy_loss=-11.9841 policy updated! \n",
      "train step 08415 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.8896 diff={max=07.8500, min=00.0316, mean=01.3481} policy_loss=-10.4308 policy updated! \n",
      "train step 08416 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1409 diff={max=07.3841, min=00.0186, mean=01.2575} policy_loss=-9.9634 policy updated! \n",
      "train step 08417 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.2880 diff={max=13.3759, min=00.0425, mean=01.6570} policy_loss=-11.5231 policy updated! \n",
      "train step 08418 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7326 diff={max=09.2469, min=00.0019, mean=01.6527} policy_loss=-10.3280 policy updated! \n",
      "train step 08419 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3530 diff={max=05.0516, min=00.0004, mean=01.3115} policy_loss=-10.5813 policy updated! \n",
      "train step 08420 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.8958 diff={max=05.4631, min=00.0018, mean=01.2210} policy_loss=-11.7878 policy updated! \n",
      "train step 08421 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.9637 diff={max=04.9686, min=00.0193, mean=01.1469} policy_loss=-12.0424 policy updated! \n",
      "train step 08422 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1257 diff={max=05.3084, min=00.0306, mean=01.1653} policy_loss=-12.4392 policy updated! \n",
      "train step 08423 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1643 diff={max=09.2616, min=00.0458, mean=01.6142} policy_loss=-12.7446 policy updated! \n",
      "train step 08424 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=10.5285 diff={max=16.0669, min=00.0165, mean=01.6601} policy_loss=-9.8902 policy updated! \n",
      "train step 08425 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.6060 diff={max=09.5567, min=00.0090, mean=01.3257} policy_loss=-11.9527 policy updated! \n",
      "train step 08426 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0348 diff={max=10.0926, min=00.0260, mean=01.4273} policy_loss=-10.1087 policy updated! \n",
      "train step 08427 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5380 diff={max=07.9072, min=00.0527, mean=01.4427} policy_loss=-11.5751 policy updated! \n",
      "train step 08428 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6115 diff={max=09.3571, min=00.0020, mean=01.6317} policy_loss=-10.6370 policy updated! \n",
      "train step 08429 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8151 diff={max=07.3724, min=00.0797, mean=01.1276} policy_loss=-10.3656 policy updated! \n",
      "train step 08430 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.5590 diff={max=09.8348, min=00.0018, mean=01.5400} policy_loss=-9.7948 policy updated! \n",
      "train step 08431 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=03.5467 diff={max=06.4666, min=00.0174, mean=01.1195} policy_loss=-11.6872 policy updated! \n",
      "train step 08432 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1753 diff={max=04.6124, min=00.0137, mean=01.2272} policy_loss=-11.9692 policy updated! \n",
      "train step 08433 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.7574 diff={max=08.6828, min=00.0023, mean=01.2622} policy_loss=-10.9414 policy updated! \n",
      "train step 08434 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1645 diff={max=06.2058, min=00.0000, mean=01.3358} policy_loss=-10.9991 policy updated! \n",
      "train step 08435 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=04.6947 diff={max=08.6917, min=00.0062, mean=01.3506} policy_loss=-9.9538 policy updated! \n",
      "train step 08436 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.9956 diff={max=06.0260, min=00.0013, mean=01.1180} policy_loss=-8.7141 policy updated! \n",
      "train step 08437 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5794 diff={max=06.6451, min=00.0292, mean=01.1148} policy_loss=-9.2698 policy updated! \n",
      "train step 08438 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0235 diff={max=03.8054, min=00.0003, mean=01.0269} policy_loss=-10.8038 policy updated! \n",
      "train step 08439 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9146 diff={max=10.6010, min=00.0075, mean=01.1133} policy_loss=-9.8071 policy updated! \n",
      "train step 08440 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.3312 diff={max=12.7202, min=00.0109, mean=01.7413} policy_loss=-11.0641 policy updated! \n",
      "train step 08441 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0932 diff={max=09.4939, min=00.0074, mean=01.1690} policy_loss=-11.2471 policy updated! \n",
      "train step 08442 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3868 diff={max=09.0289, min=00.0010, mean=01.2679} policy_loss=-8.0591 policy updated! \n",
      "train step 08443 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0078 diff={max=05.7236, min=00.0287, mean=01.1026} policy_loss=-8.6095 policy updated! \n",
      "train step 08444 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8975 diff={max=06.3708, min=00.0661, mean=01.6553} policy_loss=-10.4731 policy updated! \n",
      "train step 08445 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.1364 diff={max=05.4396, min=00.0287, mean=00.9368} policy_loss=-9.2969 policy updated! \n",
      "train step 08446 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2909 diff={max=11.4823, min=00.0228, mean=01.0230} policy_loss=-9.3321 policy updated! \n",
      "train step 08447 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9923 diff={max=04.5455, min=00.0417, mean=01.2745} policy_loss=-11.0079 policy updated! \n",
      "train step 08448 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1133 diff={max=09.4251, min=00.0058, mean=01.3836} policy_loss=-11.3022 policy updated! \n",
      "train step 08449 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5039 diff={max=04.6614, min=00.0043, mean=01.0594} policy_loss=-10.7737 policy updated! \n",
      "train step 08450 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=05.0752 diff={max=07.3914, min=00.0101, mean=01.3652} policy_loss=-12.0640 policy updated! \n",
      "train step 08451 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=03.5837 diff={max=08.2112, min=00.0217, mean=01.0380} policy_loss=-9.6372 policy updated! \n",
      "train step 08452 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0416 diff={max=07.2848, min=00.0016, mean=00.8437} policy_loss=-10.5578 policy updated! \n",
      "train step 08453 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5331 diff={max=05.2531, min=00.0096, mean=01.0525} policy_loss=-7.7741 policy updated! \n",
      "train step 08454 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6621 diff={max=07.8152, min=00.0407, mean=01.4331} policy_loss=-8.9831 policy updated! \n",
      "train step 08455 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.6887 diff={max=04.4618, min=00.0238, mean=00.8880} policy_loss=-10.3733 policy updated! \n",
      "train step 08456 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5231 diff={max=06.6188, min=00.0860, mean=01.1886} policy_loss=-12.3615 policy updated! \n",
      "train step 08457 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7515 diff={max=09.3473, min=00.0468, mean=01.0978} policy_loss=-9.1713 policy updated! \n",
      "train step 08458 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=03.8179 diff={max=09.0247, min=00.0312, mean=01.0950} policy_loss=-9.5279 policy updated! \n",
      "train step 08459 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8052 diff={max=10.2977, min=00.0090, mean=01.2488} policy_loss=-10.2391 policy updated! \n",
      "train step 08460 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.2357 diff={max=03.7478, min=00.0092, mean=01.0439} policy_loss=-9.0259 policy updated! \n",
      "train step 08461 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.2401 diff={max=04.7431, min=00.0305, mean=01.0709} policy_loss=-11.0670 policy updated! \n",
      "train step 08462 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9069 diff={max=07.8906, min=00.0085, mean=01.2961} policy_loss=-10.6823 policy updated! \n",
      "train step 08463 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1233 diff={max=09.8991, min=00.0046, mean=01.1818} policy_loss=-8.8066 policy updated! \n",
      "train step 08464 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8807 diff={max=09.1626, min=00.0132, mean=01.2270} policy_loss=-11.9190 policy updated! \n",
      "train step 08465 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.1812 diff={max=07.0412, min=00.0605, mean=01.2657} policy_loss=-10.6218 policy updated! \n",
      "train step 08466 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8780 diff={max=09.4433, min=00.0236, mean=01.3599} policy_loss=-13.3942 policy updated! \n",
      "train step 08467 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7885 diff={max=09.0958, min=00.0132, mean=01.1684} policy_loss=-12.0310 policy updated! \n",
      "train step 08468 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9904 diff={max=03.9678, min=00.0072, mean=00.9545} policy_loss=-11.4303 policy updated! \n",
      "train step 08469 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=01.6421 diff={max=04.1479, min=00.0007, mean=00.8917} policy_loss=-10.2724 policy updated! \n",
      "train step 08470 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.4361 diff={max=07.6088, min=00.0260, mean=01.0087} policy_loss=-10.6773 policy updated! \n",
      "train step 08471 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4661 diff={max=06.6141, min=00.0108, mean=00.9586} policy_loss=-10.1339 policy updated! \n",
      "train step 08472 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8399 diff={max=08.5725, min=00.0305, mean=01.1365} policy_loss=-9.2417 policy updated! \n",
      "train step 08473 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0869 diff={max=11.9445, min=00.0495, mean=01.1431} policy_loss=-9.5042 policy updated! \n",
      "train step 08474 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.7144 diff={max=08.0187, min=00.0161, mean=01.3492} policy_loss=-10.0661 policy updated! \n",
      "train step 08475 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=07.3859 diff={max=09.1496, min=00.0034, mean=01.5040} policy_loss=-12.1770 policy updated! \n",
      "train step 08476 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6718 diff={max=06.8346, min=00.0068, mean=01.4662} policy_loss=-11.3880 policy updated! \n",
      "train step 08477 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4402 diff={max=08.3406, min=00.0055, mean=01.2574} policy_loss=-11.8234 policy updated! \n",
      "train step 08478 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.4555 diff={max=07.9116, min=00.0007, mean=01.1535} policy_loss=-8.8641 policy updated! \n",
      "train step 08479 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9888 diff={max=07.2680, min=00.0114, mean=01.2924} policy_loss=-9.6695 policy updated! \n",
      "train step 08480 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.9357 diff={max=08.1750, min=00.0108, mean=01.0356} policy_loss=-10.0596 policy updated! \n",
      "train step 08481 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.5864 diff={max=08.6615, min=00.0163, mean=01.5429} policy_loss=-11.4005 policy updated! \n",
      "train step 08482 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0407 diff={max=05.6139, min=00.0147, mean=01.0282} policy_loss=-10.0098 policy updated! \n",
      "train step 08483 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9649 diff={max=09.2683, min=00.0438, mean=01.3261} policy_loss=-9.8024 policy updated! \n",
      "train step 08484 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3390 diff={max=05.5045, min=00.0253, mean=01.0013} policy_loss=-10.2471 policy updated! \n",
      "train step 08485 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.2950 diff={max=07.6407, min=00.0003, mean=01.0186} policy_loss=-9.6880 policy updated! \n",
      "train step 08486 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8776 diff={max=07.4338, min=00.0112, mean=00.9344} policy_loss=-9.4651 policy updated! \n",
      "train step 08487 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6566 diff={max=08.7970, min=00.0101, mean=01.2471} policy_loss=-9.7088 policy updated! \n",
      "train step 08488 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3920 diff={max=04.0000, min=00.0234, mean=01.1342} policy_loss=-11.7368 policy updated! \n",
      "train step 08489 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=04.5271 diff={max=06.3266, min=00.0782, mean=01.4450} policy_loss=-10.5781 policy updated! \n",
      "train step 08490 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.5192 diff={max=05.7609, min=00.0003, mean=00.9760} policy_loss=-8.9124 policy updated! \n",
      "train step 08491 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1352 diff={max=10.4358, min=00.0110, mean=01.2798} policy_loss=-13.7722 policy updated! \n",
      "train step 08492 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8878 diff={max=09.1449, min=00.0387, mean=01.1738} policy_loss=-9.5625 policy updated! \n",
      "train step 08493 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.5505 diff={max=13.0948, min=00.0057, mean=01.6358} policy_loss=-10.8727 policy updated! \n",
      "train step 08494 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1554 diff={max=07.3671, min=00.0251, mean=00.8619} policy_loss=-9.4727 policy updated! \n",
      "train step 08495 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.8476 diff={max=08.3614, min=00.0020, mean=01.1542} policy_loss=-11.3345 policy updated! \n",
      "train step 08496 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9413 diff={max=11.4230, min=00.0122, mean=01.4274} policy_loss=-9.4900 policy updated! \n",
      "train step 08497 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7552 diff={max=07.2636, min=00.0173, mean=01.8447} policy_loss=-12.9861 policy updated! \n",
      "train step 08498 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3787 diff={max=09.0804, min=00.0037, mean=01.6223} policy_loss=-11.0202 policy updated! \n",
      "train step 08499 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=01.9168 diff={max=04.4366, min=00.0016, mean=00.9211} policy_loss=-8.6014 policy updated! \n",
      "train step 08500 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.8320 diff={max=09.1547, min=00.0147, mean=01.2019} policy_loss=-9.5818 policy updated! \n",
      "train step 08501 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=02.9193 diff={max=06.2523, min=00.0017, mean=00.9326} policy_loss=-9.7546 policy updated! \n",
      "train step 08502 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9828 diff={max=09.6425, min=00.0213, mean=01.1942} policy_loss=-9.5188 policy updated! \n",
      "train step 08503 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.3480 diff={max=12.6844, min=00.0084, mean=01.2395} policy_loss=-10.5024 policy updated! \n",
      "train step 08504 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.9988 diff={max=09.0868, min=00.0008, mean=01.7308} policy_loss=-9.2548 policy updated! \n",
      "train step 08505 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=06.8920 diff={max=08.4115, min=00.0075, mean=01.6250} policy_loss=-11.5555 policy updated! \n",
      "train step 08506 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3334 diff={max=09.2390, min=00.0209, mean=01.2288} policy_loss=-8.2214 policy updated! \n",
      "train step 08507 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7445 diff={max=13.6109, min=00.0131, mean=01.3449} policy_loss=-11.3854 policy updated! \n",
      "train step 08508 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=02.9406 diff={max=07.7104, min=00.0412, mean=01.0375} policy_loss=-10.1624 policy updated! \n",
      "train step 08509 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.3868 diff={max=19.5196, min=00.0095, mean=01.6625} policy_loss=-11.6616 policy updated! \n",
      "train step 08510 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=05.2855 diff={max=08.4951, min=00.0571, mean=01.3550} policy_loss=-11.0700 policy updated! \n",
      "train step 08511 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9112 diff={max=05.8549, min=00.0336, mean=01.0411} policy_loss=-9.8655 policy updated! \n",
      "train step 08512 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.4671 diff={max=14.1891, min=00.0270, mean=01.5340} policy_loss=-10.8468 policy updated! \n",
      "train step 08513 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.4363 diff={max=11.6960, min=00.0019, mean=01.4377} policy_loss=-11.3267 policy updated! \n",
      "train step 08514 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=02.6314 diff={max=05.2839, min=00.0088, mean=01.0811} policy_loss=-10.1418 policy updated! \n",
      "train step 08515 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.4221 diff={max=08.8705, min=00.0010, mean=01.2187} policy_loss=-8.1600 policy updated! \n",
      "train step 08516 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5350 diff={max=07.5192, min=00.0146, mean=01.1004} policy_loss=-9.5100 policy updated! \n",
      "train step 08517 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2583 diff={max=09.8798, min=00.0008, mean=01.1273} policy_loss=-11.6948 policy updated! \n",
      "train step 08518 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1695 diff={max=08.1761, min=00.0157, mean=01.0021} policy_loss=-9.9773 policy updated! \n",
      "train step 08519 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=03.3292 diff={max=06.4248, min=00.0009, mean=01.1335} policy_loss=-8.6761 policy updated! \n",
      "train step 08520 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=06.6312 diff={max=14.8048, min=00.0397, mean=01.2040} policy_loss=-9.8164 policy updated! \n",
      "train step 08521 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=03.0105 diff={max=05.0251, min=00.0099, mean=01.1960} policy_loss=-8.4813 policy updated! \n",
      "train step 08522 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0663 diff={max=09.3340, min=00.0094, mean=01.1885} policy_loss=-10.0017 policy updated! \n",
      "train step 08523 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8720 diff={max=08.5653, min=00.0139, mean=01.2579} policy_loss=-10.4276 policy updated! \n",
      "train step 08524 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2073 diff={max=03.6284, min=00.0081, mean=01.1112} policy_loss=-10.6878 policy updated! \n",
      "train step 08525 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.0760 diff={max=09.6540, min=00.0386, mean=01.1757} policy_loss=-10.8971 policy updated! \n",
      "train step 08526 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0454 diff={max=06.8314, min=00.0052, mean=01.4098} policy_loss=-11.4758 policy updated! \n",
      "train step 08527 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5416 diff={max=11.1324, min=00.0019, mean=01.4308} policy_loss=-10.1447 policy updated! \n",
      "train step 08528 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3295 diff={max=13.1824, min=00.0037, mean=01.3572} policy_loss=-8.8764 policy updated! \n",
      "train step 08529 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5040 diff={max=10.0124, min=00.0298, mean=01.2657} policy_loss=-9.7516 policy updated! \n",
      "train step 08530 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.9974 diff={max=07.5557, min=00.0264, mean=01.5792} policy_loss=-8.3893 policy updated! \n",
      "train step 08531 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=03.3505 diff={max=07.0208, min=00.0301, mean=01.1957} policy_loss=-7.0912 policy updated! \n",
      "train step 08532 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4718 diff={max=05.6171, min=00.1301, mean=01.5097} policy_loss=-10.3585 policy updated! \n",
      "train step 08533 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3594 diff={max=08.7429, min=00.0220, mean=01.3104} policy_loss=-11.3910 policy updated! \n",
      "train step 08534 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=04.1931 diff={max=09.2821, min=00.0198, mean=01.2716} policy_loss=-9.1277 policy updated! \n",
      "train step 08535 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.8896 diff={max=06.9434, min=00.0378, mean=01.0238} policy_loss=-9.2055 policy updated! \n",
      "train step 08536 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0878 diff={max=04.2092, min=00.0275, mean=01.0289} policy_loss=-11.9373 policy updated! \n",
      "train step 08537 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8567 diff={max=12.0561, min=00.0412, mean=01.5810} policy_loss=-11.8855 policy updated! \n",
      "train step 08538 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=07.7965 diff={max=09.3046, min=00.0012, mean=01.7669} policy_loss=-11.3044 policy updated! \n",
      "train step 08539 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=03.3782 diff={max=07.4260, min=00.0406, mean=01.1803} policy_loss=-10.0511 policy updated! \n",
      "train step 08540 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.8017 diff={max=05.2374, min=00.0196, mean=01.0995} policy_loss=-11.2061 policy updated! \n",
      "train step 08541 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4268 diff={max=05.4973, min=00.0279, mean=01.2399} policy_loss=-12.6345 policy updated! \n",
      "train step 08542 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8097 diff={max=04.9248, min=00.0337, mean=01.1494} policy_loss=-11.1289 policy updated! \n",
      "train step 08543 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2615 diff={max=05.7938, min=00.0055, mean=01.0563} policy_loss=-10.0384 policy updated! \n",
      "train step 08544 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=04.5741 diff={max=07.9101, min=00.0590, mean=01.3707} policy_loss=-9.6513 policy updated! \n",
      "train step 08545 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=06.4072 diff={max=11.0165, min=00.0115, mean=01.5798} policy_loss=-9.9079 policy updated! \n",
      "train step 08546 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=06.7596 diff={max=11.9012, min=00.0007, mean=01.6330} policy_loss=-10.3552 policy updated! \n",
      "train step 08547 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.1264 diff={max=12.7932, min=00.0333, mean=01.6839} policy_loss=-11.3614 policy updated! \n",
      "train step 08548 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6462 diff={max=11.0883, min=00.0206, mean=01.4555} policy_loss=-12.1730 policy updated! \n",
      "train step 08549 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.7079 diff={max=15.4827, min=00.0067, mean=01.9514} policy_loss=-11.9357 policy updated! \n",
      "train step 08550 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=06.1936 diff={max=10.4677, min=00.0646, mean=01.5119} policy_loss=-12.2070 policy updated! \n",
      "train step 08551 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3460 diff={max=08.2724, min=00.0071, mean=01.5771} policy_loss=-12.7313 policy updated! \n",
      "train step 08552 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2606 diff={max=10.5122, min=00.0015, mean=01.5414} policy_loss=-11.6072 policy updated! \n",
      "train step 08553 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.6976 diff={max=15.0207, min=00.0308, mean=01.7044} policy_loss=-11.2612 policy updated! \n",
      "train step 08554 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=04.7521 diff={max=08.4086, min=00.0178, mean=01.4378} policy_loss=-11.5404 policy updated! \n",
      "train step 08555 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.8848 diff={max=06.1259, min=00.0081, mean=01.1738} policy_loss=-10.7991 policy updated! \n",
      "train step 08556 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8855 diff={max=06.5337, min=00.0090, mean=01.5437} policy_loss=-9.9706 policy updated! \n",
      "train step 08557 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4789 diff={max=06.5907, min=00.0003, mean=01.4545} policy_loss=-11.1892 policy updated! \n",
      "train step 08558 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4788 diff={max=08.9088, min=00.0073, mean=01.5243} policy_loss=-10.8707 policy updated! \n",
      "train step 08559 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=04.1053 diff={max=07.2300, min=00.0126, mean=01.4466} policy_loss=-9.0177 policy updated! \n",
      "train step 08560 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.0024 diff={max=03.7417, min=00.0307, mean=01.0759} policy_loss=-10.6379 policy updated! \n",
      "train step 08561 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=07.0667 diff={max=09.7701, min=00.0113, mean=01.4019} policy_loss=-10.7569 policy updated! \n",
      "train step 08562 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8155 diff={max=05.0825, min=00.0226, mean=01.0560} policy_loss=-10.4753 policy updated! \n",
      "train step 08563 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=07.7246 diff={max=10.8593, min=00.0509, mean=01.7314} policy_loss=-10.7851 policy updated! \n",
      "train step 08564 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7417 diff={max=04.9657, min=00.0549, mean=01.0749} policy_loss=-11.1562 policy updated! \n",
      "train step 08565 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=06.2576 diff={max=07.5705, min=00.0430, mean=01.6457} policy_loss=-11.3543 policy updated! \n",
      "train step 08566 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.2359 diff={max=15.6912, min=00.0222, mean=01.8382} policy_loss=-11.2226 policy updated! \n",
      "train step 08567 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.0513 diff={max=11.6543, min=00.0044, mean=02.0127} policy_loss=-10.4205 policy updated! \n",
      "train step 08568 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5576 diff={max=07.4506, min=00.0312, mean=01.2583} policy_loss=-10.5659 policy updated! \n",
      "train step 08569 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=05.8398 diff={max=09.0078, min=00.0132, mean=01.4890} policy_loss=-11.4917 policy updated! \n",
      "train step 08570 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.6602 diff={max=04.7672, min=00.0003, mean=01.1298} policy_loss=-9.0693 policy updated! \n",
      "train step 08571 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0789 diff={max=09.0624, min=00.0084, mean=01.7260} policy_loss=-11.0055 policy updated! \n",
      "train step 08572 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.7889 diff={max=07.5191, min=00.0083, mean=01.9050} policy_loss=-12.7505 policy updated! \n",
      "train step 08573 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.5236 diff={max=12.6539, min=00.0050, mean=01.4399} policy_loss=-10.2499 policy updated! \n",
      "train step 08574 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3519 diff={max=10.1641, min=00.0110, mean=01.3470} policy_loss=-11.8520 policy updated! \n",
      "train step 08575 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.9557 diff={max=05.5895, min=00.0152, mean=01.2929} policy_loss=-10.7867 policy updated! \n",
      "train step 08576 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3085 diff={max=07.8721, min=00.0047, mean=01.6411} policy_loss=-10.7810 policy updated! \n",
      "train step 08577 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9773 diff={max=04.7080, min=00.0164, mean=00.9890} policy_loss=-9.1310 policy updated! \n",
      "train step 08578 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=07.6845 diff={max=12.4069, min=00.0037, mean=01.6295} policy_loss=-11.4378 policy updated! \n",
      "train step 08579 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=02.7154 diff={max=04.4127, min=00.0183, mean=01.2483} policy_loss=-10.1729 policy updated! \n",
      "train step 08580 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.4477 diff={max=05.3768, min=00.0164, mean=01.2439} policy_loss=-13.3822 policy updated! \n",
      "train step 08581 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6025 diff={max=09.1550, min=00.0027, mean=01.5979} policy_loss=-11.1900 policy updated! \n",
      "train step 08582 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1032 diff={max=05.3788, min=00.0200, mean=01.1069} policy_loss=-9.6790 policy updated! \n",
      "train step 08583 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2073 diff={max=10.8376, min=00.0182, mean=01.3897} policy_loss=-9.8456 policy updated! \n",
      "train step 08584 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1298 diff={max=07.2779, min=00.0253, mean=01.3437} policy_loss=-11.1620 policy updated! \n",
      "train step 08585 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.4936 diff={max=07.6991, min=00.0054, mean=01.3373} policy_loss=-11.4473 policy updated! \n",
      "train step 08586 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6229 diff={max=06.7666, min=00.0181, mean=00.9568} policy_loss=-10.4956 policy updated! \n",
      "train step 08587 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2683 diff={max=09.0352, min=00.0165, mean=01.0026} policy_loss=-9.8082 policy updated! \n",
      "train step 08588 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.2444 diff={max=14.3798, min=00.0998, mean=01.5181} policy_loss=-10.3584 policy updated! \n",
      "train step 08589 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8898 diff={max=08.7156, min=00.0408, mean=01.3770} policy_loss=-9.0027 policy updated! \n",
      "train step 08590 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=07.9310 diff={max=12.6708, min=00.0361, mean=01.4179} policy_loss=-9.6071 policy updated! \n",
      "train step 08591 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=03.0178 diff={max=05.6253, min=00.0037, mean=01.0797} policy_loss=-12.2042 policy updated! \n",
      "train step 08592 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.6524 diff={max=15.2452, min=00.0152, mean=01.3701} policy_loss=-9.5486 policy updated! \n",
      "train step 08593 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4737 diff={max=05.6905, min=00.0028, mean=01.1862} policy_loss=-10.6308 policy updated! \n",
      "train step 08594 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9874 diff={max=05.7505, min=00.0209, mean=01.3741} policy_loss=-11.0245 policy updated! \n",
      "train step 08595 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=05.0742 diff={max=06.4072, min=00.0269, mean=01.5496} policy_loss=-11.9601 policy updated! \n",
      "train step 08596 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7987 diff={max=08.5664, min=00.0176, mean=00.9983} policy_loss=-9.0847 policy updated! \n",
      "train step 08597 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2405 diff={max=04.2490, min=00.0508, mean=01.1739} policy_loss=-11.6589 policy updated! \n",
      "train step 08598 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=03.0706 diff={max=05.6780, min=00.0388, mean=01.0452} policy_loss=-10.2446 policy updated! \n",
      "train step 08599 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.5696 diff={max=08.7622, min=00.0098, mean=01.8084} policy_loss=-11.6360 policy updated! \n",
      "train step 08600 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.5937 diff={max=05.2101, min=00.0087, mean=01.0078} policy_loss=-8.9294 policy updated! \n",
      "train step 08601 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2124 diff={max=09.9375, min=00.0242, mean=01.5155} policy_loss=-12.1987 policy updated! \n",
      "train step 08602 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4149 diff={max=05.4903, min=00.0117, mean=01.2751} policy_loss=-11.4834 policy updated! \n",
      "train step 08603 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=03.2381 diff={max=06.6401, min=00.0358, mean=01.2573} policy_loss=-10.5756 policy updated! \n",
      "train step 08604 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6874 diff={max=08.9815, min=00.0391, mean=01.3640} policy_loss=-11.3400 policy updated! \n",
      "train step 08605 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=06.2963 diff={max=11.5665, min=00.0217, mean=01.4930} policy_loss=-11.4150 policy updated! \n",
      "train step 08606 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=04.7971 diff={max=10.0269, min=00.0073, mean=01.3741} policy_loss=-9.3205 policy updated! \n",
      "train step 08607 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8610 diff={max=08.9784, min=00.0174, mean=01.3188} policy_loss=-9.8515 policy updated! \n",
      "train step 08608 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2988 diff={max=09.2883, min=00.0077, mean=01.3070} policy_loss=-12.0335 policy updated! \n",
      "train step 08609 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8845 diff={max=06.2907, min=00.0683, mean=01.0256} policy_loss=-10.4250 policy updated! \n",
      "train step 08610 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.4758 diff={max=09.3229, min=00.0672, mean=01.2133} policy_loss=-12.4464 policy updated! \n",
      "train step 08611 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8103 diff={max=05.1960, min=00.0009, mean=01.3264} policy_loss=-11.3901 policy updated! \n",
      "train step 08612 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0562 diff={max=08.1209, min=00.0312, mean=01.5424} policy_loss=-13.5563 policy updated! \n",
      "train step 08613 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2121 diff={max=07.8746, min=00.0448, mean=01.3683} policy_loss=-11.1523 policy updated! \n",
      "train step 08614 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=02.9744 diff={max=04.6130, min=00.0208, mean=01.2261} policy_loss=-11.8763 policy updated! \n",
      "train step 08615 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.0277 diff={max=08.1788, min=00.0233, mean=00.9004} policy_loss=-9.8357 policy updated! \n",
      "train step 08616 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7760 diff={max=06.9652, min=00.0254, mean=01.2856} policy_loss=-11.0121 policy updated! \n",
      "train step 08617 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5029 diff={max=07.6727, min=00.0468, mean=01.3701} policy_loss=-9.9658 policy updated! \n",
      "train step 08618 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=01.5758 diff={max=03.3373, min=00.0075, mean=00.9158} policy_loss=-10.2160 policy updated! \n",
      "train step 08619 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.9263 diff={max=14.4286, min=00.0138, mean=01.6514} policy_loss=-9.8565 policy updated! \n",
      "train step 08620 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.2930 diff={max=06.2397, min=00.0122, mean=01.0265} policy_loss=-9.7809 policy updated! \n",
      "train step 08621 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=04.3850 diff={max=07.8893, min=00.0159, mean=01.3360} policy_loss=-11.6950 policy updated! \n",
      "train step 08622 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0500 diff={max=08.4777, min=00.0263, mean=01.4172} policy_loss=-8.4367 policy updated! \n",
      "train step 08623 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=01.9270 diff={max=05.3731, min=00.0025, mean=00.9028} policy_loss=-11.1687 policy updated! \n",
      "train step 08624 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8863 diff={max=07.5814, min=00.0004, mean=01.0613} policy_loss=-11.8492 policy updated! \n",
      "train step 08625 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.7726 diff={max=09.0086, min=00.0140, mean=01.0895} policy_loss=-10.3805 policy updated! \n",
      "train step 08626 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=03.2659 diff={max=06.1618, min=00.0107, mean=01.0736} policy_loss=-11.7981 policy updated! \n",
      "train step 08627 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1422 diff={max=10.9525, min=00.0047, mean=01.1621} policy_loss=-10.5495 policy updated! \n",
      "train step 08628 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1903 diff={max=10.5657, min=00.0356, mean=01.2723} policy_loss=-9.5658 policy updated! \n",
      "train step 08629 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0640 diff={max=05.2382, min=00.0668, mean=00.9743} policy_loss=-12.3100 policy updated! \n",
      "train step 08630 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.4320 diff={max=08.6314, min=00.0376, mean=01.3179} policy_loss=-10.9678 policy updated! \n",
      "train step 08631 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=04.7917 diff={max=08.4768, min=00.0604, mean=01.3654} policy_loss=-9.4257 policy updated! \n",
      "train step 08632 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=05.6847 diff={max=10.4755, min=00.0199, mean=01.2765} policy_loss=-11.2697 policy updated! \n",
      "train step 08633 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9785 diff={max=05.5821, min=00.0090, mean=01.1036} policy_loss=-8.7102 policy updated! \n",
      "train step 08634 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6493 diff={max=08.6466, min=00.0113, mean=01.6777} policy_loss=-9.5520 policy updated! \n",
      "train step 08635 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.7631 diff={max=06.5651, min=00.0326, mean=01.2877} policy_loss=-10.8945 policy updated! \n",
      "train step 08636 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7128 diff={max=07.7709, min=00.0455, mean=01.0477} policy_loss=-9.0307 policy updated! \n",
      "train step 08637 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3102 diff={max=10.9280, min=00.0107, mean=01.1832} policy_loss=-10.2673 policy updated! \n",
      "train step 08638 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0936 diff={max=05.8963, min=00.0154, mean=01.1357} policy_loss=-11.7744 policy updated! \n",
      "train step 08639 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8527 diff={max=05.9618, min=00.0426, mean=01.1163} policy_loss=-11.1983 policy updated! \n",
      "train step 08640 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.1078 diff={max=07.7722, min=00.0236, mean=01.1829} policy_loss=-10.5343 policy updated! \n",
      "train step 08641 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=12.1472 diff={max=14.6837, min=00.0330, mean=02.0512} policy_loss=-9.7436 policy updated! \n",
      "train step 08642 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9496 diff={max=04.2261, min=00.0201, mean=01.0058} policy_loss=-8.9048 policy updated! \n",
      "train step 08643 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.6848 diff={max=08.8357, min=00.0147, mean=01.8126} policy_loss=-10.9620 policy updated! \n",
      "train step 08644 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2629 diff={max=07.1821, min=00.0035, mean=01.3642} policy_loss=-11.6229 policy updated! \n",
      "train step 08645 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=07.1206 diff={max=10.2877, min=00.0394, mean=01.6443} policy_loss=-11.4111 policy updated! \n",
      "train step 08646 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5185 diff={max=09.8131, min=00.0020, mean=01.4118} policy_loss=-11.1500 policy updated! \n",
      "train step 08647 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.2032 diff={max=09.7963, min=00.0353, mean=01.5727} policy_loss=-9.5912 policy updated! \n",
      "train step 08648 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8620 diff={max=04.7698, min=00.0065, mean=00.8986} policy_loss=-10.0692 policy updated! \n",
      "train step 08649 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.2185 diff={max=10.6215, min=00.0014, mean=01.8028} policy_loss=-10.3743 policy updated! \n",
      "train step 08650 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=08.2223 diff={max=09.9613, min=00.0260, mean=01.7728} policy_loss=-11.0726 policy updated! \n",
      "train step 08651 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=07.4512 diff={max=09.0364, min=00.0131, mean=01.6710} policy_loss=-8.8364 policy updated! \n",
      "train step 08652 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2132 diff={max=07.8671, min=00.0104, mean=01.5402} policy_loss=-9.1490 policy updated! \n",
      "train step 08653 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=02.9731 diff={max=05.9925, min=00.0058, mean=01.0866} policy_loss=-10.0412 policy updated! \n",
      "train step 08654 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4292 diff={max=08.2542, min=00.0036, mean=01.5549} policy_loss=-11.0756 policy updated! \n",
      "train step 08655 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.5799 diff={max=07.9084, min=00.0075, mean=01.1610} policy_loss=-11.1423 policy updated! \n",
      "train step 08656 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=07.3953 diff={max=11.5341, min=00.0300, mean=01.4255} policy_loss=-11.0419 policy updated! \n",
      "train step 08657 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1362 diff={max=06.6729, min=00.0062, mean=01.3085} policy_loss=-12.6326 policy updated! \n",
      "train step 08658 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=04.5374 diff={max=08.2440, min=00.0052, mean=01.3146} policy_loss=-10.6804 policy updated! \n",
      "train step 08659 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2588 diff={max=08.1732, min=00.0107, mean=01.5620} policy_loss=-10.9131 policy updated! \n",
      "train step 08660 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.5963 diff={max=05.1414, min=00.0276, mean=01.1377} policy_loss=-11.8520 policy updated! \n",
      "train step 08661 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5964 diff={max=10.1566, min=00.0090, mean=01.4905} policy_loss=-11.8778 policy updated! \n",
      "train step 08662 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8978 diff={max=08.3778, min=00.0598, mean=01.0949} policy_loss=-11.1163 policy updated! \n",
      "train step 08663 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8250 diff={max=09.5020, min=00.0264, mean=01.1434} policy_loss=-10.1955 policy updated! \n",
      "train step 08664 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9809 diff={max=11.1758, min=00.0099, mean=01.0184} policy_loss=-9.6647 policy updated! \n",
      "train step 08665 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=07.1346 diff={max=13.2651, min=00.0041, mean=01.4155} policy_loss=-11.8182 policy updated! \n",
      "train step 08666 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=03.0794 diff={max=05.5248, min=00.0550, mean=01.2565} policy_loss=-12.2077 policy updated! \n",
      "train step 08667 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8533 diff={max=11.3446, min=00.0158, mean=01.5572} policy_loss=-10.4618 policy updated! \n",
      "train step 08668 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5746 diff={max=08.3364, min=00.0077, mean=01.2933} policy_loss=-7.8460 policy updated! \n",
      "train step 08669 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=03.7694 diff={max=06.4130, min=00.0165, mean=01.2151} policy_loss=-11.1503 policy updated! \n",
      "train step 08670 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.6979 diff={max=04.1578, min=00.0121, mean=00.9080} policy_loss=-14.4810 policy updated! \n",
      "train step 08671 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1576 diff={max=11.8608, min=00.0233, mean=01.4454} policy_loss=-11.6091 policy updated! \n",
      "train step 08672 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4357 diff={max=04.8906, min=00.0049, mean=01.0250} policy_loss=-11.0477 policy updated! \n",
      "train step 08673 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.3691 diff={max=10.5735, min=00.0005, mean=01.6249} policy_loss=-12.1061 policy updated! \n",
      "train step 08674 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=03.2698 diff={max=06.9925, min=00.0111, mean=01.1306} policy_loss=-10.4793 policy updated! \n",
      "train step 08675 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.7937 diff={max=08.6623, min=00.0710, mean=01.4137} policy_loss=-11.5530 policy updated! \n",
      "train step 08676 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=02.7116 diff={max=06.4238, min=00.0506, mean=01.0382} policy_loss=-10.3977 policy updated! \n",
      "train step 08677 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2671 diff={max=06.5823, min=00.0245, mean=01.0199} policy_loss=-11.7087 policy updated! \n",
      "train step 08678 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.7170 diff={max=08.0375, min=00.0387, mean=01.8017} policy_loss=-10.7517 policy updated! \n",
      "train step 08679 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.3907 diff={max=12.2935, min=00.0052, mean=01.9937} policy_loss=-9.4429 policy updated! \n",
      "train step 08680 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.0910 diff={max=04.9882, min=00.0021, mean=00.9619} policy_loss=-9.2337 policy updated! \n",
      "train step 08681 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8766 diff={max=06.6018, min=00.0418, mean=01.3598} policy_loss=-10.6348 policy updated! \n",
      "train step 08682 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1731 diff={max=10.6472, min=00.0174, mean=01.2773} policy_loss=-10.5284 policy updated! \n",
      "train step 08683 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.6998 diff={max=10.0269, min=00.0441, mean=01.8415} policy_loss=-10.6414 policy updated! \n",
      "train step 08684 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6437 diff={max=10.3840, min=00.0219, mean=01.3881} policy_loss=-11.3428 policy updated! \n",
      "train step 08685 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.4732 diff={max=06.6897, min=00.0102, mean=01.1884} policy_loss=-10.0178 policy updated! \n",
      "train step 08686 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=02.0249 diff={max=04.4003, min=00.0105, mean=01.0152} policy_loss=-11.0895 policy updated! \n",
      "train step 08687 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6162 diff={max=07.0184, min=00.0727, mean=01.3557} policy_loss=-12.6164 policy updated! \n",
      "train step 08688 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=06.0524 diff={max=10.7801, min=00.0618, mean=01.4325} policy_loss=-9.4297 policy updated! \n",
      "train step 08689 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2254 diff={max=10.3728, min=00.0003, mean=01.5149} policy_loss=-11.0761 policy updated! \n",
      "train step 08690 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=05.2287 diff={max=08.7222, min=00.0078, mean=01.2474} policy_loss=-10.1777 policy updated! \n",
      "train step 08691 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6776 diff={max=07.9614, min=00.0226, mean=00.9967} policy_loss=-11.5729 policy updated! \n",
      "train step 08692 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5635 diff={max=04.2658, min=00.0056, mean=01.0956} policy_loss=-11.7113 policy updated! \n",
      "train step 08693 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5142 diff={max=06.6496, min=00.0592, mean=01.4511} policy_loss=-10.4430 policy updated! \n",
      "train step 08694 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=12.3943 diff={max=14.4270, min=00.0196, mean=01.8854} policy_loss=-10.2768 policy updated! \n",
      "train step 08695 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=03.1284 diff={max=06.5948, min=00.0391, mean=01.2346} policy_loss=-8.5241 policy updated! \n",
      "train step 08696 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=04.1380 diff={max=07.0190, min=00.0143, mean=01.4115} policy_loss=-9.8468 policy updated! \n",
      "train step 08697 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9624 diff={max=03.5153, min=00.0092, mean=01.0729} policy_loss=-8.2361 policy updated! \n",
      "train step 08698 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7942 diff={max=08.4404, min=00.0342, mean=01.1360} policy_loss=-11.0838 policy updated! \n",
      "train step 08699 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3520 diff={max=10.7645, min=00.0295, mean=01.1666} policy_loss=-11.5587 policy updated! \n",
      "train step 08700 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.3206 diff={max=05.5141, min=00.0279, mean=01.0914} policy_loss=-10.9282 policy updated! \n",
      "train step 08701 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1588 diff={max=08.2346, min=00.0042, mean=01.5983} policy_loss=-11.6329 policy updated! \n",
      "train step 08702 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1998 diff={max=05.8301, min=00.0368, mean=01.5077} policy_loss=-12.3735 policy updated! \n",
      "train step 08703 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6810 diff={max=05.3145, min=00.0479, mean=01.1273} policy_loss=-10.4574 policy updated! \n",
      "train step 08704 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3154 diff={max=08.9488, min=00.0109, mean=01.3518} policy_loss=-11.9315 policy updated! \n",
      "train step 08705 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.5800 diff={max=07.0483, min=00.0041, mean=01.3915} policy_loss=-11.4524 policy updated! \n",
      "train step 08706 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=01.9139 diff={max=05.5987, min=00.0431, mean=00.9493} policy_loss=-10.3531 policy updated! \n",
      "train step 08707 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3128 diff={max=05.0778, min=00.0057, mean=00.9616} policy_loss=-10.4340 policy updated! \n",
      "train step 08708 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1315 diff={max=08.8407, min=00.0009, mean=01.4284} policy_loss=-12.6680 policy updated! \n",
      "train step 08709 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=03.6616 diff={max=08.8817, min=00.0003, mean=01.1458} policy_loss=-12.9221 policy updated! \n",
      "train step 08710 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.6332 diff={max=07.6229, min=00.0323, mean=01.1643} policy_loss=-12.4320 policy updated! \n",
      "train step 08711 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.5793 diff={max=11.8081, min=00.0086, mean=01.6954} policy_loss=-9.2794 policy updated! \n",
      "train step 08712 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3341 diff={max=08.2479, min=00.0021, mean=01.3001} policy_loss=-10.9867 policy updated! \n",
      "train step 08713 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=05.8398 diff={max=11.5635, min=00.0066, mean=01.3267} policy_loss=-10.5182 policy updated! \n",
      "train step 08714 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=04.6054 diff={max=10.3189, min=00.0054, mean=01.0745} policy_loss=-12.1213 policy updated! \n",
      "train step 08715 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.7942 diff={max=07.0787, min=00.0250, mean=01.1860} policy_loss=-13.6665 policy updated! \n",
      "train step 08716 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8149 diff={max=03.6392, min=00.0318, mean=00.9907} policy_loss=-10.4376 policy updated! \n",
      "train step 08717 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6380 diff={max=06.3117, min=00.0160, mean=01.1335} policy_loss=-10.7912 policy updated! \n",
      "train step 08718 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=04.9467 diff={max=09.4478, min=00.0058, mean=01.4163} policy_loss=-10.7443 policy updated! \n",
      "train step 08719 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=08.0233 diff={max=09.6099, min=00.0251, mean=01.6614} policy_loss=-11.7676 policy updated! \n",
      "train step 08720 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=09.7733 diff={max=11.9924, min=00.0150, mean=01.8297} policy_loss=-12.0561 policy updated! \n",
      "train step 08721 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=06.2462 diff={max=07.5375, min=00.1063, mean=01.5916} policy_loss=-11.3906 policy updated! \n",
      "train step 08722 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.2999 diff={max=14.6130, min=00.0042, mean=01.3569} policy_loss=-10.5442 policy updated! \n",
      "train step 08723 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9453 diff={max=07.8590, min=00.0329, mean=01.2685} policy_loss=-11.4814 policy updated! \n",
      "train step 08724 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=05.8194 diff={max=10.4325, min=00.0233, mean=01.4177} policy_loss=-11.3434 policy updated! \n",
      "train step 08725 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=04.3532 diff={max=08.7258, min=00.0084, mean=01.3144} policy_loss=-10.8891 policy updated! \n",
      "train step 08726 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5194 diff={max=11.0936, min=00.0253, mean=01.3487} policy_loss=-10.3643 policy updated! \n",
      "train step 08727 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1954 diff={max=05.0408, min=00.0023, mean=01.0278} policy_loss=-9.5214 policy updated! \n",
      "train step 08728 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3516 diff={max=08.2804, min=00.0001, mean=01.1532} policy_loss=-10.6478 policy updated! \n",
      "train step 08729 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6481 diff={max=06.1718, min=00.0387, mean=01.1989} policy_loss=-11.4118 policy updated! \n",
      "train step 08730 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.1989 diff={max=07.3685, min=00.0117, mean=01.0999} policy_loss=-10.1377 policy updated! \n",
      "train step 08731 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=04.3614 diff={max=06.9085, min=00.0061, mean=01.3890} policy_loss=-11.4242 policy updated! \n",
      "train step 08732 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4277 diff={max=03.6666, min=00.0336, mean=00.8628} policy_loss=-11.5282 policy updated! \n",
      "train step 08733 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3416 diff={max=07.7732, min=00.0049, mean=01.1758} policy_loss=-10.8710 policy updated! \n",
      "train step 08734 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=04.6275 diff={max=07.1262, min=00.0126, mean=01.4031} policy_loss=-8.5791 policy updated! \n",
      "train step 08735 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.7672 diff={max=05.8280, min=00.0429, mean=01.3909} policy_loss=-9.9857 policy updated! \n",
      "train step 08736 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8663 diff={max=05.0554, min=00.0035, mean=01.2012} policy_loss=-12.9281 policy updated! \n",
      "train step 08737 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0278 diff={max=03.8396, min=00.0319, mean=01.0127} policy_loss=-9.6693 policy updated! \n",
      "train step 08738 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=05.8573 diff={max=13.2776, min=00.0020, mean=01.1876} policy_loss=-9.5554 policy updated! \n",
      "train step 08739 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3729 diff={max=05.1882, min=00.0036, mean=00.9298} policy_loss=-9.5124 policy updated! \n",
      "train step 08740 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=08.4162 diff={max=10.5120, min=00.0016, mean=01.6200} policy_loss=-10.5385 policy updated! \n",
      "train step 08741 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7211 diff={max=05.8851, min=00.0152, mean=00.8304} policy_loss=-9.0495 policy updated! \n",
      "train step 08742 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.2250 diff={max=14.1556, min=00.0184, mean=01.5630} policy_loss=-12.9794 policy updated! \n",
      "train step 08743 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=07.0932 diff={max=10.7884, min=00.0255, mean=01.6133} policy_loss=-12.3057 policy updated! \n",
      "train step 08744 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8208 diff={max=04.2795, min=00.0234, mean=00.9059} policy_loss=-10.9209 policy updated! \n",
      "train step 08745 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.2182 diff={max=06.8900, min=00.0026, mean=01.2502} policy_loss=-11.8097 policy updated! \n",
      "train step 08746 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6796 diff={max=12.9194, min=00.0110, mean=01.1988} policy_loss=-13.7477 policy updated! \n",
      "train step 08747 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7379 diff={max=05.2828, min=00.0314, mean=00.8961} policy_loss=-9.9616 policy updated! \n",
      "train step 08748 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6241 diff={max=08.7831, min=00.0282, mean=01.2732} policy_loss=-11.5955 policy updated! \n",
      "train step 08749 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=03.2562 diff={max=07.5809, min=00.0052, mean=01.0273} policy_loss=-11.0385 policy updated! \n",
      "train step 08750 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=05.1585 diff={max=10.3172, min=00.0707, mean=01.3855} policy_loss=-12.0481 policy updated! \n",
      "train step 08751 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9932 diff={max=10.0539, min=00.0148, mean=01.3165} policy_loss=-11.1938 policy updated! \n",
      "train step 08752 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8116 diff={max=06.1730, min=00.0190, mean=00.8884} policy_loss=-8.1087 policy updated! \n",
      "train step 08753 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3273 diff={max=06.7633, min=00.0052, mean=00.8896} policy_loss=-10.1089 policy updated! \n",
      "train step 08754 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0624 diff={max=07.2497, min=00.0037, mean=01.6189} policy_loss=-11.2655 policy updated! \n",
      "train step 08755 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.6558 diff={max=07.2460, min=00.0049, mean=01.0419} policy_loss=-8.9753 policy updated! \n",
      "train step 08756 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=03.5347 diff={max=07.4969, min=00.0071, mean=01.1888} policy_loss=-11.5174 policy updated! \n",
      "train step 08757 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3783 diff={max=05.9086, min=00.0007, mean=00.9156} policy_loss=-10.6947 policy updated! \n",
      "train step 08758 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=04.8312 diff={max=09.2444, min=00.0056, mean=01.3455} policy_loss=-9.9881 policy updated! \n",
      "train step 08759 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6416 diff={max=05.9363, min=00.0168, mean=01.0901} policy_loss=-12.1864 policy updated! \n",
      "train step 08760 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=08.6136 diff={max=15.7292, min=00.0407, mean=01.4839} policy_loss=-11.3813 policy updated! \n",
      "train step 08761 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7036 diff={max=12.3940, min=00.0645, mean=01.2955} policy_loss=-11.4650 policy updated! \n",
      "train step 08762 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3322 diff={max=06.7104, min=00.0185, mean=01.2084} policy_loss=-11.2058 policy updated! \n",
      "train step 08763 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=06.5481 diff={max=10.1091, min=00.0004, mean=01.4746} policy_loss=-13.7186 policy updated! \n",
      "train step 08764 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5478 diff={max=06.9530, min=00.0041, mean=01.1935} policy_loss=-8.8197 policy updated! \n",
      "train step 08765 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.5915 diff={max=05.0024, min=00.0265, mean=01.1196} policy_loss=-10.5493 policy updated! \n",
      "train step 08766 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7904 diff={max=05.5831, min=00.0308, mean=01.0743} policy_loss=-10.1467 policy updated! \n",
      "train step 08767 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4202 diff={max=09.0354, min=00.0167, mean=01.2827} policy_loss=-11.4999 policy updated! \n",
      "train step 08768 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=03.2388 diff={max=06.8050, min=00.0009, mean=01.0740} policy_loss=-12.8464 policy updated! \n",
      "train step 08769 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=08.2802 diff={max=09.7100, min=00.0263, mean=01.7954} policy_loss=-9.5038 policy updated! \n",
      "train step 08770 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=09.8701 diff={max=10.5167, min=00.0202, mean=01.7090} policy_loss=-12.1409 policy updated! \n",
      "train step 08771 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=08.2286 diff={max=15.1323, min=00.0052, mean=01.4370} policy_loss=-9.6566 policy updated! \n",
      "train step 08772 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1311 diff={max=09.3953, min=00.0467, mean=01.0691} policy_loss=-11.4934 policy updated! \n",
      "train step 08773 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=04.3644 diff={max=07.4706, min=00.0019, mean=01.2575} policy_loss=-11.3557 policy updated! \n",
      "train step 08774 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=01.7648 diff={max=05.8749, min=00.0005, mean=00.8815} policy_loss=-11.1440 policy updated! \n",
      "train step 08775 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.6175 diff={max=03.9081, min=00.0222, mean=00.8751} policy_loss=-11.0554 policy updated! \n",
      "train step 08776 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7808 diff={max=11.0115, min=00.0161, mean=01.3576} policy_loss=-12.0793 policy updated! \n",
      "train step 08777 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1797 diff={max=08.5687, min=00.0788, mean=01.0519} policy_loss=-9.1799 policy updated! \n",
      "train step 08778 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.3425 diff={max=10.0581, min=00.0254, mean=01.6767} policy_loss=-11.8436 policy updated! \n",
      "train step 08779 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5515 diff={max=07.0855, min=00.0082, mean=01.4295} policy_loss=-11.1228 policy updated! \n",
      "train step 08780 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.7923 diff={max=03.6326, min=00.0223, mean=00.9836} policy_loss=-10.6116 policy updated! \n",
      "train step 08781 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=05.0265 diff={max=08.7541, min=00.0270, mean=01.2891} policy_loss=-10.2444 policy updated! \n",
      "train step 08782 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.9311 diff={max=16.9169, min=00.0126, mean=01.5728} policy_loss=-11.7223 policy updated! \n",
      "train step 08783 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5334 diff={max=09.9336, min=00.0010, mean=01.5299} policy_loss=-16.3347 policy updated! \n",
      "train step 08784 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=03.5644 diff={max=06.5390, min=00.0752, mean=01.2087} policy_loss=-11.0464 policy updated! \n",
      "train step 08785 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=05.3342 diff={max=11.5202, min=00.0006, mean=01.3919} policy_loss=-13.6943 policy updated! \n",
      "train step 08786 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6679 diff={max=09.5251, min=00.0223, mean=01.2377} policy_loss=-10.5614 policy updated! \n",
      "train step 08787 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0250 diff={max=07.5529, min=00.0289, mean=01.4715} policy_loss=-11.0064 policy updated! \n",
      "train step 08788 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7629 diff={max=07.5084, min=00.0032, mean=00.9800} policy_loss=-10.9852 policy updated! \n",
      "train step 08789 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=02.3450 diff={max=05.6549, min=00.0310, mean=01.0053} policy_loss=-11.6710 policy updated! \n",
      "train step 08790 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.2254 diff={max=05.1255, min=00.0122, mean=01.0324} policy_loss=-10.1271 policy updated! \n",
      "train step 08791 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=06.8275 diff={max=14.3158, min=00.0286, mean=01.3759} policy_loss=-10.2385 policy updated! \n",
      "train step 08792 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.0492 diff={max=17.7689, min=00.0077, mean=02.0708} policy_loss=-12.6828 policy updated! \n",
      "train step 08793 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.2997 diff={max=13.7686, min=00.0046, mean=01.7753} policy_loss=-12.1563 policy updated! \n",
      "train step 08794 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.7958 diff={max=14.7246, min=00.0175, mean=01.5438} policy_loss=-13.3375 policy updated! \n",
      "train step 08795 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.4730 diff={max=08.7519, min=00.0696, mean=01.0823} policy_loss=-10.3926 policy updated! \n",
      "train step 08796 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8218 diff={max=05.8512, min=00.0024, mean=01.3086} policy_loss=-12.6010 policy updated! \n",
      "train step 08797 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=06.8965 diff={max=13.5195, min=00.0225, mean=01.3448} policy_loss=-9.5815 policy updated! \n",
      "train step 08798 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8478 diff={max=06.6106, min=00.0266, mean=01.2607} policy_loss=-10.6064 policy updated! \n",
      "train step 08799 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=02.6905 diff={max=07.4858, min=00.0077, mean=00.9958} policy_loss=-12.4051 policy updated! \n",
      "train step 08800 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.5919 diff={max=10.2533, min=00.0179, mean=01.1373} policy_loss=-10.1694 policy updated! \n",
      "train step 08801 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1823 diff={max=07.5915, min=00.0285, mean=01.1153} policy_loss=-11.7422 policy updated! \n",
      "train step 08802 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.8548 diff={max=17.3489, min=00.0354, mean=01.5304} policy_loss=-12.1261 policy updated! \n",
      "train step 08803 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6181 diff={max=07.4342, min=00.0048, mean=01.1619} policy_loss=-11.9257 policy updated! \n",
      "train step 08804 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0320 diff={max=09.7585, min=00.0184, mean=01.5321} policy_loss=-12.5459 policy updated! \n",
      "train step 08805 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.1647 diff={max=04.6750, min=00.0097, mean=01.0455} policy_loss=-9.7315 policy updated! \n",
      "train step 08806 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4569 diff={max=08.5108, min=00.0227, mean=01.1935} policy_loss=-10.0390 policy updated! \n",
      "train step 08807 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7387 diff={max=07.3377, min=00.0115, mean=01.5261} policy_loss=-10.6460 policy updated! \n",
      "train step 08808 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2356 diff={max=07.3001, min=00.0401, mean=01.1278} policy_loss=-10.1849 policy updated! \n",
      "train step 08809 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3017 diff={max=05.7047, min=00.0155, mean=01.4631} policy_loss=-12.0113 policy updated! \n",
      "train step 08810 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.3807 diff={max=06.5916, min=00.0057, mean=01.3043} policy_loss=-10.5622 policy updated! \n",
      "train step 08811 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8081 diff={max=09.4286, min=00.0008, mean=01.1666} policy_loss=-9.9622 policy updated! \n",
      "train step 08812 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7496 diff={max=07.7608, min=00.0069, mean=01.4058} policy_loss=-10.7655 policy updated! \n",
      "train step 08813 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3067 diff={max=07.8815, min=00.0021, mean=01.2776} policy_loss=-10.3683 policy updated! \n",
      "train step 08814 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=03.4166 diff={max=05.9213, min=00.0022, mean=01.1788} policy_loss=-10.9683 policy updated! \n",
      "train step 08815 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=05.1020 diff={max=10.2834, min=00.0415, mean=01.4453} policy_loss=-13.0217 policy updated! \n",
      "train step 08816 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4150 diff={max=13.8546, min=00.0221, mean=01.1198} policy_loss=-9.8553 policy updated! \n",
      "train step 08817 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9344 diff={max=08.1495, min=00.0125, mean=01.2182} policy_loss=-10.7710 policy updated! \n",
      "train step 08818 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=03.5466 diff={max=07.1102, min=00.0073, mean=01.1476} policy_loss=-11.6637 policy updated! \n",
      "train step 08819 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.6206 diff={max=15.2724, min=00.0177, mean=01.5413} policy_loss=-11.3069 policy updated! \n",
      "train step 08820 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=08.1986 diff={max=17.9518, min=00.0009, mean=01.2210} policy_loss=-9.7849 policy updated! \n",
      "train step 08821 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2701 diff={max=10.2215, min=00.0344, mean=01.3795} policy_loss=-9.5024 policy updated! \n",
      "train step 08822 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.5873 diff={max=14.3183, min=00.0157, mean=01.6952} policy_loss=-11.4243 policy updated! \n",
      "train step 08823 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3482 diff={max=08.2300, min=00.0386, mean=01.0406} policy_loss=-9.6940 policy updated! \n",
      "train step 08824 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4188 diff={max=06.0843, min=00.0228, mean=01.3448} policy_loss=-10.1090 policy updated! \n",
      "train step 08825 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.1090 diff={max=07.7508, min=00.0039, mean=01.0127} policy_loss=-10.0801 policy updated! \n",
      "train step 08826 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.6203 diff={max=14.5267, min=00.0052, mean=01.7670} policy_loss=-10.6182 policy updated! \n",
      "train step 08827 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7996 diff={max=08.7152, min=00.0305, mean=00.9716} policy_loss=-13.4914 policy updated! \n",
      "train step 08828 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6973 diff={max=04.4234, min=00.0077, mean=00.8578} policy_loss=-10.9613 policy updated! \n",
      "train step 08829 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4564 diff={max=06.1328, min=00.0575, mean=01.4527} policy_loss=-12.7147 policy updated! \n",
      "train step 08830 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=09.3910 diff={max=11.9560, min=00.0034, mean=01.7206} policy_loss=-11.5756 policy updated! \n",
      "train step 08831 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4844 diff={max=04.6902, min=00.0588, mean=01.0360} policy_loss=-11.0206 policy updated! \n",
      "train step 08832 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.2151 diff={max=09.2781, min=00.0174, mean=01.6969} policy_loss=-10.5347 policy updated! \n",
      "train step 08833 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=03.6363 diff={max=09.2854, min=00.0033, mean=01.1031} policy_loss=-8.7447 policy updated! \n",
      "train step 08834 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=04.1669 diff={max=06.4904, min=00.0019, mean=01.2515} policy_loss=-8.4224 policy updated! \n",
      "train step 08835 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=07.6466 diff={max=12.1034, min=00.0002, mean=01.5097} policy_loss=-11.4880 policy updated! \n",
      "train step 08836 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2314 diff={max=09.8720, min=00.0055, mean=01.3829} policy_loss=-12.0497 policy updated! \n",
      "train step 08837 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3280 diff={max=08.4774, min=00.0118, mean=01.3152} policy_loss=-11.8221 policy updated! \n",
      "train step 08838 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8384 diff={max=10.6010, min=00.0051, mean=01.0475} policy_loss=-11.2766 policy updated! \n",
      "train step 08839 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=07.4490 diff={max=14.1676, min=00.0106, mean=01.3404} policy_loss=-11.5286 policy updated! \n",
      "train step 08840 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.3076 diff={max=06.5503, min=00.0006, mean=01.2673} policy_loss=-13.2228 policy updated! \n",
      "train step 08841 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4201 diff={max=08.3287, min=00.0796, mean=01.6131} policy_loss=-12.4768 policy updated! \n",
      "train step 08842 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7301 diff={max=07.0148, min=00.0053, mean=01.0079} policy_loss=-10.1213 policy updated! \n",
      "train step 08843 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4424 diff={max=05.5687, min=00.0229, mean=01.3072} policy_loss=-11.4830 policy updated! \n",
      "train step 08844 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=02.0437 diff={max=05.2537, min=00.0566, mean=00.9179} policy_loss=-12.2050 policy updated! \n",
      "train step 08845 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=05.0908 diff={max=07.1046, min=00.0192, mean=01.3751} policy_loss=-9.8411 policy updated! \n",
      "train step 08846 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=03.1748 diff={max=06.3276, min=00.0159, mean=01.1984} policy_loss=-10.7810 policy updated! \n",
      "train step 08847 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.7919 diff={max=12.0897, min=00.0261, mean=01.6967} policy_loss=-10.3393 policy updated! \n",
      "train step 08848 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9745 diff={max=10.5569, min=00.0243, mean=01.6415} policy_loss=-10.0474 policy updated! \n",
      "train step 08849 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=04.0155 diff={max=07.5022, min=00.0081, mean=01.2272} policy_loss=-10.5143 policy updated! \n",
      "train step 08850 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.1525 diff={max=06.2888, min=00.0258, mean=01.2053} policy_loss=-10.4246 policy updated! \n",
      "train step 08851 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.3532 diff={max=17.6873, min=00.0090, mean=01.3508} policy_loss=-11.4034 policy updated! \n",
      "train step 08852 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7909 diff={max=06.9576, min=00.0192, mean=01.4713} policy_loss=-11.0125 policy updated! \n",
      "train step 08853 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=01.7432 diff={max=05.9222, min=00.0027, mean=00.8687} policy_loss=-9.5971 policy updated! \n",
      "train step 08854 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4251 diff={max=08.7817, min=00.0175, mean=01.2378} policy_loss=-11.8592 policy updated! \n",
      "train step 08855 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.8069 diff={max=06.0121, min=00.0215, mean=01.0706} policy_loss=-9.8713 policy updated! \n",
      "train step 08856 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=07.4668 diff={max=09.8783, min=00.0158, mean=01.6139} policy_loss=-12.7978 policy updated! \n",
      "train step 08857 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.5640 diff={max=16.1382, min=00.0058, mean=01.6683} policy_loss=-10.3933 policy updated! \n",
      "train step 08858 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=03.9482 diff={max=05.8173, min=00.0014, mean=01.2671} policy_loss=-11.0823 policy updated! \n",
      "train step 08859 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=05.0184 diff={max=10.1724, min=00.0464, mean=01.3424} policy_loss=-8.8748 policy updated! \n",
      "train step 08860 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=05.1062 diff={max=07.3442, min=00.0129, mean=01.4050} policy_loss=-9.9677 policy updated! \n",
      "train step 08861 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0809 diff={max=10.0111, min=00.0030, mean=01.2100} policy_loss=-9.7652 policy updated! \n",
      "train step 08862 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2114 diff={max=05.8172, min=00.0225, mean=01.1653} policy_loss=-9.3412 policy updated! \n",
      "train step 08863 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9548 diff={max=08.5936, min=00.0312, mean=01.3668} policy_loss=-13.1504 policy updated! \n",
      "train step 08864 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=03.2188 diff={max=07.0054, min=00.0036, mean=01.0113} policy_loss=-11.6095 policy updated! \n",
      "train step 08865 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=06.4368 diff={max=12.2073, min=00.0395, mean=01.4394} policy_loss=-11.5506 policy updated! \n",
      "train step 08866 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0173 diff={max=07.8623, min=00.0235, mean=01.3423} policy_loss=-11.7286 policy updated! \n",
      "train step 08867 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2735 diff={max=07.7669, min=00.0481, mean=01.4775} policy_loss=-11.5369 policy updated! \n",
      "train step 08868 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=06.0880 diff={max=07.7422, min=00.0016, mean=01.5328} policy_loss=-10.9792 policy updated! \n",
      "train step 08869 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=03.6243 diff={max=09.2544, min=00.0176, mean=01.0727} policy_loss=-9.6962 policy updated! \n",
      "train step 08870 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.7965 diff={max=05.4110, min=00.0074, mean=01.0756} policy_loss=-11.7054 policy updated! \n",
      "train step 08871 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=02.0403 diff={max=03.5571, min=00.0226, mean=01.0645} policy_loss=-11.6107 policy updated! \n",
      "train step 08872 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7840 diff={max=07.2029, min=00.0048, mean=01.3177} policy_loss=-9.7140 policy updated! \n",
      "train step 08873 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7347 diff={max=09.6280, min=00.0345, mean=01.4848} policy_loss=-10.3185 policy updated! \n",
      "train step 08874 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7544 diff={max=04.4254, min=00.0437, mean=00.8726} policy_loss=-9.1751 policy updated! \n",
      "train step 08875 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.8875 diff={max=07.9944, min=00.0679, mean=01.4630} policy_loss=-12.0500 policy updated! \n",
      "train step 08876 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=05.1728 diff={max=10.0447, min=00.0285, mean=01.3145} policy_loss=-10.2167 policy updated! \n",
      "train step 08877 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3008 diff={max=09.4531, min=00.1007, mean=01.4471} policy_loss=-11.8176 policy updated! \n",
      "train step 08878 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4495 diff={max=13.3230, min=00.0493, mean=01.2531} policy_loss=-12.4341 policy updated! \n",
      "train step 08879 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=07.1936 diff={max=09.8296, min=00.0046, mean=01.5629} policy_loss=-11.2993 policy updated! \n",
      "train step 08880 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=07.1543 diff={max=12.3424, min=00.0150, mean=01.4703} policy_loss=-9.0132 policy updated! \n",
      "train step 08881 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=07.4737 diff={max=11.9521, min=00.0003, mean=01.3960} policy_loss=-10.5494 policy updated! \n",
      "train step 08882 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2354 diff={max=09.8379, min=00.0728, mean=01.4016} policy_loss=-10.6164 policy updated! \n",
      "train step 08883 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8507 diff={max=05.2885, min=00.0082, mean=00.9162} policy_loss=-10.8826 policy updated! \n",
      "train step 08884 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2296 diff={max=07.0414, min=00.1042, mean=01.1826} policy_loss=-10.4692 policy updated! \n",
      "train step 08885 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=07.8228 diff={max=12.9546, min=00.0113, mean=01.4285} policy_loss=-11.6969 policy updated! \n",
      "train step 08886 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=07.7772 diff={max=10.0983, min=00.0099, mean=01.7188} policy_loss=-10.1292 policy updated! \n",
      "train step 08887 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7277 diff={max=07.6903, min=00.0144, mean=01.2122} policy_loss=-10.7841 policy updated! \n",
      "train step 08888 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3778 diff={max=11.3694, min=00.0007, mean=01.4535} policy_loss=-8.6724 policy updated! \n",
      "train step 08889 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.7240 diff={max=12.2597, min=00.0138, mean=01.6218} policy_loss=-11.4914 policy updated! \n",
      "train step 08890 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.9706 diff={max=04.8030, min=00.0027, mean=01.2400} policy_loss=-10.1226 policy updated! \n",
      "train step 08891 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2132 diff={max=07.6955, min=00.0252, mean=01.3030} policy_loss=-10.8826 policy updated! \n",
      "train step 08892 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.8072 diff={max=11.3355, min=00.0089, mean=01.7890} policy_loss=-11.1257 policy updated! \n",
      "train step 08893 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6763 diff={max=06.3921, min=00.0128, mean=01.2153} policy_loss=-9.9515 policy updated! \n",
      "train step 08894 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4804 diff={max=06.7263, min=00.0264, mean=01.3279} policy_loss=-10.3848 policy updated! \n",
      "train step 08895 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=11.6744 diff={max=13.6434, min=00.0064, mean=01.9429} policy_loss=-12.0977 policy updated! \n",
      "train step 08896 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4270 diff={max=11.3215, min=00.0071, mean=01.5945} policy_loss=-11.6246 policy updated! \n",
      "train step 08897 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0997 diff={max=05.2724, min=00.0337, mean=00.9819} policy_loss=-11.0372 policy updated! \n",
      "train step 08898 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9111 diff={max=06.9182, min=00.0316, mean=01.2534} policy_loss=-12.4736 policy updated! \n",
      "train step 08899 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=04.7829 diff={max=09.4615, min=00.0012, mean=01.3765} policy_loss=-12.9934 policy updated! \n",
      "train step 08900 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.3342 diff={max=08.6310, min=00.0098, mean=01.0239} policy_loss=-10.4105 policy updated! \n",
      "train step 08901 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9213 diff={max=09.4136, min=00.0063, mean=01.3493} policy_loss=-10.1641 policy updated! \n",
      "train step 08902 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.0530 diff={max=12.2208, min=00.0192, mean=01.6554} policy_loss=-10.0792 policy updated! \n",
      "train step 08903 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6968 diff={max=07.5365, min=00.0048, mean=01.4854} policy_loss=-9.8794 policy updated! \n",
      "train step 08904 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.6340 diff={max=15.0035, min=00.0059, mean=01.9071} policy_loss=-10.3617 policy updated! \n",
      "train step 08905 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=06.2084 diff={max=09.5026, min=00.0020, mean=01.4075} policy_loss=-10.4761 policy updated! \n",
      "train step 08906 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5268 diff={max=05.9529, min=00.0038, mean=00.9575} policy_loss=-10.3088 policy updated! \n",
      "train step 08907 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9480 diff={max=10.2313, min=00.0194, mean=01.1386} policy_loss=-10.5590 policy updated! \n",
      "train step 08908 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9452 diff={max=06.1764, min=00.0377, mean=01.1684} policy_loss=-9.2151 policy updated! \n",
      "train step 08909 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5677 diff={max=05.3829, min=00.0020, mean=01.0393} policy_loss=-9.7827 policy updated! \n",
      "train step 08910 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=05.6894 diff={max=08.5189, min=00.0318, mean=01.4680} policy_loss=-12.3704 policy updated! \n",
      "train step 08911 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0345 diff={max=09.4630, min=00.0086, mean=01.7225} policy_loss=-11.9893 policy updated! \n",
      "train step 08912 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4554 diff={max=06.5638, min=00.0102, mean=01.3339} policy_loss=-10.5895 policy updated! \n",
      "train step 08913 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=04.2880 diff={max=07.2521, min=00.0179, mean=01.1621} policy_loss=-12.0678 policy updated! \n",
      "train step 08914 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5674 diff={max=08.3287, min=00.0164, mean=01.5396} policy_loss=-11.3698 policy updated! \n",
      "train step 08915 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=08.9005 diff={max=11.4960, min=00.0018, mean=01.7806} policy_loss=-13.2565 policy updated! \n",
      "train step 08916 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5501 diff={max=08.2779, min=00.0424, mean=01.0486} policy_loss=-10.1780 policy updated! \n",
      "train step 08917 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9956 diff={max=07.2034, min=00.0441, mean=01.4354} policy_loss=-11.1125 policy updated! \n",
      "train step 08918 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=03.2619 diff={max=07.4722, min=00.0029, mean=01.1574} policy_loss=-9.8342 policy updated! \n",
      "train step 08919 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5699 diff={max=05.7004, min=00.0231, mean=01.0433} policy_loss=-10.3013 policy updated! \n",
      "train step 08920 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=05.7055 diff={max=08.1066, min=00.0171, mean=01.4664} policy_loss=-9.0958 policy updated! \n",
      "train step 08921 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3749 diff={max=07.6708, min=00.0033, mean=01.1396} policy_loss=-10.8596 policy updated! \n",
      "train step 08922 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7442 diff={max=06.7813, min=00.0192, mean=01.3579} policy_loss=-9.9668 policy updated! \n",
      "train step 08923 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1309 diff={max=07.2512, min=00.0258, mean=01.4978} policy_loss=-11.1335 policy updated! \n",
      "train step 08924 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3074 diff={max=07.9640, min=00.0155, mean=01.0600} policy_loss=-11.4607 policy updated! \n",
      "train step 08925 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=06.5998 diff={max=12.6789, min=00.0356, mean=01.3715} policy_loss=-13.0507 policy updated! \n",
      "train step 08926 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=03.1889 diff={max=06.7399, min=00.0285, mean=01.1623} policy_loss=-9.3849 policy updated! \n",
      "train step 08927 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5532 diff={max=06.0024, min=00.0078, mean=01.3145} policy_loss=-10.7375 policy updated! \n",
      "train step 08928 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4783 diff={max=09.3788, min=00.0289, mean=01.5427} policy_loss=-12.2221 policy updated! \n",
      "train step 08929 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8614 diff={max=05.4426, min=00.0352, mean=01.3355} policy_loss=-11.2453 policy updated! \n",
      "train step 08930 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.4729 diff={max=07.5552, min=00.0401, mean=00.9302} policy_loss=-10.2791 policy updated! \n",
      "train step 08931 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=01.9577 diff={max=05.2937, min=00.0373, mean=00.9743} policy_loss=-12.1717 policy updated! \n",
      "train step 08932 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7136 diff={max=07.2350, min=00.0063, mean=01.3739} policy_loss=-10.9592 policy updated! \n",
      "train step 08933 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4757 diff={max=08.4055, min=00.0245, mean=01.4793} policy_loss=-9.6023 policy updated! \n",
      "train step 08934 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8905 diff={max=06.6526, min=00.0014, mean=01.0938} policy_loss=-11.2757 policy updated! \n",
      "train step 08935 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=05.9769 diff={max=13.3591, min=00.0032, mean=01.3447} policy_loss=-10.1319 policy updated! \n",
      "train step 08936 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5279 diff={max=09.2629, min=00.0045, mean=01.4835} policy_loss=-10.4039 policy updated! \n",
      "train step 08937 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1263 diff={max=10.1284, min=00.1209, mean=01.6306} policy_loss=-11.5482 policy updated! \n",
      "train step 08938 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8420 diff={max=05.9893, min=00.0156, mean=01.0981} policy_loss=-11.4479 policy updated! \n",
      "train step 08939 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5742 diff={max=04.6235, min=00.0129, mean=01.1179} policy_loss=-12.7625 policy updated! \n",
      "train step 08940 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.0026 diff={max=06.1159, min=00.0285, mean=01.1907} policy_loss=-10.3512 policy updated! \n",
      "train step 08941 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=07.2008 diff={max=15.6995, min=00.0138, mean=01.2697} policy_loss=-11.6637 policy updated! \n",
      "train step 08942 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8178 diff={max=11.6773, min=00.0240, mean=01.4854} policy_loss=-13.4689 policy updated! \n",
      "train step 08943 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7731 diff={max=06.8880, min=00.0075, mean=00.9470} policy_loss=-9.2546 policy updated! \n",
      "train step 08944 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2168 diff={max=13.4265, min=00.0002, mean=01.1027} policy_loss=-10.2261 policy updated! \n",
      "train step 08945 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.5562 diff={max=06.8209, min=00.0129, mean=01.3308} policy_loss=-10.7754 policy updated! \n",
      "train step 08946 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=05.0641 diff={max=09.2268, min=00.0210, mean=01.3599} policy_loss=-9.1012 policy updated! \n",
      "train step 08947 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.4423 diff={max=12.9465, min=00.0089, mean=02.0305} policy_loss=-11.6596 policy updated! \n",
      "train step 08948 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1150 diff={max=08.8226, min=00.0177, mean=01.6288} policy_loss=-9.7623 policy updated! \n",
      "train step 08949 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2607 diff={max=06.4361, min=00.0299, mean=01.5828} policy_loss=-11.2914 policy updated! \n",
      "train step 08950 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=08.3019 diff={max=13.0572, min=00.0048, mean=01.4829} policy_loss=-10.3860 policy updated! \n",
      "train step 08951 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=04.8980 diff={max=09.4036, min=00.0296, mean=01.2770} policy_loss=-9.7048 policy updated! \n",
      "train step 08952 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4106 diff={max=08.5226, min=00.0423, mean=01.2671} policy_loss=-12.2379 policy updated! \n",
      "train step 08953 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=08.6865 diff={max=09.2661, min=00.0262, mean=01.9792} policy_loss=-11.8476 policy updated! \n",
      "train step 08954 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=09.0579 diff={max=13.9514, min=00.0956, mean=01.6481} policy_loss=-12.1388 policy updated! \n",
      "train step 08955 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.3604 diff={max=03.8911, min=00.0546, mean=01.2314} policy_loss=-12.0859 policy updated! \n",
      "train step 08956 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6794 diff={max=09.9614, min=00.0060, mean=01.4546} policy_loss=-11.1400 policy updated! \n",
      "train step 08957 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7247 diff={max=11.1344, min=00.0133, mean=01.2257} policy_loss=-10.8030 policy updated! \n",
      "train step 08958 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1090 diff={max=08.7256, min=00.0050, mean=01.2706} policy_loss=-10.6019 policy updated! \n",
      "train step 08959 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8967 diff={max=11.7795, min=00.0024, mean=01.4551} policy_loss=-11.4061 policy updated! \n",
      "train step 08960 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.5335 diff={max=05.2981, min=00.0349, mean=00.8468} policy_loss=-8.5611 policy updated! \n",
      "train step 08961 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.1611 diff={max=13.3635, min=00.0348, mean=01.6745} policy_loss=-10.7913 policy updated! \n",
      "train step 08962 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4989 diff={max=09.7364, min=00.0008, mean=01.2652} policy_loss=-10.8350 policy updated! \n",
      "train step 08963 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.2611 diff={max=11.9079, min=00.0358, mean=01.4975} policy_loss=-11.0730 policy updated! \n",
      "train step 08964 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7429 diff={max=07.4423, min=00.0124, mean=00.9705} policy_loss=-11.0977 policy updated! \n",
      "train step 08965 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.8920 diff={max=06.3812, min=00.0449, mean=01.3748} policy_loss=-11.3162 policy updated! \n",
      "train step 08966 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2367 diff={max=07.4155, min=00.0124, mean=01.1523} policy_loss=-10.1752 policy updated! \n",
      "train step 08967 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9692 diff={max=08.4052, min=00.0501, mean=01.4179} policy_loss=-13.1085 policy updated! \n",
      "train step 08968 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=05.9036 diff={max=07.5912, min=00.0136, mean=01.6338} policy_loss=-12.9367 policy updated! \n",
      "train step 08969 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=06.5724 diff={max=08.1426, min=00.0398, mean=01.4868} policy_loss=-11.1644 policy updated! \n",
      "train step 08970 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.6263 diff={max=07.5528, min=00.0165, mean=01.4034} policy_loss=-11.5208 policy updated! \n",
      "train step 08971 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=02.8250 diff={max=06.0755, min=00.0134, mean=01.0885} policy_loss=-11.5623 policy updated! \n",
      "train step 08972 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5529 diff={max=05.3561, min=00.0114, mean=01.1147} policy_loss=-10.5097 policy updated! \n",
      "train step 08973 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=17.4039 diff={max=24.7067, min=00.0130, mean=01.8482} policy_loss=-10.3713 policy updated! \n",
      "train step 08974 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5542 diff={max=11.7812, min=00.0106, mean=01.4186} policy_loss=-11.4167 policy updated! \n",
      "train step 08975 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.8380 diff={max=05.9084, min=00.0009, mean=01.1323} policy_loss=-8.8571 policy updated! \n",
      "train step 08976 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.3876 diff={max=10.8164, min=00.0010, mean=01.6745} policy_loss=-11.9958 policy updated! \n",
      "train step 08977 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6685 diff={max=08.0078, min=00.0003, mean=01.5492} policy_loss=-10.8929 policy updated! \n",
      "train step 08978 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=03.7508 diff={max=07.7145, min=00.0120, mean=01.1875} policy_loss=-9.7023 policy updated! \n",
      "train step 08979 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7417 diff={max=07.7463, min=00.0021, mean=01.1928} policy_loss=-11.1681 policy updated! \n",
      "train step 08980 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.1498 diff={max=07.3075, min=00.0194, mean=01.0274} policy_loss=-8.4962 policy updated! \n",
      "train step 08981 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.2982 diff={max=12.4990, min=00.0034, mean=01.7730} policy_loss=-10.3073 policy updated! \n",
      "train step 08982 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.3531 diff={max=17.2137, min=00.0374, mean=01.7903} policy_loss=-13.0169 policy updated! \n",
      "train step 08983 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=08.3991 diff={max=09.4657, min=00.0075, mean=01.5701} policy_loss=-10.7679 policy updated! \n",
      "train step 08984 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9997 diff={max=06.6762, min=00.0308, mean=01.3193} policy_loss=-10.0910 policy updated! \n",
      "train step 08985 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=06.0539 diff={max=07.6544, min=00.0035, mean=01.6859} policy_loss=-13.1765 policy updated! \n",
      "train step 08986 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=04.7339 diff={max=09.2615, min=00.0076, mean=01.3179} policy_loss=-11.6643 policy updated! \n",
      "train step 08987 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1466 diff={max=09.4717, min=00.0160, mean=01.5117} policy_loss=-10.8492 policy updated! \n",
      "train step 08988 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3092 diff={max=09.1421, min=00.0007, mean=01.6325} policy_loss=-10.8602 policy updated! \n",
      "train step 08989 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=08.2607 diff={max=13.6899, min=00.0134, mean=01.6188} policy_loss=-11.3428 policy updated! \n",
      "train step 08990 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=06.4502 diff={max=10.6972, min=00.0031, mean=01.5872} policy_loss=-12.4797 policy updated! \n",
      "train step 08991 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7794 diff={max=07.2924, min=00.0690, mean=01.7301} policy_loss=-11.0273 policy updated! \n",
      "train step 08992 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.6633 diff={max=13.7332, min=00.0511, mean=01.8842} policy_loss=-13.2453 policy updated! \n",
      "train step 08993 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4979 diff={max=07.0144, min=00.0031, mean=00.9766} policy_loss=-11.4207 policy updated! \n",
      "train step 08994 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.7612 diff={max=12.2192, min=00.0037, mean=01.5353} policy_loss=-8.5410 policy updated! \n",
      "train step 08995 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.6114 diff={max=06.8786, min=00.0354, mean=01.1446} policy_loss=-13.0681 policy updated! \n",
      "train step 08996 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1119 diff={max=07.7820, min=00.0323, mean=01.5559} policy_loss=-10.0574 policy updated! \n",
      "train step 08997 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9889 diff={max=07.6146, min=00.0052, mean=01.2653} policy_loss=-10.7882 policy updated! \n",
      "train step 08998 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=01.1936 diff={max=02.8078, min=00.0043, mean=00.8614} policy_loss=-10.2925 policy updated! \n",
      "train step 08999 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=03.3821 diff={max=06.0975, min=00.0049, mean=01.2498} policy_loss=-13.1906 policy updated! \n",
      "train step 09000 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.9901 diff={max=06.1495, min=00.0112, mean=01.3070} policy_loss=-12.3731 policy updated! \n",
      "train step 09001 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2398 diff={max=05.7625, min=00.0203, mean=01.2351} policy_loss=-8.9459 policy updated! \n",
      "train step 09002 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.1058 diff={max=10.7973, min=00.0266, mean=01.6378} policy_loss=-9.1079 policy updated! \n",
      "train step 09003 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0528 diff={max=03.8596, min=00.0310, mean=01.0012} policy_loss=-10.8652 policy updated! \n",
      "train step 09004 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1951 diff={max=05.4590, min=00.0733, mean=01.2846} policy_loss=-11.0313 policy updated! \n",
      "train step 09005 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=07.8966 diff={max=08.6990, min=00.0096, mean=01.8136} policy_loss=-9.4793 policy updated! \n",
      "train step 09006 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=09.6158 diff={max=12.2666, min=00.0271, mean=01.7319} policy_loss=-10.1291 policy updated! \n",
      "train step 09007 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0091 diff={max=05.4745, min=00.0055, mean=00.8799} policy_loss=-10.3390 policy updated! \n",
      "train step 09008 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5858 diff={max=10.1136, min=00.0083, mean=01.1699} policy_loss=-9.8111 policy updated! \n",
      "train step 09009 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=03.2484 diff={max=05.3186, min=00.0126, mean=01.3308} policy_loss=-12.7836 policy updated! \n",
      "train step 09010 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.5225 diff={max=06.6528, min=00.0099, mean=01.1802} policy_loss=-12.8717 policy updated! \n",
      "train step 09011 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=04.0790 diff={max=06.3845, min=00.0133, mean=01.4684} policy_loss=-10.8775 policy updated! \n",
      "train step 09012 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7639 diff={max=08.2126, min=00.0042, mean=01.3186} policy_loss=-11.0243 policy updated! \n",
      "train step 09013 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.8198 diff={max=11.2743, min=00.0050, mean=01.9338} policy_loss=-11.2752 policy updated! \n",
      "train step 09014 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=08.3926 diff={max=12.6351, min=00.0037, mean=01.5353} policy_loss=-9.4448 policy updated! \n",
      "train step 09015 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=05.3002 diff={max=12.3172, min=00.0040, mean=01.0387} policy_loss=-10.2729 policy updated! \n",
      "train step 09016 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=02.2017 diff={max=04.2716, min=00.0116, mean=01.0274} policy_loss=-10.0531 policy updated! \n",
      "train step 09017 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0199 diff={max=06.8338, min=00.0024, mean=01.0279} policy_loss=-9.9190 policy updated! \n",
      "train step 09018 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3229 diff={max=04.6961, min=00.0356, mean=01.0210} policy_loss=-13.2270 policy updated! \n",
      "train step 09019 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7673 diff={max=06.1337, min=00.0135, mean=01.1310} policy_loss=-11.1874 policy updated! \n",
      "train step 09020 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=07.1880 diff={max=12.7255, min=00.0212, mean=01.5523} policy_loss=-11.0153 policy updated! \n",
      "train step 09021 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4200 diff={max=06.2277, min=00.0051, mean=01.0093} policy_loss=-10.9599 policy updated! \n",
      "train step 09022 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7462 diff={max=05.7377, min=00.0176, mean=01.1098} policy_loss=-9.7889 policy updated! \n",
      "train step 09023 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1618 diff={max=11.3318, min=00.0048, mean=01.5969} policy_loss=-13.7287 policy updated! \n",
      "train step 09024 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7248 diff={max=08.3400, min=00.0153, mean=01.2008} policy_loss=-7.9241 policy updated! \n",
      "train step 09025 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.2674 diff={max=08.3082, min=00.0083, mean=01.0696} policy_loss=-12.4537 policy updated! \n",
      "train step 09026 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0288 diff={max=05.6147, min=00.0068, mean=00.9254} policy_loss=-11.6459 policy updated! \n",
      "train step 09027 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.2372 diff={max=12.7019, min=00.0144, mean=01.4384} policy_loss=-11.9515 policy updated! \n",
      "train step 09028 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.9398 diff={max=09.5437, min=00.0007, mean=01.9891} policy_loss=-11.3875 policy updated! \n",
      "train step 09029 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3869 diff={max=04.8613, min=00.0580, mean=00.9522} policy_loss=-10.1710 policy updated! \n",
      "train step 09030 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.5554 diff={max=07.4356, min=00.0318, mean=01.3847} policy_loss=-9.5013 policy updated! \n",
      "train step 09031 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5254 diff={max=07.6790, min=00.0471, mean=01.3916} policy_loss=-10.0136 policy updated! \n",
      "train step 09032 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5716 diff={max=05.7820, min=00.0161, mean=01.0869} policy_loss=-7.8233 policy updated! \n",
      "train step 09033 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=08.2046 diff={max=09.6781, min=00.0269, mean=01.9229} policy_loss=-9.3867 policy updated! \n",
      "train step 09034 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.2962 diff={max=11.5799, min=00.0408, mean=01.5869} policy_loss=-9.3747 policy updated! \n",
      "train step 09035 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=06.6283 diff={max=09.2943, min=00.0109, mean=01.5461} policy_loss=-11.1652 policy updated! \n",
      "train step 09036 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1925 diff={max=09.0237, min=00.0203, mean=01.0585} policy_loss=-9.1104 policy updated! \n",
      "train step 09037 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5335 diff={max=08.1701, min=00.0100, mean=01.3539} policy_loss=-11.0423 policy updated! \n",
      "train step 09038 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.8820 diff={max=15.8511, min=00.0033, mean=01.6263} policy_loss=-13.1561 policy updated! \n",
      "train step 09039 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7872 diff={max=08.7461, min=00.0111, mean=01.2248} policy_loss=-9.9475 policy updated! \n",
      "train step 09040 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=02.3818 diff={max=04.7497, min=00.0460, mean=01.0993} policy_loss=-10.3018 policy updated! \n",
      "train step 09041 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0556 diff={max=08.8908, min=00.0052, mean=01.1171} policy_loss=-9.0397 policy updated! \n",
      "train step 09042 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4353 diff={max=07.7973, min=00.0222, mean=01.2448} policy_loss=-11.7483 policy updated! \n",
      "train step 09043 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=03.1512 diff={max=08.7223, min=00.0005, mean=01.0046} policy_loss=-8.9383 policy updated! \n",
      "train step 09044 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1362 diff={max=06.2784, min=00.0133, mean=00.9965} policy_loss=-8.6778 policy updated! \n",
      "train step 09045 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.8439 diff={max=06.3790, min=00.0168, mean=01.0652} policy_loss=-9.5231 policy updated! \n",
      "train step 09046 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0854 diff={max=06.6357, min=00.0204, mean=01.2903} policy_loss=-11.5084 policy updated! \n",
      "train step 09047 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0851 diff={max=08.1817, min=00.0195, mean=01.4928} policy_loss=-12.4786 policy updated! \n",
      "train step 09048 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6980 diff={max=04.6341, min=00.0306, mean=01.1125} policy_loss=-10.3508 policy updated! \n",
      "train step 09049 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=02.6668 diff={max=04.6405, min=00.0077, mean=01.2214} policy_loss=-11.8497 policy updated! \n",
      "train step 09050 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.1705 diff={max=07.1920, min=00.0242, mean=01.2845} policy_loss=-13.9434 policy updated! \n",
      "train step 09051 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2790 diff={max=10.8037, min=00.0050, mean=01.3337} policy_loss=-12.3401 policy updated! \n",
      "train step 09052 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3093 diff={max=09.1982, min=00.0276, mean=01.4640} policy_loss=-11.9165 policy updated! \n",
      "train step 09053 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=02.9330 diff={max=07.1711, min=00.0114, mean=01.0272} policy_loss=-10.6328 policy updated! \n",
      "train step 09054 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.2442 diff={max=12.0170, min=00.0044, mean=01.5741} policy_loss=-11.4790 policy updated! \n",
      "train step 09055 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=06.2253 diff={max=11.6622, min=00.0136, mean=01.3625} policy_loss=-10.5314 policy updated! \n",
      "train step 09056 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7973 diff={max=03.6603, min=00.0124, mean=00.9679} policy_loss=-10.2260 policy updated! \n",
      "train step 09057 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6741 diff={max=07.1756, min=00.0053, mean=01.4061} policy_loss=-12.0714 policy updated! \n",
      "train step 09058 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0935 diff={max=07.2463, min=00.0204, mean=01.0618} policy_loss=-7.8949 policy updated! \n",
      "train step 09059 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1846 diff={max=08.1421, min=00.0033, mean=01.1207} policy_loss=-10.4714 policy updated! \n",
      "train step 09060 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.3190 diff={max=05.4773, min=00.0085, mean=01.2567} policy_loss=-10.5829 policy updated! \n",
      "train step 09061 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=03.0866 diff={max=04.3715, min=00.0084, mean=01.2478} policy_loss=-11.7264 policy updated! \n",
      "train step 09062 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2079 diff={max=07.9940, min=00.0027, mean=01.4020} policy_loss=-12.7162 policy updated! \n",
      "train step 09063 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9080 diff={max=09.0640, min=00.0064, mean=01.3255} policy_loss=-12.0634 policy updated! \n",
      "train step 09064 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=03.7506 diff={max=08.2348, min=00.0099, mean=01.0684} policy_loss=-11.2099 policy updated! \n",
      "train step 09065 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.5528 diff={max=05.3098, min=00.0178, mean=01.5121} policy_loss=-12.9100 policy updated! \n",
      "train step 09066 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=04.1905 diff={max=09.1532, min=00.0693, mean=01.3350} policy_loss=-12.2184 policy updated! \n",
      "train step 09067 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7897 diff={max=07.0489, min=00.0067, mean=01.2623} policy_loss=-10.7100 policy updated! \n",
      "train step 09068 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6938 diff={max=11.5173, min=00.0390, mean=01.5045} policy_loss=-10.7863 policy updated! \n",
      "train step 09069 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=06.8072 diff={max=10.8357, min=00.0170, mean=01.6528} policy_loss=-10.8266 policy updated! \n",
      "train step 09070 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.0486 diff={max=05.5206, min=00.0209, mean=01.2085} policy_loss=-11.6077 policy updated! \n",
      "train step 09071 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=11.1509 diff={max=11.6800, min=00.0041, mean=01.7973} policy_loss=-10.2921 policy updated! \n",
      "train step 09072 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8364 diff={max=10.5106, min=00.0054, mean=01.1446} policy_loss=-10.1625 policy updated! \n",
      "train step 09073 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8630 diff={max=09.0048, min=00.0244, mean=00.9448} policy_loss=-9.4697 policy updated! \n",
      "train step 09074 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2801 diff={max=10.6341, min=00.0106, mean=01.1769} policy_loss=-10.4129 policy updated! \n",
      "train step 09075 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.0824 diff={max=07.2095, min=00.0049, mean=01.3695} policy_loss=-13.3610 policy updated! \n",
      "train step 09076 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7517 diff={max=10.9264, min=00.0041, mean=01.5539} policy_loss=-12.8011 policy updated! \n",
      "train step 09077 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5392 diff={max=09.6487, min=00.0166, mean=01.2697} policy_loss=-11.4791 policy updated! \n",
      "train step 09078 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8266 diff={max=07.7337, min=00.0249, mean=01.3502} policy_loss=-11.8211 policy updated! \n",
      "train step 09079 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0850 diff={max=07.9737, min=00.0164, mean=01.1430} policy_loss=-10.7001 policy updated! \n",
      "train step 09080 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=09.1676 diff={max=11.6165, min=00.0196, mean=01.9718} policy_loss=-11.9768 policy updated! \n",
      "train step 09081 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=03.4741 diff={max=06.7995, min=00.0309, mean=01.2221} policy_loss=-10.6758 policy updated! \n",
      "train step 09082 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2028 diff={max=05.5758, min=00.0007, mean=01.3206} policy_loss=-10.1313 policy updated! \n",
      "train step 09083 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=07.5474 diff={max=09.2096, min=00.0578, mean=01.8867} policy_loss=-14.2099 policy updated! \n",
      "train step 09084 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=04.5422 diff={max=10.3740, min=00.0144, mean=01.2817} policy_loss=-10.0792 policy updated! \n",
      "train step 09085 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.5955 diff={max=07.1440, min=00.0004, mean=01.3403} policy_loss=-10.2510 policy updated! \n",
      "train step 09086 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1368 diff={max=05.0090, min=00.0468, mean=01.0001} policy_loss=-10.0511 policy updated! \n",
      "train step 09087 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.9593 diff={max=08.8179, min=00.0067, mean=01.9724} policy_loss=-11.1136 policy updated! \n",
      "train step 09088 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4629 diff={max=05.2790, min=00.0065, mean=01.5440} policy_loss=-10.2328 policy updated! \n",
      "train step 09089 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=05.5354 diff={max=10.0270, min=00.0028, mean=01.4354} policy_loss=-10.0211 policy updated! \n",
      "train step 09090 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.2020 diff={max=08.3238, min=00.0528, mean=00.9566} policy_loss=-10.2895 policy updated! \n",
      "train step 09091 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0956 diff={max=07.2789, min=00.0149, mean=01.1460} policy_loss=-12.5308 policy updated! \n",
      "train step 09092 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2885 diff={max=06.5718, min=00.0248, mean=01.1607} policy_loss=-12.5625 policy updated! \n",
      "train step 09093 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3768 diff={max=05.2884, min=00.0629, mean=01.2563} policy_loss=-12.7916 policy updated! \n",
      "train step 09094 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2750 diff={max=11.2727, min=00.0065, mean=01.1831} policy_loss=-10.9888 policy updated! \n",
      "train step 09095 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=06.5539 diff={max=07.8699, min=00.0166, mean=01.6087} policy_loss=-11.0957 policy updated! \n",
      "train step 09096 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3889 diff={max=07.8651, min=00.0154, mean=01.4867} policy_loss=-10.4570 policy updated! \n",
      "train step 09097 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9647 diff={max=05.0693, min=00.0066, mean=01.4100} policy_loss=-9.5707 policy updated! \n",
      "train step 09098 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=01.7245 diff={max=04.7459, min=00.0410, mean=00.9215} policy_loss=-9.9358 policy updated! \n",
      "train step 09099 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=06.0960 diff={max=08.9711, min=00.0051, mean=01.5175} policy_loss=-12.2539 policy updated! \n",
      "train step 09100 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.7696 diff={max=06.3176, min=00.0076, mean=01.4930} policy_loss=-8.6950 policy updated! \n",
      "train step 09101 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=03.5753 diff={max=05.6186, min=00.0020, mean=01.2915} policy_loss=-10.7808 policy updated! \n",
      "train step 09102 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5083 diff={max=09.9499, min=00.0093, mean=01.2173} policy_loss=-10.0643 policy updated! \n",
      "train step 09103 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=08.5596 diff={max=13.2903, min=00.0459, mean=01.7283} policy_loss=-12.8939 policy updated! \n",
      "train step 09104 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=03.3517 diff={max=06.8403, min=00.0192, mean=01.2002} policy_loss=-11.1008 policy updated! \n",
      "train step 09105 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=11.2666 diff={max=09.3962, min=00.0002, mean=02.1204} policy_loss=-8.9392 policy updated! \n",
      "train step 09106 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6370 diff={max=06.7388, min=00.0081, mean=01.3850} policy_loss=-10.0987 policy updated! \n",
      "train step 09107 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.0665 diff={max=09.7508, min=00.0043, mean=01.6561} policy_loss=-8.9563 policy updated! \n",
      "train step 09108 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=15.2865 diff={max=14.1381, min=00.0427, mean=02.3444} policy_loss=-11.7642 policy updated! \n",
      "train step 09109 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7838 diff={max=10.0909, min=00.0076, mean=01.3193} policy_loss=-11.6290 policy updated! \n",
      "train step 09110 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.5551 diff={max=08.5853, min=00.0144, mean=01.2130} policy_loss=-11.3629 policy updated! \n",
      "train step 09111 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=02.9098 diff={max=07.4360, min=00.0123, mean=01.0839} policy_loss=-11.8369 policy updated! \n",
      "train step 09112 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1098 diff={max=06.9601, min=00.0021, mean=01.2036} policy_loss=-10.1311 policy updated! \n",
      "train step 09113 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=04.8859 diff={max=09.1519, min=00.0257, mean=01.3780} policy_loss=-11.5090 policy updated! \n",
      "train step 09114 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=08.8295 diff={max=08.9045, min=00.0084, mean=01.8964} policy_loss=-12.4406 policy updated! \n",
      "train step 09115 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=05.1229 diff={max=10.4352, min=00.0285, mean=01.4013} policy_loss=-11.3707 policy updated! \n",
      "train step 09116 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6142 diff={max=07.5342, min=00.0102, mean=01.4012} policy_loss=-10.0847 policy updated! \n",
      "train step 09117 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4064 diff={max=09.0340, min=00.0770, mean=01.4561} policy_loss=-13.0726 policy updated! \n",
      "train step 09118 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9080 diff={max=07.9704, min=00.0251, mean=01.0079} policy_loss=-12.1677 policy updated! \n",
      "train step 09119 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=02.0418 diff={max=04.6856, min=00.0209, mean=00.9551} policy_loss=-10.5008 policy updated! \n",
      "train step 09120 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=05.2412 diff={max=11.1760, min=00.0429, mean=01.2748} policy_loss=-11.2966 policy updated! \n",
      "train step 09121 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=05.8963 diff={max=11.0522, min=00.0254, mean=01.2737} policy_loss=-10.1662 policy updated! \n",
      "train step 09122 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0518 diff={max=08.0752, min=00.0480, mean=01.4108} policy_loss=-10.6080 policy updated! \n",
      "train step 09123 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=02.3466 diff={max=06.1116, min=00.0020, mean=00.9721} policy_loss=-8.6369 policy updated! \n",
      "train step 09124 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=11.7732 diff={max=21.0793, min=00.0173, mean=01.6378} policy_loss=-13.0943 policy updated! \n",
      "train step 09125 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=09.4248 diff={max=18.3737, min=00.0027, mean=01.3480} policy_loss=-10.1057 policy updated! \n",
      "train step 09126 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4410 diff={max=08.1815, min=00.0035, mean=01.2582} policy_loss=-10.5210 policy updated! \n",
      "train step 09127 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2020 diff={max=07.8364, min=00.0005, mean=01.4388} policy_loss=-11.0968 policy updated! \n",
      "train step 09128 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=05.3232 diff={max=09.6505, min=00.0029, mean=01.3356} policy_loss=-10.2957 policy updated! \n",
      "train step 09129 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3799 diff={max=13.6325, min=00.0024, mean=01.5991} policy_loss=-9.8523 policy updated! \n",
      "train step 09130 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.8798 diff={max=07.4060, min=00.0002, mean=01.2031} policy_loss=-11.1200 policy updated! \n",
      "train step 09131 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7235 diff={max=06.1998, min=00.0006, mean=01.2662} policy_loss=-9.6802 policy updated! \n",
      "train step 09132 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5236 diff={max=06.9832, min=00.0066, mean=01.2841} policy_loss=-10.0542 policy updated! \n",
      "train step 09133 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0048 diff={max=09.9885, min=00.0283, mean=01.3402} policy_loss=-11.5451 policy updated! \n",
      "train step 09134 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=07.5684 diff={max=10.8382, min=00.0532, mean=01.5959} policy_loss=-11.6647 policy updated! \n",
      "train step 09135 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.3827 diff={max=06.6639, min=00.0027, mean=01.1776} policy_loss=-12.1004 policy updated! \n",
      "train step 09136 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=07.3478 diff={max=11.1431, min=00.0035, mean=01.5154} policy_loss=-10.3904 policy updated! \n",
      "train step 09137 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7332 diff={max=05.8842, min=00.0207, mean=01.2796} policy_loss=-10.3695 policy updated! \n",
      "train step 09138 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=06.7624 diff={max=09.0645, min=00.0148, mean=01.6143} policy_loss=-10.9756 policy updated! \n",
      "train step 09139 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=04.9400 diff={max=07.5672, min=00.0102, mean=01.3888} policy_loss=-11.1851 policy updated! \n",
      "train step 09140 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.7602 diff={max=04.4562, min=00.0063, mean=00.9163} policy_loss=-11.7367 policy updated! \n",
      "train step 09141 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6057 diff={max=08.0128, min=00.0761, mean=01.3646} policy_loss=-11.3426 policy updated! \n",
      "train step 09142 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4317 diff={max=12.1903, min=00.0375, mean=01.4026} policy_loss=-13.3330 policy updated! \n",
      "train step 09143 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0758 diff={max=09.5705, min=00.0262, mean=01.3852} policy_loss=-11.0793 policy updated! \n",
      "train step 09144 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=07.7196 diff={max=15.0460, min=00.0045, mean=01.3658} policy_loss=-8.7340 policy updated! \n",
      "train step 09145 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.6173 diff={max=07.1023, min=00.0226, mean=01.2162} policy_loss=-10.7103 policy updated! \n",
      "train step 09146 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=03.6705 diff={max=08.0791, min=00.0004, mean=01.1784} policy_loss=-10.9632 policy updated! \n",
      "train step 09147 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1929 diff={max=03.8093, min=00.0076, mean=00.7550} policy_loss=-9.8217 policy updated! \n",
      "train step 09148 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.8220 diff={max=17.0379, min=00.0238, mean=01.5829} policy_loss=-11.4247 policy updated! \n",
      "train step 09149 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=06.8231 diff={max=09.5741, min=00.0026, mean=01.3615} policy_loss=-8.9644 policy updated! \n",
      "train step 09150 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.8628 diff={max=08.4033, min=00.0020, mean=01.4381} policy_loss=-10.4492 policy updated! \n",
      "train step 09151 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0559 diff={max=05.9140, min=00.1136, mean=01.3303} policy_loss=-13.1527 policy updated! \n",
      "train step 09152 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8763 diff={max=08.3413, min=00.0598, mean=01.3239} policy_loss=-11.5307 policy updated! \n",
      "train step 09153 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=04.2745 diff={max=08.8113, min=00.0660, mean=01.2262} policy_loss=-11.5597 policy updated! \n",
      "train step 09154 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1367 diff={max=07.7626, min=00.0328, mean=01.0959} policy_loss=-12.0895 policy updated! \n",
      "train step 09155 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=05.5861 diff={max=12.4434, min=00.0037, mean=01.3476} policy_loss=-11.2638 policy updated! \n",
      "train step 09156 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=05.1254 diff={max=08.2138, min=00.0012, mean=01.5504} policy_loss=-11.4742 policy updated! \n",
      "train step 09157 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=05.6391 diff={max=11.2234, min=00.0105, mean=01.4483} policy_loss=-10.0373 policy updated! \n",
      "train step 09158 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.8986 diff={max=10.0251, min=00.0340, mean=01.8080} policy_loss=-9.8390 policy updated! \n",
      "train step 09159 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1320 diff={max=07.3623, min=00.0158, mean=01.3341} policy_loss=-11.6984 policy updated! \n",
      "train step 09160 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=06.4351 diff={max=11.3425, min=00.0090, mean=01.3906} policy_loss=-11.2256 policy updated! \n",
      "train step 09161 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=05.3615 diff={max=07.4811, min=00.0159, mean=01.4740} policy_loss=-9.5535 policy updated! \n",
      "train step 09162 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.9694 diff={max=10.2739, min=00.0027, mean=01.6751} policy_loss=-12.6037 policy updated! \n",
      "train step 09163 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=05.1844 diff={max=09.9994, min=00.0609, mean=01.3094} policy_loss=-12.2498 policy updated! \n",
      "train step 09164 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8798 diff={max=06.1987, min=00.0115, mean=01.3670} policy_loss=-10.0289 policy updated! \n",
      "train step 09165 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.7013 diff={max=07.1008, min=00.0053, mean=01.3484} policy_loss=-8.2775 policy updated! \n",
      "train step 09166 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=02.4756 diff={max=06.0244, min=00.0065, mean=00.9832} policy_loss=-10.6353 policy updated! \n",
      "train step 09167 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2155 diff={max=06.6789, min=00.0265, mean=01.1118} policy_loss=-9.9071 policy updated! \n",
      "train step 09168 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=02.5240 diff={max=05.4221, min=00.0218, mean=01.1150} policy_loss=-12.0857 policy updated! \n",
      "train step 09169 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.0641 diff={max=12.6318, min=00.0272, mean=01.7370} policy_loss=-12.5604 policy updated! \n",
      "train step 09170 reward={max=06.0000, min=05.0000, mean=05.8000} optimizing loss=05.5588 diff={max=09.8571, min=00.0115, mean=01.2796} policy_loss=-11.2267 policy updated! \n",
      "train step 09171 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.3874 diff={max=18.3360, min=00.0144, mean=01.6764} policy_loss=-12.8709 policy updated! \n",
      "train step 09172 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9755 diff={max=04.6930, min=00.0290, mean=00.8769} policy_loss=-8.7733 policy updated! \n",
      "train step 09173 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6801 diff={max=13.9362, min=00.0335, mean=01.3851} policy_loss=-10.8655 policy updated! \n",
      "train step 09174 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6867 diff={max=06.6862, min=00.0074, mean=01.3933} policy_loss=-12.8904 policy updated! \n",
      "train step 09175 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=10.5833 diff={max=09.8208, min=00.0266, mean=02.1005} policy_loss=-9.7741 policy updated! \n",
      "train step 09176 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=03.0714 diff={max=09.0653, min=00.0102, mean=01.0094} policy_loss=-11.9361 policy updated! \n",
      "train step 09177 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7369 diff={max=05.4249, min=00.0062, mean=00.9577} policy_loss=-12.3144 policy updated! \n",
      "train step 09178 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=02.5379 diff={max=06.2127, min=00.0184, mean=00.9732} policy_loss=-10.8439 policy updated! \n",
      "train step 09179 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0641 diff={max=07.1388, min=00.0127, mean=01.2279} policy_loss=-10.9347 policy updated! \n",
      "train step 09180 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=01.3081 diff={max=03.7862, min=00.0007, mean=00.7620} policy_loss=-10.5779 policy updated! \n",
      "train step 09181 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=04.0389 diff={max=09.8467, min=00.0121, mean=01.2216} policy_loss=-12.6533 policy updated! \n",
      "train step 09182 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.9975 diff={max=16.5232, min=00.0019, mean=01.5577} policy_loss=-11.0255 policy updated! \n",
      "train step 09183 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1084 diff={max=10.4639, min=00.0205, mean=01.5090} policy_loss=-11.0011 policy updated! \n",
      "train step 09184 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9682 diff={max=05.6268, min=00.0007, mean=01.1754} policy_loss=-10.4589 policy updated! \n",
      "train step 09185 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.7266 diff={max=06.3006, min=00.0123, mean=01.0013} policy_loss=-9.4273 policy updated! \n",
      "train step 09186 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0237 diff={max=12.4593, min=00.0149, mean=01.5719} policy_loss=-12.8120 policy updated! \n",
      "train step 09187 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1249 diff={max=07.3875, min=00.0052, mean=01.4327} policy_loss=-12.9620 policy updated! \n",
      "train step 09188 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8240 diff={max=09.5334, min=00.0096, mean=01.4097} policy_loss=-12.8770 policy updated! \n",
      "train step 09189 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.0732 diff={max=10.9383, min=00.0076, mean=01.8867} policy_loss=-11.4527 policy updated! \n",
      "train step 09190 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=06.8245 diff={max=09.5378, min=00.0908, mean=01.6127} policy_loss=-12.5854 policy updated! \n",
      "train step 09191 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5775 diff={max=08.7247, min=00.0085, mean=01.2629} policy_loss=-10.7878 policy updated! \n",
      "train step 09192 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0973 diff={max=05.3595, min=00.0040, mean=00.9536} policy_loss=-9.1174 policy updated! \n",
      "train step 09193 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=02.6028 diff={max=05.3698, min=00.0400, mean=01.1280} policy_loss=-9.9956 policy updated! \n",
      "train step 09194 reward={max=07.0000, min=00.0000, mean=05.0000} optimizing loss=03.7330 diff={max=06.9949, min=00.0065, mean=01.1778} policy_loss=-10.2150 policy updated! \n",
      "train step 09195 reward={max=07.0000, min=06.0000, mean=06.2000} optimizing loss=05.7733 diff={max=10.2296, min=00.0127, mean=01.3577} policy_loss=-9.9074 policy updated! \n",
      "train step 09196 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8114 diff={max=05.1283, min=00.0334, mean=01.2049} policy_loss=-11.3481 policy updated! \n",
      "train step 09197 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0000 diff={max=07.8031, min=00.0100, mean=01.2375} policy_loss=-11.2740 policy updated! \n",
      "train step 09198 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.2658 diff={max=09.1500, min=00.0440, mean=01.8874} policy_loss=-11.1581 policy updated! \n",
      "train step 09199 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4966 diff={max=05.5437, min=00.0393, mean=01.2303} policy_loss=-9.9353 policy updated! \n",
      "train step 09200 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.2645 diff={max=07.6335, min=00.0077, mean=01.6255} policy_loss=-12.1668 policy updated! \n",
      "train step 09201 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=05.9959 diff={max=08.3658, min=00.0045, mean=01.6014} policy_loss=-10.5766 policy updated! \n",
      "train step 09202 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5173 diff={max=07.0471, min=00.0385, mean=01.0784} policy_loss=-9.8503 policy updated! \n",
      "train step 09203 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1893 diff={max=09.3600, min=00.0200, mean=01.3000} policy_loss=-11.4820 policy updated! \n",
      "train step 09204 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=11.9007 diff={max=12.7844, min=00.0398, mean=01.8745} policy_loss=-13.7186 policy updated! \n",
      "train step 09205 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.7253 diff={max=06.7451, min=00.0053, mean=01.3414} policy_loss=-11.0476 policy updated! \n",
      "train step 09206 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3676 diff={max=11.4101, min=00.0391, mean=01.3437} policy_loss=-10.2209 policy updated! \n",
      "train step 09207 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.2881 diff={max=13.7040, min=00.0222, mean=01.9446} policy_loss=-13.4273 policy updated! \n",
      "train step 09208 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=03.4286 diff={max=07.7236, min=00.0122, mean=01.1263} policy_loss=-10.8642 policy updated! \n",
      "train step 09209 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4952 diff={max=05.0811, min=00.0181, mean=01.0599} policy_loss=-9.7307 policy updated! \n",
      "train step 09210 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.1017 diff={max=07.3739, min=00.0351, mean=01.1703} policy_loss=-12.1103 policy updated! \n",
      "train step 09211 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.2334 diff={max=07.7022, min=00.0109, mean=01.6745} policy_loss=-12.4540 policy updated! \n",
      "train step 09212 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3830 diff={max=04.8871, min=00.0023, mean=01.0421} policy_loss=-12.7050 policy updated! \n",
      "train step 09213 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4112 diff={max=07.2730, min=00.0137, mean=01.7529} policy_loss=-10.6064 policy updated! \n",
      "train step 09214 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6625 diff={max=06.3153, min=00.0581, mean=01.2653} policy_loss=-12.3148 policy updated! \n",
      "train step 09215 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=03.6637 diff={max=06.2027, min=00.0323, mean=01.2190} policy_loss=-11.7362 policy updated! \n",
      "train step 09216 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.4754 diff={max=15.9436, min=00.0155, mean=01.3741} policy_loss=-11.2155 policy updated! \n",
      "train step 09217 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8354 diff={max=05.4842, min=00.0352, mean=01.1873} policy_loss=-10.8390 policy updated! \n",
      "train step 09218 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=03.4331 diff={max=08.7607, min=00.0002, mean=01.0449} policy_loss=-10.1323 policy updated! \n",
      "train step 09219 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8991 diff={max=05.0099, min=00.0136, mean=00.9938} policy_loss=-9.4316 policy updated! \n",
      "train step 09220 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.5695 diff={max=05.6591, min=00.0182, mean=01.0709} policy_loss=-10.7220 policy updated! \n",
      "train step 09221 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=03.9086 diff={max=06.5759, min=00.0027, mean=01.3510} policy_loss=-14.4612 policy updated! \n",
      "train step 09222 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7184 diff={max=09.3122, min=00.0078, mean=01.4935} policy_loss=-11.8307 policy updated! \n",
      "train step 09223 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.1092 diff={max=15.4521, min=00.0197, mean=01.4740} policy_loss=-12.4828 policy updated! \n",
      "train step 09224 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.4178 diff={max=18.4358, min=00.0407, mean=01.3807} policy_loss=-8.3466 policy updated! \n",
      "train step 09225 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.6227 diff={max=04.3453, min=00.0017, mean=01.1616} policy_loss=-10.8003 policy updated! \n",
      "train step 09226 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8267 diff={max=11.2134, min=00.0044, mean=01.5315} policy_loss=-10.0699 policy updated! \n",
      "train step 09227 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7486 diff={max=07.9111, min=00.0004, mean=01.3352} policy_loss=-9.7843 policy updated! \n",
      "train step 09228 reward={max=06.0000, min=00.0000, mean=02.2000} optimizing loss=04.5203 diff={max=08.4213, min=00.0489, mean=01.3651} policy_loss=-9.2124 policy updated! \n",
      "train step 09229 reward={max=07.0000, min=00.0000, mean=05.0000} optimizing loss=03.3242 diff={max=08.8819, min=00.0106, mean=01.0124} policy_loss=-10.4891 policy updated! \n",
      "train step 09230 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.2918 diff={max=09.3801, min=00.0198, mean=01.0259} policy_loss=-8.5709 policy updated! \n",
      "train step 09231 reward={max=07.0000, min=00.0000, mean=03.8000} optimizing loss=05.2405 diff={max=08.1873, min=00.0405, mean=01.4098} policy_loss=-10.5487 policy updated! \n",
      "train step 09232 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4991 diff={max=12.0414, min=00.0054, mean=01.3399} policy_loss=-11.0557 policy updated! \n",
      "train step 09233 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=07.2602 diff={max=10.9437, min=00.0017, mean=01.5747} policy_loss=-12.3188 policy updated! \n",
      "train step 09234 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4695 diff={max=08.0405, min=00.0003, mean=01.2670} policy_loss=-11.9752 policy updated! \n",
      "train step 09235 reward={max=06.0000, min=00.0000, mean=02.2000} optimizing loss=02.1851 diff={max=05.3271, min=00.0145, mean=00.8390} policy_loss=-8.9080 policy updated! \n",
      "train step 09236 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4340 diff={max=09.9067, min=00.0068, mean=01.4354} policy_loss=-9.7628 policy updated! \n",
      "train step 09237 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8587 diff={max=13.5536, min=00.0276, mean=01.3742} policy_loss=-11.1270 policy updated! \n",
      "train step 09238 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9435 diff={max=07.5596, min=00.0037, mean=01.1258} policy_loss=-10.9695 policy updated! \n",
      "train step 09239 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5644 diff={max=15.1533, min=00.0082, mean=01.2398} policy_loss=-11.2375 policy updated! \n",
      "train step 09240 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.6771 diff={max=09.9453, min=00.0176, mean=01.2941} policy_loss=-9.9013 policy updated! \n",
      "train step 09241 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=06.4525 diff={max=11.0927, min=00.0073, mean=01.5243} policy_loss=-11.4805 policy updated! \n",
      "train step 09242 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4531 diff={max=08.5039, min=00.0305, mean=01.1246} policy_loss=-10.2923 policy updated! \n",
      "train step 09243 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.9332 diff={max=09.2370, min=00.0003, mean=01.7665} policy_loss=-11.4286 policy updated! \n",
      "train step 09244 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8859 diff={max=06.6155, min=00.0006, mean=01.1320} policy_loss=-9.4396 policy updated! \n",
      "train step 09245 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.5067 diff={max=04.5055, min=00.0089, mean=01.1413} policy_loss=-10.7561 policy updated! \n",
      "train step 09246 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=04.0315 diff={max=08.6614, min=00.0046, mean=01.1648} policy_loss=-12.6932 policy updated! \n",
      "train step 09247 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1063 diff={max=03.6042, min=00.0735, mean=00.7657} policy_loss=-9.4549 policy updated! \n",
      "train step 09248 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.5491 diff={max=11.0875, min=00.0305, mean=01.4967} policy_loss=-10.9711 policy updated! \n",
      "train step 09249 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3931 diff={max=12.4165, min=00.0050, mean=01.3415} policy_loss=-13.4562 policy updated! \n",
      "train step 09250 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=12.7512 diff={max=13.5472, min=00.0099, mean=01.9333} policy_loss=-10.2580 policy updated! \n",
      "train step 09251 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.1502 diff={max=04.2185, min=00.0045, mean=01.0493} policy_loss=-10.9936 policy updated! \n",
      "train step 09252 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.6642 diff={max=15.1693, min=00.0209, mean=01.4082} policy_loss=-10.1205 policy updated! \n",
      "train step 09253 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.6480 diff={max=13.7757, min=00.0091, mean=01.6336} policy_loss=-12.1933 policy updated! \n",
      "train step 09254 reward={max=07.0000, min=00.0000, mean=05.2000} optimizing loss=03.4125 diff={max=07.3717, min=00.0425, mean=01.0990} policy_loss=-11.1651 policy updated! \n",
      "train step 09255 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.7051 diff={max=07.8222, min=00.0911, mean=00.9664} policy_loss=-9.5664 policy updated! \n",
      "train step 09256 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.1139 diff={max=06.5694, min=00.0053, mean=01.1915} policy_loss=-12.4069 policy updated! \n",
      "train step 09257 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2837 diff={max=10.6885, min=00.0268, mean=01.3845} policy_loss=-12.0896 policy updated! \n",
      "train step 09258 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=02.8923 diff={max=06.7696, min=00.0449, mean=01.0862} policy_loss=-10.1363 policy updated! \n",
      "train step 09259 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=04.3652 diff={max=06.2174, min=00.1321, mean=01.4904} policy_loss=-11.9000 policy updated! \n",
      "train step 09260 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.3954 diff={max=07.2818, min=00.0009, mean=01.1140} policy_loss=-11.9881 policy updated! \n",
      "train step 09261 reward={max=06.0000, min=00.0000, mean=03.2000} optimizing loss=01.9683 diff={max=06.8093, min=00.0108, mean=00.8944} policy_loss=-8.4636 policy updated! \n",
      "train step 09262 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6138 diff={max=07.1224, min=00.0292, mean=01.3128} policy_loss=-11.9408 policy updated! \n",
      "train step 09263 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=04.4557 diff={max=08.1812, min=00.0468, mean=01.3642} policy_loss=-11.9182 policy updated! \n",
      "train step 09264 reward={max=06.0000, min=00.0000, mean=04.2000} optimizing loss=02.4751 diff={max=06.9980, min=00.0141, mean=00.8944} policy_loss=-10.3679 policy updated! \n",
      "train step 09265 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.4348 diff={max=06.2106, min=00.0302, mean=01.0356} policy_loss=-9.8833 policy updated! \n",
      "train step 09266 reward={max=07.0000, min=00.0000, mean=04.0000} optimizing loss=06.5196 diff={max=09.2366, min=00.0013, mean=01.5276} policy_loss=-10.3724 policy updated! \n",
      "train step 09267 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0053 diff={max=08.0263, min=00.0005, mean=01.0942} policy_loss=-10.6589 policy updated! \n",
      "train step 09268 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.1150 diff={max=08.9430, min=00.0182, mean=01.2331} policy_loss=-11.0399 policy updated! \n",
      "train step 09269 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5862 diff={max=10.6897, min=00.0140, mean=01.5265} policy_loss=-9.9147 policy updated! \n",
      "train step 09270 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=05.1550 diff={max=07.3793, min=00.0247, mean=01.5379} policy_loss=-11.6416 policy updated! \n",
      "train step 09271 reward={max=07.0000, min=00.0000, mean=04.0000} optimizing loss=07.4651 diff={max=09.3277, min=00.0132, mean=01.8085} policy_loss=-13.8033 policy updated! \n",
      "train step 09272 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9060 diff={max=11.1013, min=00.0418, mean=01.3491} policy_loss=-11.7327 policy updated! \n",
      "train step 09273 reward={max=06.0000, min=00.0000, mean=02.2000} optimizing loss=05.0124 diff={max=08.3961, min=00.0307, mean=01.3477} policy_loss=-10.9730 policy updated! \n",
      "train step 09274 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3924 diff={max=08.0600, min=00.0258, mean=01.4121} policy_loss=-10.8083 policy updated! \n",
      "train step 09275 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.3260 diff={max=05.0478, min=00.0077, mean=01.0236} policy_loss=-11.2270 policy updated! \n",
      "train step 09276 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6673 diff={max=12.1361, min=00.0137, mean=01.4496} policy_loss=-11.0008 policy updated! \n",
      "train step 09277 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2154 diff={max=07.9476, min=00.0079, mean=01.3541} policy_loss=-10.9214 policy updated! \n",
      "train step 09278 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=03.3183 diff={max=04.9683, min=00.0169, mean=01.2216} policy_loss=-11.0103 policy updated! \n",
      "train step 09279 reward={max=07.0000, min=00.0000, mean=04.8000} optimizing loss=05.4060 diff={max=10.4871, min=00.0081, mean=01.3528} policy_loss=-10.6339 policy updated! \n",
      "train step 09280 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=07.2209 diff={max=10.8906, min=00.0018, mean=01.4622} policy_loss=-10.8025 policy updated! \n",
      "train step 09281 reward={max=07.0000, min=00.0000, mean=03.6000} optimizing loss=03.8282 diff={max=10.1945, min=00.0473, mean=01.1568} policy_loss=-11.7145 policy updated! \n",
      "train step 09282 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6691 diff={max=09.7002, min=00.0068, mean=01.4259} policy_loss=-9.1560 policy updated! \n",
      "train step 09283 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1170 diff={max=09.4323, min=00.0244, mean=01.2371} policy_loss=-8.3893 policy updated! \n",
      "train step 09284 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=07.4626 diff={max=12.6483, min=00.0460, mean=01.3629} policy_loss=-9.4486 policy updated! \n",
      "train step 09285 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.5528 diff={max=06.7750, min=00.0267, mean=01.2610} policy_loss=-13.5328 policy updated! \n",
      "train step 09286 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4533 diff={max=04.8421, min=00.0775, mean=01.2942} policy_loss=-9.8545 policy updated! \n",
      "train step 09287 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1677 diff={max=05.2338, min=00.0156, mean=00.9773} policy_loss=-9.8282 policy updated! \n",
      "train step 09288 reward={max=07.0000, min=00.0000, mean=02.6000} optimizing loss=03.3464 diff={max=05.2889, min=00.0207, mean=01.2714} policy_loss=-12.0352 policy updated! \n",
      "train step 09289 reward={max=07.0000, min=00.0000, mean=05.0000} optimizing loss=05.3206 diff={max=12.2178, min=00.0021, mean=01.4226} policy_loss=-12.0664 policy updated! \n",
      "train step 09290 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.5301 diff={max=08.3999, min=00.0005, mean=01.2628} policy_loss=-11.6860 policy updated! \n",
      "train step 09291 reward={max=07.0000, min=00.0000, mean=05.4000} optimizing loss=02.7964 diff={max=07.5053, min=00.0330, mean=01.0363} policy_loss=-10.8207 policy updated! \n",
      "train step 09292 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8422 diff={max=09.6139, min=00.0476, mean=01.3513} policy_loss=-10.6175 policy updated! \n",
      "train step 09293 reward={max=07.0000, min=00.0000, mean=02.4000} optimizing loss=05.9809 diff={max=08.5224, min=00.0302, mean=01.5412} policy_loss=-10.8518 policy updated! \n",
      "train step 09294 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=04.4740 diff={max=09.3449, min=00.0217, mean=01.1750} policy_loss=-11.0421 policy updated! \n",
      "train step 09295 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.5298 diff={max=08.9979, min=00.0012, mean=01.0842} policy_loss=-10.4088 policy updated! \n",
      "train step 09296 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8523 diff={max=09.5229, min=00.0030, mean=01.3810} policy_loss=-12.4538 policy updated! \n",
      "train step 09297 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.1172 diff={max=12.9369, min=00.0050, mean=01.8151} policy_loss=-11.7805 policy updated! \n",
      "train step 09298 reward={max=07.0000, min=00.0000, mean=02.6000} optimizing loss=03.7252 diff={max=08.4719, min=00.0226, mean=01.0753} policy_loss=-8.8926 policy updated! \n",
      "train step 09299 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=05.3948 diff={max=09.7259, min=00.0141, mean=01.3434} policy_loss=-13.2303 policy updated! \n",
      "train step 09300 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=04.7395 diff={max=06.6825, min=00.0012, mean=01.4024} policy_loss=-12.3472 policy updated! \n",
      "train step 09301 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3096 diff={max=05.3691, min=00.0188, mean=00.9575} policy_loss=-11.9294 policy updated! \n",
      "train step 09302 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7493 diff={max=07.7781, min=00.0021, mean=01.3943} policy_loss=-12.7264 policy updated! \n",
      "train step 09303 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=02.3730 diff={max=04.7659, min=00.0092, mean=01.0113} policy_loss=-9.3795 policy updated! \n",
      "train step 09304 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8630 diff={max=12.7736, min=00.0072, mean=01.3647} policy_loss=-12.3928 policy updated! \n",
      "train step 09305 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=04.3709 diff={max=07.4035, min=00.0648, mean=01.3516} policy_loss=-13.3551 policy updated! \n",
      "train step 09306 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3899 diff={max=08.8920, min=00.0186, mean=01.5120} policy_loss=-13.7579 policy updated! \n",
      "train step 09307 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.1500 diff={max=05.4889, min=00.0018, mean=00.9785} policy_loss=-10.4032 policy updated! \n",
      "train step 09308 reward={max=06.0000, min=00.0000, mean=02.2000} optimizing loss=07.2522 diff={max=11.0523, min=00.0062, mean=01.6629} policy_loss=-11.3980 policy updated! \n",
      "train step 09309 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9349 diff={max=08.2551, min=00.0233, mean=01.5673} policy_loss=-10.5065 policy updated! \n",
      "train step 09310 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.2607 diff={max=11.5416, min=00.0513, mean=01.6030} policy_loss=-10.7501 policy updated! \n",
      "train step 09311 reward={max=06.0000, min=00.0000, mean=02.2000} optimizing loss=09.9535 diff={max=10.2384, min=00.0026, mean=01.8418} policy_loss=-9.0432 policy updated! \n",
      "train step 09312 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0750 diff={max=05.7556, min=00.0011, mean=01.0721} policy_loss=-10.2517 policy updated! \n",
      "train step 09313 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=05.7186 diff={max=07.5986, min=00.0678, mean=01.5055} policy_loss=-12.0653 policy updated! \n",
      "train step 09314 reward={max=06.0000, min=00.0000, mean=04.2000} optimizing loss=04.8637 diff={max=08.4758, min=00.0090, mean=01.3232} policy_loss=-11.6183 policy updated! \n",
      "train step 09315 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=05.1469 diff={max=09.4828, min=00.0042, mean=01.3584} policy_loss=-12.9601 policy updated! \n",
      "train step 09316 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.0628 diff={max=11.5625, min=00.0069, mean=01.7875} policy_loss=-12.4831 policy updated! \n",
      "train step 09317 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9560 diff={max=10.9281, min=00.0034, mean=01.6873} policy_loss=-13.8494 policy updated! \n",
      "train step 09318 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=04.0805 diff={max=07.0897, min=00.0070, mean=01.4353} policy_loss=-11.9829 policy updated! \n",
      "train step 09319 reward={max=07.0000, min=00.0000, mean=05.4000} optimizing loss=03.2415 diff={max=05.7458, min=00.0439, mean=01.3335} policy_loss=-13.1548 policy updated! \n",
      "train step 09320 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.9511 diff={max=07.1110, min=00.0072, mean=01.0357} policy_loss=-9.8482 policy updated! \n",
      "train step 09321 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2965 diff={max=08.1140, min=00.0141, mean=01.4522} policy_loss=-12.5477 policy updated! \n",
      "train step 09322 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8500 diff={max=11.2824, min=00.0117, mean=01.2813} policy_loss=-10.7457 policy updated! \n",
      "train step 09323 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0528 diff={max=09.9202, min=00.0031, mean=01.4743} policy_loss=-10.0685 policy updated! \n",
      "train step 09324 reward={max=07.0000, min=00.0000, mean=05.4000} optimizing loss=03.0592 diff={max=07.2191, min=00.0342, mean=01.1032} policy_loss=-10.4950 policy updated! \n",
      "train step 09325 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.0376 diff={max=07.7336, min=00.0063, mean=01.2816} policy_loss=-10.3831 policy updated! \n",
      "train step 09326 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.3927 diff={max=16.7427, min=00.0495, mean=01.7722} policy_loss=-11.0536 policy updated! \n",
      "train step 09327 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8592 diff={max=08.6147, min=00.0004, mean=01.2044} policy_loss=-9.3237 policy updated! \n",
      "train step 09328 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2042 diff={max=10.2058, min=00.0126, mean=01.4827} policy_loss=-11.9637 policy updated! \n",
      "train step 09329 reward={max=06.0000, min=00.0000, mean=03.2000} optimizing loss=06.9517 diff={max=13.4515, min=00.0055, mean=01.5965} policy_loss=-11.8014 policy updated! \n",
      "train step 09330 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=08.7735 diff={max=09.0244, min=00.0515, mean=01.9602} policy_loss=-11.0679 policy updated! \n",
      "train step 09331 reward={max=07.0000, min=00.0000, mean=04.0000} optimizing loss=05.2115 diff={max=08.1776, min=00.0011, mean=01.5186} policy_loss=-11.4413 policy updated! \n",
      "train step 09332 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2512 diff={max=07.7640, min=00.0053, mean=01.3585} policy_loss=-13.8271 policy updated! \n",
      "train step 09333 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.4261 diff={max=14.6061, min=00.0298, mean=01.5796} policy_loss=-10.4423 policy updated! \n",
      "train step 09334 reward={max=06.0000, min=00.0000, mean=04.6000} optimizing loss=03.0382 diff={max=07.7733, min=00.0123, mean=01.0184} policy_loss=-11.3776 policy updated! \n",
      "train step 09335 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.8417 diff={max=09.1300, min=00.0410, mean=01.2701} policy_loss=-11.2245 policy updated! \n",
      "train step 09336 reward={max=06.0000, min=00.0000, mean=03.4000} optimizing loss=04.5173 diff={max=07.6057, min=00.0016, mean=01.3229} policy_loss=-10.4019 policy updated! \n",
      "train step 09337 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3092 diff={max=09.6961, min=00.0152, mean=01.3691} policy_loss=-9.6899 policy updated! \n",
      "train step 09338 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.1436 diff={max=12.0755, min=00.0200, mean=01.7678} policy_loss=-9.1219 policy updated! \n",
      "train step 09339 reward={max=07.0000, min=00.0000, mean=05.0000} optimizing loss=04.7798 diff={max=08.3488, min=00.0283, mean=01.3195} policy_loss=-9.2085 policy updated! \n",
      "train step 09340 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=05.8753 diff={max=07.4221, min=00.0217, mean=01.5699} policy_loss=-10.9942 policy updated! \n",
      "train step 09341 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6851 diff={max=07.9992, min=00.0079, mean=01.4052} policy_loss=-10.6480 policy updated! \n",
      "train step 09342 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7157 diff={max=04.6983, min=00.0337, mean=01.2442} policy_loss=-11.4897 policy updated! \n",
      "train step 09343 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.8911 diff={max=13.6387, min=00.0193, mean=01.6086} policy_loss=-11.0049 policy updated! \n",
      "train step 09344 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=02.2826 diff={max=05.3527, min=00.0071, mean=01.0069} policy_loss=-12.5287 policy updated! \n",
      "train step 09345 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.8487 diff={max=05.4694, min=00.0095, mean=01.1199} policy_loss=-9.5673 policy updated! \n",
      "train step 09346 reward={max=07.0000, min=00.0000, mean=03.4000} optimizing loss=06.9046 diff={max=10.3787, min=00.0053, mean=01.5374} policy_loss=-11.4242 policy updated! \n",
      "train step 09347 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.9439 diff={max=13.9447, min=00.0056, mean=01.9543} policy_loss=-10.7623 policy updated! \n",
      "train step 09348 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.2762 diff={max=11.4827, min=00.0005, mean=01.6607} policy_loss=-9.2782 policy updated! \n",
      "train step 09349 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0010 diff={max=10.8451, min=00.0371, mean=01.4839} policy_loss=-12.2288 policy updated! \n",
      "train step 09350 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.1592 diff={max=05.5573, min=00.0175, mean=01.2249} policy_loss=-10.9241 policy updated! \n",
      "train step 09351 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0638 diff={max=12.4668, min=00.0186, mean=01.6180} policy_loss=-13.3024 policy updated! \n",
      "train step 09352 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.8129 diff={max=14.7751, min=00.0367, mean=01.5484} policy_loss=-12.2729 policy updated! \n",
      "train step 09353 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9947 diff={max=08.4650, min=00.0095, mean=01.1859} policy_loss=-10.5421 policy updated! \n",
      "train step 09354 reward={max=07.0000, min=00.0000, mean=05.0000} optimizing loss=02.2327 diff={max=04.0726, min=00.0277, mean=01.1502} policy_loss=-10.6422 policy updated! \n",
      "train step 09355 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.8724 diff={max=03.6869, min=00.0031, mean=00.9754} policy_loss=-8.8635 policy updated! \n",
      "train step 09356 reward={max=06.0000, min=00.0000, mean=03.4000} optimizing loss=05.4377 diff={max=09.6087, min=00.0101, mean=01.4421} policy_loss=-9.9939 policy updated! \n",
      "train step 09357 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.6039 diff={max=09.9833, min=00.0203, mean=01.7595} policy_loss=-10.4681 policy updated! \n",
      "train step 09358 reward={max=06.0000, min=00.0000, mean=02.2000} optimizing loss=10.1378 diff={max=13.4819, min=00.0244, mean=01.7319} policy_loss=-11.3865 policy updated! \n",
      "train step 09359 reward={max=07.0000, min=00.0000, mean=04.8000} optimizing loss=05.3411 diff={max=09.4852, min=00.0016, mean=01.3439} policy_loss=-11.0171 policy updated! \n",
      "train step 09360 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=05.7387 diff={max=08.8425, min=00.0057, mean=01.5397} policy_loss=-11.3510 policy updated! \n",
      "train step 09361 reward={max=07.0000, min=00.0000, mean=04.8000} optimizing loss=03.1665 diff={max=05.8565, min=00.0050, mean=01.1797} policy_loss=-11.5935 policy updated! \n",
      "train step 09362 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6417 diff={max=06.6232, min=00.0276, mean=01.0645} policy_loss=-13.9338 policy updated! \n",
      "train step 09363 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.9993 diff={max=13.5130, min=00.0418, mean=01.4591} policy_loss=-11.7332 policy updated! \n",
      "train step 09364 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0332 diff={max=05.3134, min=00.1002, mean=01.2500} policy_loss=-10.4583 policy updated! \n",
      "train step 09365 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.9932 diff={max=05.0447, min=00.0109, mean=00.9458} policy_loss=-10.6356 policy updated! \n",
      "train step 09366 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.4283 diff={max=10.5390, min=00.0074, mean=02.2328} policy_loss=-14.0861 policy updated! \n",
      "train step 09367 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6058 diff={max=05.2315, min=00.0425, mean=01.0668} policy_loss=-10.1967 policy updated! \n",
      "train step 09368 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9919 diff={max=06.3739, min=00.0000, mean=01.2407} policy_loss=-12.1079 policy updated! \n",
      "train step 09369 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8723 diff={max=06.8709, min=00.0127, mean=01.0143} policy_loss=-10.8358 policy updated! \n",
      "train step 09370 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.8854 diff={max=07.6240, min=00.0010, mean=01.0202} policy_loss=-9.0926 policy updated! \n",
      "train step 09371 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=06.6332 diff={max=12.2229, min=00.0295, mean=01.4428} policy_loss=-8.5323 policy updated! \n",
      "train step 09372 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4591 diff={max=07.2031, min=00.0027, mean=01.1603} policy_loss=-9.8704 policy updated! \n",
      "train step 09373 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=04.4344 diff={max=08.1650, min=00.0168, mean=01.2655} policy_loss=-11.1184 policy updated! \n",
      "train step 09374 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0328 diff={max=05.7102, min=00.0164, mean=01.1460} policy_loss=-7.6283 policy updated! \n",
      "train step 09375 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.0348 diff={max=06.6503, min=00.0000, mean=01.4967} policy_loss=-12.8461 policy updated! \n",
      "train step 09376 reward={max=07.0000, min=00.0000, mean=03.8000} optimizing loss=04.3613 diff={max=06.7083, min=00.0122, mean=01.4021} policy_loss=-11.0754 policy updated! \n",
      "train step 09377 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.9975 diff={max=10.4010, min=00.0384, mean=01.7889} policy_loss=-12.0053 policy updated! \n",
      "train step 09378 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=05.8827 diff={max=05.9047, min=00.0461, mean=01.7961} policy_loss=-13.6927 policy updated! \n",
      "train step 09379 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=02.7910 diff={max=04.9799, min=00.0073, mean=01.1877} policy_loss=-9.8505 policy updated! \n",
      "train step 09380 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=05.5231 diff={max=06.9614, min=00.0114, mean=01.5255} policy_loss=-11.7896 policy updated! \n",
      "train step 09381 reward={max=05.0000, min=05.0000, mean=05.0000} optimizing loss=04.1792 diff={max=07.3359, min=00.0124, mean=01.3453} policy_loss=-10.4290 policy updated! \n",
      "train step 09382 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7104 diff={max=08.1993, min=00.0323, mean=01.0960} policy_loss=-10.7679 policy updated! \n",
      "train step 09383 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3200 diff={max=08.1867, min=00.0068, mean=01.2650} policy_loss=-11.7759 policy updated! \n",
      "train step 09384 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=03.1827 diff={max=06.6987, min=00.0014, mean=00.9601} policy_loss=-7.2528 policy updated! \n",
      "train step 09385 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.2868 diff={max=08.1307, min=00.0064, mean=01.4774} policy_loss=-11.3446 policy updated! \n",
      "train step 09386 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1726 diff={max=06.6398, min=00.0097, mean=01.4745} policy_loss=-10.4306 policy updated! \n",
      "train step 09387 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5531 diff={max=06.0932, min=00.0012, mean=01.0082} policy_loss=-9.7165 policy updated! \n",
      "train step 09388 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5185 diff={max=07.4271, min=00.0197, mean=00.8602} policy_loss=-9.9489 policy updated! \n",
      "train step 09389 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=03.6028 diff={max=08.4780, min=00.0696, mean=01.1153} policy_loss=-11.2255 policy updated! \n",
      "train step 09390 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=05.4770 diff={max=08.0208, min=00.0119, mean=01.4172} policy_loss=-10.2899 policy updated! \n",
      "train step 09391 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4220 diff={max=07.9170, min=00.0578, mean=01.1130} policy_loss=-10.3112 policy updated! \n",
      "train step 09392 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0860 diff={max=08.6239, min=00.0140, mean=01.4620} policy_loss=-10.8081 policy updated! \n",
      "train step 09393 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1163 diff={max=07.8507, min=00.0072, mean=01.8169} policy_loss=-12.8469 policy updated! \n",
      "train step 09394 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=04.0352 diff={max=07.3858, min=00.0057, mean=01.2823} policy_loss=-12.8744 policy updated! \n",
      "train step 09395 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.0391 diff={max=12.2005, min=00.0475, mean=01.1838} policy_loss=-9.8462 policy updated! \n",
      "train step 09396 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0521 diff={max=04.2890, min=00.0469, mean=01.0290} policy_loss=-10.9441 policy updated! \n",
      "train step 09397 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4801 diff={max=09.2417, min=00.0068, mean=01.4804} policy_loss=-11.0271 policy updated! \n",
      "train step 09398 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.9775 diff={max=05.8870, min=00.0180, mean=01.1499} policy_loss=-11.6612 policy updated! \n",
      "train step 09399 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1669 diff={max=11.3587, min=00.0003, mean=01.5671} policy_loss=-12.5561 policy updated! \n",
      "train step 09400 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=05.1751 diff={max=08.1327, min=00.0038, mean=01.4053} policy_loss=-13.0615 policy updated! \n",
      "train step 09401 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=04.5565 diff={max=08.4573, min=00.0107, mean=01.1288} policy_loss=-11.5584 policy updated! \n",
      "train step 09402 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6299 diff={max=07.6719, min=00.0230, mean=01.2727} policy_loss=-12.3840 policy updated! \n",
      "train step 09403 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1081 diff={max=04.2272, min=00.0110, mean=01.0085} policy_loss=-9.8660 policy updated! \n",
      "train step 09404 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5661 diff={max=07.2803, min=00.0205, mean=01.5420} policy_loss=-11.7976 policy updated! \n",
      "train step 09405 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=03.6841 diff={max=06.8538, min=00.0132, mean=01.2104} policy_loss=-9.7058 policy updated! \n",
      "train step 09406 reward={max=07.0000, min=00.0000, mean=05.0000} optimizing loss=04.9356 diff={max=08.3467, min=00.0097, mean=01.4180} policy_loss=-11.5976 policy updated! \n",
      "train step 09407 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9401 diff={max=07.9760, min=00.0067, mean=01.0777} policy_loss=-10.7450 policy updated! \n",
      "train step 09408 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=09.5703 diff={max=15.9516, min=00.0131, mean=01.5424} policy_loss=-11.4806 policy updated! \n",
      "train step 09409 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3400 diff={max=04.6341, min=00.0202, mean=01.0519} policy_loss=-11.2783 policy updated! \n",
      "train step 09410 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.8665 diff={max=07.1377, min=00.0008, mean=01.1770} policy_loss=-10.8201 policy updated! \n",
      "train step 09411 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.8001 diff={max=06.1507, min=00.0067, mean=01.1180} policy_loss=-12.3550 policy updated! \n",
      "train step 09412 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.5157 diff={max=08.4638, min=00.0005, mean=01.3628} policy_loss=-11.2575 policy updated! \n",
      "train step 09413 reward={max=08.0000, min=00.0000, mean=03.0000} optimizing loss=05.2597 diff={max=08.0965, min=00.0019, mean=01.4039} policy_loss=-9.5523 policy updated! \n",
      "train step 09414 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5015 diff={max=04.9842, min=00.0191, mean=01.0512} policy_loss=-13.1457 policy updated! \n",
      "train step 09415 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.8391 diff={max=04.5219, min=00.0078, mean=01.1546} policy_loss=-9.7347 policy updated! \n",
      "train step 09416 reward={max=07.0000, min=00.0000, mean=03.8000} optimizing loss=01.5259 diff={max=04.1136, min=00.0576, mean=00.8912} policy_loss=-10.9358 policy updated! \n",
      "train step 09417 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7966 diff={max=06.6127, min=00.0172, mean=01.3209} policy_loss=-10.1402 policy updated! \n",
      "train step 09418 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7241 diff={max=06.9751, min=00.0053, mean=01.4156} policy_loss=-9.4220 policy updated! \n",
      "train step 09419 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2431 diff={max=06.8786, min=00.0216, mean=00.9441} policy_loss=-8.9736 policy updated! \n",
      "train step 09420 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.9505 diff={max=06.7684, min=00.0093, mean=01.1021} policy_loss=-11.9133 policy updated! \n",
      "train step 09421 reward={max=05.0000, min=05.0000, mean=05.0000} optimizing loss=07.6169 diff={max=10.3134, min=00.0058, mean=01.5556} policy_loss=-12.2357 policy updated! \n",
      "train step 09422 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.0690 diff={max=06.3270, min=00.0258, mean=01.1485} policy_loss=-11.1675 policy updated! \n",
      "train step 09423 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.3915 diff={max=05.2090, min=00.0043, mean=01.0180} policy_loss=-8.6281 policy updated! \n",
      "train step 09424 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9793 diff={max=11.9240, min=00.0039, mean=01.4502} policy_loss=-11.5450 policy updated! \n",
      "train step 09425 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=03.1682 diff={max=07.0865, min=00.0072, mean=01.0078} policy_loss=-9.3358 policy updated! \n",
      "train step 09426 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.7932 diff={max=09.3286, min=00.0557, mean=01.3330} policy_loss=-14.4665 policy updated! \n",
      "train step 09427 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8611 diff={max=05.9596, min=00.0097, mean=01.1503} policy_loss=-10.0437 policy updated! \n",
      "train step 09428 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5704 diff={max=05.0692, min=00.0008, mean=01.3049} policy_loss=-13.0573 policy updated! \n",
      "train step 09429 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.0078 diff={max=21.3353, min=00.0058, mean=01.1868} policy_loss=-10.4606 policy updated! \n",
      "train step 09430 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.6746 diff={max=10.1171, min=00.0078, mean=01.0587} policy_loss=-10.9399 policy updated! \n",
      "train step 09431 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.1666 diff={max=15.2451, min=00.0102, mean=01.7287} policy_loss=-9.3463 policy updated! \n",
      "train step 09432 reward={max=07.0000, min=00.0000, mean=05.2000} optimizing loss=04.7247 diff={max=07.3933, min=00.0039, mean=01.3898} policy_loss=-9.8334 policy updated! \n",
      "train step 09433 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=04.4946 diff={max=07.8478, min=00.0095, mean=01.4236} policy_loss=-11.8310 policy updated! \n",
      "train step 09434 reward={max=06.0000, min=00.0000, mean=04.2000} optimizing loss=02.5664 diff={max=04.5802, min=00.0097, mean=01.1272} policy_loss=-10.4435 policy updated! \n",
      "train step 09435 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=08.6326 diff={max=12.8442, min=00.0005, mean=01.6463} policy_loss=-12.6098 policy updated! \n",
      "train step 09436 reward={max=07.0000, min=00.0000, mean=03.4000} optimizing loss=04.1341 diff={max=07.1399, min=00.0057, mean=01.3029} policy_loss=-11.2786 policy updated! \n",
      "train step 09437 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3065 diff={max=05.3343, min=00.0302, mean=01.1481} policy_loss=-11.1460 policy updated! \n",
      "train step 09438 reward={max=07.0000, min=00.0000, mean=02.6000} optimizing loss=03.5374 diff={max=08.1131, min=00.0230, mean=01.1095} policy_loss=-11.0958 policy updated! \n",
      "train step 09439 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5241 diff={max=06.3780, min=00.0112, mean=01.2754} policy_loss=-11.7101 policy updated! \n",
      "train step 09440 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.3232 diff={max=06.1774, min=00.0023, mean=01.3612} policy_loss=-13.0116 policy updated! \n",
      "train step 09441 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=08.5908 diff={max=10.8018, min=00.0441, mean=01.8574} policy_loss=-12.3892 policy updated! \n",
      "train step 09442 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6818 diff={max=05.4908, min=00.0415, mean=01.3715} policy_loss=-12.1528 policy updated! \n",
      "train step 09443 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1003 diff={max=09.9897, min=00.0032, mean=01.5683} policy_loss=-10.1212 policy updated! \n",
      "train step 09444 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0401 diff={max=06.5123, min=00.0581, mean=01.5029} policy_loss=-12.5004 policy updated! \n",
      "train step 09445 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.5277 diff={max=08.2209, min=00.0344, mean=01.3284} policy_loss=-11.6959 policy updated! \n",
      "train step 09446 reward={max=06.0000, min=00.0000, mean=03.2000} optimizing loss=02.3505 diff={max=06.1032, min=00.0068, mean=00.9662} policy_loss=-8.6388 policy updated! \n",
      "train step 09447 reward={max=07.0000, min=00.0000, mean=03.8000} optimizing loss=02.9658 diff={max=06.7238, min=00.0316, mean=01.0631} policy_loss=-10.6290 policy updated! \n",
      "train step 09448 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.9980 diff={max=10.2049, min=00.0542, mean=01.7471} policy_loss=-10.6320 policy updated! \n",
      "train step 09449 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5107 diff={max=06.7610, min=00.0129, mean=01.0038} policy_loss=-9.9310 policy updated! \n",
      "train step 09450 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.4627 diff={max=06.8391, min=00.0165, mean=01.1135} policy_loss=-9.1711 policy updated! \n",
      "train step 09451 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9802 diff={max=11.6861, min=00.0030, mean=01.1069} policy_loss=-10.3036 policy updated! \n",
      "train step 09452 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7214 diff={max=05.7067, min=00.0051, mean=01.5335} policy_loss=-10.9631 policy updated! \n",
      "train step 09453 reward={max=07.0000, min=00.0000, mean=02.6000} optimizing loss=03.6694 diff={max=08.1537, min=00.0097, mean=01.1891} policy_loss=-9.9333 policy updated! \n",
      "train step 09454 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0424 diff={max=09.3971, min=00.0051, mean=01.3778} policy_loss=-12.9247 policy updated! \n",
      "train step 09455 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.5718 diff={max=08.6806, min=00.0085, mean=01.5247} policy_loss=-11.7750 policy updated! \n",
      "train step 09456 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.9264 diff={max=07.0832, min=00.0124, mean=01.5026} policy_loss=-11.3707 policy updated! \n",
      "train step 09457 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5481 diff={max=10.8593, min=00.0003, mean=01.3942} policy_loss=-11.5862 policy updated! \n",
      "train step 09458 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9929 diff={max=05.0770, min=00.0087, mean=01.1245} policy_loss=-11.0494 policy updated! \n",
      "train step 09459 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.0599 diff={max=09.4760, min=00.0548, mean=01.7020} policy_loss=-12.9481 policy updated! \n",
      "train step 09460 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=02.2923 diff={max=05.8968, min=00.0058, mean=01.0670} policy_loss=-12.0406 policy updated! \n",
      "train step 09461 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.1250 diff={max=08.2994, min=00.0129, mean=01.2928} policy_loss=-9.2979 policy updated! \n",
      "train step 09462 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3985 diff={max=05.3468, min=00.0052, mean=01.0748} policy_loss=-10.2421 policy updated! \n",
      "train step 09463 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6261 diff={max=07.9237, min=00.0267, mean=01.3929} policy_loss=-10.3519 policy updated! \n",
      "train step 09464 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5436 diff={max=10.6275, min=00.0502, mean=01.1596} policy_loss=-12.9490 policy updated! \n",
      "train step 09465 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.2699 diff={max=07.4361, min=00.0431, mean=01.3817} policy_loss=-10.9593 policy updated! \n",
      "train step 09466 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5195 diff={max=04.9606, min=00.0238, mean=01.0276} policy_loss=-12.6187 policy updated! \n",
      "train step 09467 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2638 diff={max=08.0785, min=00.0039, mean=01.3411} policy_loss=-10.9172 policy updated! \n",
      "train step 09468 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.7105 diff={max=09.2566, min=00.0139, mean=01.1833} policy_loss=-12.9957 policy updated! \n",
      "train step 09469 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7015 diff={max=12.5316, min=00.0070, mean=01.4241} policy_loss=-12.5807 policy updated! \n",
      "train step 09470 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.6906 diff={max=06.5031, min=00.0038, mean=01.1249} policy_loss=-12.0310 policy updated! \n",
      "train step 09471 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6976 diff={max=05.6908, min=00.0247, mean=01.2139} policy_loss=-10.8910 policy updated! \n",
      "train step 09472 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=07.2720 diff={max=10.4044, min=00.0047, mean=01.6424} policy_loss=-13.5964 policy updated! \n",
      "train step 09473 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.3968 diff={max=10.2550, min=00.0357, mean=01.4243} policy_loss=-11.5747 policy updated! \n",
      "train step 09474 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2603 diff={max=06.6465, min=00.0120, mean=01.3595} policy_loss=-14.2226 policy updated! \n",
      "train step 09475 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.4326 diff={max=05.3411, min=00.0024, mean=01.2545} policy_loss=-11.5683 policy updated! \n",
      "train step 09476 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.7112 diff={max=06.8236, min=00.0281, mean=01.3484} policy_loss=-12.1559 policy updated! \n",
      "train step 09477 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8644 diff={max=09.3862, min=00.0116, mean=01.1467} policy_loss=-12.5972 policy updated! \n",
      "train step 09478 reward={max=07.0000, min=00.0000, mean=02.4000} optimizing loss=04.9156 diff={max=08.2278, min=00.0285, mean=01.3440} policy_loss=-10.7144 policy updated! \n",
      "train step 09479 reward={max=07.0000, min=00.0000, mean=05.4000} optimizing loss=06.7638 diff={max=12.9450, min=00.0131, mean=01.3731} policy_loss=-12.8513 policy updated! \n",
      "train step 09480 reward={max=07.0000, min=06.0000, mean=06.2000} optimizing loss=06.7346 diff={max=08.0124, min=00.0182, mean=01.6135} policy_loss=-14.1106 policy updated! \n",
      "train step 09481 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.9429 diff={max=08.7611, min=00.0155, mean=01.1716} policy_loss=-10.6993 policy updated! \n",
      "train step 09482 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.8623 diff={max=02.8372, min=00.0048, mean=00.6595} policy_loss=-11.9505 policy updated! \n",
      "train step 09483 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7986 diff={max=07.0026, min=00.0215, mean=01.4106} policy_loss=-12.7852 policy updated! \n",
      "train step 09484 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.6296 diff={max=13.8315, min=00.0178, mean=01.7070} policy_loss=-11.9093 policy updated! \n",
      "train step 09485 reward={max=07.0000, min=00.0000, mean=03.6000} optimizing loss=10.6080 diff={max=21.0536, min=00.0160, mean=01.2818} policy_loss=-11.6308 policy updated! \n",
      "train step 09486 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1297 diff={max=04.5183, min=00.0024, mean=00.7480} policy_loss=-10.0841 policy updated! \n",
      "train step 09487 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9407 diff={max=08.1401, min=00.0105, mean=01.2357} policy_loss=-12.0580 policy updated! \n",
      "train step 09488 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.5186 diff={max=10.0610, min=00.0014, mean=01.0664} policy_loss=-12.7373 policy updated! \n",
      "train step 09489 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=10.5989 diff={max=14.4588, min=00.0074, mean=01.5490} policy_loss=-10.9751 policy updated! \n",
      "train step 09490 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.7442 diff={max=14.2739, min=00.0268, mean=01.4403} policy_loss=-9.6555 policy updated! \n",
      "train step 09491 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3164 diff={max=07.8386, min=00.0095, mean=01.2551} policy_loss=-10.7036 policy updated! \n",
      "train step 09492 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8144 diff={max=05.8160, min=00.0026, mean=01.0841} policy_loss=-10.4849 policy updated! \n",
      "train step 09493 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.8042 diff={max=12.0781, min=00.0004, mean=01.2728} policy_loss=-11.2318 policy updated! \n",
      "train step 09494 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8279 diff={max=09.2418, min=00.0147, mean=01.3848} policy_loss=-9.9158 policy updated! \n",
      "train step 09495 reward={max=06.0000, min=06.0000, mean=06.0000} optimizing loss=07.8943 diff={max=12.0921, min=00.0210, mean=01.7253} policy_loss=-11.2758 policy updated! \n",
      "train step 09496 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6771 diff={max=07.9974, min=00.0019, mean=01.1444} policy_loss=-10.1774 policy updated! \n",
      "train step 09497 reward={max=05.0000, min=00.0000, mean=02.0000} optimizing loss=04.6198 diff={max=07.0727, min=00.0660, mean=01.3621} policy_loss=-10.9749 policy updated! \n",
      "train step 09498 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9988 diff={max=07.6031, min=00.0055, mean=01.2409} policy_loss=-11.8366 policy updated! \n",
      "train step 09499 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.2308 diff={max=09.3421, min=00.0126, mean=01.7914} policy_loss=-10.9273 policy updated! \n",
      "train step 09500 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.1794 diff={max=06.9894, min=00.0358, mean=01.2778} policy_loss=-11.8629 policy updated! \n",
      "train step 09501 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3256 diff={max=10.6130, min=00.0391, mean=01.7706} policy_loss=-12.5838 policy updated! \n",
      "train step 09502 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3566 diff={max=09.5297, min=00.0031, mean=01.4880} policy_loss=-11.8562 policy updated! \n",
      "train step 09503 reward={max=07.0000, min=00.0000, mean=04.0000} optimizing loss=03.3214 diff={max=07.3810, min=00.0144, mean=01.1289} policy_loss=-11.3831 policy updated! \n",
      "train step 09504 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.2541 diff={max=14.6871, min=00.0012, mean=01.4676} policy_loss=-8.7429 policy updated! \n",
      "train step 09505 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=01.8064 diff={max=04.4856, min=00.0029, mean=00.8353} policy_loss=-9.1207 policy updated! \n",
      "train step 09506 reward={max=08.0000, min=00.0000, mean=04.4000} optimizing loss=08.1986 diff={max=11.3849, min=00.0052, mean=01.6395} policy_loss=-10.5539 policy updated! \n",
      "train step 09507 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4755 diff={max=05.0908, min=00.0129, mean=01.2998} policy_loss=-10.4475 policy updated! \n",
      "train step 09508 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3109 diff={max=08.1994, min=00.0195, mean=01.1144} policy_loss=-10.8045 policy updated! \n",
      "train step 09509 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6008 diff={max=07.1917, min=00.0139, mean=01.2524} policy_loss=-11.6547 policy updated! \n",
      "train step 09510 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.5998 diff={max=09.6658, min=00.0469, mean=01.3268} policy_loss=-11.5753 policy updated! \n",
      "train step 09511 reward={max=06.0000, min=00.0000, mean=03.2000} optimizing loss=07.6976 diff={max=09.5138, min=00.0005, mean=01.5434} policy_loss=-10.6402 policy updated! \n",
      "train step 09512 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4159 diff={max=11.4536, min=00.0063, mean=01.3418} policy_loss=-8.5111 policy updated! \n",
      "train step 09513 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=11.6752 diff={max=16.8922, min=00.0085, mean=01.7902} policy_loss=-14.5988 policy updated! \n",
      "train step 09514 reward={max=07.0000, min=00.0000, mean=04.8000} optimizing loss=05.9899 diff={max=10.2157, min=00.0731, mean=01.4243} policy_loss=-11.8352 policy updated! \n",
      "train step 09515 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.2070 diff={max=09.9309, min=00.0088, mean=01.6330} policy_loss=-14.5971 policy updated! \n",
      "train step 09516 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6134 diff={max=08.7670, min=00.0087, mean=01.4678} policy_loss=-12.6426 policy updated! \n",
      "train step 09517 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6287 diff={max=07.7087, min=00.0084, mean=00.9645} policy_loss=-10.4113 policy updated! \n",
      "train step 09518 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3503 diff={max=05.6655, min=00.0123, mean=01.2685} policy_loss=-12.0338 policy updated! \n",
      "train step 09519 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.9092 diff={max=08.6676, min=00.0273, mean=01.2831} policy_loss=-11.4682 policy updated! \n",
      "train step 09520 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.6579 diff={max=06.9709, min=00.0163, mean=01.3302} policy_loss=-11.9274 policy updated! \n",
      "train step 09521 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.1741 diff={max=19.3871, min=00.0062, mean=01.4624} policy_loss=-11.5819 policy updated! \n",
      "train step 09522 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.1220 diff={max=14.6124, min=00.0146, mean=01.6685} policy_loss=-12.8817 policy updated! \n",
      "train step 09523 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1872 diff={max=12.1001, min=00.0018, mean=01.3733} policy_loss=-11.0713 policy updated! \n",
      "train step 09524 reward={max=05.0000, min=00.0000, mean=04.0000} optimizing loss=02.3759 diff={max=05.4729, min=00.0058, mean=00.8580} policy_loss=-10.5240 policy updated! \n",
      "train step 09525 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.1106 diff={max=07.4325, min=00.0086, mean=01.1971} policy_loss=-12.1022 policy updated! \n",
      "train step 09526 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2989 diff={max=06.1869, min=00.0020, mean=01.1021} policy_loss=-11.2677 policy updated! \n",
      "train step 09527 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5854 diff={max=08.9365, min=00.0188, mean=01.2298} policy_loss=-12.1486 policy updated! \n",
      "train step 09528 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.4890 diff={max=08.8875, min=00.0113, mean=01.4979} policy_loss=-10.8694 policy updated! \n",
      "train step 09529 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4954 diff={max=06.7845, min=00.0002, mean=00.8137} policy_loss=-10.4481 policy updated! \n",
      "train step 09530 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.8507 diff={max=05.6667, min=00.0108, mean=00.8547} policy_loss=-9.9001 policy updated! \n",
      "train step 09531 reward={max=05.0000, min=00.0000, mean=03.0000} optimizing loss=03.0797 diff={max=05.3118, min=00.0300, mean=01.0974} policy_loss=-14.6413 policy updated! \n",
      "train step 09532 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.6265 diff={max=10.3131, min=00.0536, mean=01.8681} policy_loss=-12.9374 policy updated! \n",
      "train step 09533 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8297 diff={max=07.6558, min=00.0733, mean=01.3143} policy_loss=-8.6819 policy updated! \n",
      "train step 09534 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=08.0774 diff={max=18.2110, min=00.0011, mean=01.2227} policy_loss=-8.8874 policy updated! \n",
      "train step 09535 reward={max=07.0000, min=00.0000, mean=04.8000} optimizing loss=04.9084 diff={max=09.4064, min=00.0096, mean=01.3175} policy_loss=-10.9524 policy updated! \n",
      "train step 09536 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.9887 diff={max=14.5798, min=00.0401, mean=01.5365} policy_loss=-12.5577 policy updated! \n",
      "train step 09537 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.1986 diff={max=17.1424, min=00.0070, mean=01.9904} policy_loss=-11.6665 policy updated! \n",
      "train step 09538 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7634 diff={max=10.8577, min=00.0041, mean=01.6987} policy_loss=-11.3649 policy updated! \n",
      "train step 09539 reward={max=07.0000, min=00.0000, mean=05.2000} optimizing loss=06.8914 diff={max=10.2579, min=00.0171, mean=01.6680} policy_loss=-14.2825 policy updated! \n",
      "train step 09540 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.8941 diff={max=04.8243, min=00.0111, mean=01.1507} policy_loss=-10.2794 policy updated! \n",
      "train step 09541 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3839 diff={max=07.2094, min=00.0362, mean=01.5199} policy_loss=-11.8501 policy updated! \n",
      "train step 09542 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2107 diff={max=12.7187, min=00.0563, mean=01.2645} policy_loss=-13.2762 policy updated! \n",
      "train step 09543 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.3277 diff={max=13.5242, min=00.0127, mean=01.9267} policy_loss=-13.7805 policy updated! \n",
      "train step 09544 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0854 diff={max=08.3508, min=00.0820, mean=01.5081} policy_loss=-12.2616 policy updated! \n",
      "train step 09545 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.8744 diff={max=11.3160, min=00.0157, mean=01.5468} policy_loss=-12.8768 policy updated! \n",
      "train step 09546 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3456 diff={max=11.3800, min=00.0384, mean=01.4476} policy_loss=-10.3754 policy updated! \n",
      "train step 09547 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2236 diff={max=06.9682, min=00.0205, mean=01.3731} policy_loss=-10.1669 policy updated! \n",
      "train step 09548 reward={max=06.0000, min=00.0000, mean=02.2000} optimizing loss=02.0889 diff={max=04.1469, min=00.0015, mean=01.0597} policy_loss=-11.0436 policy updated! \n",
      "train step 09549 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3025 diff={max=08.7592, min=00.0011, mean=01.8711} policy_loss=-12.4109 policy updated! \n",
      "train step 09550 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=21.1923 diff={max=27.1109, min=00.0169, mean=02.2226} policy_loss=-11.1757 policy updated! \n",
      "train step 09551 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1903 diff={max=07.6421, min=00.0272, mean=01.4627} policy_loss=-13.5630 policy updated! \n",
      "train step 09552 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.5586 diff={max=13.3071, min=00.0012, mean=02.0769} policy_loss=-12.2013 policy updated! \n",
      "train step 09553 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6272 diff={max=06.6406, min=00.0620, mean=01.4213} policy_loss=-12.2449 policy updated! \n",
      "train step 09554 reward={max=06.0000, min=00.0000, mean=04.2000} optimizing loss=12.5468 diff={max=19.8441, min=00.0062, mean=01.7779} policy_loss=-10.9803 policy updated! \n",
      "train step 09555 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.7643 diff={max=08.1399, min=00.0031, mean=01.3879} policy_loss=-12.6653 policy updated! \n",
      "train step 09556 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2145 diff={max=07.2178, min=00.0133, mean=01.2690} policy_loss=-12.3349 policy updated! \n",
      "train step 09557 reward={max=06.0000, min=00.0000, mean=03.2000} optimizing loss=11.0999 diff={max=17.2031, min=00.0238, mean=01.6362} policy_loss=-11.8680 policy updated! \n",
      "train step 09558 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9519 diff={max=05.9872, min=00.0164, mean=01.3971} policy_loss=-11.1434 policy updated! \n",
      "train step 09559 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9852 diff={max=10.2464, min=00.0372, mean=01.6335} policy_loss=-12.4227 policy updated! \n",
      "train step 09560 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.0408 diff={max=05.4649, min=00.0067, mean=00.8790} policy_loss=-8.5329 policy updated! \n",
      "train step 09561 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.4176 diff={max=17.5672, min=00.0117, mean=01.7399} policy_loss=-12.9462 policy updated! \n",
      "train step 09562 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.6878 diff={max=06.4740, min=00.0004, mean=01.0579} policy_loss=-9.9276 policy updated! \n",
      "train step 09563 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.2395 diff={max=12.4897, min=00.0227, mean=01.6043} policy_loss=-12.9385 policy updated! \n",
      "train step 09564 reward={max=07.0000, min=00.0000, mean=05.2000} optimizing loss=05.7192 diff={max=08.4235, min=00.0248, mean=01.3331} policy_loss=-10.5795 policy updated! \n",
      "train step 09565 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.2413 diff={max=06.4721, min=00.0097, mean=01.1426} policy_loss=-11.6688 policy updated! \n",
      "train step 09566 reward={max=08.0000, min=00.0000, mean=04.4000} optimizing loss=05.0279 diff={max=06.9185, min=00.0122, mean=01.5180} policy_loss=-9.6849 policy updated! \n",
      "train step 09567 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=16.0029 diff={max=19.9894, min=00.0576, mean=02.1645} policy_loss=-12.6249 policy updated! \n",
      "train step 09568 reward={max=08.0000, min=00.0000, mean=03.0000} optimizing loss=07.4164 diff={max=16.3511, min=00.0392, mean=01.4103} policy_loss=-9.0248 policy updated! \n",
      "train step 09569 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4764 diff={max=07.8331, min=00.0675, mean=01.4823} policy_loss=-10.9048 policy updated! \n",
      "train step 09570 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.7972 diff={max=08.8218, min=00.0003, mean=01.6064} policy_loss=-9.8608 policy updated! \n",
      "train step 09571 reward={max=08.0000, min=00.0000, mean=04.8000} optimizing loss=06.5195 diff={max=09.0918, min=00.0016, mean=01.5278} policy_loss=-11.7917 policy updated! \n",
      "train step 09572 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2184 diff={max=07.0279, min=00.0017, mean=01.3714} policy_loss=-10.5131 policy updated! \n",
      "train step 09573 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2323 diff={max=06.0411, min=00.0246, mean=01.4862} policy_loss=-13.4809 policy updated! \n",
      "train step 09574 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0286 diff={max=09.4106, min=00.0339, mean=01.6250} policy_loss=-13.0204 policy updated! \n",
      "train step 09575 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=06.6854 diff={max=08.7558, min=00.0139, mean=01.6552} policy_loss=-11.7598 policy updated! \n",
      "train step 09576 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=14.9398 diff={max=22.4571, min=00.0422, mean=01.7795} policy_loss=-10.1459 policy updated! \n",
      "train step 09577 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8126 diff={max=08.1995, min=00.0102, mean=01.2811} policy_loss=-11.8015 policy updated! \n",
      "train step 09578 reward={max=07.0000, min=00.0000, mean=02.4000} optimizing loss=02.5819 diff={max=06.6305, min=00.0480, mean=01.0310} policy_loss=-11.8787 policy updated! \n",
      "train step 09579 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2725 diff={max=05.7714, min=00.0066, mean=00.9198} policy_loss=-9.1134 policy updated! \n",
      "train step 09580 reward={max=05.0000, min=00.0000, mean=01.0000} optimizing loss=07.2687 diff={max=10.3228, min=00.0154, mean=01.6042} policy_loss=-14.2901 policy updated! \n",
      "train step 09581 reward={max=06.0000, min=00.0000, mean=03.4000} optimizing loss=15.5357 diff={max=23.4412, min=00.0361, mean=01.7141} policy_loss=-10.7538 policy updated! \n",
      "train step 09582 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2495 diff={max=05.4800, min=00.0207, mean=01.2564} policy_loss=-12.1189 policy updated! \n",
      "train step 09583 reward={max=06.0000, min=00.0000, mean=02.2000} optimizing loss=05.0815 diff={max=10.6510, min=00.0464, mean=01.2887} policy_loss=-11.9204 policy updated! \n",
      "train step 09584 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3438 diff={max=05.8698, min=00.0372, mean=01.0352} policy_loss=-12.7573 policy updated! \n",
      "train step 09585 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.3775 diff={max=07.9362, min=00.0253, mean=01.4209} policy_loss=-11.9501 policy updated! \n",
      "train step 09586 reward={max=08.0000, min=00.0000, mean=05.6000} optimizing loss=03.5930 diff={max=06.0577, min=00.0883, mean=01.2255} policy_loss=-11.9157 policy updated! \n",
      "train step 09587 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8860 diff={max=06.1700, min=00.0058, mean=01.0895} policy_loss=-11.6798 policy updated! \n",
      "train step 09588 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3633 diff={max=08.1682, min=00.0004, mean=01.1975} policy_loss=-11.1436 policy updated! \n",
      "train step 09589 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9621 diff={max=07.8283, min=00.0032, mean=01.0287} policy_loss=-11.1872 policy updated! \n",
      "train step 09590 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.8596 diff={max=08.8410, min=00.0054, mean=01.2853} policy_loss=-9.7415 policy updated! \n",
      "train step 09591 reward={max=08.0000, min=00.0000, mean=04.4000} optimizing loss=06.9144 diff={max=08.2102, min=00.0242, mean=01.7760} policy_loss=-14.8585 policy updated! \n",
      "train step 09592 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0972 diff={max=04.8639, min=00.0041, mean=01.2777} policy_loss=-10.4478 policy updated! \n",
      "train step 09593 reward={max=06.0000, min=00.0000, mean=02.2000} optimizing loss=05.3632 diff={max=08.8789, min=00.0063, mean=01.4159} policy_loss=-13.2957 policy updated! \n",
      "train step 09594 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.0141 diff={max=12.7885, min=00.0189, mean=01.8431} policy_loss=-13.0274 policy updated! \n",
      "train step 09595 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=10.1852 diff={max=15.3408, min=00.0739, mean=01.4765} policy_loss=-11.6890 policy updated! \n",
      "train step 09596 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6125 diff={max=13.6747, min=00.0243, mean=01.3835} policy_loss=-10.9580 policy updated! \n",
      "train step 09597 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8690 diff={max=13.3230, min=00.1061, mean=01.4438} policy_loss=-10.9655 policy updated! \n",
      "train step 09598 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0648 diff={max=10.1118, min=00.0125, mean=01.2418} policy_loss=-9.9562 policy updated! \n",
      "train step 09599 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0926 diff={max=09.9099, min=00.0047, mean=01.4451} policy_loss=-11.3352 policy updated! \n",
      "train step 09600 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.6996 diff={max=06.2231, min=00.0686, mean=01.2978} policy_loss=-12.5804 policy updated! \n",
      "train step 09601 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5706 diff={max=07.3277, min=00.0265, mean=01.1371} policy_loss=-12.3123 policy updated! \n",
      "train step 09602 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5789 diff={max=05.1897, min=00.0096, mean=01.2916} policy_loss=-11.9857 policy updated! \n",
      "train step 09603 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0462 diff={max=04.8746, min=00.0341, mean=01.1717} policy_loss=-10.9601 policy updated! \n",
      "train step 09604 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.7080 diff={max=10.3427, min=00.0193, mean=01.2842} policy_loss=-10.7807 policy updated! \n",
      "train step 09605 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.9012 diff={max=07.0941, min=00.0077, mean=01.2049} policy_loss=-10.1507 policy updated! \n",
      "train step 09606 reward={max=07.0000, min=00.0000, mean=04.0000} optimizing loss=06.8075 diff={max=12.1169, min=00.0160, mean=01.4567} policy_loss=-12.1155 policy updated! \n",
      "train step 09607 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.6423 diff={max=13.2185, min=00.0845, mean=01.7469} policy_loss=-12.3772 policy updated! \n",
      "train step 09608 reward={max=07.0000, min=00.0000, mean=02.6000} optimizing loss=08.3841 diff={max=10.5133, min=00.0291, mean=01.8667} policy_loss=-14.0787 policy updated! \n",
      "train step 09609 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7073 diff={max=06.7598, min=00.0013, mean=01.4581} policy_loss=-14.4495 policy updated! \n",
      "train step 09610 reward={max=06.0000, min=00.0000, mean=04.8000} optimizing loss=04.2551 diff={max=06.6944, min=00.0104, mean=01.3292} policy_loss=-9.9816 policy updated! \n",
      "train step 09611 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8861 diff={max=05.3550, min=00.0629, mean=00.9541} policy_loss=-9.5610 policy updated! \n",
      "train step 09612 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2900 diff={max=08.7180, min=00.0054, mean=01.6175} policy_loss=-12.2470 policy updated! \n",
      "train step 09613 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.3863 diff={max=05.9326, min=00.0038, mean=01.4158} policy_loss=-14.1825 policy updated! \n",
      "train step 09614 reward={max=07.0000, min=00.0000, mean=05.2000} optimizing loss=03.8523 diff={max=06.2055, min=00.0149, mean=01.2764} policy_loss=-13.7890 policy updated! \n",
      "train step 09615 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.7749 diff={max=08.8928, min=00.0110, mean=01.5280} policy_loss=-11.1933 policy updated! \n",
      "train step 09616 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2488 diff={max=04.6334, min=00.0032, mean=01.2474} policy_loss=-11.8218 policy updated! \n",
      "train step 09617 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9100 diff={max=07.8438, min=00.0072, mean=01.2652} policy_loss=-11.5792 policy updated! \n",
      "train step 09618 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=10.4120 diff={max=16.2319, min=00.0069, mean=01.7286} policy_loss=-11.9780 policy updated! \n",
      "train step 09619 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8035 diff={max=08.4923, min=00.0073, mean=01.1188} policy_loss=-8.9821 policy updated! \n",
      "train step 09620 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=11.8323 diff={max=14.9142, min=00.0275, mean=02.0128} policy_loss=-10.9578 policy updated! \n",
      "train step 09621 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7420 diff={max=06.3339, min=00.0453, mean=01.0164} policy_loss=-13.0894 policy updated! \n",
      "train step 09622 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2390 diff={max=08.1711, min=00.0432, mean=01.5568} policy_loss=-11.6653 policy updated! \n",
      "train step 09623 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5249 diff={max=06.2605, min=00.0272, mean=01.3177} policy_loss=-9.5951 policy updated! \n",
      "train step 09624 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1961 diff={max=04.8401, min=00.0094, mean=01.0814} policy_loss=-10.7527 policy updated! \n",
      "train step 09625 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=03.9355 diff={max=06.4909, min=00.0535, mean=01.4057} policy_loss=-12.4719 policy updated! \n",
      "train step 09626 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0372 diff={max=08.6171, min=00.0044, mean=01.4939} policy_loss=-13.3175 policy updated! \n",
      "train step 09627 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7652 diff={max=10.9914, min=00.0059, mean=01.3825} policy_loss=-13.2127 policy updated! \n",
      "train step 09628 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=07.8786 diff={max=13.8032, min=00.0031, mean=01.4817} policy_loss=-8.2473 policy updated! \n",
      "train step 09629 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3993 diff={max=08.4387, min=00.0081, mean=01.4279} policy_loss=-10.4623 policy updated! \n",
      "train step 09630 reward={max=08.0000, min=06.0000, mean=06.8000} optimizing loss=05.6095 diff={max=09.5043, min=00.0122, mean=01.3169} policy_loss=-9.7109 policy updated! \n",
      "train step 09631 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8679 diff={max=05.9700, min=00.0074, mean=01.0992} policy_loss=-10.0031 policy updated! \n",
      "train step 09632 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2691 diff={max=09.7327, min=00.0534, mean=01.2281} policy_loss=-11.4626 policy updated! \n",
      "train step 09633 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=10.5133 diff={max=13.5684, min=00.0321, mean=01.7610} policy_loss=-9.8939 policy updated! \n",
      "train step 09634 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4168 diff={max=10.9164, min=00.0032, mean=01.3580} policy_loss=-9.7055 policy updated! \n",
      "train step 09635 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.5810 diff={max=10.9249, min=00.0017, mean=01.3641} policy_loss=-15.1112 policy updated! \n",
      "train step 09636 reward={max=07.0000, min=00.0000, mean=04.0000} optimizing loss=05.7450 diff={max=08.6197, min=00.0209, mean=01.4706} policy_loss=-11.5928 policy updated! \n",
      "train step 09637 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1697 diff={max=06.7488, min=00.0574, mean=01.5482} policy_loss=-12.6987 policy updated! \n",
      "train step 09638 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=06.3804 diff={max=09.5853, min=00.0164, mean=01.5849} policy_loss=-12.1124 policy updated! \n",
      "train step 09639 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.1781 diff={max=07.9702, min=00.0598, mean=01.6090} policy_loss=-11.1685 policy updated! \n",
      "train step 09640 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.1935 diff={max=04.5378, min=00.0001, mean=00.9552} policy_loss=-8.2705 policy updated! \n",
      "train step 09641 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9987 diff={max=04.7181, min=00.0059, mean=01.0372} policy_loss=-10.3516 policy updated! \n",
      "train step 09642 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0156 diff={max=07.3351, min=00.0008, mean=01.5511} policy_loss=-11.8816 policy updated! \n",
      "train step 09643 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5819 diff={max=07.4138, min=00.0179, mean=01.1711} policy_loss=-13.3372 policy updated! \n",
      "train step 09644 reward={max=06.0000, min=00.0000, mean=02.4000} optimizing loss=04.8692 diff={max=10.3944, min=00.0115, mean=01.3406} policy_loss=-11.8994 policy updated! \n",
      "train step 09645 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.1638 diff={max=09.2169, min=00.0156, mean=01.8459} policy_loss=-11.7769 policy updated! \n",
      "train step 09646 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.1117 diff={max=11.5456, min=00.1147, mean=01.9858} policy_loss=-14.5465 policy updated! \n",
      "train step 09647 reward={max=06.0000, min=00.0000, mean=03.6000} optimizing loss=03.6791 diff={max=07.0570, min=00.0171, mean=01.2905} policy_loss=-11.3401 policy updated! \n",
      "train step 09648 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0983 diff={max=06.9025, min=00.0769, mean=01.1388} policy_loss=-12.4740 policy updated! \n",
      "train step 09649 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1460 diff={max=06.0001, min=00.0342, mean=01.0169} policy_loss=-10.9266 policy updated! \n",
      "train step 09650 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.6316 diff={max=06.5184, min=00.0083, mean=01.2503} policy_loss=-12.5291 policy updated! \n",
      "train step 09651 reward={max=08.0000, min=00.0000, mean=04.4000} optimizing loss=06.6089 diff={max=09.1223, min=00.0127, mean=01.5286} policy_loss=-10.4779 policy updated! \n",
      "train step 09652 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6733 diff={max=07.5197, min=00.0413, mean=01.5987} policy_loss=-13.9348 policy updated! \n",
      "train step 09653 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7738 diff={max=05.6908, min=00.0058, mean=01.0763} policy_loss=-11.5174 policy updated! \n",
      "train step 09654 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6295 diff={max=07.3154, min=00.0149, mean=01.2690} policy_loss=-11.7905 policy updated! \n",
      "train step 09655 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.4965 diff={max=07.4656, min=00.0408, mean=01.3306} policy_loss=-12.1412 policy updated! \n",
      "train step 09656 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=02.1923 diff={max=06.0248, min=00.0059, mean=00.9264} policy_loss=-9.8055 policy updated! \n",
      "train step 09657 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9570 diff={max=05.5598, min=00.0580, mean=01.4090} policy_loss=-12.1170 policy updated! \n",
      "train step 09658 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=05.3873 diff={max=08.5396, min=00.0389, mean=01.3918} policy_loss=-10.7265 policy updated! \n",
      "train step 09659 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8576 diff={max=07.6987, min=00.0117, mean=01.2340} policy_loss=-12.2174 policy updated! \n",
      "train step 09660 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.1331 diff={max=04.3889, min=00.0089, mean=01.0245} policy_loss=-13.5795 policy updated! \n",
      "train step 09661 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=08.7682 diff={max=13.5633, min=00.0203, mean=01.6146} policy_loss=-14.9539 policy updated! \n",
      "train step 09662 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6056 diff={max=11.7160, min=00.0135, mean=01.1186} policy_loss=-10.0227 policy updated! \n",
      "train step 09663 reward={max=07.0000, min=00.0000, mean=04.0000} optimizing loss=03.2279 diff={max=07.4463, min=00.0068, mean=01.1785} policy_loss=-11.6705 policy updated! \n",
      "train step 09664 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.2502 diff={max=13.4940, min=00.0188, mean=01.2906} policy_loss=-11.5819 policy updated! \n",
      "train step 09665 reward={max=07.0000, min=00.0000, mean=03.8000} optimizing loss=04.8876 diff={max=09.0026, min=00.0002, mean=01.3438} policy_loss=-7.8323 policy updated! \n",
      "train step 09666 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.1516 diff={max=06.3878, min=00.0197, mean=01.1021} policy_loss=-10.0310 policy updated! \n",
      "train step 09667 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4234 diff={max=05.7150, min=00.0333, mean=01.0789} policy_loss=-10.6064 policy updated! \n",
      "train step 09668 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1829 diff={max=07.8053, min=00.0240, mean=01.5011} policy_loss=-11.9576 policy updated! \n",
      "train step 09669 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4752 diff={max=08.8962, min=00.0050, mean=01.1858} policy_loss=-10.6123 policy updated! \n",
      "train step 09670 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.1283 diff={max=03.4555, min=00.0128, mean=01.0591} policy_loss=-11.9007 policy updated! \n",
      "train step 09671 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.7117 diff={max=18.1368, min=00.0103, mean=01.7196} policy_loss=-10.4428 policy updated! \n",
      "train step 09672 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4964 diff={max=07.4206, min=00.0136, mean=01.4361} policy_loss=-11.9815 policy updated! \n",
      "train step 09673 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=09.6107 diff={max=16.5753, min=00.0113, mean=01.6247} policy_loss=-12.2428 policy updated! \n",
      "train step 09674 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.5291 diff={max=13.0475, min=00.0239, mean=02.0920} policy_loss=-13.5321 policy updated! \n",
      "train step 09675 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.5279 diff={max=12.8740, min=00.0305, mean=01.6243} policy_loss=-14.2595 policy updated! \n",
      "train step 09676 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.4401 diff={max=04.1732, min=00.0114, mean=01.0571} policy_loss=-10.5674 policy updated! \n",
      "train step 09677 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.2561 diff={max=10.0770, min=00.0126, mean=01.6426} policy_loss=-12.1503 policy updated! \n",
      "train step 09678 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.9197 diff={max=12.5775, min=00.0075, mean=01.6667} policy_loss=-10.0553 policy updated! \n",
      "train step 09679 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.3651 diff={max=09.6180, min=00.0038, mean=01.2644} policy_loss=-13.9722 policy updated! \n",
      "train step 09680 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.9299 diff={max=07.5535, min=00.0062, mean=01.1134} policy_loss=-10.6637 policy updated! \n",
      "train step 09681 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9979 diff={max=07.2829, min=00.0245, mean=01.5057} policy_loss=-12.7599 policy updated! \n",
      "train step 09682 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9756 diff={max=08.8264, min=00.0058, mean=01.2769} policy_loss=-10.3642 policy updated! \n",
      "train step 09683 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2502 diff={max=05.8663, min=00.0335, mean=01.3710} policy_loss=-10.2938 policy updated! \n",
      "train step 09684 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.8266 diff={max=09.1462, min=00.0041, mean=01.2166} policy_loss=-9.9823 policy updated! \n",
      "train step 09685 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.7355 diff={max=07.5719, min=00.0481, mean=01.3334} policy_loss=-11.6706 policy updated! \n",
      "train step 09686 reward={max=08.0000, min=00.0000, mean=04.2000} optimizing loss=07.2574 diff={max=12.4183, min=00.0420, mean=01.6265} policy_loss=-9.8703 policy updated! \n",
      "train step 09687 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.5753 diff={max=14.1507, min=00.0144, mean=01.4409} policy_loss=-10.0373 policy updated! \n",
      "train step 09688 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.6882 diff={max=07.6523, min=00.0152, mean=01.0371} policy_loss=-10.0386 policy updated! \n",
      "train step 09689 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8799 diff={max=06.7404, min=00.0034, mean=01.4321} policy_loss=-8.8011 policy updated! \n",
      "train step 09690 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.8680 diff={max=07.1415, min=00.0051, mean=01.0913} policy_loss=-10.1321 policy updated! \n",
      "train step 09691 reward={max=08.0000, min=00.0000, mean=04.4000} optimizing loss=03.4105 diff={max=06.5452, min=00.0058, mean=01.2325} policy_loss=-12.8821 policy updated! \n",
      "train step 09692 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3558 diff={max=14.2157, min=00.0060, mean=01.5246} policy_loss=-11.0134 policy updated! \n",
      "train step 09693 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4263 diff={max=03.8166, min=00.0021, mean=00.8907} policy_loss=-13.7178 policy updated! \n",
      "train step 09694 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0433 diff={max=07.3039, min=00.0611, mean=01.4064} policy_loss=-13.4331 policy updated! \n",
      "train step 09695 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.0621 diff={max=06.9045, min=00.0068, mean=01.1368} policy_loss=-10.1669 policy updated! \n",
      "train step 09696 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.5432 diff={max=08.2927, min=00.0045, mean=01.2729} policy_loss=-9.8978 policy updated! \n",
      "train step 09697 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3616 diff={max=07.3623, min=00.0163, mean=01.2134} policy_loss=-11.7110 policy updated! \n",
      "train step 09698 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7896 diff={max=13.4215, min=00.0130, mean=01.3995} policy_loss=-12.2775 policy updated! \n",
      "train step 09699 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=07.5278 diff={max=11.1830, min=00.0450, mean=01.5213} policy_loss=-11.8011 policy updated! \n",
      "train step 09700 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.0449 diff={max=06.0517, min=00.0222, mean=01.3898} policy_loss=-12.1752 policy updated! \n",
      "train step 09701 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0656 diff={max=08.1314, min=00.0095, mean=01.2698} policy_loss=-10.5765 policy updated! \n",
      "train step 09702 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6788 diff={max=08.0331, min=00.0439, mean=01.2316} policy_loss=-10.8740 policy updated! \n",
      "train step 09703 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3034 diff={max=09.3205, min=00.0071, mean=01.4210} policy_loss=-11.5966 policy updated! \n",
      "train step 09704 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9588 diff={max=06.3109, min=00.0318, mean=01.3361} policy_loss=-10.6276 policy updated! \n",
      "train step 09705 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.9900 diff={max=10.5597, min=00.0177, mean=01.1237} policy_loss=-12.3396 policy updated! \n",
      "train step 09706 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1736 diff={max=09.3791, min=00.0216, mean=01.0294} policy_loss=-12.0560 policy updated! \n",
      "train step 09707 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8130 diff={max=09.5214, min=00.0329, mean=00.9387} policy_loss=-11.2479 policy updated! \n",
      "train step 09708 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7566 diff={max=08.2675, min=00.0094, mean=01.1863} policy_loss=-11.5100 policy updated! \n",
      "train step 09709 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3649 diff={max=03.6799, min=00.0075, mean=00.7769} policy_loss=-9.2243 policy updated! \n",
      "train step 09710 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.4290 diff={max=08.7950, min=00.0269, mean=01.3848} policy_loss=-9.6035 policy updated! \n",
      "train step 09711 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9350 diff={max=07.3069, min=00.0113, mean=01.0585} policy_loss=-13.2926 policy updated! \n",
      "train step 09712 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0466 diff={max=10.0598, min=00.0111, mean=01.4028} policy_loss=-11.2165 policy updated! \n",
      "train step 09713 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.4497 diff={max=05.2195, min=00.0371, mean=01.0645} policy_loss=-10.9224 policy updated! \n",
      "train step 09714 reward={max=08.0000, min=00.0000, mean=05.8000} optimizing loss=07.1045 diff={max=13.9182, min=00.0004, mean=01.3611} policy_loss=-11.3783 policy updated! \n",
      "train step 09715 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.9088 diff={max=05.8168, min=00.0142, mean=01.1022} policy_loss=-10.9350 policy updated! \n",
      "train step 09716 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8308 diff={max=05.7609, min=00.0253, mean=01.0114} policy_loss=-11.5992 policy updated! \n",
      "train step 09717 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3431 diff={max=06.8285, min=00.0049, mean=01.4627} policy_loss=-12.9297 policy updated! \n",
      "train step 09718 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0032 diff={max=07.7031, min=00.0291, mean=01.0488} policy_loss=-11.5943 policy updated! \n",
      "train step 09719 reward={max=07.0000, min=00.0000, mean=05.4000} optimizing loss=07.4292 diff={max=08.8617, min=00.0169, mean=01.6376} policy_loss=-13.0654 policy updated! \n",
      "train step 09720 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=04.9413 diff={max=09.0923, min=00.0062, mean=01.3334} policy_loss=-9.1661 policy updated! \n",
      "train step 09721 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3655 diff={max=05.0871, min=00.0424, mean=01.0393} policy_loss=-12.3705 policy updated! \n",
      "train step 09722 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.2947 diff={max=15.1565, min=00.0094, mean=01.3792} policy_loss=-10.8689 policy updated! \n",
      "train step 09723 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7018 diff={max=07.5278, min=00.0027, mean=01.3785} policy_loss=-11.0568 policy updated! \n",
      "train step 09724 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=08.6588 diff={max=11.8172, min=00.0363, mean=01.8888} policy_loss=-9.8565 policy updated! \n",
      "train step 09725 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.4277 diff={max=12.5986, min=00.0155, mean=01.3746} policy_loss=-10.4446 policy updated! \n",
      "train step 09726 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.7206 diff={max=07.4682, min=00.0656, mean=01.2092} policy_loss=-11.1948 policy updated! \n",
      "train step 09727 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.9999 diff={max=13.6497, min=00.0001, mean=01.9679} policy_loss=-12.2182 policy updated! \n",
      "train step 09728 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2988 diff={max=09.9917, min=00.0030, mean=01.1765} policy_loss=-10.3645 policy updated! \n",
      "train step 09729 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6085 diff={max=08.0126, min=00.0142, mean=01.2511} policy_loss=-10.7655 policy updated! \n",
      "train step 09730 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=10.1828 diff={max=13.4200, min=00.0323, mean=01.7803} policy_loss=-11.4800 policy updated! \n",
      "train step 09731 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6774 diff={max=09.2620, min=00.0013, mean=01.1737} policy_loss=-12.9493 policy updated! \n",
      "train step 09732 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7780 diff={max=07.9109, min=00.0013, mean=01.3569} policy_loss=-9.6480 policy updated! \n",
      "train step 09733 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6646 diff={max=10.4338, min=00.0140, mean=01.4940} policy_loss=-12.8408 policy updated! \n",
      "train step 09734 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6524 diff={max=10.1215, min=00.0045, mean=01.4488} policy_loss=-11.3460 policy updated! \n",
      "train step 09735 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.3275 diff={max=08.9317, min=00.0032, mean=01.5057} policy_loss=-12.2898 policy updated! \n",
      "train step 09736 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4296 diff={max=06.1324, min=00.0192, mean=01.3808} policy_loss=-14.5199 policy updated! \n",
      "train step 09737 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.7572 diff={max=15.4497, min=00.0195, mean=01.4937} policy_loss=-12.2377 policy updated! \n",
      "train step 09738 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.4176 diff={max=12.4862, min=00.0005, mean=01.3583} policy_loss=-13.5385 policy updated! \n",
      "train step 09739 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2715 diff={max=06.7473, min=00.0151, mean=01.0759} policy_loss=-9.9725 policy updated! \n",
      "train step 09740 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.1184 diff={max=04.3750, min=00.0961, mean=01.3076} policy_loss=-11.7002 policy updated! \n",
      "train step 09741 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2939 diff={max=06.7145, min=00.0045, mean=01.4142} policy_loss=-10.2720 policy updated! \n",
      "train step 09742 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4590 diff={max=09.8954, min=00.0013, mean=01.3423} policy_loss=-9.7586 policy updated! \n",
      "train step 09743 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0943 diff={max=10.0203, min=00.0141, mean=01.3123} policy_loss=-10.6554 policy updated! \n",
      "train step 09744 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0653 diff={max=05.3391, min=00.0142, mean=00.8965} policy_loss=-9.7672 policy updated! \n",
      "train step 09745 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.8480 diff={max=09.3398, min=00.0323, mean=01.4607} policy_loss=-11.7159 policy updated! \n",
      "train step 09746 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2270 diff={max=07.4057, min=00.0247, mean=01.1020} policy_loss=-12.3589 policy updated! \n",
      "train step 09747 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4691 diff={max=09.3682, min=00.0033, mean=01.4640} policy_loss=-11.1958 policy updated! \n",
      "train step 09748 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3791 diff={max=09.0443, min=00.0087, mean=01.1772} policy_loss=-11.7479 policy updated! \n",
      "train step 09749 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.8290 diff={max=07.0504, min=00.0267, mean=01.0680} policy_loss=-11.6775 policy updated! \n",
      "train step 09750 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=11.1114 diff={max=10.4706, min=00.0034, mean=01.9756} policy_loss=-14.1498 policy updated! \n",
      "train step 09751 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4970 diff={max=06.1891, min=00.0049, mean=01.2866} policy_loss=-11.7112 policy updated! \n",
      "train step 09752 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6901 diff={max=05.0640, min=00.0053, mean=01.1357} policy_loss=-9.8326 policy updated! \n",
      "train step 09753 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.3140 diff={max=15.0227, min=00.0191, mean=01.6133} policy_loss=-12.2050 policy updated! \n",
      "train step 09754 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.9503 diff={max=09.1102, min=00.0060, mean=01.3447} policy_loss=-11.3049 policy updated! \n",
      "train step 09755 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=12.0567 diff={max=15.9398, min=00.0156, mean=01.7017} policy_loss=-12.8657 policy updated! \n",
      "train step 09756 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.7512 diff={max=12.6388, min=00.0003, mean=01.4276} policy_loss=-9.1712 policy updated! \n",
      "train step 09757 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2416 diff={max=07.5547, min=00.0077, mean=01.0624} policy_loss=-10.5153 policy updated! \n",
      "train step 09758 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9392 diff={max=07.7696, min=00.0156, mean=01.1853} policy_loss=-11.8950 policy updated! \n",
      "train step 09759 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2298 diff={max=07.4169, min=00.0118, mean=01.0408} policy_loss=-9.4381 policy updated! \n",
      "train step 09760 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.5418 diff={max=04.8535, min=00.0078, mean=01.1307} policy_loss=-10.3592 policy updated! \n",
      "train step 09761 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.7779 diff={max=10.5118, min=00.0075, mean=01.7576} policy_loss=-11.2846 policy updated! \n",
      "train step 09762 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8588 diff={max=08.9937, min=00.0252, mean=01.4051} policy_loss=-10.1692 policy updated! \n",
      "train step 09763 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4433 diff={max=10.8888, min=00.0003, mean=01.3709} policy_loss=-12.6652 policy updated! \n",
      "train step 09764 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5777 diff={max=07.3044, min=00.0249, mean=01.0411} policy_loss=-10.0113 policy updated! \n",
      "train step 09765 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.5271 diff={max=08.1181, min=00.0202, mean=01.3943} policy_loss=-11.7881 policy updated! \n",
      "train step 09766 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0349 diff={max=09.9214, min=00.0352, mean=01.3453} policy_loss=-12.1710 policy updated! \n",
      "train step 09767 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2185 diff={max=11.4201, min=00.0389, mean=01.5170} policy_loss=-12.8466 policy updated! \n",
      "train step 09768 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.3709 diff={max=06.1053, min=00.0458, mean=01.1943} policy_loss=-10.4285 policy updated! \n",
      "train step 09769 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.3624 diff={max=08.9051, min=00.0000, mean=01.1342} policy_loss=-10.8947 policy updated! \n",
      "train step 09770 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.6169 diff={max=08.3621, min=00.0302, mean=01.4934} policy_loss=-10.9245 policy updated! \n",
      "train step 09771 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=11.6101 diff={max=18.1747, min=00.0051, mean=01.6960} policy_loss=-11.9605 policy updated! \n",
      "train step 09772 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4520 diff={max=10.8096, min=00.0238, mean=01.4924} policy_loss=-8.9901 policy updated! \n",
      "train step 09773 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.9913 diff={max=07.6230, min=00.0069, mean=01.1068} policy_loss=-10.6660 policy updated! \n",
      "train step 09774 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7942 diff={max=06.9097, min=00.0593, mean=01.2439} policy_loss=-11.3604 policy updated! \n",
      "train step 09775 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.5290 diff={max=11.4873, min=00.0181, mean=01.8191} policy_loss=-12.3518 policy updated! \n",
      "train step 09776 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8671 diff={max=07.9786, min=00.0085, mean=01.3804} policy_loss=-10.2953 policy updated! \n",
      "train step 09777 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5984 diff={max=05.3100, min=00.0101, mean=01.0454} policy_loss=-12.6985 policy updated! \n",
      "train step 09778 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=10.2149 diff={max=12.4301, min=00.0016, mean=01.6418} policy_loss=-11.2880 policy updated! \n",
      "train step 09779 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6293 diff={max=04.2908, min=00.0378, mean=01.3869} policy_loss=-11.3714 policy updated! \n",
      "train step 09780 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.2962 diff={max=10.2102, min=00.0095, mean=01.4884} policy_loss=-11.3361 policy updated! \n",
      "train step 09781 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.5236 diff={max=10.3936, min=00.0417, mean=01.5517} policy_loss=-11.0853 policy updated! \n",
      "train step 09782 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9871 diff={max=10.4259, min=00.0005, mean=01.6195} policy_loss=-10.9385 policy updated! \n",
      "train step 09783 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3006 diff={max=09.7330, min=00.0169, mean=01.2489} policy_loss=-10.3572 policy updated! \n",
      "train step 09784 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5367 diff={max=09.1844, min=00.0016, mean=01.4086} policy_loss=-11.4037 policy updated! \n",
      "train step 09785 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.1693 diff={max=08.9391, min=00.0074, mean=01.1567} policy_loss=-10.8609 policy updated! \n",
      "train step 09786 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=08.0390 diff={max=08.2684, min=00.0023, mean=01.6633} policy_loss=-11.4660 policy updated! \n",
      "train step 09787 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0276 diff={max=13.6625, min=00.0047, mean=01.3177} policy_loss=-12.7169 policy updated! \n",
      "train step 09788 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.2941 diff={max=09.4192, min=00.0250, mean=01.4853} policy_loss=-9.8841 policy updated! \n",
      "train step 09789 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.0388 diff={max=12.1779, min=00.0039, mean=01.4347} policy_loss=-11.6163 policy updated! \n",
      "train step 09790 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.7866 diff={max=09.2599, min=00.0010, mean=01.2377} policy_loss=-11.7748 policy updated! \n",
      "train step 09791 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5338 diff={max=09.5273, min=00.0107, mean=01.4286} policy_loss=-12.2168 policy updated! \n",
      "train step 09792 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=14.3411 diff={max=14.4968, min=00.0064, mean=02.1928} policy_loss=-11.6029 policy updated! \n",
      "train step 09793 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.4356 diff={max=12.6792, min=00.0435, mean=01.7453} policy_loss=-12.5548 policy updated! \n",
      "train step 09794 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.5664 diff={max=14.2186, min=00.0321, mean=02.2609} policy_loss=-12.9842 policy updated! \n",
      "train step 09795 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.4840 diff={max=07.3897, min=00.0002, mean=01.8140} policy_loss=-13.7031 policy updated! \n",
      "train step 09796 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7112 diff={max=07.4354, min=00.0440, mean=01.2498} policy_loss=-12.9630 policy updated! \n",
      "train step 09797 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.6669 diff={max=15.3540, min=00.0364, mean=01.5415} policy_loss=-12.0724 policy updated! \n",
      "train step 09798 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0151 diff={max=11.8113, min=00.0127, mean=01.4421} policy_loss=-10.5518 policy updated! \n",
      "train step 09799 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8472 diff={max=08.8744, min=00.0277, mean=01.6149} policy_loss=-9.3448 policy updated! \n",
      "train step 09800 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.0867 diff={max=10.6380, min=00.0182, mean=01.3939} policy_loss=-10.5491 policy updated! \n",
      "train step 09801 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0424 diff={max=05.3222, min=00.0919, mean=01.2062} policy_loss=-10.6579 policy updated! \n",
      "train step 09802 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1685 diff={max=06.6994, min=00.0083, mean=01.4411} policy_loss=-11.7693 policy updated! \n",
      "train step 09803 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5710 diff={max=04.8226, min=00.0301, mean=01.0585} policy_loss=-12.1876 policy updated! \n",
      "train step 09804 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7637 diff={max=07.8045, min=00.0353, mean=01.5295} policy_loss=-13.9429 policy updated! \n",
      "train step 09805 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=10.4474 diff={max=11.4288, min=00.0131, mean=01.7579} policy_loss=-10.6331 policy updated! \n",
      "train step 09806 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=16.3130 diff={max=15.1603, min=00.0631, mean=02.1706} policy_loss=-11.6270 policy updated! \n",
      "train step 09807 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6795 diff={max=11.0732, min=00.0378, mean=01.2563} policy_loss=-10.2401 policy updated! \n",
      "train step 09808 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=08.0876 diff={max=10.9021, min=00.0020, mean=01.7820} policy_loss=-12.4065 policy updated! \n",
      "train step 09809 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5203 diff={max=08.3327, min=00.0118, mean=01.0779} policy_loss=-9.9906 policy updated! \n",
      "train step 09810 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.3228 diff={max=13.0272, min=00.0053, mean=01.3848} policy_loss=-10.6715 policy updated! \n",
      "train step 09811 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0042 diff={max=08.7564, min=00.0074, mean=01.3701} policy_loss=-9.3566 policy updated! \n",
      "train step 09812 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6389 diff={max=07.9987, min=00.0050, mean=01.4446} policy_loss=-11.0961 policy updated! \n",
      "train step 09813 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.8820 diff={max=11.2501, min=00.0256, mean=01.7761} policy_loss=-9.6948 policy updated! \n",
      "train step 09814 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9781 diff={max=04.7889, min=00.0132, mean=01.0097} policy_loss=-10.9401 policy updated! \n",
      "train step 09815 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.7015 diff={max=05.9314, min=00.0160, mean=01.1054} policy_loss=-12.1556 policy updated! \n",
      "train step 09816 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5548 diff={max=07.8216, min=00.0361, mean=01.2746} policy_loss=-10.5666 policy updated! \n",
      "train step 09817 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8125 diff={max=08.3198, min=00.0126, mean=01.3314} policy_loss=-13.7065 policy updated! \n",
      "train step 09818 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5721 diff={max=10.0690, min=00.0099, mean=01.3482} policy_loss=-14.1934 policy updated! \n",
      "train step 09819 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=05.9075 diff={max=11.5225, min=00.0161, mean=01.4734} policy_loss=-12.6556 policy updated! \n",
      "train step 09820 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=13.3984 diff={max=17.6933, min=00.0185, mean=01.8774} policy_loss=-9.0429 policy updated! \n",
      "train step 09821 reward={max=07.0000, min=00.0000, mean=04.0000} optimizing loss=06.2180 diff={max=07.0103, min=00.1311, mean=01.7745} policy_loss=-12.6213 policy updated! \n",
      "train step 09822 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1116 diff={max=06.2426, min=00.0052, mean=01.4696} policy_loss=-10.7234 policy updated! \n",
      "train step 09823 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6599 diff={max=07.0422, min=00.0321, mean=01.3836} policy_loss=-11.5811 policy updated! \n",
      "train step 09824 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2642 diff={max=07.1271, min=00.0110, mean=01.6404} policy_loss=-12.0709 policy updated! \n",
      "train step 09825 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.3291 diff={max=09.3693, min=00.0669, mean=01.3550} policy_loss=-13.4425 policy updated! \n",
      "train step 09826 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.4393 diff={max=07.4703, min=00.0151, mean=01.4722} policy_loss=-12.7815 policy updated! \n",
      "train step 09827 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8903 diff={max=07.3689, min=00.0103, mean=01.4021} policy_loss=-12.1926 policy updated! \n",
      "train step 09828 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1894 diff={max=06.6464, min=00.0174, mean=01.5614} policy_loss=-11.5643 policy updated! \n",
      "train step 09829 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3080 diff={max=06.2097, min=00.0337, mean=01.2425} policy_loss=-11.7182 policy updated! \n",
      "train step 09830 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.7148 diff={max=09.0977, min=00.0316, mean=01.4752} policy_loss=-12.2272 policy updated! \n",
      "train step 09831 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.4226 diff={max=14.2714, min=00.0024, mean=01.5569} policy_loss=-12.8278 policy updated! \n",
      "train step 09832 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3939 diff={max=07.9899, min=00.0278, mean=01.1765} policy_loss=-10.8145 policy updated! \n",
      "train step 09833 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.6887 diff={max=13.7363, min=00.0209, mean=01.8787} policy_loss=-11.5857 policy updated! \n",
      "train step 09834 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9344 diff={max=05.0904, min=00.0120, mean=01.0370} policy_loss=-9.0433 policy updated! \n",
      "train step 09835 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.7954 diff={max=05.7207, min=00.0022, mean=00.8866} policy_loss=-9.9563 policy updated! \n",
      "train step 09836 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.6904 diff={max=08.8441, min=00.0195, mean=01.5486} policy_loss=-13.7818 policy updated! \n",
      "train step 09837 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0103 diff={max=08.8903, min=00.0479, mean=01.4992} policy_loss=-11.3060 policy updated! \n",
      "train step 09838 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8837 diff={max=06.5846, min=00.0504, mean=01.3009} policy_loss=-10.8191 policy updated! \n",
      "train step 09839 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.4320 diff={max=06.2292, min=00.0202, mean=01.0576} policy_loss=-9.6595 policy updated! \n",
      "train step 09840 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.7173 diff={max=14.5008, min=00.0886, mean=01.3298} policy_loss=-11.6804 policy updated! \n",
      "train step 09841 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.6370 diff={max=10.8941, min=00.0511, mean=02.0766} policy_loss=-14.2710 policy updated! \n",
      "train step 09842 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0670 diff={max=05.9501, min=00.0066, mean=01.1962} policy_loss=-12.0580 policy updated! \n",
      "train step 09843 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=07.9736 diff={max=10.4963, min=00.0116, mean=01.4583} policy_loss=-9.5597 policy updated! \n",
      "train step 09844 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=07.8419 diff={max=14.7996, min=00.0183, mean=01.3974} policy_loss=-11.6005 policy updated! \n",
      "train step 09845 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.3220 diff={max=09.1551, min=00.0262, mean=01.7636} policy_loss=-13.4346 policy updated! \n",
      "train step 09846 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.0965 diff={max=10.0900, min=00.0725, mean=01.7748} policy_loss=-10.0416 policy updated! \n",
      "train step 09847 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9460 diff={max=10.7545, min=00.0193, mean=01.4824} policy_loss=-9.2826 policy updated! \n",
      "train step 09848 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3027 diff={max=10.8097, min=00.0459, mean=01.4376} policy_loss=-13.5513 policy updated! \n",
      "train step 09849 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=07.6396 diff={max=08.7205, min=00.0589, mean=01.7561} policy_loss=-12.9988 policy updated! \n",
      "train step 09850 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.5456 diff={max=11.3773, min=00.0093, mean=01.2281} policy_loss=-12.6077 policy updated! \n",
      "train step 09851 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3167 diff={max=04.5132, min=00.0241, mean=01.0728} policy_loss=-10.4660 policy updated! \n",
      "train step 09852 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3533 diff={max=07.9090, min=00.0184, mean=01.3052} policy_loss=-11.3826 policy updated! \n",
      "train step 09853 reward={max=07.0000, min=00.0000, mean=02.6000} optimizing loss=06.9012 diff={max=07.7570, min=00.0562, mean=01.6122} policy_loss=-12.8118 policy updated! \n",
      "train step 09854 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.9078 diff={max=04.9956, min=00.0127, mean=01.3874} policy_loss=-10.9613 policy updated! \n",
      "train step 09855 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.8321 diff={max=07.2714, min=00.0226, mean=01.2508} policy_loss=-11.3058 policy updated! \n",
      "train step 09856 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9206 diff={max=11.2852, min=00.0138, mean=01.2944} policy_loss=-12.4806 policy updated! \n",
      "train step 09857 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.6360 diff={max=13.5257, min=00.0160, mean=01.5911} policy_loss=-11.3259 policy updated! \n",
      "train step 09858 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.0566 diff={max=09.9362, min=00.0394, mean=01.2845} policy_loss=-11.5633 policy updated! \n",
      "train step 09859 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.2828 diff={max=08.7625, min=00.0037, mean=01.4512} policy_loss=-12.2257 policy updated! \n",
      "train step 09860 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.0298 diff={max=08.6480, min=00.0179, mean=01.3571} policy_loss=-11.9075 policy updated! \n",
      "train step 09861 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.4365 diff={max=14.3735, min=00.0272, mean=01.4771} policy_loss=-11.4689 policy updated! \n",
      "train step 09862 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3870 diff={max=07.5788, min=00.0056, mean=01.4811} policy_loss=-11.4353 policy updated! \n",
      "train step 09863 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=11.4916 diff={max=13.8091, min=00.0485, mean=02.0556} policy_loss=-11.8796 policy updated! \n",
      "train step 09864 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.8244 diff={max=10.6852, min=00.0147, mean=01.4379} policy_loss=-9.9352 policy updated! \n",
      "train step 09865 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.5572 diff={max=11.3899, min=00.0246, mean=01.6409} policy_loss=-11.9024 policy updated! \n",
      "train step 09866 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.6775 diff={max=13.2095, min=00.0055, mean=01.3025} policy_loss=-11.3302 policy updated! \n",
      "train step 09867 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.8200 diff={max=12.7045, min=00.0082, mean=01.5530} policy_loss=-10.0098 policy updated! \n",
      "train step 09868 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4012 diff={max=09.9444, min=00.1104, mean=01.3436} policy_loss=-12.5038 policy updated! \n",
      "train step 09869 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4798 diff={max=06.9186, min=00.0203, mean=01.2341} policy_loss=-9.8984 policy updated! \n",
      "train step 09870 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.3967 diff={max=07.6884, min=00.0526, mean=01.2406} policy_loss=-14.0301 policy updated! \n",
      "train step 09871 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.3005 diff={max=09.1087, min=00.0050, mean=01.2966} policy_loss=-9.4422 policy updated! \n",
      "train step 09872 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8045 diff={max=10.7317, min=00.0275, mean=01.4788} policy_loss=-13.4082 policy updated! \n",
      "train step 09873 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2280 diff={max=04.6594, min=00.0097, mean=01.0685} policy_loss=-9.5600 policy updated! \n",
      "train step 09874 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5048 diff={max=05.3653, min=00.0030, mean=01.0882} policy_loss=-10.0468 policy updated! \n",
      "train step 09875 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.9082 diff={max=05.9561, min=00.0067, mean=01.1118} policy_loss=-12.7340 policy updated! \n",
      "train step 09876 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9597 diff={max=09.6758, min=00.0033, mean=01.1797} policy_loss=-10.7970 policy updated! \n",
      "train step 09877 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0215 diff={max=06.8469, min=00.0100, mean=01.3392} policy_loss=-10.4667 policy updated! \n",
      "train step 09878 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.0799 diff={max=06.9704, min=00.0370, mean=01.3041} policy_loss=-11.7741 policy updated! \n",
      "train step 09879 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3634 diff={max=05.2974, min=00.0375, mean=00.9626} policy_loss=-8.1640 policy updated! \n",
      "train step 09880 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.4084 diff={max=17.6317, min=00.0027, mean=01.5225} policy_loss=-10.9556 policy updated! \n",
      "train step 09881 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0008 diff={max=07.1198, min=00.0094, mean=01.0277} policy_loss=-10.5555 policy updated! \n",
      "train step 09882 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3874 diff={max=05.6911, min=00.0006, mean=01.1522} policy_loss=-8.9635 policy updated! \n",
      "train step 09883 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.0589 diff={max=05.4839, min=00.0004, mean=01.0984} policy_loss=-11.6345 policy updated! \n",
      "train step 09884 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2708 diff={max=07.4317, min=00.0281, mean=01.4157} policy_loss=-10.5580 policy updated! \n",
      "train step 09885 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.8859 diff={max=07.9085, min=00.0211, mean=00.9183} policy_loss=-9.5756 policy updated! \n",
      "train step 09886 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.1388 diff={max=07.8055, min=00.0062, mean=01.0511} policy_loss=-13.1069 policy updated! \n",
      "train step 09887 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8509 diff={max=08.1528, min=00.0299, mean=01.1772} policy_loss=-10.7325 policy updated! \n",
      "train step 09888 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8193 diff={max=05.4307, min=00.0527, mean=01.1301} policy_loss=-11.4141 policy updated! \n",
      "train step 09889 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3148 diff={max=05.0017, min=00.0009, mean=00.9869} policy_loss=-12.3108 policy updated! \n",
      "train step 09890 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.8882 diff={max=07.0932, min=00.0316, mean=01.0676} policy_loss=-11.9166 policy updated! \n",
      "train step 09891 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.9058 diff={max=09.4681, min=00.0054, mean=01.4104} policy_loss=-10.5927 policy updated! \n",
      "train step 09892 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8269 diff={max=08.2421, min=00.0034, mean=01.0099} policy_loss=-9.1631 policy updated! \n",
      "train step 09893 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3360 diff={max=05.7341, min=00.0056, mean=00.9013} policy_loss=-10.2807 policy updated! \n",
      "train step 09894 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9449 diff={max=03.6327, min=00.0269, mean=01.0334} policy_loss=-11.0734 policy updated! \n",
      "train step 09895 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.3058 diff={max=08.5421, min=00.0012, mean=01.3419} policy_loss=-11.3950 policy updated! \n",
      "train step 09896 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.3445 diff={max=20.4453, min=00.0016, mean=01.6022} policy_loss=-12.4295 policy updated! \n",
      "train step 09897 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3601 diff={max=06.1697, min=00.0144, mean=01.0940} policy_loss=-13.4457 policy updated! \n",
      "train step 09898 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0241 diff={max=06.7969, min=00.0032, mean=01.4116} policy_loss=-11.6515 policy updated! \n",
      "train step 09899 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.6428 diff={max=03.8545, min=00.0174, mean=00.9437} policy_loss=-11.6364 policy updated! \n",
      "train step 09900 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.8657 diff={max=14.9962, min=00.0000, mean=01.7505} policy_loss=-12.4035 policy updated! \n",
      "train step 09901 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0877 diff={max=05.8970, min=00.0519, mean=01.1845} policy_loss=-12.6447 policy updated! \n",
      "train step 09902 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3713 diff={max=05.1744, min=00.0100, mean=00.9686} policy_loss=-10.2795 policy updated! \n",
      "train step 09903 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5035 diff={max=05.8621, min=00.0046, mean=01.1932} policy_loss=-11.3591 policy updated! \n",
      "train step 09904 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.2327 diff={max=20.2867, min=00.0070, mean=01.3849} policy_loss=-8.7079 policy updated! \n",
      "train step 09905 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.1282 diff={max=06.3810, min=00.0176, mean=01.3052} policy_loss=-11.7809 policy updated! \n",
      "train step 09906 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1533 diff={max=06.4987, min=00.0370, mean=01.1768} policy_loss=-11.8468 policy updated! \n",
      "train step 09907 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0558 diff={max=07.7730, min=00.0036, mean=01.2193} policy_loss=-11.4410 policy updated! \n",
      "train step 09908 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2177 diff={max=09.9552, min=00.0281, mean=01.1553} policy_loss=-11.0565 policy updated! \n",
      "train step 09909 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.5523 diff={max=06.0298, min=00.0121, mean=01.2952} policy_loss=-8.7858 policy updated! \n",
      "train step 09910 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.3205 diff={max=08.9258, min=00.0084, mean=01.2275} policy_loss=-14.3215 policy updated! \n",
      "train step 09911 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6212 diff={max=06.9500, min=00.0030, mean=01.1349} policy_loss=-13.0197 policy updated! \n",
      "train step 09912 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5393 diff={max=07.6993, min=00.0079, mean=01.4066} policy_loss=-10.9295 policy updated! \n",
      "train step 09913 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7048 diff={max=09.3860, min=00.0087, mean=00.8586} policy_loss=-10.7062 policy updated! \n",
      "train step 09914 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9275 diff={max=06.4572, min=00.0033, mean=00.9158} policy_loss=-9.9880 policy updated! \n",
      "train step 09915 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.0169 diff={max=07.4738, min=00.0226, mean=01.2760} policy_loss=-10.6498 policy updated! \n",
      "train step 09916 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.0064 diff={max=08.2602, min=00.0034, mean=01.3504} policy_loss=-13.0373 policy updated! \n",
      "train step 09917 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4624 diff={max=06.4463, min=00.0262, mean=01.1574} policy_loss=-12.8110 policy updated! \n",
      "train step 09918 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.9298 diff={max=12.5104, min=00.0201, mean=01.6550} policy_loss=-12.5767 policy updated! \n",
      "train step 09919 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=10.9537 diff={max=18.2749, min=00.0038, mean=01.6277} policy_loss=-10.1463 policy updated! \n",
      "train step 09920 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.1557 diff={max=13.6382, min=00.0158, mean=01.2837} policy_loss=-12.3824 policy updated! \n",
      "train step 09921 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1371 diff={max=09.1830, min=00.0422, mean=01.0930} policy_loss=-9.7241 policy updated! \n",
      "train step 09922 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8864 diff={max=08.7500, min=00.0492, mean=01.6078} policy_loss=-11.2036 policy updated! \n",
      "train step 09923 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.0837 diff={max=08.9159, min=00.0238, mean=01.3523} policy_loss=-10.7607 policy updated! \n",
      "train step 09924 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7848 diff={max=08.3184, min=00.0126, mean=01.5093} policy_loss=-11.3770 policy updated! \n",
      "train step 09925 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.8688 diff={max=07.8991, min=00.0250, mean=01.2309} policy_loss=-11.5780 policy updated! \n",
      "train step 09926 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6868 diff={max=06.0587, min=00.0083, mean=00.9936} policy_loss=-13.1956 policy updated! \n",
      "train step 09927 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.8144 diff={max=09.7292, min=00.0090, mean=01.7232} policy_loss=-11.1020 policy updated! \n",
      "train step 09928 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8620 diff={max=05.5669, min=00.0391, mean=01.3192} policy_loss=-11.6615 policy updated! \n",
      "train step 09929 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.2228 diff={max=07.5447, min=00.0307, mean=01.4183} policy_loss=-12.7845 policy updated! \n",
      "train step 09930 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.8657 diff={max=08.6159, min=00.0056, mean=00.9782} policy_loss=-12.2415 policy updated! \n",
      "train step 09931 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.1557 diff={max=08.5878, min=00.0375, mean=01.5910} policy_loss=-10.1853 policy updated! \n",
      "train step 09932 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7510 diff={max=06.0389, min=00.0134, mean=01.0844} policy_loss=-11.0385 policy updated! \n",
      "train step 09933 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5971 diff={max=05.0801, min=00.0080, mean=01.1182} policy_loss=-7.8548 policy updated! \n",
      "train step 09934 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6745 diff={max=03.9462, min=00.0315, mean=00.9505} policy_loss=-9.7651 policy updated! \n",
      "train step 09935 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=07.5579 diff={max=12.4184, min=00.0124, mean=01.5530} policy_loss=-10.8145 policy updated! \n",
      "train step 09936 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1645 diff={max=08.4565, min=00.0080, mean=01.4247} policy_loss=-9.3392 policy updated! \n",
      "train step 09937 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0084 diff={max=08.0631, min=00.0098, mean=01.3556} policy_loss=-10.2203 policy updated! \n",
      "train step 09938 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.6430 diff={max=08.4725, min=00.0324, mean=01.7615} policy_loss=-13.0521 policy updated! \n",
      "train step 09939 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.6392 diff={max=06.5885, min=00.0063, mean=01.0970} policy_loss=-11.0668 policy updated! \n",
      "train step 09940 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.4709 diff={max=07.4021, min=00.0137, mean=01.1305} policy_loss=-10.7818 policy updated! \n",
      "train step 09941 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4692 diff={max=09.5062, min=00.0203, mean=01.2515} policy_loss=-14.5758 policy updated! \n",
      "train step 09942 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2933 diff={max=09.9328, min=00.0297, mean=01.3420} policy_loss=-11.1629 policy updated! \n",
      "train step 09943 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.6141 diff={max=11.5432, min=00.0066, mean=01.7804} policy_loss=-10.5806 policy updated! \n",
      "train step 09944 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6496 diff={max=06.3390, min=00.0124, mean=01.2681} policy_loss=-13.1174 policy updated! \n",
      "train step 09945 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.6261 diff={max=03.7462, min=00.0021, mean=01.0065} policy_loss=-13.3397 policy updated! \n",
      "train step 09946 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2604 diff={max=07.8569, min=00.0377, mean=00.9748} policy_loss=-8.0966 policy updated! \n",
      "train step 09947 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4502 diff={max=08.2920, min=00.0106, mean=01.0578} policy_loss=-8.9232 policy updated! \n",
      "train step 09948 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9050 diff={max=07.2574, min=00.0540, mean=01.2641} policy_loss=-13.7466 policy updated! \n",
      "train step 09949 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8023 diff={max=06.1779, min=00.0369, mean=01.1250} policy_loss=-12.5066 policy updated! \n",
      "train step 09950 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.8199 diff={max=05.8080, min=00.0014, mean=01.0705} policy_loss=-11.2588 policy updated! \n",
      "train step 09951 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.6397 diff={max=05.8850, min=00.0065, mean=01.2702} policy_loss=-12.3690 policy updated! \n",
      "train step 09952 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2134 diff={max=08.3935, min=00.0538, mean=01.6207} policy_loss=-10.6454 policy updated! \n",
      "train step 09953 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.3878 diff={max=04.8057, min=00.0019, mean=01.0883} policy_loss=-9.5971 policy updated! \n",
      "train step 09954 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.4664 diff={max=07.1580, min=00.0141, mean=01.4064} policy_loss=-11.5471 policy updated! \n",
      "train step 09955 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.3337 diff={max=09.8634, min=00.0133, mean=01.2913} policy_loss=-11.6143 policy updated! \n",
      "train step 09956 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5107 diff={max=08.5186, min=00.0201, mean=01.3652} policy_loss=-13.0523 policy updated! \n",
      "train step 09957 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7933 diff={max=08.2207, min=00.0041, mean=00.9158} policy_loss=-9.0760 policy updated! \n",
      "train step 09958 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4408 diff={max=07.6724, min=00.0028, mean=01.4701} policy_loss=-9.8146 policy updated! \n",
      "train step 09959 reward={max=07.0000, min=00.0000, mean=05.4000} optimizing loss=04.1726 diff={max=07.9175, min=00.0043, mean=01.3369} policy_loss=-12.6384 policy updated! \n",
      "train step 09960 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.9156 diff={max=15.6429, min=00.0127, mean=01.6463} policy_loss=-11.0131 policy updated! \n",
      "train step 09961 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=07.6983 diff={max=10.0062, min=00.0026, mean=01.6648} policy_loss=-10.1521 policy updated! \n",
      "train step 09962 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0691 diff={max=11.7973, min=00.0545, mean=01.0898} policy_loss=-9.5533 policy updated! \n",
      "train step 09963 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=08.7046 diff={max=10.6203, min=00.0192, mean=01.7113} policy_loss=-10.9383 policy updated! \n",
      "train step 09964 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5551 diff={max=07.2521, min=00.0510, mean=01.3048} policy_loss=-11.5419 policy updated! \n",
      "train step 09965 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=17.2561 diff={max=16.9405, min=00.0040, mean=02.5103} policy_loss=-14.3025 policy updated! \n",
      "train step 09966 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1352 diff={max=05.0200, min=00.0438, mean=01.0284} policy_loss=-10.7704 policy updated! \n",
      "train step 09967 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3363 diff={max=03.2071, min=00.0454, mean=00.8211} policy_loss=-8.8572 policy updated! \n",
      "train step 09968 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1742 diff={max=07.4356, min=00.0281, mean=01.3713} policy_loss=-13.2805 policy updated! \n",
      "train step 09969 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.4953 diff={max=10.5008, min=00.0316, mean=01.9610} policy_loss=-12.9202 policy updated! \n",
      "train step 09970 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.7332 diff={max=08.4406, min=00.0083, mean=01.2204} policy_loss=-11.5935 policy updated! \n",
      "train step 09971 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2860 diff={max=08.7208, min=00.0081, mean=00.8242} policy_loss=-10.7332 policy updated! \n",
      "train step 09972 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=01.6482 diff={max=05.9144, min=00.0050, mean=00.8690} policy_loss=-12.3942 policy updated! \n",
      "train step 09973 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5817 diff={max=06.3965, min=00.0251, mean=01.2161} policy_loss=-11.4757 policy updated! \n",
      "train step 09974 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5605 diff={max=09.3593, min=00.0040, mean=01.2437} policy_loss=-10.0724 policy updated! \n",
      "train step 09975 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.4827 diff={max=04.9942, min=00.0035, mean=01.2305} policy_loss=-12.7365 policy updated! \n",
      "train step 09976 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2764 diff={max=07.8787, min=00.0142, mean=01.2265} policy_loss=-12.1193 policy updated! \n",
      "train step 09977 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7907 diff={max=07.8895, min=00.0102, mean=01.6831} policy_loss=-12.3813 policy updated! \n",
      "train step 09978 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2888 diff={max=05.1071, min=00.0407, mean=01.2393} policy_loss=-11.7670 policy updated! \n",
      "train step 09979 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.3924 diff={max=06.6939, min=00.0246, mean=01.4133} policy_loss=-13.3552 policy updated! \n",
      "train step 09980 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.0135 diff={max=11.8348, min=00.0123, mean=01.2488} policy_loss=-13.6140 policy updated! \n",
      "train step 09981 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7626 diff={max=08.3641, min=00.0066, mean=01.4087} policy_loss=-10.5434 policy updated! \n",
      "train step 09982 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7985 diff={max=14.8471, min=00.0475, mean=01.3867} policy_loss=-11.9083 policy updated! \n",
      "train step 09983 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.6897 diff={max=12.7959, min=00.0362, mean=01.4765} policy_loss=-10.1579 policy updated! \n",
      "train step 09984 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8598 diff={max=07.7099, min=00.0020, mean=01.6104} policy_loss=-16.2071 policy updated! \n",
      "train step 09985 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.2664 diff={max=08.0085, min=00.0418, mean=01.3173} policy_loss=-11.3145 policy updated! \n",
      "train step 09986 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6658 diff={max=06.7548, min=00.0743, mean=01.2550} policy_loss=-11.4284 policy updated! \n",
      "train step 09987 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4974 diff={max=09.4846, min=00.0216, mean=00.9498} policy_loss=-10.7976 policy updated! \n",
      "train step 09988 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.9198 diff={max=07.3713, min=00.0054, mean=00.9525} policy_loss=-9.0804 policy updated! \n",
      "train step 09989 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8715 diff={max=04.9148, min=00.0155, mean=01.1359} policy_loss=-13.3234 policy updated! \n",
      "train step 09990 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.9555 diff={max=05.5634, min=00.0053, mean=00.9612} policy_loss=-12.4002 policy updated! \n",
      "train step 09991 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.8835 diff={max=15.5647, min=00.0239, mean=01.6221} policy_loss=-11.7674 policy updated! \n",
      "train step 09992 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4675 diff={max=09.9848, min=00.0006, mean=01.3330} policy_loss=-12.2931 policy updated! \n",
      "train step 09993 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3948 diff={max=07.2339, min=00.0215, mean=01.3767} policy_loss=-11.2257 policy updated! \n",
      "train step 09994 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0783 diff={max=08.4182, min=00.0017, mean=01.5191} policy_loss=-12.4424 policy updated! \n",
      "train step 09995 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.6406 diff={max=03.9712, min=00.0161, mean=00.9381} policy_loss=-10.7837 policy updated! \n",
      "train step 09996 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2527 diff={max=08.1268, min=00.0131, mean=01.2602} policy_loss=-10.2924 policy updated! \n",
      "train step 09997 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7935 diff={max=03.6478, min=00.0018, mean=00.9251} policy_loss=-8.9632 policy updated! \n",
      "train step 09998 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.9455 diff={max=06.4747, min=00.0033, mean=01.3587} policy_loss=-10.1481 policy updated! \n",
      "train step 09999 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8780 diff={max=07.8543, min=00.0432, mean=01.5436} policy_loss=-9.7375 policy updated! \n",
      "train step 10000 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.1887 diff={max=13.3500, min=00.0246, mean=01.1310} policy_loss=-11.8790 policy updated! \n",
      "train step 10001 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.2072 diff={max=04.1413, min=00.0295, mean=00.6946} policy_loss=-11.6303 policy updated! \n",
      "train step 10002 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4112 diff={max=13.9189, min=00.0276, mean=01.3323} policy_loss=-11.0361 policy updated! \n",
      "train step 10003 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6818 diff={max=11.4360, min=00.0030, mean=01.3180} policy_loss=-11.1995 policy updated! \n",
      "train step 10004 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4549 diff={max=06.5826, min=00.0116, mean=01.6987} policy_loss=-11.0594 policy updated! \n",
      "train step 10005 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.4005 diff={max=12.5720, min=00.0045, mean=01.6046} policy_loss=-11.3737 policy updated! \n",
      "train step 10006 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7051 diff={max=09.8539, min=00.0057, mean=01.4478} policy_loss=-11.8357 policy updated! \n",
      "train step 10007 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.1150 diff={max=18.5999, min=00.0223, mean=01.4267} policy_loss=-11.5116 policy updated! \n",
      "train step 10008 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2337 diff={max=08.4278, min=00.0237, mean=01.4799} policy_loss=-12.4778 policy updated! \n",
      "train step 10009 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5641 diff={max=07.3785, min=00.0122, mean=01.5018} policy_loss=-9.4151 policy updated! \n",
      "train step 10010 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.0273 diff={max=05.2828, min=00.0445, mean=01.1572} policy_loss=-9.8638 policy updated! \n",
      "train step 10011 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0669 diff={max=14.6908, min=00.0195, mean=01.4345} policy_loss=-11.7474 policy updated! \n",
      "train step 10012 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.7709 diff={max=10.5373, min=00.0104, mean=01.7039} policy_loss=-11.2614 policy updated! \n",
      "train step 10013 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.3137 diff={max=05.9922, min=00.0046, mean=01.1176} policy_loss=-10.7622 policy updated! \n",
      "train step 10014 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9050 diff={max=08.7037, min=00.0135, mean=01.3416} policy_loss=-9.0608 policy updated! \n",
      "train step 10015 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.0323 diff={max=10.2559, min=00.0053, mean=01.4598} policy_loss=-11.4295 policy updated! \n",
      "train step 10016 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2037 diff={max=09.0112, min=00.0024, mean=01.0160} policy_loss=-12.2717 policy updated! \n",
      "train step 10017 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0690 diff={max=07.7559, min=00.0240, mean=01.6361} policy_loss=-12.7890 policy updated! \n",
      "train step 10018 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4741 diff={max=07.2980, min=00.0128, mean=01.5540} policy_loss=-16.1855 policy updated! \n",
      "train step 10019 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=07.5354 diff={max=10.1170, min=00.0049, mean=01.4753} policy_loss=-10.9108 policy updated! \n",
      "train step 10020 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.2364 diff={max=16.5098, min=00.0024, mean=01.4761} policy_loss=-10.8776 policy updated! \n",
      "train step 10021 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7748 diff={max=05.8058, min=00.0190, mean=00.7753} policy_loss=-11.0249 policy updated! \n",
      "train step 10022 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.5785 diff={max=10.5549, min=00.0393, mean=01.5855} policy_loss=-10.9643 policy updated! \n",
      "train step 10023 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.8892 diff={max=06.7268, min=00.0026, mean=01.0280} policy_loss=-10.0619 policy updated! \n",
      "train step 10024 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8213 diff={max=07.8755, min=00.0102, mean=01.4922} policy_loss=-10.8489 policy updated! \n",
      "train step 10025 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=11.3330 diff={max=12.1412, min=00.0125, mean=01.9797} policy_loss=-12.1459 policy updated! \n",
      "train step 10026 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2459 diff={max=08.8386, min=00.0043, mean=01.6304} policy_loss=-11.4643 policy updated! \n",
      "train step 10027 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4628 diff={max=07.8106, min=00.0013, mean=01.5893} policy_loss=-11.8092 policy updated! \n",
      "train step 10028 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0724 diff={max=07.9661, min=00.0369, mean=01.4220} policy_loss=-12.0109 policy updated! \n",
      "train step 10029 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4480 diff={max=08.1500, min=00.0180, mean=01.2691} policy_loss=-11.1914 policy updated! \n",
      "train step 10030 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.8213 diff={max=06.1462, min=00.0027, mean=01.1526} policy_loss=-11.4974 policy updated! \n",
      "train step 10031 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.7149 diff={max=05.8437, min=00.0344, mean=01.0920} policy_loss=-11.0044 policy updated! \n",
      "train step 10032 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9367 diff={max=07.7859, min=00.0211, mean=01.9158} policy_loss=-11.8233 policy updated! \n",
      "train step 10033 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.1969 diff={max=13.3241, min=00.0126, mean=02.0966} policy_loss=-11.1052 policy updated! \n",
      "train step 10034 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0491 diff={max=06.6590, min=00.0080, mean=01.3393} policy_loss=-11.8919 policy updated! \n",
      "train step 10035 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.4562 diff={max=04.8860, min=00.0111, mean=01.3114} policy_loss=-11.5391 policy updated! \n",
      "train step 10036 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.7542 diff={max=07.8733, min=00.0104, mean=01.3365} policy_loss=-14.2935 policy updated! \n",
      "train step 10037 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.7897 diff={max=13.3053, min=00.0039, mean=01.7648} policy_loss=-13.3311 policy updated! \n",
      "train step 10038 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1553 diff={max=15.8577, min=00.0175, mean=01.3574} policy_loss=-13.0523 policy updated! \n",
      "train step 10039 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3460 diff={max=04.4772, min=00.0212, mean=01.0512} policy_loss=-12.2533 policy updated! \n",
      "train step 10040 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.2452 diff={max=16.5567, min=00.0787, mean=01.6518} policy_loss=-11.9410 policy updated! \n",
      "train step 10041 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.3900 diff={max=04.7293, min=00.0037, mean=01.1465} policy_loss=-11.1242 policy updated! \n",
      "train step 10042 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1577 diff={max=07.4678, min=00.0633, mean=01.5524} policy_loss=-12.7385 policy updated! \n",
      "train step 10043 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.3164 diff={max=05.9213, min=00.0062, mean=01.4385} policy_loss=-12.3148 policy updated! \n",
      "train step 10044 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=07.7515 diff={max=12.6182, min=00.0075, mean=01.4382} policy_loss=-9.5622 policy updated! \n",
      "train step 10045 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.6775 diff={max=04.4330, min=00.0358, mean=01.1534} policy_loss=-11.3381 policy updated! \n",
      "train step 10046 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.7519 diff={max=12.5822, min=00.0457, mean=01.9900} policy_loss=-13.6857 policy updated! \n",
      "train step 10047 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8828 diff={max=06.9634, min=00.0446, mean=01.4195} policy_loss=-10.4331 policy updated! \n",
      "train step 10048 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4539 diff={max=07.0836, min=00.0008, mean=01.6561} policy_loss=-11.1528 policy updated! \n",
      "train step 10049 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=07.8447 diff={max=10.7768, min=00.0070, mean=01.7935} policy_loss=-12.1983 policy updated! \n",
      "train step 10050 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.6026 diff={max=09.6744, min=00.0230, mean=01.5300} policy_loss=-12.0860 policy updated! \n",
      "train step 10051 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.3935 diff={max=14.7567, min=00.0397, mean=02.0440} policy_loss=-14.1425 policy updated! \n",
      "train step 10052 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=01.9796 diff={max=04.1698, min=00.0159, mean=00.9552} policy_loss=-12.3520 policy updated! \n",
      "train step 10053 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9118 diff={max=10.9930, min=00.0089, mean=01.2532} policy_loss=-10.5517 policy updated! \n",
      "train step 10054 reward={max=07.0000, min=00.0000, mean=05.4000} optimizing loss=05.6710 diff={max=10.0286, min=00.0482, mean=01.4693} policy_loss=-10.3737 policy updated! \n",
      "train step 10055 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.7387 diff={max=08.4734, min=00.0361, mean=01.3173} policy_loss=-12.9882 policy updated! \n",
      "train step 10056 reward={max=07.0000, min=00.0000, mean=04.0000} optimizing loss=03.8009 diff={max=06.0366, min=00.0052, mean=01.4601} policy_loss=-13.9814 policy updated! \n",
      "train step 10057 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4343 diff={max=06.7144, min=00.0002, mean=01.4102} policy_loss=-12.0347 policy updated! \n",
      "train step 10058 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=08.3053 diff={max=09.7977, min=00.0230, mean=01.8442} policy_loss=-11.4192 policy updated! \n",
      "train step 10059 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3772 diff={max=09.4924, min=00.0152, mean=01.3617} policy_loss=-11.6770 policy updated! \n",
      "train step 10060 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=02.7089 diff={max=05.2161, min=00.0683, mean=01.1935} policy_loss=-11.2302 policy updated! \n",
      "train step 10061 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.8461 diff={max=13.4676, min=00.0235, mean=01.8427} policy_loss=-11.1509 policy updated! \n",
      "train step 10062 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.6103 diff={max=11.6591, min=00.0020, mean=02.0747} policy_loss=-12.7844 policy updated! \n",
      "train step 10063 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=06.0878 diff={max=12.6918, min=00.0026, mean=01.3888} policy_loss=-13.7122 policy updated! \n",
      "train step 10064 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=08.7749 diff={max=10.3849, min=00.0134, mean=01.8365} policy_loss=-15.9758 policy updated! \n",
      "train step 10065 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.1663 diff={max=09.5325, min=00.0427, mean=01.3324} policy_loss=-10.0399 policy updated! \n",
      "train step 10066 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4667 diff={max=06.3926, min=00.0084, mean=00.9981} policy_loss=-12.8021 policy updated! \n",
      "train step 10067 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5928 diff={max=08.0323, min=00.0082, mean=01.4996} policy_loss=-12.1278 policy updated! \n",
      "train step 10068 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0062 diff={max=11.8446, min=00.0390, mean=01.4286} policy_loss=-12.3224 policy updated! \n",
      "train step 10069 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.2863 diff={max=06.0301, min=00.0018, mean=01.4284} policy_loss=-11.0886 policy updated! \n",
      "train step 10070 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.8990 diff={max=04.5105, min=00.0019, mean=01.0357} policy_loss=-11.1523 policy updated! \n",
      "train step 10071 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.4025 diff={max=07.7160, min=00.0089, mean=01.3701} policy_loss=-13.0362 policy updated! \n",
      "train step 10072 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9962 diff={max=06.1673, min=00.0553, mean=01.3105} policy_loss=-11.6800 policy updated! \n",
      "train step 10073 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.6862 diff={max=12.8269, min=00.0275, mean=01.7027} policy_loss=-9.8921 policy updated! \n",
      "train step 10074 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.9730 diff={max=10.7899, min=00.0038, mean=01.5384} policy_loss=-13.2504 policy updated! \n",
      "train step 10075 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.2135 diff={max=06.1044, min=00.0127, mean=01.1238} policy_loss=-10.7266 policy updated! \n",
      "train step 10076 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2441 diff={max=08.4707, min=00.0043, mean=01.0864} policy_loss=-12.6764 policy updated! \n",
      "train step 10077 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.1420 diff={max=17.7556, min=00.0055, mean=01.8894} policy_loss=-10.7785 policy updated! \n",
      "train step 10078 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=06.9794 diff={max=12.2735, min=00.0046, mean=01.5533} policy_loss=-11.2907 policy updated! \n",
      "train step 10079 reward={max=07.0000, min=00.0000, mean=05.4000} optimizing loss=06.3620 diff={max=09.1029, min=00.0163, mean=01.4569} policy_loss=-12.3716 policy updated! \n",
      "train step 10080 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.3682 diff={max=08.4042, min=00.0201, mean=01.2527} policy_loss=-12.8770 policy updated! \n",
      "train step 10081 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8782 diff={max=10.2013, min=00.0103, mean=01.3842} policy_loss=-12.3732 policy updated! \n",
      "train step 10082 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.6739 diff={max=16.0613, min=00.0274, mean=01.4985} policy_loss=-12.8375 policy updated! \n",
      "train step 10083 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.9415 diff={max=05.1138, min=00.0231, mean=01.1953} policy_loss=-9.8421 policy updated! \n",
      "train step 10084 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.6399 diff={max=10.3390, min=00.0983, mean=02.0664} policy_loss=-12.0765 policy updated! \n",
      "train step 10085 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.4545 diff={max=07.3276, min=00.0073, mean=01.2174} policy_loss=-9.4074 policy updated! \n",
      "train step 10086 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=10.1940 diff={max=18.5343, min=00.0144, mean=01.4662} policy_loss=-10.3180 policy updated! \n",
      "train step 10087 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0288 diff={max=07.6702, min=00.0210, mean=01.2413} policy_loss=-14.2103 policy updated! \n",
      "train step 10088 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8833 diff={max=07.1605, min=00.0926, mean=01.3794} policy_loss=-12.3440 policy updated! \n",
      "train step 10089 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.4590 diff={max=11.6406, min=00.0035, mean=02.1302} policy_loss=-11.7632 policy updated! \n",
      "train step 10090 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.5463 diff={max=07.6990, min=00.0323, mean=01.5805} policy_loss=-8.5522 policy updated! \n",
      "train step 10091 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.5718 diff={max=07.9999, min=00.0049, mean=01.7043} policy_loss=-10.8410 policy updated! \n",
      "train step 10092 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4358 diff={max=05.9676, min=00.0018, mean=00.9993} policy_loss=-10.4082 policy updated! \n",
      "train step 10093 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.5332 diff={max=09.0681, min=00.0041, mean=01.0989} policy_loss=-12.8847 policy updated! \n",
      "train step 10094 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5111 diff={max=08.7208, min=00.0040, mean=01.5978} policy_loss=-14.2367 policy updated! \n",
      "train step 10095 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.3614 diff={max=06.4677, min=00.0063, mean=01.4661} policy_loss=-12.4895 policy updated! \n",
      "train step 10096 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3657 diff={max=04.9351, min=00.0013, mean=01.0654} policy_loss=-12.2735 policy updated! \n",
      "train step 10097 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=15.8608 diff={max=19.3344, min=00.0304, mean=01.9331} policy_loss=-9.7560 policy updated! \n",
      "train step 10098 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.9990 diff={max=08.3229, min=00.0101, mean=01.2082} policy_loss=-11.3875 policy updated! \n",
      "train step 10099 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8771 diff={max=09.3448, min=00.0000, mean=01.3048} policy_loss=-10.0159 policy updated! \n",
      "train step 10100 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.6039 diff={max=08.3159, min=00.0290, mean=01.6072} policy_loss=-10.7457 policy updated! \n",
      "train step 10101 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.0361 diff={max=08.7282, min=00.0245, mean=01.6137} policy_loss=-10.4558 policy updated! \n",
      "train step 10102 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.3645 diff={max=13.1808, min=00.0156, mean=02.1876} policy_loss=-13.7587 policy updated! \n",
      "train step 10103 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.2761 diff={max=11.5619, min=00.0043, mean=01.5756} policy_loss=-11.3599 policy updated! \n",
      "train step 10104 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4238 diff={max=07.8005, min=00.0238, mean=01.3011} policy_loss=-14.5122 policy updated! \n",
      "train step 10105 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=10.6963 diff={max=13.8108, min=00.0091, mean=01.9763} policy_loss=-12.5618 policy updated! \n",
      "train step 10106 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3267 diff={max=08.9213, min=00.0518, mean=01.6733} policy_loss=-11.9791 policy updated! \n",
      "train step 10107 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.1310 diff={max=10.3571, min=00.0017, mean=01.8552} policy_loss=-11.7110 policy updated! \n",
      "train step 10108 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3771 diff={max=05.2020, min=00.0041, mean=01.0770} policy_loss=-11.4243 policy updated! \n",
      "train step 10109 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.3584 diff={max=04.7754, min=00.0094, mean=01.1391} policy_loss=-12.8385 policy updated! \n",
      "train step 10110 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.8951 diff={max=09.0520, min=00.0050, mean=01.6609} policy_loss=-12.6610 policy updated! \n",
      "train step 10111 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.4546 diff={max=17.5769, min=00.0365, mean=01.4796} policy_loss=-12.7683 policy updated! \n",
      "train step 10112 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0613 diff={max=12.1421, min=00.0271, mean=01.2723} policy_loss=-10.3044 policy updated! \n",
      "train step 10113 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.1073 diff={max=12.0005, min=00.0429, mean=01.7595} policy_loss=-11.6280 policy updated! \n",
      "train step 10114 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5908 diff={max=08.2208, min=00.0010, mean=01.5568} policy_loss=-11.7373 policy updated! \n",
      "train step 10115 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.2507 diff={max=09.1408, min=00.0193, mean=01.2443} policy_loss=-15.0432 policy updated! \n",
      "train step 10116 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.7198 diff={max=08.3406, min=00.0150, mean=01.5235} policy_loss=-12.5557 policy updated! \n",
      "train step 10117 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.0705 diff={max=14.4527, min=00.0065, mean=01.4300} policy_loss=-10.3341 policy updated! \n",
      "train step 10118 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.1097 diff={max=05.1101, min=00.0286, mean=01.2531} policy_loss=-10.8905 policy updated! \n",
      "train step 10119 reward={max=07.0000, min=00.0000, mean=05.4000} optimizing loss=06.3000 diff={max=11.7372, min=00.0234, mean=01.4094} policy_loss=-9.9785 policy updated! \n",
      "train step 10120 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.3658 diff={max=08.0858, min=00.0455, mean=02.1390} policy_loss=-14.6365 policy updated! \n",
      "train step 10121 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.6854 diff={max=09.6163, min=00.0243, mean=01.2792} policy_loss=-12.1764 policy updated! \n",
      "train step 10122 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8429 diff={max=03.5210, min=00.0190, mean=00.9864} policy_loss=-10.1728 policy updated! \n",
      "train step 10123 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.3288 diff={max=04.8150, min=00.0078, mean=01.4993} policy_loss=-14.5700 policy updated! \n",
      "train step 10124 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4909 diff={max=08.9754, min=00.0292, mean=01.3062} policy_loss=-12.6606 policy updated! \n",
      "train step 10125 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.2220 diff={max=10.2138, min=00.0648, mean=01.3534} policy_loss=-13.7284 policy updated! \n",
      "train step 10126 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.1591 diff={max=10.5740, min=00.0133, mean=01.4598} policy_loss=-10.6245 policy updated! \n",
      "train step 10127 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.6402 diff={max=10.5061, min=00.0227, mean=01.6299} policy_loss=-10.5149 policy updated! \n",
      "train step 10128 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1087 diff={max=05.1522, min=00.0830, mean=00.9195} policy_loss=-11.5775 policy updated! \n",
      "train step 10129 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.6868 diff={max=17.0213, min=00.0066, mean=01.8025} policy_loss=-11.2588 policy updated! \n",
      "train step 10130 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.0683 diff={max=08.6445, min=00.0326, mean=01.3832} policy_loss=-11.7431 policy updated! \n",
      "train step 10131 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=23.4127 diff={max=20.4167, min=00.0838, mean=02.7279} policy_loss=-13.9822 policy updated! \n",
      "train step 10132 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.1630 diff={max=03.4630, min=00.0135, mean=00.7758} policy_loss=-11.0070 policy updated! \n",
      "train step 10133 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0939 diff={max=12.0611, min=00.0008, mean=01.7059} policy_loss=-12.0302 policy updated! \n",
      "train step 10134 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7983 diff={max=06.4042, min=00.0272, mean=01.1422} policy_loss=-9.5067 policy updated! \n",
      "train step 10135 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.9298 diff={max=12.4625, min=00.0063, mean=01.6727} policy_loss=-12.3475 policy updated! \n",
      "train step 10136 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=08.0165 diff={max=12.3566, min=00.0430, mean=01.5725} policy_loss=-12.9271 policy updated! \n",
      "train step 10137 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8584 diff={max=05.6236, min=00.0040, mean=01.2072} policy_loss=-12.3441 policy updated! \n",
      "train step 10138 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1851 diff={max=05.6246, min=00.0071, mean=01.2229} policy_loss=-10.5175 policy updated! \n",
      "train step 10139 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4850 diff={max=09.0869, min=00.0160, mean=01.2157} policy_loss=-11.6679 policy updated! \n",
      "train step 10140 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.9680 diff={max=05.2080, min=00.0152, mean=00.9432} policy_loss=-12.4782 policy updated! \n",
      "train step 10141 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.5983 diff={max=10.6428, min=00.0361, mean=01.7460} policy_loss=-11.8074 policy updated! \n",
      "train step 10142 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.2110 diff={max=12.4380, min=00.0020, mean=01.9537} policy_loss=-13.6277 policy updated! \n",
      "train step 10143 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.9470 diff={max=09.5308, min=00.0239, mean=01.2537} policy_loss=-11.3113 policy updated! \n",
      "train step 10144 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.8818 diff={max=11.2815, min=00.0027, mean=02.2257} policy_loss=-14.1279 policy updated! \n",
      "train step 10145 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.3646 diff={max=10.5135, min=00.0143, mean=01.3870} policy_loss=-10.9883 policy updated! \n",
      "train step 10146 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9225 diff={max=06.2009, min=00.0085, mean=01.7833} policy_loss=-12.5442 policy updated! \n",
      "train step 10147 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3381 diff={max=08.5528, min=00.0404, mean=01.7228} policy_loss=-12.8502 policy updated! \n",
      "train step 10148 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=14.8828 diff={max=20.2061, min=00.0214, mean=01.9712} policy_loss=-11.2223 policy updated! \n",
      "train step 10149 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=08.9309 diff={max=14.7708, min=00.0979, mean=01.8397} policy_loss=-11.9317 policy updated! \n",
      "train step 10150 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.4001 diff={max=10.0762, min=00.0209, mean=01.6686} policy_loss=-7.6205 policy updated! \n",
      "train step 10151 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.5296 diff={max=08.2385, min=00.0704, mean=02.3439} policy_loss=-11.5465 policy updated! \n",
      "train step 10152 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3274 diff={max=07.6473, min=00.0120, mean=01.9826} policy_loss=-10.0886 policy updated! \n",
      "train step 10153 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6180 diff={max=11.1046, min=00.0424, mean=01.6860} policy_loss=-10.5376 policy updated! \n",
      "train step 10154 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.7668 diff={max=09.9496, min=00.0004, mean=01.6932} policy_loss=-11.2075 policy updated! \n",
      "train step 10155 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.5038 diff={max=10.6522, min=00.0844, mean=01.3809} policy_loss=-15.0532 policy updated! \n",
      "train step 10156 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=08.7541 diff={max=11.6718, min=00.0006, mean=01.7943} policy_loss=-12.5831 policy updated! \n",
      "train step 10157 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3300 diff={max=06.7067, min=00.0060, mean=01.6015} policy_loss=-13.2702 policy updated! \n",
      "train step 10158 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.0938 diff={max=11.6530, min=00.0034, mean=02.1172} policy_loss=-13.7750 policy updated! \n",
      "train step 10159 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=08.3481 diff={max=11.6527, min=00.0016, mean=01.9141} policy_loss=-11.0622 policy updated! \n",
      "train step 10160 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.6777 diff={max=07.4419, min=00.0135, mean=01.6839} policy_loss=-12.2922 policy updated! \n",
      "train step 10161 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3249 diff={max=08.9581, min=00.0188, mean=01.3254} policy_loss=-12.5838 policy updated! \n",
      "train step 10162 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0793 diff={max=08.6771, min=00.0019, mean=00.9442} policy_loss=-9.9556 policy updated! \n",
      "train step 10163 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2177 diff={max=13.7815, min=00.0022, mean=01.2052} policy_loss=-9.4594 policy updated! \n",
      "train step 10164 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2667 diff={max=05.0761, min=00.0018, mean=01.3457} policy_loss=-10.5608 policy updated! \n",
      "train step 10165 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.3749 diff={max=07.1880, min=00.0034, mean=01.7192} policy_loss=-10.5479 policy updated! \n",
      "train step 10166 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=08.4051 diff={max=09.0315, min=00.0090, mean=01.9958} policy_loss=-10.3700 policy updated! \n",
      "train step 10167 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.3893 diff={max=13.1734, min=00.0469, mean=01.9364} policy_loss=-11.0887 policy updated! \n",
      "train step 10168 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5315 diff={max=08.9014, min=00.0243, mean=01.4640} policy_loss=-11.4724 policy updated! \n",
      "train step 10169 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.1679 diff={max=12.3092, min=00.0081, mean=01.6921} policy_loss=-13.0422 policy updated! \n",
      "train step 10170 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.6306 diff={max=06.6567, min=00.0355, mean=01.2056} policy_loss=-13.8984 policy updated! \n",
      "train step 10171 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.0120 diff={max=08.2860, min=00.0110, mean=01.2433} policy_loss=-10.8268 policy updated! \n",
      "train step 10172 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8041 diff={max=10.1542, min=00.0095, mean=01.8275} policy_loss=-11.9834 policy updated! \n",
      "train step 10173 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5875 diff={max=07.4568, min=00.0029, mean=01.7710} policy_loss=-11.7500 policy updated! \n",
      "train step 10174 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.9717 diff={max=15.5396, min=00.0262, mean=01.6313} policy_loss=-10.9323 policy updated! \n",
      "train step 10175 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.3538 diff={max=08.5281, min=00.0013, mean=01.4464} policy_loss=-11.3457 policy updated! \n",
      "train step 10176 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0135 diff={max=07.5154, min=00.0474, mean=01.3417} policy_loss=-10.4383 policy updated! \n",
      "train step 10177 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4883 diff={max=08.2440, min=00.0138, mean=01.4574} policy_loss=-9.9129 policy updated! \n",
      "train step 10178 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=07.3751 diff={max=09.2231, min=00.0186, mean=01.7337} policy_loss=-11.2908 policy updated! \n",
      "train step 10179 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=11.4698 diff={max=15.4567, min=00.0118, mean=01.7424} policy_loss=-11.2453 policy updated! \n",
      "train step 10180 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.7740 diff={max=05.9936, min=00.0227, mean=01.5404} policy_loss=-9.5402 policy updated! \n",
      "train step 10181 reward={max=07.0000, min=00.0000, mean=04.0000} optimizing loss=08.9540 diff={max=15.9309, min=00.0318, mean=01.6003} policy_loss=-11.2186 policy updated! \n",
      "train step 10182 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.6823 diff={max=12.6068, min=00.0107, mean=01.7629} policy_loss=-14.3701 policy updated! \n",
      "train step 10183 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9499 diff={max=05.5988, min=00.0158, mean=01.1198} policy_loss=-14.0446 policy updated! \n",
      "train step 10184 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.4014 diff={max=13.1832, min=00.0645, mean=01.2245} policy_loss=-9.4473 policy updated! \n",
      "train step 10185 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.3378 diff={max=05.6665, min=00.0353, mean=00.9788} policy_loss=-10.5887 policy updated! \n",
      "train step 10186 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.6070 diff={max=07.4421, min=00.0041, mean=01.3198} policy_loss=-12.4330 policy updated! \n",
      "train step 10187 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.9479 diff={max=13.2939, min=00.0058, mean=01.6272} policy_loss=-11.6311 policy updated! \n",
      "train step 10188 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.4441 diff={max=06.8810, min=00.0018, mean=01.1913} policy_loss=-9.3866 policy updated! \n",
      "train step 10189 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.8909 diff={max=06.2538, min=00.0242, mean=01.2447} policy_loss=-11.5362 policy updated! \n",
      "train step 10190 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.8043 diff={max=08.8181, min=00.0179, mean=01.4635} policy_loss=-10.4044 policy updated! \n",
      "train step 10191 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.5576 diff={max=17.9896, min=00.0043, mean=01.8622} policy_loss=-11.6628 policy updated! \n",
      "train step 10192 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6331 diff={max=09.1941, min=00.0466, mean=01.3157} policy_loss=-11.3133 policy updated! \n",
      "train step 10193 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3352 diff={max=04.2408, min=00.0092, mean=00.8248} policy_loss=-9.7584 policy updated! \n",
      "train step 10194 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.7862 diff={max=07.6090, min=00.0191, mean=01.3548} policy_loss=-12.3308 policy updated! \n",
      "train step 10195 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.3110 diff={max=12.6523, min=00.0068, mean=01.4437} policy_loss=-9.7115 policy updated! \n",
      "train step 10196 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4645 diff={max=08.2371, min=00.0008, mean=01.2547} policy_loss=-10.9194 policy updated! \n",
      "train step 10197 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7234 diff={max=03.8493, min=00.0121, mean=00.9975} policy_loss=-10.9205 policy updated! \n",
      "train step 10198 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.1675 diff={max=16.8787, min=00.0154, mean=01.5322} policy_loss=-12.8439 policy updated! \n",
      "train step 10199 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.0242 diff={max=10.8297, min=00.0300, mean=01.3074} policy_loss=-11.7234 policy updated! \n",
      "train step 10200 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.2260 diff={max=11.4627, min=00.0549, mean=01.4004} policy_loss=-13.4433 policy updated! \n",
      "train step 10201 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7558 diff={max=08.8133, min=00.0040, mean=01.4740} policy_loss=-11.9471 policy updated! \n",
      "train step 10202 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.8160 diff={max=12.8298, min=00.0176, mean=01.7253} policy_loss=-12.9713 policy updated! \n",
      "train step 10203 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.1434 diff={max=07.5126, min=00.0333, mean=01.2790} policy_loss=-11.7061 policy updated! \n",
      "train step 10204 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.8602 diff={max=05.6705, min=00.0007, mean=01.1838} policy_loss=-14.1046 policy updated! \n",
      "train step 10205 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.4046 diff={max=07.2890, min=00.0174, mean=01.3816} policy_loss=-11.8497 policy updated! \n",
      "train step 10206 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.4425 diff={max=19.8306, min=00.0322, mean=01.5919} policy_loss=-11.9715 policy updated! \n",
      "train step 10207 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4906 diff={max=07.9320, min=00.0124, mean=01.3304} policy_loss=-15.2847 policy updated! \n",
      "train step 10208 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8964 diff={max=05.4825, min=00.0012, mean=01.1463} policy_loss=-12.3490 policy updated! \n",
      "train step 10209 reward={max=08.0000, min=00.0000, mean=05.8000} optimizing loss=05.7703 diff={max=08.2033, min=00.1012, mean=01.5709} policy_loss=-12.6939 policy updated! \n",
      "train step 10210 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.1512 diff={max=08.1009, min=00.0038, mean=01.1451} policy_loss=-12.1138 policy updated! \n",
      "train step 10211 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.1365 diff={max=10.0215, min=00.0242, mean=01.1918} policy_loss=-12.0594 policy updated! \n",
      "train step 10212 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7149 diff={max=10.6108, min=00.0443, mean=01.5325} policy_loss=-11.7287 policy updated! \n",
      "train step 10213 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1223 diff={max=05.7668, min=00.0013, mean=00.9451} policy_loss=-13.2681 policy updated! \n",
      "train step 10214 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2502 diff={max=10.2567, min=00.0304, mean=01.0997} policy_loss=-9.8719 policy updated! \n",
      "train step 10215 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.5297 diff={max=09.8510, min=00.0056, mean=01.5214} policy_loss=-12.0441 policy updated! \n",
      "train step 10216 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9331 diff={max=12.8689, min=00.0018, mean=01.4382} policy_loss=-12.8556 policy updated! \n",
      "train step 10217 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3122 diff={max=07.6078, min=00.0243, mean=01.1536} policy_loss=-10.6523 policy updated! \n",
      "train step 10218 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.8803 diff={max=11.4147, min=00.0090, mean=01.8954} policy_loss=-11.0924 policy updated! \n",
      "train step 10219 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4591 diff={max=06.7073, min=00.0270, mean=01.3363} policy_loss=-11.7907 policy updated! \n",
      "train step 10220 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.9428 diff={max=06.0008, min=00.0102, mean=01.0826} policy_loss=-11.7261 policy updated! \n",
      "train step 10221 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.7442 diff={max=06.8602, min=00.0371, mean=01.2599} policy_loss=-13.3706 policy updated! \n",
      "train step 10222 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8758 diff={max=11.5061, min=00.0023, mean=01.3119} policy_loss=-11.7955 policy updated! \n",
      "train step 10223 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1487 diff={max=06.4215, min=00.0132, mean=00.9161} policy_loss=-10.3120 policy updated! \n",
      "train step 10224 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.0021 diff={max=06.4789, min=00.0038, mean=01.2649} policy_loss=-11.0334 policy updated! \n",
      "train step 10225 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.9222 diff={max=07.3918, min=00.0223, mean=01.2021} policy_loss=-9.9453 policy updated! \n",
      "train step 10226 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.5626 diff={max=12.1751, min=00.0080, mean=01.8305} policy_loss=-13.8391 policy updated! \n",
      "train step 10227 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7242 diff={max=09.4113, min=00.0135, mean=01.2539} policy_loss=-12.5852 policy updated! \n",
      "train step 10228 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4306 diff={max=06.0764, min=00.0267, mean=00.9057} policy_loss=-10.8035 policy updated! \n",
      "train step 10229 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.7482 diff={max=07.9485, min=00.0295, mean=01.2689} policy_loss=-11.5843 policy updated! \n",
      "train step 10230 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.0596 diff={max=07.8079, min=00.0064, mean=01.3296} policy_loss=-9.5083 policy updated! \n",
      "train step 10231 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=07.1855 diff={max=08.0432, min=00.0305, mean=01.7440} policy_loss=-12.0315 policy updated! \n",
      "train step 10232 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0942 diff={max=05.5043, min=00.0319, mean=01.0818} policy_loss=-12.9195 policy updated! \n",
      "train step 10233 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9228 diff={max=10.3748, min=00.0073, mean=01.1386} policy_loss=-13.0912 policy updated! \n",
      "train step 10234 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.8874 diff={max=08.9756, min=00.0314, mean=01.3205} policy_loss=-12.9238 policy updated! \n",
      "train step 10235 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.6507 diff={max=12.1624, min=00.0199, mean=01.3266} policy_loss=-13.4663 policy updated! \n",
      "train step 10236 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7742 diff={max=12.6348, min=00.0110, mean=01.2666} policy_loss=-12.3420 policy updated! \n",
      "train step 10237 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=15.2282 diff={max=14.1891, min=00.0262, mean=02.3619} policy_loss=-14.3834 policy updated! \n",
      "train step 10238 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=09.2209 diff={max=12.7348, min=00.0192, mean=01.5880} policy_loss=-8.7193 policy updated! \n",
      "train step 10239 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9202 diff={max=06.2853, min=00.0003, mean=01.2546} policy_loss=-12.6583 policy updated! \n",
      "train step 10240 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.6982 diff={max=08.4220, min=00.0020, mean=01.2929} policy_loss=-8.6661 policy updated! \n",
      "train step 10241 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.7207 diff={max=09.4801, min=00.0452, mean=01.8222} policy_loss=-12.3629 policy updated! \n",
      "train step 10242 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4661 diff={max=08.3291, min=00.0021, mean=01.5863} policy_loss=-11.7467 policy updated! \n",
      "train step 10243 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0190 diff={max=05.5958, min=00.0158, mean=01.1189} policy_loss=-9.2644 policy updated! \n",
      "train step 10244 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.7762 diff={max=09.7949, min=00.0016, mean=01.3670} policy_loss=-8.5050 policy updated! \n",
      "train step 10245 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.0114 diff={max=08.8822, min=00.0158, mean=01.4441} policy_loss=-12.5239 policy updated! \n",
      "train step 10246 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4020 diff={max=08.3687, min=00.0089, mean=01.0998} policy_loss=-9.4460 policy updated! \n",
      "train step 10247 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2952 diff={max=10.3890, min=00.0408, mean=01.4368} policy_loss=-14.6243 policy updated! \n",
      "train step 10248 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.7333 diff={max=15.2989, min=00.0168, mean=01.5993} policy_loss=-11.6165 policy updated! \n",
      "train step 10249 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8538 diff={max=06.8448, min=00.0512, mean=01.6271} policy_loss=-11.9989 policy updated! \n",
      "train step 10250 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.6035 diff={max=05.6854, min=00.0013, mean=00.7633} policy_loss=-9.8950 policy updated! \n",
      "train step 10251 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2721 diff={max=04.2369, min=00.0872, mean=01.1386} policy_loss=-12.6982 policy updated! \n",
      "train step 10252 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1985 diff={max=09.4551, min=00.0359, mean=01.6714} policy_loss=-12.6821 policy updated! \n",
      "train step 10253 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.5343 diff={max=06.0894, min=00.0046, mean=01.6412} policy_loss=-12.3029 policy updated! \n",
      "train step 10254 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9826 diff={max=08.5026, min=00.0456, mean=01.3481} policy_loss=-13.0642 policy updated! \n",
      "train step 10255 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.7551 diff={max=09.5114, min=00.0658, mean=01.2163} policy_loss=-11.2656 policy updated! \n",
      "train step 10256 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5503 diff={max=09.0051, min=00.0123, mean=01.6129} policy_loss=-10.2009 policy updated! \n",
      "train step 10257 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.8462 diff={max=12.3143, min=00.0302, mean=01.9378} policy_loss=-12.4487 policy updated! \n",
      "train step 10258 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6584 diff={max=09.1014, min=00.0207, mean=01.2804} policy_loss=-12.5758 policy updated! \n",
      "train step 10259 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3395 diff={max=06.4798, min=00.0295, mean=00.9183} policy_loss=-10.7461 policy updated! \n",
      "train step 10260 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.3301 diff={max=07.7954, min=00.0309, mean=01.6387} policy_loss=-13.6190 policy updated! \n",
      "train step 10261 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0713 diff={max=06.5549, min=00.0390, mean=01.3165} policy_loss=-11.2082 policy updated! \n",
      "train step 10262 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5332 diff={max=07.2422, min=00.0290, mean=01.1380} policy_loss=-14.7101 policy updated! \n",
      "train step 10263 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.7789 diff={max=07.4232, min=00.0008, mean=01.1719} policy_loss=-13.6424 policy updated! \n",
      "train step 10264 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5440 diff={max=09.0365, min=00.0047, mean=01.1091} policy_loss=-12.8696 policy updated! \n",
      "train step 10265 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.7565 diff={max=11.1796, min=00.0273, mean=01.6666} policy_loss=-11.1349 policy updated! \n",
      "train step 10266 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.4316 diff={max=11.8841, min=00.0011, mean=01.5023} policy_loss=-11.8344 policy updated! \n",
      "train step 10267 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5096 diff={max=09.2152, min=00.0063, mean=01.6225} policy_loss=-9.0855 policy updated! \n",
      "train step 10268 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0427 diff={max=07.6735, min=00.0350, mean=01.3641} policy_loss=-11.3461 policy updated! \n",
      "train step 10269 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.0013 diff={max=13.4099, min=00.0077, mean=01.8840} policy_loss=-10.8875 policy updated! \n",
      "train step 10270 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.5972 diff={max=09.0668, min=00.0017, mean=01.3424} policy_loss=-13.4656 policy updated! \n",
      "train step 10271 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.2177 diff={max=09.4722, min=00.0021, mean=01.6535} policy_loss=-10.8924 policy updated! \n",
      "train step 10272 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.7437 diff={max=16.7728, min=00.0458, mean=01.8907} policy_loss=-12.2217 policy updated! \n",
      "train step 10273 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.6463 diff={max=09.3336, min=00.0049, mean=01.7944} policy_loss=-12.9667 policy updated! \n",
      "train step 10274 reward={max=07.0000, min=00.0000, mean=05.4000} optimizing loss=04.1963 diff={max=05.5624, min=00.0573, mean=01.5551} policy_loss=-12.4303 policy updated! \n",
      "train step 10275 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.4679 diff={max=06.3177, min=00.0070, mean=01.1905} policy_loss=-11.8357 policy updated! \n",
      "train step 10276 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7904 diff={max=06.7131, min=00.0044, mean=01.1335} policy_loss=-11.3626 policy updated! \n",
      "train step 10277 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5540 diff={max=09.7528, min=00.0017, mean=01.1960} policy_loss=-8.2075 policy updated! \n",
      "train step 10278 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=03.4407 diff={max=07.0187, min=00.0073, mean=01.2012} policy_loss=-9.9102 policy updated! \n",
      "train step 10279 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.4735 diff={max=12.3469, min=00.0096, mean=01.7077} policy_loss=-10.1573 policy updated! \n",
      "train step 10280 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.7981 diff={max=16.3866, min=00.0173, mean=01.6190} policy_loss=-11.7793 policy updated! \n",
      "train step 10281 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.1036 diff={max=05.4282, min=00.0104, mean=01.2660} policy_loss=-11.8540 policy updated! \n",
      "train step 10282 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8588 diff={max=07.3911, min=00.0012, mean=01.8433} policy_loss=-11.9010 policy updated! \n",
      "train step 10283 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9403 diff={max=07.9297, min=00.0022, mean=01.6587} policy_loss=-11.7583 policy updated! \n",
      "train step 10284 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=07.6421 diff={max=12.6039, min=00.0195, mean=01.4999} policy_loss=-10.8546 policy updated! \n",
      "train step 10285 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.1730 diff={max=06.1418, min=00.0360, mean=01.1598} policy_loss=-10.9578 policy updated! \n",
      "train step 10286 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4641 diff={max=08.0386, min=00.0110, mean=01.1318} policy_loss=-11.3597 policy updated! \n",
      "train step 10287 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6060 diff={max=10.2389, min=00.0020, mean=01.5640} policy_loss=-14.5916 policy updated! \n",
      "train step 10288 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6018 diff={max=08.0079, min=00.0362, mean=01.6770} policy_loss=-13.7231 policy updated! \n",
      "train step 10289 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5555 diff={max=07.8438, min=00.0055, mean=01.4328} policy_loss=-10.6154 policy updated! \n",
      "train step 10290 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.4771 diff={max=09.0597, min=00.0197, mean=01.4073} policy_loss=-11.7156 policy updated! \n",
      "train step 10291 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.8302 diff={max=09.7554, min=00.0408, mean=01.7292} policy_loss=-11.4698 policy updated! \n",
      "train step 10292 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.9098 diff={max=15.9076, min=00.0203, mean=01.7698} policy_loss=-11.5789 policy updated! \n",
      "train step 10293 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1856 diff={max=08.0369, min=00.0142, mean=01.5195} policy_loss=-9.5669 policy updated! \n",
      "train step 10294 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1516 diff={max=08.2600, min=00.0755, mean=01.4433} policy_loss=-10.2552 policy updated! \n",
      "train step 10295 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.0967 diff={max=07.1200, min=00.0055, mean=01.5329} policy_loss=-10.3649 policy updated! \n",
      "train step 10296 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4059 diff={max=06.6100, min=00.0002, mean=01.3463} policy_loss=-10.1214 policy updated! \n",
      "train step 10297 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3131 diff={max=09.5433, min=00.0173, mean=01.5622} policy_loss=-13.3384 policy updated! \n",
      "train step 10298 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.3308 diff={max=12.4731, min=00.0041, mean=01.2813} policy_loss=-12.7325 policy updated! \n",
      "train step 10299 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9163 diff={max=11.6577, min=00.0136, mean=01.1560} policy_loss=-11.3246 policy updated! \n",
      "train step 10300 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.1203 diff={max=07.9615, min=00.0057, mean=01.2050} policy_loss=-12.3286 policy updated! \n",
      "train step 10301 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.5832 diff={max=13.2481, min=00.0688, mean=01.9853} policy_loss=-14.4463 policy updated! \n",
      "train step 10302 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2352 diff={max=07.3519, min=00.0014, mean=01.3559} policy_loss=-14.2326 policy updated! \n",
      "train step 10303 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=08.4428 diff={max=12.4417, min=00.0271, mean=01.8462} policy_loss=-14.1431 policy updated! \n",
      "train step 10304 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.9502 diff={max=11.5208, min=00.0161, mean=01.5250} policy_loss=-14.0953 policy updated! \n",
      "train step 10305 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.1847 diff={max=08.7044, min=00.0419, mean=01.4698} policy_loss=-11.4480 policy updated! \n",
      "train step 10306 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.7010 diff={max=06.5469, min=00.0247, mean=01.6554} policy_loss=-12.0846 policy updated! \n",
      "train step 10307 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1750 diff={max=09.0050, min=00.0308, mean=01.4830} policy_loss=-12.4539 policy updated! \n",
      "train step 10308 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5136 diff={max=06.4268, min=00.0071, mean=01.1727} policy_loss=-12.0470 policy updated! \n",
      "train step 10309 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.1949 diff={max=03.4728, min=00.0416, mean=01.0779} policy_loss=-11.4707 policy updated! \n",
      "train step 10310 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=10.9598 diff={max=14.3136, min=00.0761, mean=01.8080} policy_loss=-11.4649 policy updated! \n",
      "train step 10311 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.3135 diff={max=14.0998, min=00.0353, mean=01.8387} policy_loss=-11.2283 policy updated! \n",
      "train step 10312 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4709 diff={max=07.3340, min=00.0143, mean=01.6278} policy_loss=-9.9156 policy updated! \n",
      "train step 10313 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7880 diff={max=10.1643, min=00.0030, mean=01.5721} policy_loss=-10.1827 policy updated! \n",
      "train step 10314 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7243 diff={max=05.9353, min=00.0035, mean=01.2332} policy_loss=-11.0105 policy updated! \n",
      "train step 10315 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.9825 diff={max=04.7097, min=00.0089, mean=00.9690} policy_loss=-9.3837 policy updated! \n",
      "train step 10316 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.9982 diff={max=07.6158, min=00.0163, mean=01.4493} policy_loss=-11.6784 policy updated! \n",
      "train step 10317 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9811 diff={max=07.4816, min=00.0140, mean=01.0613} policy_loss=-12.1625 policy updated! \n",
      "train step 10318 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.2197 diff={max=09.4562, min=00.0057, mean=01.2455} policy_loss=-11.0428 policy updated! \n",
      "train step 10319 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=06.7332 diff={max=12.3349, min=00.0192, mean=01.4689} policy_loss=-13.1538 policy updated! \n",
      "train step 10320 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.7161 diff={max=07.7518, min=00.0291, mean=01.7494} policy_loss=-13.1822 policy updated! \n",
      "train step 10321 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6247 diff={max=04.9305, min=00.0067, mean=01.0708} policy_loss=-12.0283 policy updated! \n",
      "train step 10322 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8392 diff={max=07.6212, min=00.0061, mean=01.4132} policy_loss=-13.0411 policy updated! \n",
      "train step 10323 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3307 diff={max=04.6380, min=00.0299, mean=01.0114} policy_loss=-13.3586 policy updated! \n",
      "train step 10324 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.4399 diff={max=11.5667, min=00.0078, mean=01.5472} policy_loss=-11.7942 policy updated! \n",
      "train step 10325 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.4944 diff={max=03.9548, min=00.0358, mean=01.1432} policy_loss=-10.7910 policy updated! \n",
      "train step 10326 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=08.7862 diff={max=11.0557, min=00.0589, mean=01.9404} policy_loss=-10.3623 policy updated! \n",
      "train step 10327 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9613 diff={max=06.4109, min=00.0252, mean=01.3313} policy_loss=-10.5734 policy updated! \n",
      "train step 10328 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1460 diff={max=05.0283, min=00.0219, mean=00.9998} policy_loss=-11.7184 policy updated! \n",
      "train step 10329 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.7193 diff={max=06.1120, min=00.0118, mean=01.3120} policy_loss=-13.4825 policy updated! \n",
      "train step 10330 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.1554 diff={max=11.6646, min=00.0037, mean=01.4352} policy_loss=-12.2021 policy updated! \n",
      "train step 10331 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=08.8757 diff={max=13.6223, min=00.0133, mean=01.6581} policy_loss=-11.7654 policy updated! \n",
      "train step 10332 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5159 diff={max=06.6190, min=00.0387, mean=01.0506} policy_loss=-13.4952 policy updated! \n",
      "train step 10333 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6317 diff={max=12.7102, min=00.0233, mean=01.4516} policy_loss=-13.6976 policy updated! \n",
      "train step 10334 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4044 diff={max=06.4526, min=00.0063, mean=01.2462} policy_loss=-11.0258 policy updated! \n",
      "train step 10335 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.8926 diff={max=05.7063, min=00.0027, mean=00.9245} policy_loss=-11.4607 policy updated! \n",
      "train step 10336 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2534 diff={max=06.4106, min=00.0451, mean=01.3838} policy_loss=-12.0912 policy updated! \n",
      "train step 10337 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2782 diff={max=08.7437, min=00.0041, mean=01.6467} policy_loss=-11.2465 policy updated! \n",
      "train step 10338 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0608 diff={max=14.8641, min=00.0136, mean=01.2192} policy_loss=-9.8666 policy updated! \n",
      "train step 10339 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.8353 diff={max=06.4773, min=00.0398, mean=01.1387} policy_loss=-10.6154 policy updated! \n",
      "train step 10340 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.9259 diff={max=11.0675, min=00.0378, mean=01.2127} policy_loss=-12.1468 policy updated! \n",
      "train step 10341 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.1794 diff={max=06.5189, min=00.0301, mean=01.0135} policy_loss=-11.6452 policy updated! \n",
      "train step 10342 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2211 diff={max=07.4339, min=00.0254, mean=01.0609} policy_loss=-10.9897 policy updated! \n",
      "train step 10343 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0856 diff={max=07.1611, min=00.0350, mean=01.4650} policy_loss=-14.0459 policy updated! \n",
      "train step 10344 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6036 diff={max=04.1261, min=00.0343, mean=00.9310} policy_loss=-11.5699 policy updated! \n",
      "train step 10345 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.1226 diff={max=09.0037, min=00.0402, mean=01.5248} policy_loss=-11.5702 policy updated! \n",
      "train step 10346 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.4117 diff={max=11.7976, min=00.0125, mean=01.6325} policy_loss=-10.5150 policy updated! \n",
      "train step 10347 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8458 diff={max=08.4924, min=00.0234, mean=01.5101} policy_loss=-12.0699 policy updated! \n",
      "train step 10348 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.0305 diff={max=04.0478, min=00.0031, mean=01.2641} policy_loss=-13.9216 policy updated! \n",
      "train step 10349 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4688 diff={max=13.7538, min=00.0087, mean=01.1785} policy_loss=-11.2179 policy updated! \n",
      "train step 10350 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.8890 diff={max=05.6915, min=00.0268, mean=01.4098} policy_loss=-14.5966 policy updated! \n",
      "train step 10351 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.8079 diff={max=05.7380, min=00.0007, mean=01.0669} policy_loss=-10.9335 policy updated! \n",
      "train step 10352 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4223 diff={max=07.4373, min=00.0038, mean=01.2393} policy_loss=-9.0552 policy updated! \n",
      "train step 10353 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6534 diff={max=08.8838, min=00.0140, mean=01.2596} policy_loss=-9.2274 policy updated! \n",
      "train step 10354 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3594 diff={max=08.7711, min=00.0215, mean=01.1541} policy_loss=-9.0905 policy updated! \n",
      "train step 10355 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.0427 diff={max=06.0152, min=00.0049, mean=01.3975} policy_loss=-11.7032 policy updated! \n",
      "train step 10356 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.7799 diff={max=19.6207, min=00.0099, mean=01.4135} policy_loss=-11.2324 policy updated! \n",
      "train step 10357 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3249 diff={max=08.3498, min=00.0254, mean=01.7999} policy_loss=-11.2908 policy updated! \n",
      "train step 10358 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1105 diff={max=05.4029, min=00.0230, mean=01.1661} policy_loss=-12.4172 policy updated! \n",
      "train step 10359 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7842 diff={max=05.3166, min=00.0399, mean=00.9033} policy_loss=-10.9892 policy updated! \n",
      "train step 10360 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.9228 diff={max=08.2424, min=00.0046, mean=01.6495} policy_loss=-11.5444 policy updated! \n",
      "train step 10361 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1287 diff={max=05.7947, min=00.0077, mean=00.9765} policy_loss=-11.3731 policy updated! \n",
      "train step 10362 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0101 diff={max=09.3035, min=00.0361, mean=01.6008} policy_loss=-10.9007 policy updated! \n",
      "train step 10363 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9354 diff={max=09.7310, min=00.0103, mean=01.2478} policy_loss=-12.1420 policy updated! \n",
      "train step 10364 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1590 diff={max=08.0103, min=00.0029, mean=01.2138} policy_loss=-12.0926 policy updated! \n",
      "train step 10365 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.4376 diff={max=09.1833, min=00.0142, mean=01.8188} policy_loss=-13.3090 policy updated! \n",
      "train step 10366 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1235 diff={max=07.7881, min=00.0050, mean=01.3602} policy_loss=-10.1430 policy updated! \n",
      "train step 10367 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.7885 diff={max=17.4404, min=00.0249, mean=01.8720} policy_loss=-11.6247 policy updated! \n",
      "train step 10368 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1053 diff={max=05.6983, min=00.0034, mean=01.3467} policy_loss=-11.1286 policy updated! \n",
      "train step 10369 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=17.3933 diff={max=14.1833, min=00.0006, mean=02.3727} policy_loss=-14.3429 policy updated! \n",
      "train step 10370 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.9243 diff={max=04.4499, min=00.0214, mean=00.9619} policy_loss=-9.4418 policy updated! \n",
      "train step 10371 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4997 diff={max=11.3726, min=00.0472, mean=01.0607} policy_loss=-9.9503 policy updated! \n",
      "train step 10372 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.3745 diff={max=11.1917, min=00.0718, mean=01.9571} policy_loss=-12.8598 policy updated! \n",
      "train step 10373 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.1302 diff={max=05.8913, min=00.0005, mean=01.5008} policy_loss=-12.5272 policy updated! \n",
      "train step 10374 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4464 diff={max=11.2061, min=00.0397, mean=01.5284} policy_loss=-11.5143 policy updated! \n",
      "train step 10375 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.6221 diff={max=06.8969, min=00.0410, mean=01.5735} policy_loss=-13.2858 policy updated! \n",
      "train step 10376 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=07.9491 diff={max=09.5154, min=00.0041, mean=01.7785} policy_loss=-13.0203 policy updated! \n",
      "train step 10377 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8598 diff={max=10.0696, min=00.0125, mean=01.6028} policy_loss=-13.6117 policy updated! \n",
      "train step 10378 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.1966 diff={max=08.1390, min=00.0469, mean=01.4209} policy_loss=-11.7478 policy updated! \n",
      "train step 10379 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.0810 diff={max=09.0064, min=00.0246, mean=01.6668} policy_loss=-13.9953 policy updated! \n",
      "train step 10380 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.2785 diff={max=12.0987, min=00.0096, mean=01.8432} policy_loss=-12.7127 policy updated! \n",
      "train step 10381 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6914 diff={max=09.8546, min=00.0201, mean=01.4679} policy_loss=-12.3725 policy updated! \n",
      "train step 10382 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=07.5192 diff={max=08.3163, min=00.0100, mean=01.7098} policy_loss=-11.3979 policy updated! \n",
      "train step 10383 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.5478 diff={max=07.3154, min=00.0378, mean=01.4561} policy_loss=-11.7308 policy updated! \n",
      "train step 10384 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1430 diff={max=07.6156, min=00.0334, mean=01.7211} policy_loss=-11.3077 policy updated! \n",
      "train step 10385 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=11.9173 diff={max=14.4248, min=00.0007, mean=01.9956} policy_loss=-13.5063 policy updated! \n",
      "train step 10386 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5846 diff={max=04.5364, min=00.0201, mean=01.1417} policy_loss=-11.4789 policy updated! \n",
      "train step 10387 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1823 diff={max=10.3371, min=00.0098, mean=01.5565} policy_loss=-13.2914 policy updated! \n",
      "train step 10388 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9392 diff={max=11.6213, min=00.0017, mean=01.3380} policy_loss=-12.3694 policy updated! \n",
      "train step 10389 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.6434 diff={max=08.0543, min=00.0075, mean=01.1877} policy_loss=-13.9945 policy updated! \n",
      "train step 10390 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.5389 diff={max=08.9521, min=00.0135, mean=01.6030} policy_loss=-11.0752 policy updated! \n",
      "train step 10391 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=07.1759 diff={max=12.4007, min=00.0067, mean=01.2481} policy_loss=-9.9565 policy updated! \n",
      "train step 10392 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.5808 diff={max=14.0705, min=00.0285, mean=01.5377} policy_loss=-12.8886 policy updated! \n",
      "train step 10393 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9216 diff={max=12.8275, min=00.0370, mean=01.5271} policy_loss=-11.4051 policy updated! \n",
      "train step 10394 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0473 diff={max=07.7600, min=00.0285, mean=01.2825} policy_loss=-11.9628 policy updated! \n",
      "train step 10395 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.6959 diff={max=06.4545, min=00.0058, mean=01.2872} policy_loss=-10.8177 policy updated! \n",
      "train step 10396 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4810 diff={max=10.6146, min=00.0157, mean=01.3450} policy_loss=-10.7070 policy updated! \n",
      "train step 10397 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4701 diff={max=08.2026, min=00.0173, mean=01.4252} policy_loss=-11.6106 policy updated! \n",
      "train step 10398 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.9536 diff={max=09.3727, min=00.0001, mean=01.2738} policy_loss=-11.1053 policy updated! \n",
      "train step 10399 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.5864 diff={max=13.2931, min=00.0112, mean=01.6454} policy_loss=-11.7914 policy updated! \n",
      "train step 10400 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.9856 diff={max=10.6591, min=00.0696, mean=01.4500} policy_loss=-9.6144 policy updated! \n",
      "train step 10401 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.9495 diff={max=08.0187, min=00.0084, mean=01.2677} policy_loss=-11.3469 policy updated! \n",
      "train step 10402 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4718 diff={max=14.8346, min=00.0145, mean=01.0341} policy_loss=-10.6767 policy updated! \n",
      "train step 10403 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=06.7546 diff={max=09.0357, min=00.0135, mean=01.7054} policy_loss=-10.4725 policy updated! \n",
      "train step 10404 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5700 diff={max=07.3144, min=00.0490, mean=01.5696} policy_loss=-11.8405 policy updated! \n",
      "train step 10405 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=11.2228 diff={max=15.6358, min=00.0113, mean=01.7204} policy_loss=-12.0402 policy updated! \n",
      "train step 10406 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.3730 diff={max=08.1406, min=00.0041, mean=01.4760} policy_loss=-14.8700 policy updated! \n",
      "train step 10407 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8476 diff={max=13.5765, min=00.0210, mean=01.3797} policy_loss=-11.8619 policy updated! \n",
      "train step 10408 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.4778 diff={max=13.5709, min=00.0027, mean=01.6348} policy_loss=-12.6259 policy updated! \n",
      "train step 10409 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.6768 diff={max=05.5359, min=00.0114, mean=01.0954} policy_loss=-13.0453 policy updated! \n",
      "train step 10410 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.4435 diff={max=08.5804, min=00.0105, mean=01.2483} policy_loss=-11.8642 policy updated! \n",
      "train step 10411 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5095 diff={max=08.8114, min=00.0046, mean=01.4214} policy_loss=-12.3466 policy updated! \n",
      "train step 10412 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.8042 diff={max=12.7905, min=00.0017, mean=01.8021} policy_loss=-9.2922 policy updated! \n",
      "train step 10413 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9628 diff={max=08.3747, min=00.0190, mean=01.6664} policy_loss=-9.9950 policy updated! \n",
      "train step 10414 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6852 diff={max=11.6800, min=00.0003, mean=01.2515} policy_loss=-11.8632 policy updated! \n",
      "train step 10415 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.1456 diff={max=08.5908, min=00.0319, mean=01.3837} policy_loss=-12.1654 policy updated! \n",
      "train step 10416 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2541 diff={max=08.4284, min=00.0329, mean=01.2393} policy_loss=-13.0155 policy updated! \n",
      "train step 10417 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3444 diff={max=05.0300, min=00.0196, mean=01.0671} policy_loss=-11.3176 policy updated! \n",
      "train step 10418 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1880 diff={max=06.3461, min=00.0971, mean=01.2069} policy_loss=-10.5932 policy updated! \n",
      "train step 10419 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.3309 diff={max=13.1081, min=00.0156, mean=01.6764} policy_loss=-12.1879 policy updated! \n",
      "train step 10420 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.3975 diff={max=14.0933, min=00.0233, mean=01.3929} policy_loss=-13.9771 policy updated! \n",
      "train step 10421 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6112 diff={max=08.4385, min=00.0140, mean=01.6188} policy_loss=-13.6947 policy updated! \n",
      "train step 10422 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2983 diff={max=08.9122, min=00.0018, mean=01.6011} policy_loss=-10.6969 policy updated! \n",
      "train step 10423 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7916 diff={max=07.0049, min=00.0035, mean=01.4576} policy_loss=-12.6706 policy updated! \n",
      "train step 10424 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.3370 diff={max=16.1633, min=00.0091, mean=01.7020} policy_loss=-12.5158 policy updated! \n",
      "train step 10425 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.4924 diff={max=11.2905, min=00.0007, mean=01.3295} policy_loss=-9.4407 policy updated! \n",
      "train step 10426 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.0869 diff={max=10.0308, min=00.0466, mean=01.5103} policy_loss=-12.5018 policy updated! \n",
      "train step 10427 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9675 diff={max=08.9651, min=00.0001, mean=01.5228} policy_loss=-11.9966 policy updated! \n",
      "train step 10428 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.4892 diff={max=11.7936, min=00.0115, mean=01.8498} policy_loss=-11.3655 policy updated! \n",
      "train step 10429 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.7400 diff={max=09.9698, min=00.0012, mean=01.8501} policy_loss=-14.1905 policy updated! \n",
      "train step 10430 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.5383 diff={max=05.8114, min=00.0089, mean=01.3957} policy_loss=-12.3172 policy updated! \n",
      "train step 10431 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=19.4911 diff={max=21.6700, min=00.0008, mean=02.4246} policy_loss=-14.2245 policy updated! \n",
      "train step 10432 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6056 diff={max=05.6452, min=00.0172, mean=01.5444} policy_loss=-11.7839 policy updated! \n",
      "train step 10433 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3831 diff={max=09.6852, min=00.0191, mean=01.2093} policy_loss=-13.8627 policy updated! \n",
      "train step 10434 reward={max=07.0000, min=00.0000, mean=05.4000} optimizing loss=04.6165 diff={max=06.2519, min=00.0387, mean=01.5071} policy_loss=-11.5416 policy updated! \n",
      "train step 10435 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.5640 diff={max=08.5148, min=00.0044, mean=01.6221} policy_loss=-10.2944 policy updated! \n",
      "train step 10436 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.1395 diff={max=11.4431, min=00.0129, mean=01.5508} policy_loss=-9.5336 policy updated! \n",
      "train step 10437 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0407 diff={max=08.9753, min=00.0544, mean=01.2075} policy_loss=-11.7214 policy updated! \n",
      "train step 10438 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0126 diff={max=10.0266, min=00.0267, mean=01.3210} policy_loss=-9.4816 policy updated! \n",
      "train step 10439 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.5976 diff={max=17.0373, min=00.0009, mean=01.6820} policy_loss=-9.8601 policy updated! \n",
      "train step 10440 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.1595 diff={max=07.4632, min=00.0076, mean=01.1146} policy_loss=-12.5774 policy updated! \n",
      "train step 10441 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4587 diff={max=11.4645, min=00.0237, mean=01.4225} policy_loss=-10.7007 policy updated! \n",
      "train step 10442 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7648 diff={max=08.3704, min=00.0088, mean=01.5180} policy_loss=-12.2661 policy updated! \n",
      "train step 10443 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9838 diff={max=08.4666, min=00.0205, mean=01.3198} policy_loss=-13.5941 policy updated! \n",
      "train step 10444 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.8148 diff={max=06.7481, min=00.0011, mean=01.4078} policy_loss=-12.2582 policy updated! \n",
      "train step 10445 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.4429 diff={max=08.2694, min=00.0278, mean=01.5514} policy_loss=-15.6986 policy updated! \n",
      "train step 10446 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.1491 diff={max=12.7424, min=00.0019, mean=01.8229} policy_loss=-13.8833 policy updated! \n",
      "train step 10447 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4781 diff={max=06.9626, min=00.0079, mean=01.1267} policy_loss=-13.0443 policy updated! \n",
      "train step 10448 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.2435 diff={max=10.1324, min=00.0623, mean=01.7186} policy_loss=-12.6206 policy updated! \n",
      "train step 10449 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1871 diff={max=07.4571, min=00.0257, mean=01.7801} policy_loss=-12.2532 policy updated! \n",
      "train step 10450 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=05.1385 diff={max=07.6894, min=00.0348, mean=01.4488} policy_loss=-11.2074 policy updated! \n",
      "train step 10451 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5615 diff={max=06.8006, min=00.0110, mean=01.5263} policy_loss=-11.4961 policy updated! \n",
      "train step 10452 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.4513 diff={max=15.2439, min=00.0245, mean=01.7532} policy_loss=-11.2243 policy updated! \n",
      "train step 10453 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0103 diff={max=08.5862, min=00.0628, mean=01.4520} policy_loss=-10.3416 policy updated! \n",
      "train step 10454 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.6931 diff={max=15.3738, min=00.0074, mean=01.7882} policy_loss=-12.8933 policy updated! \n",
      "train step 10455 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.7669 diff={max=08.8943, min=00.0024, mean=01.1549} policy_loss=-10.1632 policy updated! \n",
      "train step 10456 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.0736 diff={max=10.4293, min=00.0034, mean=01.8588} policy_loss=-11.1703 policy updated! \n",
      "train step 10457 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=15.2917 diff={max=22.5389, min=00.0009, mean=01.6255} policy_loss=-10.6046 policy updated! \n",
      "train step 10458 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.8067 diff={max=13.8880, min=00.0087, mean=01.7332} policy_loss=-11.8024 policy updated! \n",
      "train step 10459 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5412 diff={max=08.5942, min=00.0521, mean=01.4933} policy_loss=-12.1400 policy updated! \n",
      "train step 10460 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=15.2474 diff={max=15.1839, min=00.0180, mean=02.1312} policy_loss=-12.9097 policy updated! \n",
      "train step 10461 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.6512 diff={max=09.6018, min=00.0041, mean=01.5009} policy_loss=-10.7384 policy updated! \n",
      "train step 10462 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9942 diff={max=09.0610, min=00.0222, mean=01.5403} policy_loss=-9.8948 policy updated! \n",
      "train step 10463 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0884 diff={max=06.6302, min=00.0120, mean=01.0558} policy_loss=-7.8979 policy updated! \n",
      "train step 10464 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=07.6010 diff={max=09.4437, min=00.0009, mean=01.6551} policy_loss=-16.1848 policy updated! \n",
      "train step 10465 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.5031 diff={max=05.9487, min=00.0153, mean=01.4933} policy_loss=-14.2370 policy updated! \n",
      "train step 10466 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6771 diff={max=07.0552, min=00.0104, mean=01.2661} policy_loss=-14.8492 policy updated! \n",
      "train step 10467 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8255 diff={max=07.4304, min=00.0023, mean=01.6088} policy_loss=-12.7058 policy updated! \n",
      "train step 10468 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=06.0319 diff={max=09.6560, min=00.0173, mean=01.5552} policy_loss=-12.7955 policy updated! \n",
      "train step 10469 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.9600 diff={max=11.3770, min=00.0429, mean=02.0580} policy_loss=-11.5905 policy updated! \n",
      "train step 10470 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.3430 diff={max=06.7620, min=00.0411, mean=01.2951} policy_loss=-11.6542 policy updated! \n",
      "train step 10471 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.3667 diff={max=17.0867, min=00.0169, mean=01.6072} policy_loss=-11.2053 policy updated! \n",
      "train step 10472 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.0266 diff={max=12.1870, min=00.0392, mean=01.9042} policy_loss=-12.1572 policy updated! \n",
      "train step 10473 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9176 diff={max=08.1077, min=00.0257, mean=01.4180} policy_loss=-13.5812 policy updated! \n",
      "train step 10474 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3300 diff={max=06.8372, min=00.0057, mean=01.4457} policy_loss=-13.0452 policy updated! \n",
      "train step 10475 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=29.0008 diff={max=31.4662, min=00.0079, mean=02.5116} policy_loss=-14.2799 policy updated! \n",
      "train step 10476 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3210 diff={max=10.7834, min=00.0222, mean=01.3307} policy_loss=-12.5867 policy updated! \n",
      "train step 10477 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.0431 diff={max=17.8770, min=00.0071, mean=01.6562} policy_loss=-11.9142 policy updated! \n",
      "train step 10478 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.7380 diff={max=12.0075, min=00.0177, mean=01.6821} policy_loss=-10.2954 policy updated! \n",
      "train step 10479 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=09.3850 diff={max=13.6902, min=00.0022, mean=01.8428} policy_loss=-12.6225 policy updated! \n",
      "train step 10480 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.0326 diff={max=10.7703, min=00.0097, mean=01.5926} policy_loss=-12.8174 policy updated! \n",
      "train step 10481 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.5639 diff={max=11.6741, min=00.0060, mean=01.6960} policy_loss=-10.8297 policy updated! \n",
      "train step 10482 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9604 diff={max=07.7939, min=00.0555, mean=01.6470} policy_loss=-13.0881 policy updated! \n",
      "train step 10483 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.2286 diff={max=09.6553, min=00.1133, mean=02.0861} policy_loss=-12.4171 policy updated! \n",
      "train step 10484 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3685 diff={max=08.0308, min=00.0172, mean=01.3625} policy_loss=-10.6506 policy updated! \n",
      "train step 10485 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=03.6484 diff={max=07.3034, min=00.0246, mean=01.2042} policy_loss=-11.0078 policy updated! \n",
      "train step 10486 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7725 diff={max=05.5464, min=00.0215, mean=00.8891} policy_loss=-10.8231 policy updated! \n",
      "train step 10487 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.6057 diff={max=12.1194, min=00.0004, mean=01.9989} policy_loss=-12.9090 policy updated! \n",
      "train step 10488 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7482 diff={max=08.8170, min=00.0242, mean=01.4130} policy_loss=-13.6619 policy updated! \n",
      "train step 10489 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.9310 diff={max=06.9726, min=00.0021, mean=01.4268} policy_loss=-11.2644 policy updated! \n",
      "train step 10490 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.8997 diff={max=10.5504, min=00.0200, mean=01.3380} policy_loss=-10.0541 policy updated! \n",
      "train step 10491 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.9413 diff={max=12.8811, min=00.0270, mean=01.5123} policy_loss=-14.6104 policy updated! \n",
      "train step 10492 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.3010 diff={max=13.2203, min=00.0087, mean=02.0735} policy_loss=-10.5125 policy updated! \n",
      "train step 10493 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9451 diff={max=10.2552, min=00.0055, mean=01.5291} policy_loss=-10.6548 policy updated! \n",
      "train step 10494 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.5341 diff={max=09.4132, min=00.0297, mean=01.8967} policy_loss=-10.5586 policy updated! \n",
      "train step 10495 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.4277 diff={max=08.4983, min=00.0355, mean=01.3855} policy_loss=-11.1496 policy updated! \n",
      "train step 10496 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.5534 diff={max=07.3005, min=00.0242, mean=01.1298} policy_loss=-11.5562 policy updated! \n",
      "train step 10497 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8048 diff={max=04.6945, min=00.0033, mean=01.1501} policy_loss=-11.2078 policy updated! \n",
      "train step 10498 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1395 diff={max=08.4328, min=00.0042, mean=01.6260} policy_loss=-11.4248 policy updated! \n",
      "train step 10499 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9153 diff={max=04.8559, min=00.0158, mean=00.9919} policy_loss=-11.2052 policy updated! \n",
      "train step 10500 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.4208 diff={max=07.5179, min=00.0154, mean=01.7872} policy_loss=-10.8130 policy updated! \n",
      "train step 10501 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9884 diff={max=07.8596, min=00.0608, mean=01.5446} policy_loss=-14.7380 policy updated! \n",
      "train step 10502 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8895 diff={max=06.9964, min=00.0060, mean=01.4025} policy_loss=-11.8771 policy updated! \n",
      "train step 10503 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=05.5745 diff={max=08.0386, min=00.0113, mean=01.5433} policy_loss=-11.0179 policy updated! \n",
      "train step 10504 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0916 diff={max=12.4207, min=00.0398, mean=01.5512} policy_loss=-12.0099 policy updated! \n",
      "train step 10505 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.8729 diff={max=07.1383, min=00.0113, mean=01.0587} policy_loss=-9.7611 policy updated! \n",
      "train step 10506 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.9460 diff={max=12.0589, min=00.0018, mean=01.5244} policy_loss=-10.8157 policy updated! \n",
      "train step 10507 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.9634 diff={max=14.4208, min=00.0075, mean=01.6628} policy_loss=-11.9066 policy updated! \n",
      "train step 10508 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3232 diff={max=08.7074, min=00.0044, mean=01.1131} policy_loss=-11.3497 policy updated! \n",
      "train step 10509 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.4540 diff={max=04.8241, min=00.0103, mean=01.0416} policy_loss=-11.1707 policy updated! \n",
      "train step 10510 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.3582 diff={max=11.5609, min=00.0661, mean=01.4328} policy_loss=-10.5934 policy updated! \n",
      "train step 10511 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.0125 diff={max=08.4133, min=00.0239, mean=01.6224} policy_loss=-13.7564 policy updated! \n",
      "train step 10512 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5122 diff={max=09.6120, min=00.0123, mean=01.3254} policy_loss=-11.5974 policy updated! \n",
      "train step 10513 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=06.7197 diff={max=12.9248, min=00.0536, mean=01.3774} policy_loss=-13.1067 policy updated! \n",
      "train step 10514 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0793 diff={max=03.5964, min=00.0262, mean=01.0849} policy_loss=-11.6044 policy updated! \n",
      "train step 10515 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=04.8723 diff={max=07.0861, min=00.0153, mean=01.4486} policy_loss=-12.7336 policy updated! \n",
      "train step 10516 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=08.0934 diff={max=14.4784, min=00.0008, mean=01.4373} policy_loss=-12.5254 policy updated! \n",
      "train step 10517 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0542 diff={max=07.1878, min=00.0031, mean=01.6157} policy_loss=-12.1242 policy updated! \n",
      "train step 10518 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.9094 diff={max=16.5860, min=00.0078, mean=01.7902} policy_loss=-12.8293 policy updated! \n",
      "train step 10519 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=10.7785 diff={max=15.8006, min=00.0038, mean=01.7284} policy_loss=-12.8243 policy updated! \n",
      "train step 10520 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=04.5929 diff={max=09.8602, min=00.0056, mean=01.2490} policy_loss=-12.0793 policy updated! \n",
      "train step 10521 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.9664 diff={max=15.7798, min=00.0123, mean=01.9192} policy_loss=-11.8504 policy updated! \n",
      "train step 10522 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1695 diff={max=06.4240, min=00.0325, mean=01.4713} policy_loss=-10.3038 policy updated! \n",
      "train step 10523 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7340 diff={max=05.9655, min=00.0081, mean=01.1780} policy_loss=-12.3530 policy updated! \n",
      "train step 10524 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.3800 diff={max=15.8253, min=00.0222, mean=01.5057} policy_loss=-11.1101 policy updated! \n",
      "train step 10525 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.8394 diff={max=07.2093, min=00.0423, mean=01.2766} policy_loss=-11.4398 policy updated! \n",
      "train step 10526 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3458 diff={max=07.7324, min=00.0014, mean=01.3244} policy_loss=-13.7653 policy updated! \n",
      "train step 10527 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1334 diff={max=05.7924, min=00.0231, mean=01.1293} policy_loss=-10.5117 policy updated! \n",
      "train step 10528 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0150 diff={max=05.1081, min=00.0107, mean=00.9424} policy_loss=-9.9794 policy updated! \n",
      "train step 10529 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.6876 diff={max=06.2796, min=00.0535, mean=01.2607} policy_loss=-14.3122 policy updated! \n",
      "train step 10530 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.0600 diff={max=06.5315, min=00.0065, mean=01.5792} policy_loss=-15.8340 policy updated! \n",
      "train step 10531 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2055 diff={max=11.7308, min=00.0319, mean=01.4899} policy_loss=-11.7060 policy updated! \n",
      "train step 10532 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3733 diff={max=05.6929, min=00.0035, mean=01.1690} policy_loss=-10.4838 policy updated! \n",
      "train step 10533 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=06.8730 diff={max=09.1746, min=00.0018, mean=01.6664} policy_loss=-14.1667 policy updated! \n",
      "train step 10534 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.8945 diff={max=11.4480, min=00.0050, mean=01.5945} policy_loss=-12.9837 policy updated! \n",
      "train step 10535 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.1297 diff={max=12.4733, min=00.0186, mean=01.8275} policy_loss=-11.2853 policy updated! \n",
      "train step 10536 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=04.1515 diff={max=07.6980, min=00.0309, mean=01.2226} policy_loss=-12.1965 policy updated! \n",
      "train step 10537 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9301 diff={max=07.1438, min=00.0338, mean=01.2074} policy_loss=-13.2857 policy updated! \n",
      "train step 10538 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.6468 diff={max=10.6212, min=00.0104, mean=01.5658} policy_loss=-13.8004 policy updated! \n",
      "train step 10539 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4526 diff={max=06.6860, min=00.0246, mean=01.3693} policy_loss=-11.2395 policy updated! \n",
      "train step 10540 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=16.8015 diff={max=13.3755, min=00.0071, mean=02.4954} policy_loss=-14.7627 policy updated! \n",
      "train step 10541 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.8462 diff={max=09.0962, min=00.0118, mean=01.6087} policy_loss=-14.6737 policy updated! \n",
      "train step 10542 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6521 diff={max=07.3470, min=00.0011, mean=01.0890} policy_loss=-11.7976 policy updated! \n",
      "train step 10543 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2238 diff={max=13.7275, min=00.0079, mean=01.2208} policy_loss=-12.4388 policy updated! \n",
      "train step 10544 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.8126 diff={max=10.3541, min=00.0039, mean=01.5652} policy_loss=-13.2635 policy updated! \n",
      "train step 10545 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.7589 diff={max=05.3960, min=00.0387, mean=01.1647} policy_loss=-13.7722 policy updated! \n",
      "train step 10546 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.7855 diff={max=02.3891, min=00.0041, mean=00.6788} policy_loss=-10.5861 policy updated! \n",
      "train step 10547 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.7551 diff={max=12.2673, min=00.0849, mean=01.6102} policy_loss=-11.5177 policy updated! \n",
      "train step 10548 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5431 diff={max=06.2269, min=00.0129, mean=01.0207} policy_loss=-9.9291 policy updated! \n",
      "train step 10549 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.8294 diff={max=03.7193, min=00.0010, mean=00.9687} policy_loss=-9.4229 policy updated! \n",
      "train step 10550 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.5346 diff={max=10.3531, min=00.0025, mean=01.2146} policy_loss=-11.4816 policy updated! \n",
      "train step 10551 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=05.3971 diff={max=07.4193, min=00.0005, mean=01.6014} policy_loss=-14.7658 policy updated! \n",
      "train step 10552 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5682 diff={max=06.2238, min=00.0063, mean=01.2828} policy_loss=-14.8402 policy updated! \n",
      "train step 10553 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=11.6210 diff={max=12.2926, min=00.0105, mean=02.0777} policy_loss=-10.8165 policy updated! \n",
      "train step 10554 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.9706 diff={max=11.4727, min=00.0519, mean=01.4238} policy_loss=-12.3457 policy updated! \n",
      "train step 10555 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.6532 diff={max=07.3295, min=00.0035, mean=01.6792} policy_loss=-12.1259 policy updated! \n",
      "train step 10556 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.7486 diff={max=06.0972, min=00.0154, mean=01.0863} policy_loss=-10.8210 policy updated! \n",
      "train step 10557 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=15.5234 diff={max=18.5200, min=00.0177, mean=01.8089} policy_loss=-13.7225 policy updated! \n",
      "train step 10558 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6483 diff={max=12.8952, min=00.0036, mean=01.4461} policy_loss=-12.6835 policy updated! \n",
      "train step 10559 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9269 diff={max=06.3822, min=00.0313, mean=01.1804} policy_loss=-11.9473 policy updated! \n",
      "train step 10560 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.2026 diff={max=05.5236, min=00.0029, mean=01.2228} policy_loss=-10.9333 policy updated! \n",
      "train step 10561 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3916 diff={max=09.5545, min=00.0073, mean=01.1825} policy_loss=-12.6256 policy updated! \n",
      "train step 10562 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7793 diff={max=10.0850, min=00.0215, mean=01.8171} policy_loss=-16.2274 policy updated! \n",
      "train step 10563 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4942 diff={max=07.9633, min=00.0145, mean=01.1914} policy_loss=-11.7449 policy updated! \n",
      "train step 10564 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=08.3807 diff={max=09.3646, min=00.0008, mean=01.9149} policy_loss=-12.6798 policy updated! \n",
      "train step 10565 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.3728 diff={max=07.6834, min=00.0019, mean=01.5347} policy_loss=-11.8709 policy updated! \n",
      "train step 10566 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.7288 diff={max=13.2029, min=00.0118, mean=02.1167} policy_loss=-13.9224 policy updated! \n",
      "train step 10567 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1212 diff={max=07.3470, min=00.0331, mean=01.5665} policy_loss=-13.3094 policy updated! \n",
      "train step 10568 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4033 diff={max=10.3617, min=00.0047, mean=01.2502} policy_loss=-11.3257 policy updated! \n",
      "train step 10569 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.8032 diff={max=07.0549, min=00.0040, mean=01.7510} policy_loss=-12.3426 policy updated! \n",
      "train step 10570 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.4037 diff={max=06.2663, min=00.0471, mean=01.2835} policy_loss=-9.0286 policy updated! \n",
      "train step 10571 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.5844 diff={max=08.5291, min=00.0182, mean=01.1810} policy_loss=-10.9527 policy updated! \n",
      "train step 10572 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4641 diff={max=07.7457, min=00.0073, mean=01.5303} policy_loss=-10.6036 policy updated! \n",
      "train step 10573 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.1049 diff={max=07.3633, min=00.0078, mean=01.4372} policy_loss=-12.5595 policy updated! \n",
      "train step 10574 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.0948 diff={max=11.2907, min=00.0230, mean=01.6209} policy_loss=-12.1634 policy updated! \n",
      "train step 10575 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.5758 diff={max=07.7923, min=00.0455, mean=01.1374} policy_loss=-12.9707 policy updated! \n",
      "train step 10576 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8897 diff={max=07.9233, min=00.0142, mean=01.2715} policy_loss=-12.1089 policy updated! \n",
      "train step 10577 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9111 diff={max=07.5571, min=00.0044, mean=01.4369} policy_loss=-11.8944 policy updated! \n",
      "train step 10578 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7284 diff={max=07.6354, min=00.0052, mean=01.4079} policy_loss=-11.2418 policy updated! \n",
      "train step 10579 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.7351 diff={max=13.7514, min=00.0736, mean=01.8692} policy_loss=-13.7444 policy updated! \n",
      "train step 10580 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.4162 diff={max=11.3582, min=00.0028, mean=01.4661} policy_loss=-11.9702 policy updated! \n",
      "train step 10581 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.3480 diff={max=16.4100, min=00.0812, mean=01.6213} policy_loss=-12.1645 policy updated! \n",
      "train step 10582 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0233 diff={max=10.6514, min=00.0056, mean=01.4838} policy_loss=-13.3948 policy updated! \n",
      "train step 10583 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7897 diff={max=05.4644, min=00.0048, mean=01.0485} policy_loss=-10.2095 policy updated! \n",
      "train step 10584 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.6202 diff={max=09.9678, min=00.0259, mean=01.7832} policy_loss=-14.8401 policy updated! \n",
      "train step 10585 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=07.9362 diff={max=11.2804, min=00.0106, mean=01.5883} policy_loss=-13.1087 policy updated! \n",
      "train step 10586 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5615 diff={max=05.0328, min=00.0026, mean=01.0491} policy_loss=-12.8666 policy updated! \n",
      "train step 10587 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4651 diff={max=03.7724, min=00.0083, mean=00.8886} policy_loss=-11.6671 policy updated! \n",
      "train step 10588 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8337 diff={max=06.2611, min=00.0151, mean=01.1465} policy_loss=-12.7552 policy updated! \n",
      "train step 10589 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.5835 diff={max=14.2883, min=00.0142, mean=01.7996} policy_loss=-12.3830 policy updated! \n",
      "train step 10590 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.4795 diff={max=08.4997, min=00.0040, mean=01.2817} policy_loss=-11.4278 policy updated! \n",
      "train step 10591 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6631 diff={max=08.0097, min=00.0397, mean=01.4148} policy_loss=-11.0373 policy updated! \n",
      "train step 10592 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.0096 diff={max=12.5327, min=00.0283, mean=01.7367} policy_loss=-12.2061 policy updated! \n",
      "train step 10593 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3625 diff={max=08.4688, min=00.0089, mean=01.5644} policy_loss=-10.9676 policy updated! \n",
      "train step 10594 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.4736 diff={max=17.6353, min=00.0100, mean=01.7528} policy_loss=-10.7796 policy updated! \n",
      "train step 10595 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.6415 diff={max=10.2073, min=00.0101, mean=02.0505} policy_loss=-13.5840 policy updated! \n",
      "train step 10596 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0156 diff={max=08.5681, min=00.0000, mean=01.0517} policy_loss=-11.3474 policy updated! \n",
      "train step 10597 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7197 diff={max=07.4016, min=00.0149, mean=00.9434} policy_loss=-10.1913 policy updated! \n",
      "train step 10598 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=09.9519 diff={max=16.7285, min=00.0355, mean=01.6548} policy_loss=-13.3846 policy updated! \n",
      "train step 10599 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9522 diff={max=09.2942, min=00.0077, mean=01.6266} policy_loss=-10.9491 policy updated! \n",
      "train step 10600 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.8016 diff={max=09.7124, min=00.0142, mean=01.6578} policy_loss=-10.7889 policy updated! \n",
      "train step 10601 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1000 diff={max=07.9018, min=00.0126, mean=01.1541} policy_loss=-9.9169 policy updated! \n",
      "train step 10602 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8069 diff={max=06.1690, min=00.0023, mean=01.2625} policy_loss=-12.8548 policy updated! \n",
      "train step 10603 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.4541 diff={max=15.0271, min=00.0346, mean=01.8370} policy_loss=-14.3577 policy updated! \n",
      "train step 10604 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.3581 diff={max=06.2980, min=00.0295, mean=01.4651} policy_loss=-12.4799 policy updated! \n",
      "train step 10605 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.6073 diff={max=06.9248, min=00.0047, mean=01.4726} policy_loss=-14.2855 policy updated! \n",
      "train step 10606 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0635 diff={max=03.6773, min=00.0685, mean=01.1230} policy_loss=-12.6656 policy updated! \n",
      "train step 10607 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2472 diff={max=03.5417, min=00.0342, mean=01.1244} policy_loss=-12.9995 policy updated! \n",
      "train step 10608 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.5305 diff={max=05.5204, min=00.0497, mean=01.3238} policy_loss=-13.5259 policy updated! \n",
      "train step 10609 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9732 diff={max=08.9036, min=00.0016, mean=01.3280} policy_loss=-11.2699 policy updated! \n",
      "train step 10610 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=10.1042 diff={max=13.7708, min=00.0108, mean=01.8696} policy_loss=-11.0736 policy updated! \n",
      "train step 10611 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8301 diff={max=09.1355, min=00.0030, mean=01.3297} policy_loss=-12.1046 policy updated! \n",
      "train step 10612 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4298 diff={max=06.0817, min=00.0036, mean=01.2236} policy_loss=-11.4637 policy updated! \n",
      "train step 10613 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=08.9815 diff={max=12.1427, min=00.0013, mean=01.6258} policy_loss=-9.4307 policy updated! \n",
      "train step 10614 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=09.0740 diff={max=11.6154, min=00.0353, mean=01.6723} policy_loss=-11.8087 policy updated! \n",
      "train step 10615 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.9877 diff={max=11.4341, min=00.0233, mean=01.5594} policy_loss=-12.8910 policy updated! \n",
      "train step 10616 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=07.9852 diff={max=12.3400, min=00.0059, mean=01.5346} policy_loss=-10.7538 policy updated! \n",
      "train step 10617 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7460 diff={max=07.7584, min=00.0003, mean=01.2536} policy_loss=-12.1916 policy updated! \n",
      "train step 10618 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.9435 diff={max=09.5842, min=00.0082, mean=01.8279} policy_loss=-13.2674 policy updated! \n",
      "train step 10619 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9337 diff={max=06.6924, min=00.0369, mean=01.2858} policy_loss=-13.5996 policy updated! \n",
      "train step 10620 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.5606 diff={max=07.7236, min=00.0090, mean=01.4716} policy_loss=-13.3526 policy updated! \n",
      "train step 10621 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8419 diff={max=10.2417, min=00.0185, mean=01.2319} policy_loss=-12.1855 policy updated! \n",
      "train step 10622 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8775 diff={max=06.3942, min=00.0315, mean=01.1019} policy_loss=-9.3948 policy updated! \n",
      "train step 10623 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5642 diff={max=06.1048, min=00.0038, mean=01.0133} policy_loss=-9.1847 policy updated! \n",
      "train step 10624 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.7834 diff={max=05.4667, min=00.0019, mean=00.9068} policy_loss=-10.4637 policy updated! \n",
      "train step 10625 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.8652 diff={max=10.6129, min=00.0030, mean=01.6875} policy_loss=-12.6629 policy updated! \n",
      "train step 10626 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.5673 diff={max=08.6424, min=00.0041, mean=01.4053} policy_loss=-12.5717 policy updated! \n",
      "train step 10627 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3515 diff={max=07.4731, min=00.0255, mean=01.3310} policy_loss=-8.8880 policy updated! \n",
      "train step 10628 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8952 diff={max=12.3192, min=00.0259, mean=01.4886} policy_loss=-10.3240 policy updated! \n",
      "train step 10629 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9431 diff={max=04.1954, min=00.0049, mean=01.0780} policy_loss=-11.2944 policy updated! \n",
      "train step 10630 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.4775 diff={max=11.2251, min=00.0100, mean=01.2713} policy_loss=-10.7039 policy updated! \n",
      "train step 10631 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.3244 diff={max=09.7705, min=00.0133, mean=01.0056} policy_loss=-10.4051 policy updated! \n",
      "train step 10632 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0955 diff={max=07.2167, min=00.0018, mean=01.5802} policy_loss=-10.2862 policy updated! \n",
      "train step 10633 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.1377 diff={max=07.5988, min=00.0128, mean=01.4261} policy_loss=-12.4121 policy updated! \n",
      "train step 10634 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.7819 diff={max=11.7606, min=00.0476, mean=01.4212} policy_loss=-13.5343 policy updated! \n",
      "train step 10635 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.5120 diff={max=06.2520, min=00.0119, mean=01.1480} policy_loss=-11.2734 policy updated! \n",
      "train step 10636 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.2287 diff={max=13.0792, min=00.0134, mean=01.5641} policy_loss=-13.1375 policy updated! \n",
      "train step 10637 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9029 diff={max=10.0042, min=00.0102, mean=01.3250} policy_loss=-15.1643 policy updated! \n",
      "train step 10638 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.2351 diff={max=05.8509, min=00.0408, mean=01.1796} policy_loss=-9.4286 policy updated! \n",
      "train step 10639 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8150 diff={max=08.8636, min=00.0277, mean=01.3666} policy_loss=-12.7677 policy updated! \n",
      "train step 10640 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.5431 diff={max=06.5597, min=00.0303, mean=00.9905} policy_loss=-12.6678 policy updated! \n",
      "train step 10641 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.3451 diff={max=18.9647, min=00.0642, mean=01.3460} policy_loss=-12.9195 policy updated! \n",
      "train step 10642 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5903 diff={max=05.3441, min=00.0197, mean=00.7577} policy_loss=-12.1595 policy updated! \n",
      "train step 10643 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2069 diff={max=06.4597, min=00.0041, mean=01.2767} policy_loss=-9.3585 policy updated! \n",
      "train step 10644 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6390 diff={max=05.9874, min=00.0316, mean=01.3118} policy_loss=-11.8311 policy updated! \n",
      "train step 10645 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.8584 diff={max=04.6561, min=00.0041, mean=01.1592} policy_loss=-11.1156 policy updated! \n",
      "train step 10646 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.3015 diff={max=06.4596, min=00.0135, mean=01.3756} policy_loss=-12.4386 policy updated! \n",
      "train step 10647 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2262 diff={max=11.2767, min=00.0781, mean=01.2144} policy_loss=-11.0493 policy updated! \n",
      "train step 10648 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.2116 diff={max=10.1524, min=00.0137, mean=01.5411} policy_loss=-11.6229 policy updated! \n",
      "train step 10649 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6731 diff={max=12.6110, min=00.0303, mean=01.3242} policy_loss=-11.2194 policy updated! \n",
      "train step 10650 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.0169 diff={max=09.5189, min=00.0304, mean=01.0375} policy_loss=-9.6471 policy updated! \n",
      "train step 10651 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.9365 diff={max=15.7043, min=00.0172, mean=01.2947} policy_loss=-14.2828 policy updated! \n",
      "train step 10652 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.3693 diff={max=13.3496, min=00.0171, mean=01.4707} policy_loss=-11.3341 policy updated! \n",
      "train step 10653 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.8367 diff={max=13.1124, min=00.0406, mean=01.1681} policy_loss=-10.0389 policy updated! \n",
      "train step 10654 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0787 diff={max=07.7230, min=00.0063, mean=01.2895} policy_loss=-12.3740 policy updated! \n",
      "train step 10655 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.5610 diff={max=09.2276, min=00.0514, mean=01.5512} policy_loss=-12.4004 policy updated! \n",
      "train step 10656 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=07.7150 diff={max=13.2276, min=00.0101, mean=01.5643} policy_loss=-13.3772 policy updated! \n",
      "train step 10657 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7032 diff={max=08.9165, min=00.0042, mean=01.5994} policy_loss=-12.5626 policy updated! \n",
      "train step 10658 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.5940 diff={max=12.0538, min=00.0217, mean=01.7530} policy_loss=-11.7210 policy updated! \n",
      "train step 10659 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4351 diff={max=06.2328, min=00.0226, mean=01.3510} policy_loss=-13.6210 policy updated! \n",
      "train step 10660 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.6905 diff={max=04.9458, min=00.0069, mean=00.8387} policy_loss=-12.9354 policy updated! \n",
      "train step 10661 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.4603 diff={max=11.8534, min=00.0088, mean=01.4858} policy_loss=-11.2198 policy updated! \n",
      "train step 10662 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.9698 diff={max=15.9269, min=00.0256, mean=01.7193} policy_loss=-12.8616 policy updated! \n",
      "train step 10663 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.6751 diff={max=12.5458, min=00.0085, mean=01.8374} policy_loss=-11.6580 policy updated! \n",
      "train step 10664 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0710 diff={max=09.3920, min=00.0039, mean=01.1666} policy_loss=-12.5394 policy updated! \n",
      "train step 10665 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.2069 diff={max=07.6006, min=00.0206, mean=01.1086} policy_loss=-10.7148 policy updated! \n",
      "train step 10666 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3907 diff={max=10.5281, min=00.0106, mean=01.3465} policy_loss=-11.4906 policy updated! \n",
      "train step 10667 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3307 diff={max=10.5689, min=00.0020, mean=01.2767} policy_loss=-12.0210 policy updated! \n",
      "train step 10668 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0723 diff={max=07.5397, min=00.0111, mean=01.5451} policy_loss=-12.8463 policy updated! \n",
      "train step 10669 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1975 diff={max=08.7958, min=00.0349, mean=01.1461} policy_loss=-13.1452 policy updated! \n",
      "train step 10670 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.1206 diff={max=10.5012, min=00.0903, mean=01.2355} policy_loss=-13.7055 policy updated! \n",
      "train step 10671 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.6828 diff={max=09.9305, min=00.0178, mean=01.1089} policy_loss=-12.5712 policy updated! \n",
      "train step 10672 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1861 diff={max=08.3149, min=00.0115, mean=01.3953} policy_loss=-11.8077 policy updated! \n",
      "train step 10673 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6125 diff={max=10.2360, min=00.0073, mean=01.3102} policy_loss=-10.7793 policy updated! \n",
      "train step 10674 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.3962 diff={max=12.3075, min=00.0226, mean=01.6073} policy_loss=-13.9085 policy updated! \n",
      "train step 10675 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.2705 diff={max=10.0884, min=00.0055, mean=01.6789} policy_loss=-11.3956 policy updated! \n",
      "train step 10676 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9438 diff={max=06.9130, min=00.0169, mean=01.4280} policy_loss=-12.6368 policy updated! \n",
      "train step 10677 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4305 diff={max=10.2167, min=00.0411, mean=01.3497} policy_loss=-11.1609 policy updated! \n",
      "train step 10678 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.8526 diff={max=06.7797, min=00.0099, mean=01.4334} policy_loss=-10.4327 policy updated! \n",
      "train step 10679 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=09.3334 diff={max=10.9092, min=00.0364, mean=02.0103} policy_loss=-14.9125 policy updated! \n",
      "train step 10680 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.9662 diff={max=13.8311, min=00.0003, mean=01.3913} policy_loss=-12.1651 policy updated! \n",
      "train step 10681 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4419 diff={max=05.8570, min=00.0216, mean=01.0041} policy_loss=-10.9418 policy updated! \n",
      "train step 10682 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0251 diff={max=07.7619, min=00.0257, mean=01.3298} policy_loss=-11.8780 policy updated! \n",
      "train step 10683 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8901 diff={max=09.5955, min=00.0085, mean=01.1631} policy_loss=-11.7675 policy updated! \n",
      "train step 10684 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0856 diff={max=06.6373, min=00.0094, mean=01.4053} policy_loss=-13.2619 policy updated! \n",
      "train step 10685 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.8869 diff={max=06.7606, min=00.0002, mean=01.1617} policy_loss=-11.7635 policy updated! \n",
      "train step 10686 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.0827 diff={max=12.1624, min=00.0072, mean=01.7010} policy_loss=-14.5025 policy updated! \n",
      "train step 10687 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6183 diff={max=10.2168, min=00.0035, mean=01.4171} policy_loss=-11.6465 policy updated! \n",
      "train step 10688 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.4440 diff={max=13.5257, min=00.0122, mean=01.7766} policy_loss=-13.9429 policy updated! \n",
      "train step 10689 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7407 diff={max=06.9035, min=00.0124, mean=01.1304} policy_loss=-12.7397 policy updated! \n",
      "train step 10690 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.3836 diff={max=06.6414, min=00.0183, mean=01.5369} policy_loss=-12.5203 policy updated! \n",
      "train step 10691 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.4585 diff={max=08.5160, min=00.0016, mean=01.1598} policy_loss=-10.4639 policy updated! \n",
      "train step 10692 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3097 diff={max=05.2036, min=00.0051, mean=01.0527} policy_loss=-11.0249 policy updated! \n",
      "train step 10693 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.7554 diff={max=11.2958, min=00.0307, mean=01.7725} policy_loss=-12.1756 policy updated! \n",
      "train step 10694 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8186 diff={max=11.8352, min=00.0039, mean=01.3504} policy_loss=-13.8471 policy updated! \n",
      "train step 10695 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.8387 diff={max=08.4772, min=00.0062, mean=01.1332} policy_loss=-10.9542 policy updated! \n",
      "train step 10696 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.6809 diff={max=16.0389, min=00.0094, mean=01.4198} policy_loss=-12.2525 policy updated! \n",
      "train step 10697 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6739 diff={max=12.3208, min=00.0047, mean=01.4486} policy_loss=-11.4331 policy updated! \n",
      "train step 10698 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0367 diff={max=07.2484, min=00.0064, mean=01.5999} policy_loss=-13.3296 policy updated! \n",
      "train step 10699 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.8074 diff={max=03.2497, min=00.0190, mean=00.9809} policy_loss=-13.7708 policy updated! \n",
      "train step 10700 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.1920 diff={max=07.7411, min=00.0152, mean=01.2069} policy_loss=-12.4156 policy updated! \n",
      "train step 10701 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4666 diff={max=11.4571, min=00.0005, mean=01.4234} policy_loss=-12.9611 policy updated! \n",
      "train step 10702 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3709 diff={max=09.9931, min=00.0430, mean=01.2108} policy_loss=-11.9049 policy updated! \n",
      "train step 10703 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6392 diff={max=08.6119, min=00.0132, mean=01.0944} policy_loss=-12.3910 policy updated! \n",
      "train step 10704 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8115 diff={max=06.5298, min=00.0247, mean=01.1691} policy_loss=-10.5480 policy updated! \n",
      "train step 10705 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.8342 diff={max=06.4754, min=00.0487, mean=01.3529} policy_loss=-14.6953 policy updated! \n",
      "train step 10706 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8945 diff={max=09.8548, min=00.0225, mean=01.4831} policy_loss=-12.1556 policy updated! \n",
      "train step 10707 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1861 diff={max=08.4837, min=00.0647, mean=01.3642} policy_loss=-14.2585 policy updated! \n",
      "train step 10708 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.8843 diff={max=10.1098, min=00.0012, mean=01.8338} policy_loss=-15.1823 policy updated! \n",
      "train step 10709 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9754 diff={max=08.6584, min=00.0244, mean=00.9513} policy_loss=-11.8484 policy updated! \n",
      "train step 10710 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=10.0352 diff={max=10.4709, min=00.0198, mean=01.9150} policy_loss=-13.6540 policy updated! \n",
      "train step 10711 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=07.4248 diff={max=11.4301, min=00.0094, mean=01.6336} policy_loss=-12.8830 policy updated! \n",
      "train step 10712 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4076 diff={max=03.1196, min=00.0042, mean=00.9152} policy_loss=-13.1234 policy updated! \n",
      "train step 10713 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4279 diff={max=05.1767, min=00.0130, mean=01.0541} policy_loss=-13.5279 policy updated! \n",
      "train step 10714 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6495 diff={max=05.5380, min=00.0397, mean=01.1836} policy_loss=-13.6932 policy updated! \n",
      "train step 10715 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.0890 diff={max=07.4346, min=00.0079, mean=01.1128} policy_loss=-12.2313 policy updated! \n",
      "train step 10716 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0615 diff={max=06.6389, min=00.0180, mean=01.0414} policy_loss=-12.4221 policy updated! \n",
      "train step 10717 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.1555 diff={max=10.6591, min=00.0097, mean=01.9952} policy_loss=-11.7869 policy updated! \n",
      "train step 10718 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4273 diff={max=06.5464, min=00.0397, mean=01.1802} policy_loss=-13.3772 policy updated! \n",
      "train step 10719 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.2330 diff={max=19.2612, min=00.0194, mean=01.5755} policy_loss=-12.8004 policy updated! \n",
      "train step 10720 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.0536 diff={max=04.0081, min=00.0346, mean=01.0379} policy_loss=-12.2553 policy updated! \n",
      "train step 10721 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1794 diff={max=07.1386, min=00.0027, mean=00.9046} policy_loss=-10.6102 policy updated! \n",
      "train step 10722 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5932 diff={max=07.6725, min=00.0337, mean=01.4618} policy_loss=-10.0329 policy updated! \n",
      "train step 10723 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.4807 diff={max=12.2428, min=00.0203, mean=01.6610} policy_loss=-13.2618 policy updated! \n",
      "train step 10724 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0320 diff={max=11.9621, min=00.0006, mean=01.2276} policy_loss=-10.2899 policy updated! \n",
      "train step 10725 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.7603 diff={max=06.8842, min=00.0271, mean=01.6872} policy_loss=-11.7127 policy updated! \n",
      "train step 10726 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=17.7748 diff={max=22.7527, min=00.0049, mean=01.8896} policy_loss=-13.1767 policy updated! \n",
      "train step 10727 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6206 diff={max=05.4028, min=00.0022, mean=01.2580} policy_loss=-11.5202 policy updated! \n",
      "train step 10728 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7731 diff={max=08.5727, min=00.0039, mean=01.1675} policy_loss=-10.6910 policy updated! \n",
      "train step 10729 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=32.4647 diff={max=35.7551, min=00.0073, mean=02.4464} policy_loss=-12.3405 policy updated! \n",
      "train step 10730 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=09.4003 diff={max=17.5030, min=00.0444, mean=01.3990} policy_loss=-10.4168 policy updated! \n",
      "train step 10731 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3540 diff={max=14.6505, min=00.0064, mean=01.4777} policy_loss=-11.8143 policy updated! \n",
      "train step 10732 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0287 diff={max=09.9251, min=00.0183, mean=01.1201} policy_loss=-13.5648 policy updated! \n",
      "train step 10733 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7491 diff={max=05.7495, min=00.0527, mean=01.2380} policy_loss=-9.9470 policy updated! \n",
      "train step 10734 reward={max=07.0000, min=00.0000, mean=05.4000} optimizing loss=04.3378 diff={max=10.5577, min=00.0054, mean=01.1821} policy_loss=-12.4929 policy updated! \n",
      "train step 10735 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.6023 diff={max=09.5505, min=00.0090, mean=01.4593} policy_loss=-11.9116 policy updated! \n",
      "train step 10736 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=23.3897 diff={max=27.7531, min=00.0486, mean=02.3482} policy_loss=-14.3753 policy updated! \n",
      "train step 10737 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7614 diff={max=08.1013, min=00.0365, mean=01.3936} policy_loss=-13.7720 policy updated! \n",
      "train step 10738 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.2295 diff={max=14.4201, min=00.0277, mean=02.1595} policy_loss=-14.8936 policy updated! \n",
      "train step 10739 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2976 diff={max=06.9381, min=00.0552, mean=01.6242} policy_loss=-12.5033 policy updated! \n",
      "train step 10740 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.4224 diff={max=10.0693, min=00.0010, mean=01.6661} policy_loss=-12.3992 policy updated! \n",
      "train step 10741 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.4365 diff={max=08.6554, min=00.0271, mean=01.8248} policy_loss=-12.5291 policy updated! \n",
      "train step 10742 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2060 diff={max=08.4650, min=00.0060, mean=01.3275} policy_loss=-10.7811 policy updated! \n",
      "train step 10743 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8971 diff={max=07.5152, min=00.0021, mean=01.2607} policy_loss=-8.8592 policy updated! \n",
      "train step 10744 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.2620 diff={max=12.4707, min=00.0106, mean=01.7390} policy_loss=-11.1749 policy updated! \n",
      "train step 10745 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=16.4190 diff={max=11.4047, min=00.0064, mean=02.5073} policy_loss=-15.4989 policy updated! \n",
      "train step 10746 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7562 diff={max=12.6784, min=00.0188, mean=01.2253} policy_loss=-11.5365 policy updated! \n",
      "train step 10747 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5103 diff={max=11.5427, min=00.0180, mean=01.2589} policy_loss=-11.6888 policy updated! \n",
      "train step 10748 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.6752 diff={max=10.1760, min=00.0345, mean=02.0131} policy_loss=-14.1158 policy updated! \n",
      "train step 10749 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=17.2890 diff={max=17.7489, min=00.0168, mean=02.1389} policy_loss=-13.2545 policy updated! \n",
      "train step 10750 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.9241 diff={max=05.7312, min=00.0316, mean=01.1691} policy_loss=-11.9677 policy updated! \n",
      "train step 10751 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=08.3166 diff={max=11.1907, min=00.0103, mean=01.7190} policy_loss=-12.0590 policy updated! \n",
      "train step 10752 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=07.9292 diff={max=10.7229, min=00.0238, mean=01.6657} policy_loss=-13.1936 policy updated! \n",
      "train step 10753 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3227 diff={max=07.4435, min=00.0001, mean=01.7119} policy_loss=-11.4224 policy updated! \n",
      "train step 10754 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.1358 diff={max=06.4647, min=00.0097, mean=01.2136} policy_loss=-10.8982 policy updated! \n",
      "train step 10755 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.4630 diff={max=07.8892, min=00.0121, mean=01.1660} policy_loss=-10.8413 policy updated! \n",
      "train step 10756 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.5868 diff={max=13.6773, min=00.0669, mean=02.0243} policy_loss=-12.9527 policy updated! \n",
      "train step 10757 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3641 diff={max=08.9480, min=00.0095, mean=01.5538} policy_loss=-12.9590 policy updated! \n",
      "train step 10758 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7393 diff={max=08.0864, min=00.0120, mean=01.3433} policy_loss=-10.4144 policy updated! \n",
      "train step 10759 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3224 diff={max=10.0098, min=00.0020, mean=01.4376} policy_loss=-10.5388 policy updated! \n",
      "train step 10760 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.6111 diff={max=05.5752, min=00.0218, mean=01.2856} policy_loss=-8.8684 policy updated! \n",
      "train step 10761 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.2125 diff={max=09.2751, min=00.0310, mean=01.1791} policy_loss=-10.5616 policy updated! \n",
      "train step 10762 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8680 diff={max=08.1043, min=00.0168, mean=01.5905} policy_loss=-11.7898 policy updated! \n",
      "train step 10763 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5915 diff={max=13.2421, min=00.0031, mean=01.0035} policy_loss=-12.3095 policy updated! \n",
      "train step 10764 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.3245 diff={max=07.2385, min=00.0015, mean=01.0836} policy_loss=-11.5774 policy updated! \n",
      "train step 10765 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.1170 diff={max=10.5954, min=00.0136, mean=01.2084} policy_loss=-12.2526 policy updated! \n",
      "train step 10766 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=09.0241 diff={max=12.7113, min=00.0029, mean=01.7251} policy_loss=-10.6351 policy updated! \n",
      "train step 10767 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5784 diff={max=10.7053, min=00.0063, mean=01.1853} policy_loss=-13.8665 policy updated! \n",
      "train step 10768 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.7872 diff={max=16.6192, min=00.0143, mean=01.7529} policy_loss=-11.5649 policy updated! \n",
      "train step 10769 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=11.9833 diff={max=16.6635, min=00.0059, mean=01.9070} policy_loss=-12.2462 policy updated! \n",
      "train step 10770 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=14.1388 diff={max=18.4304, min=00.0221, mean=02.1822} policy_loss=-12.9060 policy updated! \n",
      "train step 10771 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.3495 diff={max=06.7034, min=00.0236, mean=01.3378} policy_loss=-11.6930 policy updated! \n",
      "train step 10772 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.8807 diff={max=11.1876, min=00.0145, mean=01.6928} policy_loss=-10.5105 policy updated! \n",
      "train step 10773 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.2295 diff={max=07.4448, min=00.0256, mean=01.3310} policy_loss=-12.0628 policy updated! \n",
      "train step 10774 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3235 diff={max=07.6743, min=00.0268, mean=01.1092} policy_loss=-10.8414 policy updated! \n",
      "train step 10775 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.1468 diff={max=05.5264, min=00.0438, mean=01.3342} policy_loss=-12.4393 policy updated! \n",
      "train step 10776 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.1427 diff={max=13.3219, min=00.0357, mean=01.9012} policy_loss=-13.2211 policy updated! \n",
      "train step 10777 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.4240 diff={max=14.4258, min=00.0445, mean=01.8217} policy_loss=-12.9557 policy updated! \n",
      "train step 10778 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4917 diff={max=07.5459, min=00.0111, mean=01.4437} policy_loss=-10.9100 policy updated! \n",
      "train step 10779 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6230 diff={max=12.0971, min=00.0041, mean=01.2304} policy_loss=-12.2732 policy updated! \n",
      "train step 10780 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.0983 diff={max=13.8040, min=00.0199, mean=01.2867} policy_loss=-9.9995 policy updated! \n",
      "train step 10781 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.5961 diff={max=10.8140, min=00.0066, mean=01.7248} policy_loss=-9.7018 policy updated! \n",
      "train step 10782 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7638 diff={max=09.0585, min=00.0565, mean=01.4201} policy_loss=-13.2379 policy updated! \n",
      "train step 10783 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9600 diff={max=13.1372, min=00.0000, mean=01.4319} policy_loss=-14.2002 policy updated! \n",
      "train step 10784 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0484 diff={max=09.9234, min=00.0014, mean=01.3026} policy_loss=-12.4614 policy updated! \n",
      "train step 10785 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.3159 diff={max=09.0694, min=00.0010, mean=01.8170} policy_loss=-10.0627 policy updated! \n",
      "train step 10786 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.5693 diff={max=06.5188, min=00.0171, mean=01.2932} policy_loss=-11.3307 policy updated! \n",
      "train step 10787 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3785 diff={max=10.7763, min=00.0086, mean=01.6613} policy_loss=-12.8753 policy updated! \n",
      "train step 10788 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0833 diff={max=08.3381, min=00.0328, mean=01.7267} policy_loss=-13.4224 policy updated! \n",
      "train step 10789 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=09.8408 diff={max=17.1680, min=00.0226, mean=01.6282} policy_loss=-11.6767 policy updated! \n",
      "train step 10790 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.7812 diff={max=11.5396, min=00.0289, mean=01.1340} policy_loss=-10.8897 policy updated! \n",
      "train step 10791 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=17.2442 diff={max=21.7987, min=00.0045, mean=01.8909} policy_loss=-10.5995 policy updated! \n",
      "train step 10792 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7302 diff={max=07.7637, min=00.0415, mean=01.2611} policy_loss=-10.0585 policy updated! \n",
      "train step 10793 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5811 diff={max=08.4055, min=00.0016, mean=01.5997} policy_loss=-11.6589 policy updated! \n",
      "train step 10794 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.9586 diff={max=10.0672, min=00.0306, mean=02.1243} policy_loss=-13.9495 policy updated! \n",
      "train step 10795 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.0687 diff={max=08.3459, min=00.0179, mean=01.0955} policy_loss=-11.2257 policy updated! \n",
      "train step 10796 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.0246 diff={max=08.6510, min=00.0178, mean=01.3601} policy_loss=-12.7361 policy updated! \n",
      "train step 10797 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.8306 diff={max=16.1379, min=00.0185, mean=01.5443} policy_loss=-12.8902 policy updated! \n",
      "train step 10798 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3469 diff={max=04.8341, min=00.0823, mean=01.0686} policy_loss=-9.8981 policy updated! \n",
      "train step 10799 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3510 diff={max=07.7954, min=00.0279, mean=01.6431} policy_loss=-13.2660 policy updated! \n",
      "train step 10800 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=11.9824 diff={max=15.1834, min=00.0287, mean=01.9087} policy_loss=-13.2529 policy updated! \n",
      "train step 10801 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.2254 diff={max=08.5812, min=00.0147, mean=01.8119} policy_loss=-12.2721 policy updated! \n",
      "train step 10802 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8956 diff={max=08.3797, min=00.0261, mean=01.3508} policy_loss=-13.6852 policy updated! \n",
      "train step 10803 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0310 diff={max=09.6562, min=00.0280, mean=01.5800} policy_loss=-12.7366 policy updated! \n",
      "train step 10804 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.8888 diff={max=13.2683, min=00.0130, mean=01.3640} policy_loss=-12.6325 policy updated! \n",
      "train step 10805 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.2995 diff={max=09.7661, min=00.0040, mean=01.7607} policy_loss=-12.7128 policy updated! \n",
      "train step 10806 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.7489 diff={max=12.3306, min=00.0030, mean=01.7301} policy_loss=-12.1804 policy updated! \n",
      "train step 10807 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.9991 diff={max=09.5833, min=00.0215, mean=01.9222} policy_loss=-13.5944 policy updated! \n",
      "train step 10808 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2917 diff={max=09.8480, min=00.0311, mean=01.5822} policy_loss=-12.2664 policy updated! \n",
      "train step 10809 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6655 diff={max=06.2562, min=00.0195, mean=01.3471} policy_loss=-11.6411 policy updated! \n",
      "train step 10810 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.6130 diff={max=08.4934, min=00.0058, mean=01.6660} policy_loss=-11.9008 policy updated! \n",
      "train step 10811 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3838 diff={max=09.1036, min=00.0042, mean=01.3597} policy_loss=-12.8299 policy updated! \n",
      "train step 10812 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.7211 diff={max=15.7444, min=00.0184, mean=01.4319} policy_loss=-14.4255 policy updated! \n",
      "train step 10813 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0186 diff={max=09.6715, min=00.0216, mean=01.5493} policy_loss=-14.7521 policy updated! \n",
      "train step 10814 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2657 diff={max=08.7874, min=00.0029, mean=01.5000} policy_loss=-11.0936 policy updated! \n",
      "train step 10815 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.1034 diff={max=08.7994, min=00.0109, mean=01.4030} policy_loss=-13.2564 policy updated! \n",
      "train step 10816 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.6878 diff={max=06.4746, min=00.0363, mean=01.2697} policy_loss=-12.5773 policy updated! \n",
      "train step 10817 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0470 diff={max=11.2426, min=00.0300, mean=01.3302} policy_loss=-11.7130 policy updated! \n",
      "train step 10818 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6549 diff={max=09.8106, min=00.0200, mean=01.6376} policy_loss=-15.8083 policy updated! \n",
      "train step 10819 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8639 diff={max=06.9876, min=00.0339, mean=01.4591} policy_loss=-9.6838 policy updated! \n",
      "train step 10820 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.9329 diff={max=08.8665, min=00.0089, mean=01.3149} policy_loss=-12.8083 policy updated! \n",
      "train step 10821 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8159 diff={max=08.0745, min=00.0002, mean=01.0177} policy_loss=-9.6210 policy updated! \n",
      "train step 10822 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.4362 diff={max=16.2334, min=00.0938, mean=01.7773} policy_loss=-11.2263 policy updated! \n",
      "train step 10823 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.5712 diff={max=07.6208, min=00.0401, mean=01.8976} policy_loss=-12.7852 policy updated! \n",
      "train step 10824 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.8936 diff={max=07.0564, min=00.0186, mean=01.3277} policy_loss=-13.6731 policy updated! \n",
      "train step 10825 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.9458 diff={max=08.2968, min=00.0094, mean=01.8427} policy_loss=-13.0029 policy updated! \n",
      "train step 10826 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.7360 diff={max=12.6652, min=00.0159, mean=01.7830} policy_loss=-14.4576 policy updated! \n",
      "train step 10827 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9934 diff={max=07.1091, min=00.0091, mean=01.2449} policy_loss=-13.5664 policy updated! \n",
      "train step 10828 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8382 diff={max=07.6451, min=00.0099, mean=01.2662} policy_loss=-11.5278 policy updated! \n",
      "train step 10829 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4452 diff={max=07.4989, min=00.0113, mean=01.2169} policy_loss=-12.6332 policy updated! \n",
      "train step 10830 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=12.9177 diff={max=16.6114, min=00.0021, mean=01.7772} policy_loss=-10.0395 policy updated! \n",
      "train step 10831 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.5856 diff={max=07.8710, min=00.0007, mean=01.7475} policy_loss=-13.4491 policy updated! \n",
      "train step 10832 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5235 diff={max=03.4681, min=00.0301, mean=00.8802} policy_loss=-8.7875 policy updated! \n",
      "train step 10833 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2903 diff={max=10.2395, min=00.0110, mean=01.4075} policy_loss=-10.2030 policy updated! \n",
      "train step 10834 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8083 diff={max=07.4565, min=00.0182, mean=01.3674} policy_loss=-14.3438 policy updated! \n",
      "train step 10835 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.5192 diff={max=04.0082, min=00.0041, mean=01.1577} policy_loss=-10.6789 policy updated! \n",
      "train step 10836 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.4251 diff={max=07.2997, min=00.0344, mean=01.4648} policy_loss=-13.3247 policy updated! \n",
      "train step 10837 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1483 diff={max=06.9413, min=00.0441, mean=01.1856} policy_loss=-10.3710 policy updated! \n",
      "train step 10838 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4112 diff={max=08.1682, min=00.0416, mean=01.4915} policy_loss=-10.6162 policy updated! \n",
      "train step 10839 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9315 diff={max=06.4757, min=00.0043, mean=01.3072} policy_loss=-13.5798 policy updated! \n",
      "train step 10840 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.8784 diff={max=09.1502, min=00.0352, mean=01.3032} policy_loss=-11.8061 policy updated! \n",
      "train step 10841 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5662 diff={max=07.3764, min=00.0053, mean=01.1894} policy_loss=-10.7034 policy updated! \n",
      "train step 10842 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3316 diff={max=09.0845, min=00.1529, mean=01.6767} policy_loss=-12.9673 policy updated! \n",
      "train step 10843 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2751 diff={max=09.1474, min=00.0023, mean=01.2093} policy_loss=-11.8782 policy updated! \n",
      "train step 10844 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.8652 diff={max=14.9902, min=00.0828, mean=01.6242} policy_loss=-12.0014 policy updated! \n",
      "train step 10845 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.3875 diff={max=05.5205, min=00.0447, mean=00.9992} policy_loss=-10.3935 policy updated! \n",
      "train step 10846 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.7239 diff={max=05.6181, min=00.0113, mean=01.1291} policy_loss=-10.2014 policy updated! \n",
      "train step 10847 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8013 diff={max=06.9188, min=00.0079, mean=01.2116} policy_loss=-12.0663 policy updated! \n",
      "train step 10848 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4565 diff={max=06.2717, min=00.0112, mean=01.2305} policy_loss=-13.1477 policy updated! \n",
      "train step 10849 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6649 diff={max=10.7524, min=00.0021, mean=01.3197} policy_loss=-9.7120 policy updated! \n",
      "train step 10850 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.4611 diff={max=11.8921, min=00.0018, mean=01.5848} policy_loss=-9.5819 policy updated! \n",
      "train step 10851 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.0470 diff={max=07.0557, min=00.0219, mean=01.4942} policy_loss=-12.5090 policy updated! \n",
      "train step 10852 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.4971 diff={max=10.3758, min=00.0558, mean=01.8401} policy_loss=-10.7707 policy updated! \n",
      "train step 10853 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.8797 diff={max=09.6459, min=00.0058, mean=01.8687} policy_loss=-12.1410 policy updated! \n",
      "train step 10854 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.3974 diff={max=06.9847, min=00.0137, mean=01.4020} policy_loss=-13.1135 policy updated! \n",
      "train step 10855 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.6873 diff={max=04.1515, min=00.0066, mean=00.9084} policy_loss=-10.2868 policy updated! \n",
      "train step 10856 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.1177 diff={max=11.5494, min=00.0151, mean=01.8354} policy_loss=-13.4403 policy updated! \n",
      "train step 10857 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6174 diff={max=12.2989, min=00.0317, mean=01.5183} policy_loss=-11.5824 policy updated! \n",
      "train step 10858 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.8393 diff={max=10.9115, min=00.1043, mean=01.8786} policy_loss=-15.1645 policy updated! \n",
      "train step 10859 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0788 diff={max=07.7684, min=00.0628, mean=01.4496} policy_loss=-11.9017 policy updated! \n",
      "train step 10860 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.1995 diff={max=08.0587, min=00.0429, mean=01.6677} policy_loss=-11.1203 policy updated! \n",
      "train step 10861 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0060 diff={max=08.8542, min=00.0446, mean=01.5683} policy_loss=-12.2423 policy updated! \n",
      "train step 10862 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.1317 diff={max=14.2073, min=00.0014, mean=01.6861} policy_loss=-13.3800 policy updated! \n",
      "train step 10863 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6446 diff={max=08.4352, min=00.0248, mean=01.6141} policy_loss=-11.4799 policy updated! \n",
      "train step 10864 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.5225 diff={max=11.0179, min=00.0057, mean=02.0579} policy_loss=-12.3929 policy updated! \n",
      "train step 10865 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.2850 diff={max=08.3334, min=00.0700, mean=01.3468} policy_loss=-9.8860 policy updated! \n",
      "train step 10866 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.6955 diff={max=15.3168, min=00.0068, mean=01.4769} policy_loss=-11.1535 policy updated! \n",
      "train step 10867 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6433 diff={max=05.6006, min=00.0173, mean=01.3326} policy_loss=-11.8606 policy updated! \n",
      "train step 10868 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5070 diff={max=07.9003, min=00.0148, mean=01.6418} policy_loss=-11.9092 policy updated! \n",
      "train step 10869 reward={max=07.0000, min=00.0000, mean=05.4000} optimizing loss=06.2521 diff={max=07.6467, min=00.0245, mean=01.7310} policy_loss=-14.9581 policy updated! \n",
      "train step 10870 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.6293 diff={max=06.0724, min=00.0052, mean=01.2787} policy_loss=-11.7475 policy updated! \n",
      "train step 10871 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.1840 diff={max=09.5487, min=00.0233, mean=01.5383} policy_loss=-13.0385 policy updated! \n",
      "train step 10872 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1731 diff={max=07.3252, min=00.0441, mean=01.4345} policy_loss=-13.0740 policy updated! \n",
      "train step 10873 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8024 diff={max=09.8391, min=00.0300, mean=01.4136} policy_loss=-12.9716 policy updated! \n",
      "train step 10874 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=16.6208 diff={max=25.3213, min=00.0233, mean=01.8240} policy_loss=-12.9171 policy updated! \n",
      "train step 10875 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.9105 diff={max=07.8316, min=00.0118, mean=01.1741} policy_loss=-11.8167 policy updated! \n",
      "train step 10876 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4844 diff={max=07.2393, min=00.0097, mean=00.9891} policy_loss=-9.7225 policy updated! \n",
      "train step 10877 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3169 diff={max=12.4878, min=00.0198, mean=01.4009} policy_loss=-11.2554 policy updated! \n",
      "train step 10878 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.5322 diff={max=11.0664, min=00.0263, mean=01.6257} policy_loss=-13.1009 policy updated! \n",
      "train step 10879 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5146 diff={max=10.2753, min=00.0159, mean=01.1345} policy_loss=-10.7373 policy updated! \n",
      "train step 10880 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.6516 diff={max=05.4509, min=00.0192, mean=00.8126} policy_loss=-13.4598 policy updated! \n",
      "train step 10881 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4632 diff={max=06.7765, min=00.0251, mean=01.4198} policy_loss=-14.3191 policy updated! \n",
      "train step 10882 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7302 diff={max=07.0954, min=00.0441, mean=01.6729} policy_loss=-11.6717 policy updated! \n",
      "train step 10883 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.9111 diff={max=11.1105, min=00.0318, mean=01.6852} policy_loss=-11.2875 policy updated! \n",
      "train step 10884 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.3176 diff={max=11.5392, min=00.0213, mean=01.3406} policy_loss=-10.7221 policy updated! \n",
      "train step 10885 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.1961 diff={max=06.9860, min=00.0600, mean=01.4066} policy_loss=-14.1387 policy updated! \n",
      "train step 10886 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3950 diff={max=11.2548, min=00.0139, mean=01.4614} policy_loss=-10.3326 policy updated! \n",
      "train step 10887 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.6109 diff={max=13.8245, min=00.0053, mean=01.5326} policy_loss=-14.0153 policy updated! \n",
      "train step 10888 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.0927 diff={max=07.3543, min=00.0115, mean=01.1093} policy_loss=-12.9915 policy updated! \n",
      "train step 10889 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6936 diff={max=09.5739, min=00.0094, mean=01.4438} policy_loss=-13.0523 policy updated! \n",
      "train step 10890 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.1060 diff={max=14.6429, min=00.0111, mean=01.6002} policy_loss=-11.1283 policy updated! \n",
      "train step 10891 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0099 diff={max=06.9078, min=00.0018, mean=01.5708} policy_loss=-9.7232 policy updated! \n",
      "train step 10892 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3424 diff={max=08.6175, min=00.0061, mean=01.2254} policy_loss=-9.3347 policy updated! \n",
      "train step 10893 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3922 diff={max=10.6331, min=00.0217, mean=01.4316} policy_loss=-11.2446 policy updated! \n",
      "train step 10894 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=07.9094 diff={max=12.0978, min=00.0162, mean=01.7277} policy_loss=-8.5384 policy updated! \n",
      "train step 10895 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.8739 diff={max=07.9991, min=00.0059, mean=01.4051} policy_loss=-11.1973 policy updated! \n",
      "train step 10896 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9553 diff={max=06.9867, min=00.0422, mean=01.2197} policy_loss=-11.9366 policy updated! \n",
      "train step 10897 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9269 diff={max=09.9697, min=00.0067, mean=01.3337} policy_loss=-9.8114 policy updated! \n",
      "train step 10898 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0981 diff={max=13.2414, min=00.0117, mean=01.3117} policy_loss=-10.1502 policy updated! \n",
      "train step 10899 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5067 diff={max=08.3461, min=00.0555, mean=01.7389} policy_loss=-14.0213 policy updated! \n",
      "train step 10900 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.8313 diff={max=08.5203, min=00.0097, mean=01.3804} policy_loss=-13.8521 policy updated! \n",
      "train step 10901 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0801 diff={max=09.9083, min=00.0141, mean=01.3669} policy_loss=-10.5289 policy updated! \n",
      "train step 10902 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6364 diff={max=07.7708, min=00.0373, mean=01.6838} policy_loss=-14.6179 policy updated! \n",
      "train step 10903 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.9992 diff={max=09.0797, min=00.0275, mean=01.4607} policy_loss=-13.6561 policy updated! \n",
      "train step 10904 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2105 diff={max=06.6698, min=00.0164, mean=01.3810} policy_loss=-11.7791 policy updated! \n",
      "train step 10905 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.4467 diff={max=06.4426, min=00.0355, mean=01.3815} policy_loss=-12.1333 policy updated! \n",
      "train step 10906 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7893 diff={max=11.9582, min=00.0085, mean=01.4727} policy_loss=-10.8548 policy updated! \n",
      "train step 10907 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4243 diff={max=06.5264, min=00.0100, mean=01.3700} policy_loss=-11.4499 policy updated! \n",
      "train step 10908 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.5098 diff={max=10.1305, min=00.0333, mean=01.7258} policy_loss=-10.2575 policy updated! \n",
      "train step 10909 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4543 diff={max=08.9287, min=00.0996, mean=01.5809} policy_loss=-12.9035 policy updated! \n",
      "train step 10910 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.2548 diff={max=06.7630, min=00.0251, mean=01.5655} policy_loss=-13.4978 policy updated! \n",
      "train step 10911 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4343 diff={max=12.7404, min=00.0520, mean=01.4430} policy_loss=-13.1799 policy updated! \n",
      "train step 10912 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3418 diff={max=07.1692, min=00.0222, mean=01.3966} policy_loss=-14.4749 policy updated! \n",
      "train step 10913 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5256 diff={max=06.9108, min=00.0286, mean=01.0213} policy_loss=-11.3708 policy updated! \n",
      "train step 10914 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0601 diff={max=06.9672, min=00.0033, mean=01.3179} policy_loss=-13.6017 policy updated! \n",
      "train step 10915 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.6790 diff={max=07.0871, min=00.0399, mean=01.1585} policy_loss=-11.9549 policy updated! \n",
      "train step 10916 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0969 diff={max=07.1369, min=00.0002, mean=01.1025} policy_loss=-10.7876 policy updated! \n",
      "train step 10917 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.0940 diff={max=14.1285, min=00.0797, mean=01.5227} policy_loss=-11.6582 policy updated! \n",
      "train step 10918 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3018 diff={max=06.2233, min=00.0390, mean=01.4120} policy_loss=-13.3035 policy updated! \n",
      "train step 10919 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.9269 diff={max=08.9096, min=00.0015, mean=01.2133} policy_loss=-11.3126 policy updated! \n",
      "train step 10920 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.2312 diff={max=07.7999, min=00.0475, mean=01.3798} policy_loss=-13.8880 policy updated! \n",
      "train step 10921 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1154 diff={max=09.3530, min=00.0037, mean=01.3933} policy_loss=-12.9442 policy updated! \n",
      "train step 10922 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1698 diff={max=08.9603, min=00.0550, mean=01.4435} policy_loss=-12.1143 policy updated! \n",
      "train step 10923 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.2606 diff={max=18.1290, min=00.0647, mean=01.4610} policy_loss=-12.9310 policy updated! \n",
      "train step 10924 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.3899 diff={max=09.5436, min=00.0015, mean=01.3845} policy_loss=-12.8510 policy updated! \n",
      "train step 10925 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.0292 diff={max=06.9465, min=00.0219, mean=01.4481} policy_loss=-15.7555 policy updated! \n",
      "train step 10926 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3904 diff={max=06.8336, min=00.0011, mean=01.1100} policy_loss=-10.7569 policy updated! \n",
      "train step 10927 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3403 diff={max=12.0181, min=00.0231, mean=01.2779} policy_loss=-11.7146 policy updated! \n",
      "train step 10928 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.8441 diff={max=11.2278, min=00.0160, mean=01.6432} policy_loss=-12.3206 policy updated! \n",
      "train step 10929 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8804 diff={max=09.4676, min=00.0205, mean=01.3323} policy_loss=-12.7209 policy updated! \n",
      "train step 10930 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.6542 diff={max=06.7883, min=00.0033, mean=01.4217} policy_loss=-12.0528 policy updated! \n",
      "train step 10931 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0976 diff={max=08.2004, min=00.0086, mean=01.2316} policy_loss=-12.2846 policy updated! \n",
      "train step 10932 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.9940 diff={max=16.5752, min=00.0115, mean=01.7199} policy_loss=-12.8604 policy updated! \n",
      "train step 10933 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.5517 diff={max=04.9777, min=00.0260, mean=01.3234} policy_loss=-15.4973 policy updated! \n",
      "train step 10934 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5271 diff={max=09.6397, min=00.0691, mean=01.2358} policy_loss=-12.3940 policy updated! \n",
      "train step 10935 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.5698 diff={max=14.2114, min=00.0171, mean=01.5722} policy_loss=-12.0348 policy updated! \n",
      "train step 10936 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3281 diff={max=04.4850, min=00.0149, mean=01.0811} policy_loss=-13.1632 policy updated! \n",
      "train step 10937 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.4667 diff={max=11.9438, min=00.0534, mean=01.7572} policy_loss=-13.1946 policy updated! \n",
      "train step 10938 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.1448 diff={max=12.3756, min=00.0388, mean=01.9697} policy_loss=-12.3952 policy updated! \n",
      "train step 10939 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2192 diff={max=05.9564, min=00.0987, mean=01.4189} policy_loss=-12.8672 policy updated! \n",
      "train step 10940 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=11.9709 diff={max=17.3708, min=00.0693, mean=01.7973} policy_loss=-12.3208 policy updated! \n",
      "train step 10941 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.7280 diff={max=07.6801, min=00.0003, mean=00.9213} policy_loss=-9.9259 policy updated! \n",
      "train step 10942 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.7966 diff={max=11.3435, min=00.0219, mean=02.0628} policy_loss=-13.2511 policy updated! \n",
      "train step 10943 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0767 diff={max=09.3719, min=00.0769, mean=01.6444} policy_loss=-11.8653 policy updated! \n",
      "train step 10944 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5346 diff={max=06.6296, min=00.0262, mean=01.5279} policy_loss=-8.9391 policy updated! \n",
      "train step 10945 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.1351 diff={max=07.2468, min=00.0389, mean=01.3438} policy_loss=-8.3544 policy updated! \n",
      "train step 10946 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4792 diff={max=05.8285, min=00.0064, mean=01.2497} policy_loss=-9.5124 policy updated! \n",
      "train step 10947 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1550 diff={max=06.3528, min=00.0036, mean=01.1346} policy_loss=-12.5243 policy updated! \n",
      "train step 10948 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=09.1246 diff={max=09.9929, min=00.0337, mean=01.9814} policy_loss=-14.7443 policy updated! \n",
      "train step 10949 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=08.7920 diff={max=12.8470, min=00.0154, mean=01.7540} policy_loss=-12.5369 policy updated! \n",
      "train step 10950 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=05.3078 diff={max=07.4527, min=00.0441, mean=01.5908} policy_loss=-12.8084 policy updated! \n",
      "train step 10951 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.5979 diff={max=08.7776, min=00.0059, mean=01.6642} policy_loss=-11.4069 policy updated! \n",
      "train step 10952 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0103 diff={max=06.6053, min=00.0085, mean=01.1571} policy_loss=-11.7063 policy updated! \n",
      "train step 10953 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.2742 diff={max=10.0890, min=00.0029, mean=01.9381} policy_loss=-14.6704 policy updated! \n",
      "train step 10954 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0473 diff={max=07.7139, min=00.0158, mean=01.0930} policy_loss=-9.8173 policy updated! \n",
      "train step 10955 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.0023 diff={max=06.9165, min=00.0080, mean=01.1410} policy_loss=-10.8328 policy updated! \n",
      "train step 10956 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5000 diff={max=06.1736, min=00.0169, mean=01.2491} policy_loss=-14.4991 policy updated! \n",
      "train step 10957 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8639 diff={max=06.1782, min=00.0011, mean=01.3323} policy_loss=-14.7715 policy updated! \n",
      "train step 10958 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=10.7564 diff={max=11.6807, min=00.0735, mean=02.0891} policy_loss=-11.6910 policy updated! \n",
      "train step 10959 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.8928 diff={max=11.7497, min=00.0073, mean=01.7906} policy_loss=-8.7661 policy updated! \n",
      "train step 10960 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.1799 diff={max=10.9034, min=00.0622, mean=01.5746} policy_loss=-13.0865 policy updated! \n",
      "train step 10961 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8923 diff={max=05.3563, min=00.0121, mean=00.9680} policy_loss=-11.4628 policy updated! \n",
      "train step 10962 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7888 diff={max=06.3856, min=00.0018, mean=01.2847} policy_loss=-11.2390 policy updated! \n",
      "train step 10963 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7950 diff={max=08.9176, min=00.0028, mean=01.4114} policy_loss=-14.6290 policy updated! \n",
      "train step 10964 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=07.0798 diff={max=11.1742, min=00.0074, mean=01.5376} policy_loss=-12.0617 policy updated! \n",
      "train step 10965 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.0784 diff={max=09.4700, min=00.0269, mean=01.3028} policy_loss=-14.5290 policy updated! \n",
      "train step 10966 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1689 diff={max=11.6651, min=00.0058, mean=01.4335} policy_loss=-11.5448 policy updated! \n",
      "train step 10967 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4782 diff={max=07.6572, min=00.0032, mean=01.3712} policy_loss=-12.8473 policy updated! \n",
      "train step 10968 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5728 diff={max=05.4313, min=00.0052, mean=01.1157} policy_loss=-13.4345 policy updated! \n",
      "train step 10969 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1834 diff={max=08.5539, min=00.0221, mean=01.4839} policy_loss=-12.1526 policy updated! \n",
      "train step 10970 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.5405 diff={max=14.5985, min=00.0107, mean=01.2533} policy_loss=-14.7584 policy updated! \n",
      "train step 10971 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=14.4194 diff={max=21.9054, min=00.0241, mean=01.9397} policy_loss=-14.3278 policy updated! \n",
      "train step 10972 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7740 diff={max=06.3565, min=00.0009, mean=01.0569} policy_loss=-10.3242 policy updated! \n",
      "train step 10973 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4457 diff={max=07.6094, min=00.0161, mean=01.5556} policy_loss=-11.3296 policy updated! \n",
      "train step 10974 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3397 diff={max=06.4079, min=00.0287, mean=01.3258} policy_loss=-11.3734 policy updated! \n",
      "train step 10975 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.0304 diff={max=13.6603, min=00.0106, mean=01.7195} policy_loss=-13.7078 policy updated! \n",
      "train step 10976 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9986 diff={max=06.8820, min=00.0296, mean=01.1483} policy_loss=-11.0067 policy updated! \n",
      "train step 10977 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2048 diff={max=05.9905, min=00.0004, mean=01.4243} policy_loss=-10.9179 policy updated! \n",
      "train step 10978 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3222 diff={max=06.4204, min=00.0019, mean=01.4242} policy_loss=-13.6149 policy updated! \n",
      "train step 10979 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4746 diff={max=08.2794, min=00.0017, mean=01.4474} policy_loss=-9.9469 policy updated! \n",
      "train step 10980 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.2928 diff={max=05.7988, min=00.0329, mean=01.4162} policy_loss=-14.5461 policy updated! \n",
      "train step 10981 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0067 diff={max=07.8236, min=00.0747, mean=01.3075} policy_loss=-12.1416 policy updated! \n",
      "train step 10982 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.5763 diff={max=10.7001, min=00.0015, mean=01.8380} policy_loss=-10.2677 policy updated! \n",
      "train step 10983 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7192 diff={max=06.5197, min=00.0094, mean=01.4506} policy_loss=-13.7460 policy updated! \n",
      "train step 10984 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.2903 diff={max=08.1581, min=00.0018, mean=01.3773} policy_loss=-13.4022 policy updated! \n",
      "train step 10985 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.6105 diff={max=10.9551, min=00.0076, mean=01.5927} policy_loss=-18.7575 policy updated! \n",
      "train step 10986 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.9318 diff={max=20.6076, min=00.0386, mean=01.7178} policy_loss=-12.9732 policy updated! \n",
      "train step 10987 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.4567 diff={max=15.0721, min=00.0063, mean=01.3093} policy_loss=-13.0075 policy updated! \n",
      "train step 10988 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.9257 diff={max=09.2943, min=00.0026, mean=01.3325} policy_loss=-11.2856 policy updated! \n",
      "train step 10989 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.7482 diff={max=05.5344, min=00.0113, mean=01.1774} policy_loss=-10.6799 policy updated! \n",
      "train step 10990 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.6511 diff={max=11.5162, min=00.0623, mean=01.4146} policy_loss=-12.2002 policy updated! \n",
      "train step 10991 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1240 diff={max=05.3802, min=00.0332, mean=01.4226} policy_loss=-12.1222 policy updated! \n",
      "train step 10992 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5477 diff={max=09.9342, min=00.0113, mean=01.5523} policy_loss=-9.7733 policy updated! \n",
      "train step 10993 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1278 diff={max=04.7173, min=00.0086, mean=01.2238} policy_loss=-11.6606 policy updated! \n",
      "train step 10994 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.2097 diff={max=12.3591, min=00.0271, mean=01.7329} policy_loss=-14.4809 policy updated! \n",
      "train step 10995 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.0402 diff={max=06.4952, min=00.0225, mean=01.2813} policy_loss=-12.9488 policy updated! \n",
      "train step 10996 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.7499 diff={max=10.4608, min=00.0018, mean=01.5061} policy_loss=-13.6283 policy updated! \n",
      "train step 10997 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3052 diff={max=06.2352, min=00.0065, mean=01.3449} policy_loss=-14.1425 policy updated! \n",
      "train step 10998 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=05.7874 diff={max=09.5152, min=00.0004, mean=01.5163} policy_loss=-12.2051 policy updated! \n",
      "train step 10999 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3763 diff={max=07.5645, min=00.0116, mean=01.0733} policy_loss=-10.7177 policy updated! \n",
      "train step 11000 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.9143 diff={max=07.5283, min=00.0178, mean=01.2806} policy_loss=-14.3229 policy updated! \n",
      "train step 11001 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.0628 diff={max=13.9438, min=00.0140, mean=02.0287} policy_loss=-15.1373 policy updated! \n",
      "train step 11002 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.4951 diff={max=16.9650, min=00.0014, mean=01.6990} policy_loss=-11.0573 policy updated! \n",
      "train step 11003 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.9947 diff={max=07.1334, min=00.0163, mean=01.9228} policy_loss=-13.0593 policy updated! \n",
      "train step 11004 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0864 diff={max=05.9100, min=00.0074, mean=01.1269} policy_loss=-10.2792 policy updated! \n",
      "train step 11005 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.7314 diff={max=08.6546, min=00.0088, mean=01.5747} policy_loss=-13.2963 policy updated! \n",
      "train step 11006 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.1092 diff={max=19.1430, min=00.0055, mean=01.5933} policy_loss=-11.9981 policy updated! \n",
      "train step 11007 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1144 diff={max=07.0409, min=00.0362, mean=01.4753} policy_loss=-15.5663 policy updated! \n",
      "train step 11008 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8858 diff={max=10.5914, min=00.0360, mean=01.1515} policy_loss=-13.9406 policy updated! \n",
      "train step 11009 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.9175 diff={max=05.8481, min=00.0235, mean=01.0763} policy_loss=-11.7966 policy updated! \n",
      "train step 11010 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.9914 diff={max=08.9216, min=00.0235, mean=01.4449} policy_loss=-10.9695 policy updated! \n",
      "train step 11011 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=13.6273 diff={max=10.9505, min=00.0331, mean=02.6025} policy_loss=-14.5484 policy updated! \n",
      "train step 11012 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1693 diff={max=08.9184, min=00.0376, mean=01.7554} policy_loss=-12.6765 policy updated! \n",
      "train step 11013 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=06.9654 diff={max=14.0068, min=00.0297, mean=01.4468} policy_loss=-13.6061 policy updated! \n",
      "train step 11014 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1588 diff={max=09.8658, min=00.0100, mean=01.5707} policy_loss=-13.0943 policy updated! \n",
      "train step 11015 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=12.3671 diff={max=18.8387, min=00.0470, mean=01.8754} policy_loss=-11.7046 policy updated! \n",
      "train step 11016 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1593 diff={max=05.0706, min=00.0375, mean=01.0129} policy_loss=-12.5953 policy updated! \n",
      "train step 11017 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8909 diff={max=07.1866, min=00.0034, mean=01.3466} policy_loss=-12.9879 policy updated! \n",
      "train step 11018 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.0793 diff={max=09.5708, min=00.0011, mean=01.4504} policy_loss=-14.3058 policy updated! \n",
      "train step 11019 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=07.3910 diff={max=10.2262, min=00.0013, mean=01.4780} policy_loss=-12.4196 policy updated! \n",
      "train step 11020 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.6799 diff={max=05.0377, min=00.0013, mean=01.1798} policy_loss=-13.1027 policy updated! \n",
      "train step 11021 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.6761 diff={max=15.9190, min=00.0288, mean=01.4402} policy_loss=-12.9595 policy updated! \n",
      "train step 11022 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8253 diff={max=06.7140, min=00.0210, mean=01.3296} policy_loss=-10.9687 policy updated! \n",
      "train step 11023 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=07.7356 diff={max=12.8950, min=00.0133, mean=01.5906} policy_loss=-11.4677 policy updated! \n",
      "train step 11024 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2889 diff={max=09.2489, min=00.0095, mean=01.5738} policy_loss=-11.0275 policy updated! \n",
      "train step 11025 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.0973 diff={max=08.7038, min=00.0367, mean=01.3982} policy_loss=-12.4255 policy updated! \n",
      "train step 11026 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.9599 diff={max=04.7059, min=00.0038, mean=01.1581} policy_loss=-10.3204 policy updated! \n",
      "train step 11027 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.8147 diff={max=16.2908, min=00.0179, mean=01.5688} policy_loss=-12.1471 policy updated! \n",
      "train step 11028 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.2497 diff={max=10.5373, min=00.0485, mean=01.6711} policy_loss=-14.0034 policy updated! \n",
      "train step 11029 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2887 diff={max=07.0720, min=00.0404, mean=01.3002} policy_loss=-10.2065 policy updated! \n",
      "train step 11030 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.6638 diff={max=06.9422, min=00.0068, mean=01.2829} policy_loss=-16.3258 policy updated! \n",
      "train step 11031 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9808 diff={max=07.1732, min=00.0680, mean=01.5318} policy_loss=-11.5142 policy updated! \n",
      "train step 11032 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.5631 diff={max=13.6909, min=00.0027, mean=01.5418} policy_loss=-12.7541 policy updated! \n",
      "train step 11033 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.5268 diff={max=11.9070, min=00.0574, mean=01.6143} policy_loss=-15.1165 policy updated! \n",
      "train step 11034 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3687 diff={max=07.4340, min=00.0100, mean=01.0813} policy_loss=-11.4076 policy updated! \n",
      "train step 11035 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.7609 diff={max=06.0644, min=00.0010, mean=01.4007} policy_loss=-11.5575 policy updated! \n",
      "train step 11036 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9072 diff={max=05.2079, min=00.0042, mean=01.1801} policy_loss=-11.8060 policy updated! \n",
      "train step 11037 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2159 diff={max=10.0513, min=00.0355, mean=01.4136} policy_loss=-13.8404 policy updated! \n",
      "train step 11038 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=06.5600 diff={max=09.4146, min=00.0299, mean=01.6088} policy_loss=-11.3771 policy updated! \n",
      "train step 11039 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=08.9549 diff={max=11.1970, min=00.0016, mean=01.7342} policy_loss=-13.9561 policy updated! \n",
      "train step 11040 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=07.0911 diff={max=13.5244, min=00.0395, mean=01.5467} policy_loss=-12.5875 policy updated! \n",
      "train step 11041 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.3585 diff={max=11.1603, min=00.0298, mean=01.5046} policy_loss=-12.0109 policy updated! \n",
      "train step 11042 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4256 diff={max=10.0046, min=00.0397, mean=01.2447} policy_loss=-12.5297 policy updated! \n",
      "train step 11043 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8899 diff={max=05.0247, min=00.0682, mean=01.1956} policy_loss=-10.7779 policy updated! \n",
      "train step 11044 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.3180 diff={max=08.3048, min=00.0027, mean=01.4486} policy_loss=-12.4009 policy updated! \n",
      "train step 11045 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.3483 diff={max=10.7035, min=00.0127, mean=01.5319} policy_loss=-11.9300 policy updated! \n",
      "train step 11046 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.1595 diff={max=11.1241, min=00.0080, mean=01.5128} policy_loss=-14.9362 policy updated! \n",
      "train step 11047 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0036 diff={max=10.5085, min=00.0384, mean=01.7080} policy_loss=-11.9350 policy updated! \n",
      "train step 11048 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8697 diff={max=07.5165, min=00.0096, mean=01.3639} policy_loss=-11.7202 policy updated! \n",
      "train step 11049 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2889 diff={max=05.0588, min=00.0303, mean=01.0182} policy_loss=-12.6043 policy updated! \n",
      "train step 11050 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=10.8416 diff={max=12.4136, min=00.0009, mean=02.2044} policy_loss=-15.2214 policy updated! \n",
      "train step 11051 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=07.6106 diff={max=11.8204, min=00.0306, mean=01.5393} policy_loss=-11.8923 policy updated! \n",
      "train step 11052 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7415 diff={max=11.6134, min=00.0045, mean=01.3893} policy_loss=-11.8109 policy updated! \n",
      "train step 11053 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.2971 diff={max=12.7816, min=00.0035, mean=01.6934} policy_loss=-12.3449 policy updated! \n",
      "train step 11054 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.1942 diff={max=07.8067, min=00.0163, mean=01.3106} policy_loss=-9.7089 policy updated! \n",
      "train step 11055 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.9135 diff={max=08.0267, min=00.0052, mean=01.4334} policy_loss=-15.0184 policy updated! \n",
      "train step 11056 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0858 diff={max=07.6946, min=00.0100, mean=01.3127} policy_loss=-14.0162 policy updated! \n",
      "train step 11057 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.9478 diff={max=09.7307, min=00.0071, mean=02.2198} policy_loss=-13.3188 policy updated! \n",
      "train step 11058 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.2803 diff={max=12.8351, min=00.0070, mean=01.6794} policy_loss=-12.0064 policy updated! \n",
      "train step 11059 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=07.8990 diff={max=14.4072, min=00.0034, mean=01.2639} policy_loss=-9.6141 policy updated! \n",
      "train step 11060 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.4037 diff={max=13.4121, min=00.0374, mean=01.3658} policy_loss=-11.9321 policy updated! \n",
      "train step 11061 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3154 diff={max=09.9856, min=00.0430, mean=01.5749} policy_loss=-10.7171 policy updated! \n",
      "train step 11062 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2777 diff={max=05.9272, min=00.0089, mean=01.1718} policy_loss=-11.2620 policy updated! \n",
      "train step 11063 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1633 diff={max=04.6716, min=00.0158, mean=01.0728} policy_loss=-12.7260 policy updated! \n",
      "train step 11064 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1722 diff={max=09.5652, min=00.0014, mean=01.6868} policy_loss=-11.8287 policy updated! \n",
      "train step 11065 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=09.9006 diff={max=13.8434, min=00.0342, mean=01.9679} policy_loss=-15.5735 policy updated! \n",
      "train step 11066 reward={max=07.0000, min=00.0000, mean=04.0000} optimizing loss=07.7579 diff={max=09.1416, min=00.0029, mean=01.6398} policy_loss=-10.3987 policy updated! \n",
      "train step 11067 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.3533 diff={max=14.1656, min=00.0191, mean=01.6981} policy_loss=-12.1088 policy updated! \n",
      "train step 11068 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8069 diff={max=13.1565, min=00.0058, mean=01.4459} policy_loss=-12.6525 policy updated! \n",
      "train step 11069 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=13.5076 diff={max=19.9904, min=00.0351, mean=02.0635} policy_loss=-11.9972 policy updated! \n",
      "train step 11070 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.8691 diff={max=04.9146, min=00.0109, mean=00.9399} policy_loss=-11.5267 policy updated! \n",
      "train step 11071 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.2599 diff={max=15.7539, min=00.0164, mean=01.7260} policy_loss=-11.0384 policy updated! \n",
      "train step 11072 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0196 diff={max=05.4977, min=00.0074, mean=01.3104} policy_loss=-12.0344 policy updated! \n",
      "train step 11073 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3948 diff={max=04.2425, min=00.0483, mean=01.0896} policy_loss=-11.6433 policy updated! \n",
      "train step 11074 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6357 diff={max=06.1239, min=00.0538, mean=01.3786} policy_loss=-10.7647 policy updated! \n",
      "train step 11075 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.7509 diff={max=11.6331, min=00.0192, mean=01.6049} policy_loss=-11.5915 policy updated! \n",
      "train step 11076 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9201 diff={max=07.3414, min=00.0262, mean=01.4768} policy_loss=-12.8218 policy updated! \n",
      "train step 11077 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.3681 diff={max=09.7148, min=00.0298, mean=01.8983} policy_loss=-14.9510 policy updated! \n",
      "train step 11078 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7334 diff={max=09.3534, min=00.0560, mean=01.6734} policy_loss=-12.9619 policy updated! \n",
      "train step 11079 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8934 diff={max=05.6793, min=00.0331, mean=01.4300} policy_loss=-11.6453 policy updated! \n",
      "train step 11080 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.1644 diff={max=04.7785, min=00.0118, mean=00.9811} policy_loss=-12.1437 policy updated! \n",
      "train step 11081 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.3625 diff={max=13.0787, min=00.0149, mean=01.8201} policy_loss=-11.4501 policy updated! \n",
      "train step 11082 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.8955 diff={max=18.0247, min=00.0168, mean=01.6616} policy_loss=-11.8541 policy updated! \n",
      "train step 11083 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.8433 diff={max=07.4706, min=00.0307, mean=01.2058} policy_loss=-11.9752 policy updated! \n",
      "train step 11084 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8301 diff={max=08.0124, min=00.0242, mean=01.7217} policy_loss=-14.0223 policy updated! \n",
      "train step 11085 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.8183 diff={max=06.5812, min=00.0293, mean=01.2405} policy_loss=-10.1110 policy updated! \n",
      "train step 11086 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.5446 diff={max=14.8776, min=00.0264, mean=01.8189} policy_loss=-13.2629 policy updated! \n",
      "train step 11087 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8597 diff={max=08.3282, min=00.0032, mean=01.2489} policy_loss=-12.8780 policy updated! \n",
      "train step 11088 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=09.4265 diff={max=10.7131, min=00.0019, mean=01.7990} policy_loss=-13.2216 policy updated! \n",
      "train step 11089 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8199 diff={max=06.2108, min=00.0012, mean=01.1864} policy_loss=-14.7133 policy updated! \n",
      "train step 11090 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.2752 diff={max=06.9345, min=00.0316, mean=01.8221} policy_loss=-13.0745 policy updated! \n",
      "train step 11091 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.2404 diff={max=06.4084, min=00.0046, mean=01.1032} policy_loss=-11.3678 policy updated! \n",
      "train step 11092 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.4266 diff={max=09.9141, min=00.0039, mean=01.7810} policy_loss=-12.4391 policy updated! \n",
      "train step 11093 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.5007 diff={max=14.5373, min=00.0329, mean=01.9432} policy_loss=-16.1752 policy updated! \n",
      "train step 11094 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.7722 diff={max=10.0234, min=00.0634, mean=01.7271} policy_loss=-13.3323 policy updated! \n",
      "train step 11095 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.2804 diff={max=11.5031, min=00.0059, mean=01.4119} policy_loss=-14.1419 policy updated! \n",
      "train step 11096 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7681 diff={max=08.2020, min=00.0094, mean=01.1767} policy_loss=-14.0242 policy updated! \n",
      "train step 11097 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9412 diff={max=07.1025, min=00.0328, mean=01.6232} policy_loss=-13.6295 policy updated! \n",
      "train step 11098 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2274 diff={max=08.2840, min=00.0155, mean=01.5424} policy_loss=-12.6458 policy updated! \n",
      "train step 11099 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=11.0310 diff={max=13.2539, min=00.0056, mean=01.7651} policy_loss=-10.8722 policy updated! \n",
      "train step 11100 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.7278 diff={max=07.3870, min=00.0065, mean=01.0835} policy_loss=-10.8386 policy updated! \n",
      "train step 11101 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.3566 diff={max=15.3633, min=00.0337, mean=01.6179} policy_loss=-13.5395 policy updated! \n",
      "train step 11102 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.6240 diff={max=09.6676, min=00.0118, mean=01.8302} policy_loss=-13.6018 policy updated! \n",
      "train step 11103 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4194 diff={max=05.7563, min=00.0039, mean=01.0121} policy_loss=-12.2934 policy updated! \n",
      "train step 11104 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.3424 diff={max=07.5575, min=00.0108, mean=01.1279} policy_loss=-10.9242 policy updated! \n",
      "train step 11105 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.9477 diff={max=08.2550, min=00.0008, mean=01.0203} policy_loss=-13.0229 policy updated! \n",
      "train step 11106 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6071 diff={max=08.9498, min=00.0025, mean=01.4661} policy_loss=-11.2565 policy updated! \n",
      "train step 11107 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4633 diff={max=08.0004, min=00.0582, mean=01.2493} policy_loss=-12.2059 policy updated! \n",
      "train step 11108 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=19.1880 diff={max=23.8772, min=00.0326, mean=02.1821} policy_loss=-14.0117 policy updated! \n",
      "train step 11109 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3126 diff={max=07.6608, min=00.0143, mean=01.2976} policy_loss=-12.0970 policy updated! \n",
      "train step 11110 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.7370 diff={max=06.0001, min=00.0364, mean=01.0135} policy_loss=-13.1624 policy updated! \n",
      "train step 11111 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.1044 diff={max=08.1902, min=00.0021, mean=01.3489} policy_loss=-10.8914 policy updated! \n",
      "train step 11112 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.6361 diff={max=08.1968, min=00.0123, mean=01.8686} policy_loss=-14.4734 policy updated! \n",
      "train step 11113 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=15.8931 diff={max=18.1267, min=00.0165, mean=01.8527} policy_loss=-12.3327 policy updated! \n",
      "train step 11114 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=07.7042 diff={max=11.9101, min=00.0083, mean=01.5055} policy_loss=-12.7309 policy updated! \n",
      "train step 11115 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.3493 diff={max=10.9532, min=00.1068, mean=01.5919} policy_loss=-10.8234 policy updated! \n",
      "train step 11116 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6323 diff={max=07.3823, min=00.0077, mean=01.0904} policy_loss=-11.1692 policy updated! \n",
      "train step 11117 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3652 diff={max=05.8517, min=00.0015, mean=01.1376} policy_loss=-9.7478 policy updated! \n",
      "train step 11118 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7104 diff={max=09.0647, min=00.0485, mean=01.4679} policy_loss=-12.6440 policy updated! \n",
      "train step 11119 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0166 diff={max=09.6538, min=00.0160, mean=01.4634} policy_loss=-9.9284 policy updated! \n",
      "train step 11120 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.3967 diff={max=06.5837, min=00.0450, mean=01.4312} policy_loss=-13.3075 policy updated! \n",
      "train step 11121 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3511 diff={max=06.5661, min=00.0450, mean=01.5145} policy_loss=-14.3233 policy updated! \n",
      "train step 11122 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4621 diff={max=09.1355, min=00.0566, mean=01.2853} policy_loss=-12.1917 policy updated! \n",
      "train step 11123 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8948 diff={max=04.2420, min=00.0006, mean=01.0135} policy_loss=-13.2917 policy updated! \n",
      "train step 11124 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.7241 diff={max=13.6746, min=00.0436, mean=01.7060} policy_loss=-9.9450 policy updated! \n",
      "train step 11125 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.8227 diff={max=06.9206, min=00.0796, mean=01.1327} policy_loss=-11.8652 policy updated! \n",
      "train step 11126 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=08.1391 diff={max=12.5297, min=00.1031, mean=01.7842} policy_loss=-13.5333 policy updated! \n",
      "train step 11127 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3984 diff={max=12.1113, min=00.0573, mean=01.6017} policy_loss=-11.7872 policy updated! \n",
      "train step 11128 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=14.1043 diff={max=18.5661, min=00.0042, mean=01.8291} policy_loss=-12.3486 policy updated! \n",
      "train step 11129 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.7456 diff={max=13.0495, min=00.0063, mean=01.6309} policy_loss=-12.9326 policy updated! \n",
      "train step 11130 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.9040 diff={max=07.6259, min=00.0062, mean=01.5427} policy_loss=-15.7521 policy updated! \n",
      "train step 11131 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3860 diff={max=08.2395, min=00.0010, mean=01.3623} policy_loss=-10.3308 policy updated! \n",
      "train step 11132 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1230 diff={max=08.0906, min=00.0277, mean=01.4971} policy_loss=-15.4770 policy updated! \n",
      "train step 11133 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.5113 diff={max=06.9925, min=00.0004, mean=01.1313} policy_loss=-11.9261 policy updated! \n",
      "train step 11134 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9229 diff={max=05.1043, min=00.0027, mean=00.9252} policy_loss=-10.9218 policy updated! \n",
      "train step 11135 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.4757 diff={max=05.9675, min=00.0001, mean=01.2424} policy_loss=-14.2788 policy updated! \n",
      "train step 11136 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=13.2221 diff={max=16.0300, min=00.0098, mean=01.8903} policy_loss=-12.7068 policy updated! \n",
      "train step 11137 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.3006 diff={max=17.8334, min=00.0147, mean=01.6267} policy_loss=-10.5675 policy updated! \n",
      "train step 11138 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.1778 diff={max=08.7355, min=00.0075, mean=01.2432} policy_loss=-10.2240 policy updated! \n",
      "train step 11139 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7251 diff={max=05.9645, min=00.0121, mean=01.3976} policy_loss=-12.2447 policy updated! \n",
      "train step 11140 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.6553 diff={max=08.9034, min=00.0206, mean=01.4135} policy_loss=-12.2201 policy updated! \n",
      "train step 11141 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7532 diff={max=08.9402, min=00.0334, mean=01.3755} policy_loss=-12.1295 policy updated! \n",
      "train step 11142 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=06.3122 diff={max=09.3292, min=00.0139, mean=01.5826} policy_loss=-12.8711 policy updated! \n",
      "train step 11143 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.6694 diff={max=05.1665, min=00.0264, mean=01.1453} policy_loss=-11.1083 policy updated! \n",
      "train step 11144 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.0341 diff={max=08.2182, min=00.0619, mean=01.3145} policy_loss=-12.2389 policy updated! \n",
      "train step 11145 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.5083 diff={max=06.9524, min=00.0057, mean=01.4852} policy_loss=-11.1564 policy updated! \n",
      "train step 11146 reward={max=07.0000, min=00.0000, mean=04.0000} optimizing loss=04.6690 diff={max=07.7938, min=00.0118, mean=01.4936} policy_loss=-14.0409 policy updated! \n",
      "train step 11147 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6692 diff={max=03.9026, min=00.0090, mean=01.2360} policy_loss=-12.2770 policy updated! \n",
      "train step 11148 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.3388 diff={max=16.8591, min=00.0161, mean=01.7218} policy_loss=-12.5710 policy updated! \n",
      "train step 11149 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=07.9027 diff={max=14.7577, min=00.0621, mean=01.6420} policy_loss=-11.6474 policy updated! \n",
      "train step 11150 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.6985 diff={max=06.5186, min=00.0022, mean=01.0520} policy_loss=-12.3614 policy updated! \n",
      "train step 11151 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=09.0863 diff={max=10.5780, min=00.0139, mean=01.6902} policy_loss=-16.3084 policy updated! \n",
      "train step 11152 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9223 diff={max=07.1155, min=00.0091, mean=01.1662} policy_loss=-9.6570 policy updated! \n",
      "train step 11153 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.1788 diff={max=18.8668, min=00.0325, mean=01.9898} policy_loss=-10.6640 policy updated! \n",
      "train step 11154 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.7642 diff={max=08.5390, min=00.0324, mean=01.4374} policy_loss=-12.4062 policy updated! \n",
      "train step 11155 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.4267 diff={max=06.9773, min=00.0032, mean=01.7908} policy_loss=-15.1171 policy updated! \n",
      "train step 11156 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.8512 diff={max=08.2199, min=00.0044, mean=01.8909} policy_loss=-14.0736 policy updated! \n",
      "train step 11157 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0551 diff={max=10.2349, min=00.0068, mean=01.1898} policy_loss=-13.2363 policy updated! \n",
      "train step 11158 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.2807 diff={max=06.3696, min=00.0031, mean=01.3842} policy_loss=-12.4678 policy updated! \n",
      "train step 11159 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.9524 diff={max=13.9239, min=00.0149, mean=02.0138} policy_loss=-16.1701 policy updated! \n",
      "train step 11160 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.0946 diff={max=06.2763, min=00.0146, mean=01.2149} policy_loss=-13.7443 policy updated! \n",
      "train step 11161 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9839 diff={max=09.4766, min=00.0047, mean=01.5078} policy_loss=-11.0948 policy updated! \n",
      "train step 11162 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0686 diff={max=07.4185, min=00.1267, mean=01.5728} policy_loss=-13.9495 policy updated! \n",
      "train step 11163 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9080 diff={max=06.4177, min=00.0018, mean=01.3299} policy_loss=-14.2027 policy updated! \n",
      "train step 11164 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=14.1432 diff={max=21.2986, min=00.0555, mean=01.8617} policy_loss=-15.1982 policy updated! \n",
      "train step 11165 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.4491 diff={max=05.4708, min=00.1070, mean=01.3380} policy_loss=-14.4521 policy updated! \n",
      "train step 11166 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7207 diff={max=07.5325, min=00.0041, mean=01.1077} policy_loss=-9.6391 policy updated! \n",
      "train step 11167 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9217 diff={max=05.3811, min=00.0350, mean=01.2334} policy_loss=-11.0264 policy updated! \n",
      "train step 11168 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0501 diff={max=08.1873, min=00.0034, mean=01.7629} policy_loss=-10.8926 policy updated! \n",
      "train step 11169 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.4425 diff={max=13.0568, min=00.0012, mean=02.1102} policy_loss=-13.5503 policy updated! \n",
      "train step 11170 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=10.9734 diff={max=13.2009, min=00.0205, mean=01.9859} policy_loss=-12.8239 policy updated! \n",
      "train step 11171 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.2397 diff={max=14.8973, min=00.0258, mean=01.8611} policy_loss=-10.7034 policy updated! \n",
      "train step 11172 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.7391 diff={max=09.5954, min=00.0361, mean=01.6670} policy_loss=-10.6383 policy updated! \n",
      "train step 11173 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.7483 diff={max=14.4114, min=00.0000, mean=01.6171} policy_loss=-10.8984 policy updated! \n",
      "train step 11174 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.4218 diff={max=11.3102, min=00.0135, mean=01.9498} policy_loss=-13.2217 policy updated! \n",
      "train step 11175 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.1073 diff={max=06.3601, min=00.0595, mean=01.1799} policy_loss=-14.1152 policy updated! \n",
      "train step 11176 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4070 diff={max=05.7531, min=00.0121, mean=01.4382} policy_loss=-12.6362 policy updated! \n",
      "train step 11177 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4439 diff={max=05.4659, min=00.0361, mean=01.3039} policy_loss=-14.7686 policy updated! \n",
      "train step 11178 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0614 diff={max=13.1987, min=00.0175, mean=01.5272} policy_loss=-12.3153 policy updated! \n",
      "train step 11179 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.4284 diff={max=07.4354, min=00.0109, mean=01.2334} policy_loss=-12.4469 policy updated! \n",
      "train step 11180 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.7189 diff={max=07.3432, min=00.0223, mean=01.3709} policy_loss=-11.9882 policy updated! \n",
      "train step 11181 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.2326 diff={max=06.4793, min=00.0485, mean=01.4152} policy_loss=-11.8187 policy updated! \n",
      "train step 11182 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5124 diff={max=08.1215, min=00.0078, mean=01.7341} policy_loss=-11.9844 policy updated! \n",
      "train step 11183 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5304 diff={max=06.0574, min=00.0217, mean=01.1986} policy_loss=-10.8985 policy updated! \n",
      "train step 11184 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9027 diff={max=06.1422, min=00.0074, mean=01.3214} policy_loss=-11.7188 policy updated! \n",
      "train step 11185 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.1524 diff={max=08.9225, min=00.0633, mean=01.1090} policy_loss=-12.7544 policy updated! \n",
      "train step 11186 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6382 diff={max=05.3167, min=00.0198, mean=01.1075} policy_loss=-10.9217 policy updated! \n",
      "train step 11187 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3842 diff={max=11.8867, min=00.0052, mean=01.0197} policy_loss=-11.4061 policy updated! \n",
      "train step 11188 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8571 diff={max=10.1078, min=00.0064, mean=01.4625} policy_loss=-12.6956 policy updated! \n",
      "train step 11189 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9724 diff={max=10.3630, min=00.0066, mean=01.2777} policy_loss=-12.7886 policy updated! \n",
      "train step 11190 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.9830 diff={max=11.5253, min=00.0119, mean=01.5661} policy_loss=-13.2820 policy updated! \n",
      "train step 11191 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.4241 diff={max=10.9549, min=00.0124, mean=01.7211} policy_loss=-13.3183 policy updated! \n",
      "train step 11192 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4181 diff={max=07.0313, min=00.0071, mean=01.6870} policy_loss=-12.1991 policy updated! \n",
      "train step 11193 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4545 diff={max=10.4472, min=00.0217, mean=01.2101} policy_loss=-10.3115 policy updated! \n",
      "train step 11194 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5328 diff={max=05.1687, min=00.0051, mean=01.1664} policy_loss=-11.2209 policy updated! \n",
      "train step 11195 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.6765 diff={max=10.6485, min=00.0108, mean=01.1927} policy_loss=-10.3322 policy updated! \n",
      "train step 11196 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7428 diff={max=09.2583, min=00.0376, mean=01.4638} policy_loss=-9.0074 policy updated! \n",
      "train step 11197 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.5848 diff={max=12.1971, min=00.0048, mean=01.6409} policy_loss=-11.9835 policy updated! \n",
      "train step 11198 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=09.1076 diff={max=13.1207, min=00.0096, mean=01.7345} policy_loss=-11.9468 policy updated! \n",
      "train step 11199 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9402 diff={max=07.6341, min=00.0609, mean=01.3439} policy_loss=-13.2606 policy updated! \n",
      "train step 11200 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.1361 diff={max=09.4428, min=00.0108, mean=01.5623} policy_loss=-14.5251 policy updated! \n",
      "train step 11201 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.3512 diff={max=14.8924, min=00.0177, mean=01.5047} policy_loss=-12.8601 policy updated! \n",
      "train step 11202 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2816 diff={max=12.2846, min=00.0086, mean=01.4516} policy_loss=-11.8311 policy updated! \n",
      "train step 11203 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9147 diff={max=08.5551, min=00.0115, mean=01.3917} policy_loss=-12.6439 policy updated! \n",
      "train step 11204 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.2016 diff={max=05.7997, min=00.0702, mean=01.4098} policy_loss=-11.7542 policy updated! \n",
      "train step 11205 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.8771 diff={max=04.8789, min=00.0561, mean=01.4157} policy_loss=-12.4577 policy updated! \n",
      "train step 11206 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6702 diff={max=07.9161, min=00.0029, mean=01.3170} policy_loss=-11.3828 policy updated! \n",
      "train step 11207 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.4037 diff={max=15.7928, min=00.0244, mean=01.5918} policy_loss=-12.2921 policy updated! \n",
      "train step 11208 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.6644 diff={max=13.1642, min=00.0055, mean=01.6704} policy_loss=-10.3141 policy updated! \n",
      "train step 11209 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.8386 diff={max=08.9732, min=00.0024, mean=01.5147} policy_loss=-11.4304 policy updated! \n",
      "train step 11210 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.3303 diff={max=11.9412, min=00.0314, mean=01.8217} policy_loss=-12.7880 policy updated! \n",
      "train step 11211 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0894 diff={max=10.2680, min=00.0213, mean=01.5378} policy_loss=-11.5558 policy updated! \n",
      "train step 11212 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.5183 diff={max=13.3248, min=00.0006, mean=01.9613} policy_loss=-11.5239 policy updated! \n",
      "train step 11213 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.6050 diff={max=12.2306, min=00.0037, mean=01.4790} policy_loss=-11.2874 policy updated! \n",
      "train step 11214 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4749 diff={max=07.3351, min=00.0054, mean=01.3379} policy_loss=-12.4465 policy updated! \n",
      "train step 11215 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.4893 diff={max=03.9276, min=00.0380, mean=01.1607} policy_loss=-12.1131 policy updated! \n",
      "train step 11216 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=11.5409 diff={max=15.6631, min=00.0018, mean=01.8861} policy_loss=-16.9358 policy updated! \n",
      "train step 11217 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4508 diff={max=10.7901, min=00.0205, mean=01.4217} policy_loss=-14.8476 policy updated! \n",
      "train step 11218 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.8942 diff={max=05.1361, min=00.0074, mean=01.2624} policy_loss=-11.9847 policy updated! \n",
      "train step 11219 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5059 diff={max=10.8929, min=00.0093, mean=01.4179} policy_loss=-11.2524 policy updated! \n",
      "train step 11220 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.4544 diff={max=08.8602, min=00.0154, mean=01.3128} policy_loss=-10.9900 policy updated! \n",
      "train step 11221 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0246 diff={max=06.5458, min=00.0145, mean=01.3079} policy_loss=-10.0208 policy updated! \n",
      "train step 11222 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6460 diff={max=09.0510, min=00.0002, mean=01.4823} policy_loss=-12.0060 policy updated! \n",
      "train step 11223 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.9839 diff={max=09.7536, min=00.0108, mean=01.6603} policy_loss=-11.7816 policy updated! \n",
      "train step 11224 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.8734 diff={max=07.6572, min=00.0133, mean=01.5359} policy_loss=-12.3219 policy updated! \n",
      "train step 11225 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.9543 diff={max=08.8176, min=00.0143, mean=01.0225} policy_loss=-11.9997 policy updated! \n",
      "train step 11226 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=11.3982 diff={max=20.3964, min=00.0242, mean=01.6070} policy_loss=-15.9960 policy updated! \n",
      "train step 11227 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2103 diff={max=09.7928, min=00.0661, mean=01.5222} policy_loss=-12.4115 policy updated! \n",
      "train step 11228 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8623 diff={max=09.3596, min=00.0455, mean=01.3916} policy_loss=-13.9532 policy updated! \n",
      "train step 11229 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.1619 diff={max=12.1959, min=00.0066, mean=01.9547} policy_loss=-16.0472 policy updated! \n",
      "train step 11230 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.8903 diff={max=06.3481, min=00.0127, mean=01.1491} policy_loss=-13.3441 policy updated! \n",
      "train step 11231 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.1198 diff={max=10.1830, min=00.0103, mean=01.6511} policy_loss=-12.5090 policy updated! \n",
      "train step 11232 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=18.0947 diff={max=18.9268, min=00.0304, mean=02.1593} policy_loss=-12.2034 policy updated! \n",
      "train step 11233 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.0247 diff={max=13.0966, min=00.0184, mean=01.6673} policy_loss=-12.7650 policy updated! \n",
      "train step 11234 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0557 diff={max=11.5304, min=00.0084, mean=01.4043} policy_loss=-12.0871 policy updated! \n",
      "train step 11235 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.2006 diff={max=08.6946, min=00.0153, mean=01.3794} policy_loss=-11.3912 policy updated! \n",
      "train step 11236 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7088 diff={max=07.3370, min=00.0004, mean=01.3331} policy_loss=-13.0567 policy updated! \n",
      "train step 11237 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.0971 diff={max=19.8626, min=00.0035, mean=01.4236} policy_loss=-12.5875 policy updated! \n",
      "train step 11238 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3693 diff={max=11.5660, min=00.0404, mean=01.4511} policy_loss=-13.8987 policy updated! \n",
      "train step 11239 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.9200 diff={max=09.6514, min=00.0199, mean=01.3243} policy_loss=-9.5763 policy updated! \n",
      "train step 11240 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.0539 diff={max=04.7974, min=00.0119, mean=01.1885} policy_loss=-12.3068 policy updated! \n",
      "train step 11241 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=10.2937 diff={max=10.0739, min=00.0044, mean=02.0337} policy_loss=-16.5374 policy updated! \n",
      "train step 11242 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1698 diff={max=07.5967, min=00.0577, mean=01.1353} policy_loss=-14.3094 policy updated! \n",
      "train step 11243 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=07.7088 diff={max=11.5952, min=00.0284, mean=01.6050} policy_loss=-12.3606 policy updated! \n",
      "train step 11244 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=09.6308 diff={max=11.3452, min=00.0088, mean=01.9575} policy_loss=-15.1497 policy updated! \n",
      "train step 11245 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.5231 diff={max=04.7437, min=00.0019, mean=01.0691} policy_loss=-12.9271 policy updated! \n",
      "train step 11246 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5500 diff={max=07.9574, min=00.0096, mean=01.5710} policy_loss=-13.7297 policy updated! \n",
      "train step 11247 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.2817 diff={max=09.0695, min=00.0209, mean=01.8459} policy_loss=-13.1993 policy updated! \n",
      "train step 11248 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=15.3501 diff={max=22.0922, min=00.0225, mean=01.8080} policy_loss=-11.1691 policy updated! \n",
      "train step 11249 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5385 diff={max=11.3340, min=00.0182, mean=01.3895} policy_loss=-13.8635 policy updated! \n",
      "train step 11250 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=08.0004 diff={max=09.8456, min=00.0245, mean=01.7998} policy_loss=-10.7461 policy updated! \n",
      "train step 11251 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8074 diff={max=07.3190, min=00.0338, mean=01.3956} policy_loss=-13.9646 policy updated! \n",
      "train step 11252 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.5760 diff={max=14.5045, min=00.0665, mean=01.8663} policy_loss=-13.4069 policy updated! \n",
      "train step 11253 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.8084 diff={max=12.9371, min=00.0401, mean=01.5469} policy_loss=-14.9257 policy updated! \n",
      "train step 11254 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9636 diff={max=04.2978, min=00.0007, mean=01.0242} policy_loss=-14.0162 policy updated! \n",
      "train step 11255 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.3208 diff={max=10.1616, min=00.0070, mean=01.4414} policy_loss=-11.1231 policy updated! \n",
      "train step 11256 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6315 diff={max=11.7801, min=00.0233, mean=01.3159} policy_loss=-12.2849 policy updated! \n",
      "train step 11257 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4428 diff={max=06.2314, min=00.0023, mean=01.2981} policy_loss=-11.0375 policy updated! \n",
      "train step 11258 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6088 diff={max=07.0551, min=00.0067, mean=01.4646} policy_loss=-14.0947 policy updated! \n",
      "train step 11259 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.5662 diff={max=14.5965, min=00.0325, mean=01.8535} policy_loss=-12.8395 policy updated! \n",
      "train step 11260 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.0324 diff={max=06.9468, min=00.0065, mean=01.3776} policy_loss=-11.4734 policy updated! \n",
      "train step 11261 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.3177 diff={max=14.1527, min=00.0301, mean=01.7782} policy_loss=-12.4901 policy updated! \n",
      "train step 11262 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.4700 diff={max=15.1038, min=00.0396, mean=01.7653} policy_loss=-12.2855 policy updated! \n",
      "train step 11263 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5262 diff={max=07.4402, min=00.0434, mean=01.6712} policy_loss=-10.0336 policy updated! \n",
      "train step 11264 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.7516 diff={max=06.9267, min=00.0278, mean=01.3248} policy_loss=-11.5088 policy updated! \n",
      "train step 11265 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.4937 diff={max=08.8849, min=00.0362, mean=01.6052} policy_loss=-14.3097 policy updated! \n",
      "train step 11266 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.0600 diff={max=07.3885, min=00.0074, mean=01.3357} policy_loss=-11.4999 policy updated! \n",
      "train step 11267 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5982 diff={max=10.9902, min=00.0718, mean=01.4849} policy_loss=-12.7659 policy updated! \n",
      "train step 11268 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2972 diff={max=08.7074, min=00.0143, mean=01.3170} policy_loss=-11.7990 policy updated! \n",
      "train step 11269 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.6150 diff={max=14.6344, min=00.0088, mean=01.6944} policy_loss=-11.4978 policy updated! \n",
      "train step 11270 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.7093 diff={max=05.5497, min=00.0787, mean=01.4061} policy_loss=-12.5588 policy updated! \n",
      "train step 11271 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9791 diff={max=07.7116, min=00.0101, mean=01.5010} policy_loss=-14.0798 policy updated! \n",
      "train step 11272 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.7469 diff={max=10.8646, min=00.0256, mean=01.8171} policy_loss=-13.1358 policy updated! \n",
      "train step 11273 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=11.4611 diff={max=16.4421, min=00.0309, mean=01.9349} policy_loss=-13.1077 policy updated! \n",
      "train step 11274 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0285 diff={max=05.9211, min=00.0363, mean=01.6342} policy_loss=-14.3713 policy updated! \n",
      "train step 11275 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.3184 diff={max=06.7660, min=00.0026, mean=01.6359} policy_loss=-12.4978 policy updated! \n",
      "train step 11276 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0797 diff={max=11.1464, min=00.0060, mean=01.2491} policy_loss=-11.2862 policy updated! \n",
      "train step 11277 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3081 diff={max=09.8674, min=00.0246, mean=01.3717} policy_loss=-12.0699 policy updated! \n",
      "train step 11278 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.6371 diff={max=08.8610, min=00.0040, mean=01.4606} policy_loss=-12.4029 policy updated! \n",
      "train step 11279 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2911 diff={max=09.1942, min=00.0812, mean=01.6250} policy_loss=-15.0318 policy updated! \n",
      "train step 11280 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.7468 diff={max=03.9358, min=00.0027, mean=00.9373} policy_loss=-11.0467 policy updated! \n",
      "train step 11281 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2020 diff={max=07.1341, min=00.0355, mean=01.3438} policy_loss=-11.4168 policy updated! \n",
      "train step 11282 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.7534 diff={max=18.3428, min=00.0335, mean=02.0980} policy_loss=-14.9704 policy updated! \n",
      "train step 11283 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9295 diff={max=07.5642, min=00.0023, mean=01.4650} policy_loss=-11.0116 policy updated! \n",
      "train step 11284 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.1162 diff={max=10.6867, min=00.0099, mean=01.7310} policy_loss=-14.3984 policy updated! \n",
      "train step 11285 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.7960 diff={max=06.6280, min=00.0009, mean=01.2357} policy_loss=-11.1695 policy updated! \n",
      "train step 11286 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7969 diff={max=09.1973, min=00.0185, mean=01.4345} policy_loss=-13.2032 policy updated! \n",
      "train step 11287 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5497 diff={max=05.5245, min=00.0074, mean=00.9836} policy_loss=-10.3173 policy updated! \n",
      "train step 11288 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8220 diff={max=11.2278, min=00.0033, mean=01.4776} policy_loss=-12.7794 policy updated! \n",
      "train step 11289 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6929 diff={max=08.1783, min=00.0288, mean=01.4151} policy_loss=-13.4637 policy updated! \n",
      "train step 11290 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.8325 diff={max=09.0512, min=00.0342, mean=01.3124} policy_loss=-13.1677 policy updated! \n",
      "train step 11291 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=07.4758 diff={max=09.5854, min=00.0109, mean=01.7107} policy_loss=-13.6666 policy updated! \n",
      "train step 11292 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8375 diff={max=07.2588, min=00.0019, mean=01.5995} policy_loss=-10.7453 policy updated! \n",
      "train step 11293 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0883 diff={max=07.5307, min=00.0051, mean=01.3916} policy_loss=-12.2902 policy updated! \n",
      "train step 11294 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.2907 diff={max=13.4479, min=00.0612, mean=02.2302} policy_loss=-12.4765 policy updated! \n",
      "train step 11295 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.0111 diff={max=09.0006, min=00.0417, mean=01.6079} policy_loss=-12.4527 policy updated! \n",
      "train step 11296 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7478 diff={max=07.9565, min=00.0096, mean=01.2685} policy_loss=-9.0880 policy updated! \n",
      "train step 11297 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3056 diff={max=05.1822, min=00.0304, mean=01.3326} policy_loss=-12.5540 policy updated! \n",
      "train step 11298 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.7186 diff={max=14.8331, min=00.0003, mean=01.7199} policy_loss=-15.0987 policy updated! \n",
      "train step 11299 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.3896 diff={max=06.5314, min=00.0053, mean=01.0807} policy_loss=-12.3388 policy updated! \n",
      "train step 11300 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=10.9583 diff={max=18.5717, min=00.0054, mean=01.5041} policy_loss=-10.7627 policy updated! \n",
      "train step 11301 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.4194 diff={max=06.6281, min=00.0037, mean=00.9978} policy_loss=-11.8588 policy updated! \n",
      "train step 11302 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1115 diff={max=03.5598, min=00.0446, mean=01.1147} policy_loss=-10.6041 policy updated! \n",
      "train step 11303 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=18.5621 diff={max=19.0452, min=00.0095, mean=02.4480} policy_loss=-13.1265 policy updated! \n",
      "train step 11304 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=10.2091 diff={max=11.9684, min=00.0217, mean=01.9192} policy_loss=-12.3200 policy updated! \n",
      "train step 11305 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.3141 diff={max=05.7690, min=00.0481, mean=01.1903} policy_loss=-12.1168 policy updated! \n",
      "train step 11306 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.8150 diff={max=13.6677, min=00.1308, mean=01.6771} policy_loss=-13.2081 policy updated! \n",
      "train step 11307 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9277 diff={max=13.0504, min=00.0130, mean=01.4998} policy_loss=-12.2420 policy updated! \n",
      "train step 11308 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0759 diff={max=06.5169, min=00.0034, mean=01.4824} policy_loss=-12.7408 policy updated! \n",
      "train step 11309 reward={max=07.0000, min=00.0000, mean=05.4000} optimizing loss=03.0681 diff={max=04.8257, min=00.0535, mean=01.2094} policy_loss=-13.9928 policy updated! \n",
      "train step 11310 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.9492 diff={max=06.5612, min=00.0396, mean=01.3895} policy_loss=-12.6127 policy updated! \n",
      "train step 11311 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.4145 diff={max=06.4709, min=00.0374, mean=01.2379} policy_loss=-13.1387 policy updated! \n",
      "train step 11312 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2410 diff={max=09.5644, min=00.0314, mean=01.5715} policy_loss=-13.7793 policy updated! \n",
      "train step 11313 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.1708 diff={max=07.9304, min=00.0883, mean=01.4104} policy_loss=-13.0248 policy updated! \n",
      "train step 11314 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.0303 diff={max=12.2521, min=00.0154, mean=02.0036} policy_loss=-13.1279 policy updated! \n",
      "train step 11315 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.2040 diff={max=11.0511, min=00.0931, mean=01.6524} policy_loss=-13.9201 policy updated! \n",
      "train step 11316 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.3380 diff={max=02.9550, min=00.0586, mean=00.9350} policy_loss=-11.5869 policy updated! \n",
      "train step 11317 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=11.2191 diff={max=15.3362, min=00.0579, mean=01.9293} policy_loss=-12.5534 policy updated! \n",
      "train step 11318 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=07.5462 diff={max=11.0891, min=00.0419, mean=01.6828} policy_loss=-13.3805 policy updated! \n",
      "train step 11319 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.0346 diff={max=06.8656, min=00.0449, mean=01.3200} policy_loss=-13.1419 policy updated! \n",
      "train step 11320 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.3349 diff={max=08.8209, min=00.0287, mean=01.4889} policy_loss=-12.1938 policy updated! \n",
      "train step 11321 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.9220 diff={max=11.8555, min=00.0083, mean=01.7904} policy_loss=-11.6583 policy updated! \n",
      "train step 11322 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2181 diff={max=08.0268, min=00.0120, mean=00.9796} policy_loss=-9.9525 policy updated! \n",
      "train step 11323 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.1800 diff={max=10.9058, min=00.0310, mean=02.2330} policy_loss=-12.8563 policy updated! \n",
      "train step 11324 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.1022 diff={max=11.4135, min=00.0026, mean=01.5928} policy_loss=-14.7981 policy updated! \n",
      "train step 11325 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.2239 diff={max=14.8326, min=00.0000, mean=01.7957} policy_loss=-11.8050 policy updated! \n",
      "train step 11326 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=09.2787 diff={max=20.1542, min=00.0198, mean=01.1802} policy_loss=-14.7414 policy updated! \n",
      "train step 11327 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.9780 diff={max=11.7696, min=00.0241, mean=01.8851} policy_loss=-12.4086 policy updated! \n",
      "train step 11328 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.8839 diff={max=07.4770, min=00.0795, mean=01.2865} policy_loss=-12.3524 policy updated! \n",
      "train step 11329 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.3157 diff={max=09.2210, min=00.0081, mean=01.4440} policy_loss=-12.5647 policy updated! \n",
      "train step 11330 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.3759 diff={max=06.5968, min=00.0223, mean=01.4918} policy_loss=-12.4043 policy updated! \n",
      "train step 11331 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.9902 diff={max=23.5047, min=00.0447, mean=01.6649} policy_loss=-15.1156 policy updated! \n",
      "train step 11332 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4358 diff={max=05.0602, min=00.0071, mean=01.0364} policy_loss=-11.7611 policy updated! \n",
      "train step 11333 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8726 diff={max=06.6636, min=00.0139, mean=01.4157} policy_loss=-16.0830 policy updated! \n",
      "train step 11334 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.6482 diff={max=10.3482, min=00.0036, mean=01.8745} policy_loss=-12.9556 policy updated! \n",
      "train step 11335 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.6107 diff={max=08.9841, min=00.0599, mean=01.6507} policy_loss=-16.1652 policy updated! \n",
      "train step 11336 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9404 diff={max=04.3070, min=00.0348, mean=00.8986} policy_loss=-10.3902 policy updated! \n",
      "train step 11337 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3025 diff={max=06.8322, min=00.0183, mean=01.3422} policy_loss=-12.1150 policy updated! \n",
      "train step 11338 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.9745 diff={max=09.2031, min=00.0368, mean=01.3271} policy_loss=-10.4326 policy updated! \n",
      "train step 11339 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5633 diff={max=10.1242, min=00.0072, mean=01.4906} policy_loss=-10.3937 policy updated! \n",
      "train step 11340 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=07.3820 diff={max=15.7717, min=00.0204, mean=01.3138} policy_loss=-12.7945 policy updated! \n",
      "train step 11341 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.6669 diff={max=12.8822, min=00.0520, mean=01.7435} policy_loss=-11.9472 policy updated! \n",
      "train step 11342 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0503 diff={max=07.1891, min=00.0364, mean=01.1384} policy_loss=-13.0743 policy updated! \n",
      "train step 11343 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9646 diff={max=08.8254, min=00.0043, mean=01.4288} policy_loss=-12.4715 policy updated! \n",
      "train step 11344 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4977 diff={max=06.0664, min=00.0113, mean=01.2688} policy_loss=-12.1815 policy updated! \n",
      "train step 11345 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.6171 diff={max=06.2961, min=00.0279, mean=01.2760} policy_loss=-11.9475 policy updated! \n",
      "train step 11346 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.0881 diff={max=13.3621, min=00.0205, mean=02.0686} policy_loss=-16.0219 policy updated! \n",
      "train step 11347 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7446 diff={max=09.7554, min=00.0107, mean=01.3968} policy_loss=-10.1179 policy updated! \n",
      "train step 11348 reward={max=07.0000, min=00.0000, mean=02.6000} optimizing loss=11.3006 diff={max=19.8427, min=00.0021, mean=01.3102} policy_loss=-13.0863 policy updated! \n",
      "train step 11349 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7802 diff={max=04.0237, min=00.0359, mean=00.9621} policy_loss=-12.1643 policy updated! \n",
      "train step 11350 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.9711 diff={max=07.7565, min=00.0084, mean=01.2508} policy_loss=-10.8560 policy updated! \n",
      "train step 11351 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.9738 diff={max=06.9690, min=00.0394, mean=01.2410} policy_loss=-11.7277 policy updated! \n",
      "train step 11352 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7657 diff={max=07.5781, min=00.0071, mean=01.2401} policy_loss=-11.7893 policy updated! \n",
      "train step 11353 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9117 diff={max=10.4896, min=00.0228, mean=01.7119} policy_loss=-11.3050 policy updated! \n",
      "train step 11354 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.4513 diff={max=07.8904, min=00.0087, mean=01.1014} policy_loss=-10.9994 policy updated! \n",
      "train step 11355 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=12.3179 diff={max=16.4871, min=00.1040, mean=01.8603} policy_loss=-13.7165 policy updated! \n",
      "train step 11356 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=10.0438 diff={max=17.3194, min=00.0003, mean=01.5738} policy_loss=-12.0085 policy updated! \n",
      "train step 11357 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9287 diff={max=10.9817, min=00.0046, mean=01.2165} policy_loss=-12.2652 policy updated! \n",
      "train step 11358 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4418 diff={max=07.1842, min=00.0040, mean=01.0713} policy_loss=-11.1157 policy updated! \n",
      "train step 11359 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.0100 diff={max=04.2453, min=00.0126, mean=01.2343} policy_loss=-12.0876 policy updated! \n",
      "train step 11360 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.9135 diff={max=05.5722, min=00.0276, mean=01.1635} policy_loss=-12.7908 policy updated! \n",
      "train step 11361 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7387 diff={max=08.7893, min=00.0018, mean=01.1447} policy_loss=-10.1371 policy updated! \n",
      "train step 11362 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5852 diff={max=08.0980, min=00.0017, mean=01.1590} policy_loss=-10.8638 policy updated! \n",
      "train step 11363 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9205 diff={max=06.5062, min=00.0310, mean=01.0329} policy_loss=-12.1111 policy updated! \n",
      "train step 11364 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7526 diff={max=12.7300, min=00.0011, mean=01.3220} policy_loss=-11.2622 policy updated! \n",
      "train step 11365 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.9651 diff={max=06.5824, min=00.0130, mean=01.2754} policy_loss=-11.9053 policy updated! \n",
      "train step 11366 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.2811 diff={max=13.5827, min=00.0473, mean=01.3874} policy_loss=-11.7833 policy updated! \n",
      "train step 11367 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.0781 diff={max=10.7087, min=00.0288, mean=01.7335} policy_loss=-10.3752 policy updated! \n",
      "train step 11368 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.6169 diff={max=06.8691, min=00.0762, mean=01.5401} policy_loss=-10.5520 policy updated! \n",
      "train step 11369 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9176 diff={max=11.6155, min=00.1102, mean=01.3301} policy_loss=-11.0012 policy updated! \n",
      "train step 11370 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.6417 diff={max=07.0296, min=00.0023, mean=00.9927} policy_loss=-11.2991 policy updated! \n",
      "train step 11371 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=08.4044 diff={max=13.7559, min=00.0007, mean=01.7356} policy_loss=-10.3103 policy updated! \n",
      "train step 11372 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3760 diff={max=05.6900, min=00.0199, mean=01.2699} policy_loss=-11.8380 policy updated! \n",
      "train step 11373 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5591 diff={max=09.7785, min=00.0114, mean=01.3526} policy_loss=-10.8585 policy updated! \n",
      "train step 11374 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0345 diff={max=06.4356, min=00.0409, mean=01.3743} policy_loss=-11.0876 policy updated! \n",
      "train step 11375 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.7523 diff={max=05.6993, min=00.0033, mean=01.1516} policy_loss=-12.7360 policy updated! \n",
      "train step 11376 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=21.6757 diff={max=18.8215, min=00.0427, mean=02.6739} policy_loss=-14.9842 policy updated! \n",
      "train step 11377 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2035 diff={max=07.9344, min=00.0099, mean=01.1866} policy_loss=-12.0461 policy updated! \n",
      "train step 11378 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.4442 diff={max=12.8255, min=00.0032, mean=01.5626} policy_loss=-12.2915 policy updated! \n",
      "train step 11379 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.1098 diff={max=16.0778, min=00.0136, mean=01.9714} policy_loss=-14.1016 policy updated! \n",
      "train step 11380 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=10.4654 diff={max=15.8528, min=00.0033, mean=01.4641} policy_loss=-9.7443 policy updated! \n",
      "train step 11381 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=12.5342 diff={max=15.0227, min=00.0494, mean=01.9929} policy_loss=-11.7228 policy updated! \n",
      "train step 11382 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3333 diff={max=07.7642, min=00.0030, mean=01.2741} policy_loss=-9.1515 policy updated! \n",
      "train step 11383 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.3943 diff={max=06.7465, min=00.0012, mean=01.5478} policy_loss=-10.8526 policy updated! \n",
      "train step 11384 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0504 diff={max=10.0622, min=00.0029, mean=01.6407} policy_loss=-14.4415 policy updated! \n",
      "train step 11385 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.2667 diff={max=10.3263, min=00.0175, mean=01.8938} policy_loss=-10.5070 policy updated! \n",
      "train step 11386 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1240 diff={max=11.1547, min=00.0203, mean=01.5809} policy_loss=-13.3600 policy updated! \n",
      "train step 11387 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6103 diff={max=08.7794, min=00.0313, mean=01.3817} policy_loss=-11.6206 policy updated! \n",
      "train step 11388 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=15.1352 diff={max=18.8186, min=00.0031, mean=02.2446} policy_loss=-12.0282 policy updated! \n",
      "train step 11389 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=07.4462 diff={max=14.7662, min=00.0274, mean=01.4955} policy_loss=-10.9865 policy updated! \n",
      "train step 11390 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.0723 diff={max=09.6316, min=00.0271, mean=01.6792} policy_loss=-11.9862 policy updated! \n",
      "train step 11391 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0896 diff={max=08.9615, min=00.0045, mean=01.5730} policy_loss=-10.8249 policy updated! \n",
      "train step 11392 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.1643 diff={max=07.5031, min=00.0029, mean=01.3135} policy_loss=-12.7784 policy updated! \n",
      "train step 11393 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4505 diff={max=08.3532, min=00.0355, mean=01.5253} policy_loss=-11.3358 policy updated! \n",
      "train step 11394 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2324 diff={max=09.7380, min=00.0064, mean=01.3729} policy_loss=-9.4834 policy updated! \n",
      "train step 11395 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=10.0578 diff={max=10.5862, min=00.0353, mean=01.8627} policy_loss=-9.5454 policy updated! \n",
      "train step 11396 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3575 diff={max=13.5405, min=00.0032, mean=01.5571} policy_loss=-13.3675 policy updated! \n",
      "train step 11397 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0271 diff={max=06.8094, min=00.0335, mean=01.1855} policy_loss=-13.3772 policy updated! \n",
      "train step 11398 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.9162 diff={max=20.2597, min=00.0254, mean=01.9031} policy_loss=-10.3296 policy updated! \n",
      "train step 11399 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7925 diff={max=07.9345, min=00.0076, mean=01.6725} policy_loss=-12.0262 policy updated! \n",
      "train step 11400 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.1611 diff={max=05.4807, min=00.0260, mean=01.0751} policy_loss=-11.6059 policy updated! \n",
      "train step 11401 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.3621 diff={max=14.2089, min=00.0214, mean=01.7710} policy_loss=-14.7898 policy updated! \n",
      "train step 11402 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5747 diff={max=06.6887, min=00.0022, mean=01.4137} policy_loss=-10.7580 policy updated! \n",
      "train step 11403 reward={max=07.0000, min=06.0000, mean=06.8000} optimizing loss=13.7733 diff={max=16.8333, min=00.0064, mean=01.9662} policy_loss=-12.3551 policy updated! \n",
      "train step 11404 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.4170 diff={max=05.4412, min=00.0332, mean=01.0620} policy_loss=-10.1137 policy updated! \n",
      "train step 11405 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.0400 diff={max=08.9430, min=00.0059, mean=01.3232} policy_loss=-12.1469 policy updated! \n",
      "train step 11406 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4317 diff={max=08.1071, min=00.0428, mean=01.5430} policy_loss=-11.1983 policy updated! \n",
      "train step 11407 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.5258 diff={max=10.9280, min=00.0296, mean=01.7654} policy_loss=-11.3218 policy updated! \n",
      "train step 11408 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8547 diff={max=05.5677, min=00.0039, mean=01.0959} policy_loss=-9.5571 policy updated! \n",
      "train step 11409 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.3287 diff={max=12.0513, min=00.0102, mean=01.9839} policy_loss=-14.0493 policy updated! \n",
      "train step 11410 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.7719 diff={max=07.3550, min=00.0426, mean=01.3560} policy_loss=-11.2831 policy updated! \n",
      "train step 11411 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.8538 diff={max=06.1176, min=00.0257, mean=01.1389} policy_loss=-13.4202 policy updated! \n",
      "train step 11412 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9840 diff={max=09.0605, min=00.0568, mean=01.4051} policy_loss=-13.8842 policy updated! \n",
      "train step 11413 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=08.0866 diff={max=11.0688, min=00.0133, mean=01.7343} policy_loss=-12.6945 policy updated! \n",
      "train step 11414 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2940 diff={max=11.5181, min=00.0402, mean=01.3695} policy_loss=-11.9582 policy updated! \n",
      "train step 11415 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.3520 diff={max=06.5784, min=00.0025, mean=01.2498} policy_loss=-14.7193 policy updated! \n",
      "train step 11416 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.4767 diff={max=15.1381, min=00.0085, mean=01.5642} policy_loss=-11.6442 policy updated! \n",
      "train step 11417 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1268 diff={max=06.5797, min=00.0609, mean=01.5037} policy_loss=-11.5076 policy updated! \n",
      "train step 11418 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.1166 diff={max=14.6297, min=00.0139, mean=01.7013} policy_loss=-14.6385 policy updated! \n",
      "train step 11419 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.0637 diff={max=09.4974, min=00.0340, mean=01.5288} policy_loss=-13.0499 policy updated! \n",
      "train step 11420 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=12.2427 diff={max=16.4260, min=00.0172, mean=02.0895} policy_loss=-13.9412 policy updated! \n",
      "train step 11421 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6971 diff={max=07.3124, min=00.0139, mean=01.2802} policy_loss=-10.9397 policy updated! \n",
      "train step 11422 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1315 diff={max=09.7596, min=00.0120, mean=01.5394} policy_loss=-13.3219 policy updated! \n",
      "train step 11423 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9639 diff={max=07.5500, min=00.0449, mean=01.4273} policy_loss=-12.8465 policy updated! \n",
      "train step 11424 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9907 diff={max=09.0084, min=00.0261, mean=01.2551} policy_loss=-12.3163 policy updated! \n",
      "train step 11425 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.9686 diff={max=03.5799, min=00.0080, mean=01.0613} policy_loss=-11.1925 policy updated! \n",
      "train step 11426 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.7535 diff={max=10.8913, min=00.1397, mean=01.5732} policy_loss=-13.2317 policy updated! \n",
      "train step 11427 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.6303 diff={max=09.3438, min=00.0120, mean=01.8546} policy_loss=-12.3706 policy updated! \n",
      "train step 11428 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.9337 diff={max=10.6068, min=00.0026, mean=01.6050} policy_loss=-12.9468 policy updated! \n",
      "train step 11429 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7629 diff={max=07.0862, min=00.0196, mean=01.0530} policy_loss=-13.3777 policy updated! \n",
      "train step 11430 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.7647 diff={max=15.3188, min=00.0430, mean=01.5456} policy_loss=-10.5040 policy updated! \n",
      "train step 11431 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.2383 diff={max=12.7395, min=00.0281, mean=01.7364} policy_loss=-12.8354 policy updated! \n",
      "train step 11432 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.7812 diff={max=16.1157, min=00.0594, mean=01.5882} policy_loss=-12.4729 policy updated! \n",
      "train step 11433 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.8997 diff={max=07.9777, min=00.0226, mean=01.2692} policy_loss=-11.1244 policy updated! \n",
      "train step 11434 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1787 diff={max=09.5744, min=00.0257, mean=01.2296} policy_loss=-9.7063 policy updated! \n",
      "train step 11435 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.2881 diff={max=09.1826, min=00.0205, mean=01.2830} policy_loss=-11.3626 policy updated! \n",
      "train step 11436 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3299 diff={max=06.6099, min=00.0136, mean=01.0877} policy_loss=-11.7282 policy updated! \n",
      "train step 11437 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8877 diff={max=10.0093, min=00.0022, mean=01.4629} policy_loss=-9.6865 policy updated! \n",
      "train step 11438 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.5099 diff={max=11.3575, min=00.0272, mean=01.8724} policy_loss=-12.0713 policy updated! \n",
      "train step 11439 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.9947 diff={max=10.0956, min=00.0018, mean=01.5331} policy_loss=-10.1624 policy updated! \n",
      "train step 11440 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.6931 diff={max=14.6718, min=00.0064, mean=01.3350} policy_loss=-13.7628 policy updated! \n",
      "train step 11441 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0929 diff={max=14.5614, min=00.0660, mean=01.4028} policy_loss=-13.5825 policy updated! \n",
      "train step 11442 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6196 diff={max=07.6688, min=00.0043, mean=01.7360} policy_loss=-12.3027 policy updated! \n",
      "train step 11443 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=07.6655 diff={max=08.8989, min=00.0537, mean=01.9057} policy_loss=-14.5565 policy updated! \n",
      "train step 11444 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.2750 diff={max=11.1461, min=00.0029, mean=01.3309} policy_loss=-11.4136 policy updated! \n",
      "train step 11445 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.7890 diff={max=13.6890, min=00.0096, mean=01.8234} policy_loss=-13.3921 policy updated! \n",
      "train step 11446 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.8532 diff={max=12.1338, min=00.0399, mean=01.7656} policy_loss=-12.6256 policy updated! \n",
      "train step 11447 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4188 diff={max=07.8100, min=00.0007, mean=01.2232} policy_loss=-11.5404 policy updated! \n",
      "train step 11448 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8543 diff={max=07.7282, min=00.0004, mean=01.3022} policy_loss=-13.2686 policy updated! \n",
      "train step 11449 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7584 diff={max=07.1815, min=00.0280, mean=01.3060} policy_loss=-11.0944 policy updated! \n",
      "train step 11450 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.7574 diff={max=06.5504, min=00.0148, mean=01.1093} policy_loss=-13.5512 policy updated! \n",
      "train step 11451 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6529 diff={max=06.3289, min=00.0037, mean=01.3224} policy_loss=-10.8038 policy updated! \n",
      "train step 11452 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.8919 diff={max=14.8950, min=00.0000, mean=01.5479} policy_loss=-12.0064 policy updated! \n",
      "train step 11453 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=06.8261 diff={max=10.3178, min=00.0015, mean=01.4172} policy_loss=-14.3959 policy updated! \n",
      "train step 11454 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9954 diff={max=05.5832, min=00.0022, mean=01.1725} policy_loss=-11.2296 policy updated! \n",
      "train step 11455 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.6341 diff={max=15.1830, min=00.0197, mean=01.7446} policy_loss=-12.8386 policy updated! \n",
      "train step 11456 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1937 diff={max=08.5382, min=00.0053, mean=01.3560} policy_loss=-13.8691 policy updated! \n",
      "train step 11457 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0246 diff={max=09.1321, min=00.0067, mean=01.4602} policy_loss=-10.4352 policy updated! \n",
      "train step 11458 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=08.8706 diff={max=13.1200, min=00.0470, mean=01.7662} policy_loss=-15.3857 policy updated! \n",
      "train step 11459 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.7411 diff={max=10.8802, min=00.0371, mean=01.5620} policy_loss=-13.0332 policy updated! \n",
      "train step 11460 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.3961 diff={max=10.1928, min=00.0068, mean=01.3373} policy_loss=-12.7957 policy updated! \n",
      "train step 11461 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1203 diff={max=07.7616, min=00.0210, mean=01.4359} policy_loss=-12.0661 policy updated! \n",
      "train step 11462 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1710 diff={max=08.6269, min=00.0052, mean=01.6670} policy_loss=-15.7136 policy updated! \n",
      "train step 11463 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.7862 diff={max=08.2582, min=00.0115, mean=01.2440} policy_loss=-12.5295 policy updated! \n",
      "train step 11464 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2280 diff={max=07.5008, min=00.0014, mean=01.2813} policy_loss=-12.0230 policy updated! \n",
      "train step 11465 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.0774 diff={max=10.1488, min=00.0119, mean=01.2036} policy_loss=-12.6427 policy updated! \n",
      "train step 11466 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.6149 diff={max=06.6940, min=00.0115, mean=01.2599} policy_loss=-10.9100 policy updated! \n",
      "train step 11467 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5499 diff={max=09.4416, min=00.0040, mean=01.3400} policy_loss=-11.3545 policy updated! \n",
      "train step 11468 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0032 diff={max=08.1170, min=00.0383, mean=01.3836} policy_loss=-12.5724 policy updated! \n",
      "train step 11469 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8476 diff={max=11.4490, min=00.0691, mean=01.4336} policy_loss=-13.9719 policy updated! \n",
      "train step 11470 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.5156 diff={max=10.3284, min=00.0399, mean=01.5581} policy_loss=-15.5683 policy updated! \n",
      "train step 11471 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.1555 diff={max=17.3307, min=00.0354, mean=01.5459} policy_loss=-13.6427 policy updated! \n",
      "train step 11472 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4661 diff={max=05.9852, min=00.0025, mean=01.1069} policy_loss=-12.5818 policy updated! \n",
      "train step 11473 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6246 diff={max=06.6981, min=00.0306, mean=01.5257} policy_loss=-14.2800 policy updated! \n",
      "train step 11474 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3788 diff={max=08.3476, min=00.0134, mean=01.5331} policy_loss=-12.0979 policy updated! \n",
      "train step 11475 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.6588 diff={max=08.9702, min=00.0139, mean=01.5447} policy_loss=-13.4668 policy updated! \n",
      "train step 11476 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9545 diff={max=10.6725, min=00.0158, mean=01.4267} policy_loss=-12.7595 policy updated! \n",
      "train step 11477 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.6410 diff={max=11.9832, min=00.0388, mean=01.4198} policy_loss=-13.3443 policy updated! \n",
      "train step 11478 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7271 diff={max=05.1093, min=00.0056, mean=01.0764} policy_loss=-13.7388 policy updated! \n",
      "train step 11479 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1222 diff={max=06.9012, min=00.0112, mean=01.6574} policy_loss=-9.3187 policy updated! \n",
      "train step 11480 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=10.6901 diff={max=14.8632, min=00.0099, mean=01.8413} policy_loss=-13.6451 policy updated! \n",
      "train step 11481 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.8944 diff={max=06.1300, min=00.0047, mean=01.3148} policy_loss=-12.2821 policy updated! \n",
      "train step 11482 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=14.6697 diff={max=14.1864, min=00.0025, mean=02.6372} policy_loss=-14.0883 policy updated! \n",
      "train step 11483 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.6494 diff={max=11.1006, min=00.0701, mean=01.9239} policy_loss=-11.9927 policy updated! \n",
      "train step 11484 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0785 diff={max=05.9265, min=00.0362, mean=01.1377} policy_loss=-9.8172 policy updated! \n",
      "train step 11485 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.5020 diff={max=10.6096, min=00.0357, mean=01.5568} policy_loss=-11.0351 policy updated! \n",
      "train step 11486 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0671 diff={max=09.6833, min=00.0007, mean=01.2992} policy_loss=-9.2004 policy updated! \n",
      "train step 11487 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4386 diff={max=07.5466, min=00.0178, mean=01.1418} policy_loss=-10.7687 policy updated! \n",
      "train step 11488 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9435 diff={max=09.8751, min=00.0563, mean=01.5076} policy_loss=-10.6071 policy updated! \n",
      "train step 11489 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.8406 diff={max=05.7484, min=00.0331, mean=01.3008} policy_loss=-15.3566 policy updated! \n",
      "train step 11490 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.2814 diff={max=03.7402, min=00.0024, mean=01.1358} policy_loss=-13.4532 policy updated! \n",
      "train step 11491 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2590 diff={max=06.4199, min=00.0159, mean=01.3620} policy_loss=-11.4617 policy updated! \n",
      "train step 11492 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.5405 diff={max=14.9123, min=00.0060, mean=01.5050} policy_loss=-11.4478 policy updated! \n",
      "train step 11493 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5449 diff={max=06.8528, min=00.0026, mean=01.1879} policy_loss=-11.9480 policy updated! \n",
      "train step 11494 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.8553 diff={max=06.4122, min=00.0075, mean=01.2550} policy_loss=-14.1479 policy updated! \n",
      "train step 11495 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.7175 diff={max=07.7154, min=00.0351, mean=01.2282} policy_loss=-9.1866 policy updated! \n",
      "train step 11496 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.4220 diff={max=04.1303, min=00.0197, mean=01.2134} policy_loss=-11.6133 policy updated! \n",
      "train step 11497 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0963 diff={max=05.5932, min=00.0297, mean=01.2661} policy_loss=-14.2311 policy updated! \n",
      "train step 11498 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6665 diff={max=04.8353, min=00.0342, mean=01.1085} policy_loss=-11.1835 policy updated! \n",
      "train step 11499 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=08.3804 diff={max=07.7458, min=00.0051, mean=01.7438} policy_loss=-11.6865 policy updated! \n",
      "train step 11500 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.9558 diff={max=04.7617, min=00.0098, mean=01.0196} policy_loss=-10.7721 policy updated! \n",
      "train step 11501 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9440 diff={max=07.3748, min=00.0379, mean=01.2871} policy_loss=-12.2934 policy updated! \n",
      "train step 11502 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4043 diff={max=07.1490, min=00.0077, mean=01.0422} policy_loss=-8.7150 policy updated! \n",
      "train step 11503 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6917 diff={max=06.9645, min=00.0002, mean=01.3039} policy_loss=-11.1624 policy updated! \n",
      "train step 11504 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1384 diff={max=06.6351, min=00.0427, mean=01.3161} policy_loss=-11.6312 policy updated! \n",
      "train step 11505 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.5228 diff={max=06.6310, min=00.0300, mean=01.2116} policy_loss=-12.4969 policy updated! \n",
      "train step 11506 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.6247 diff={max=04.5460, min=00.0154, mean=00.9256} policy_loss=-12.2063 policy updated! \n",
      "train step 11507 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.4799 diff={max=14.2211, min=00.0069, mean=01.8074} policy_loss=-12.9815 policy updated! \n",
      "train step 11508 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=00.9507 diff={max=02.4856, min=00.0112, mean=00.7846} policy_loss=-10.2888 policy updated! \n",
      "train step 11509 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0066 diff={max=06.4709, min=00.0073, mean=01.0687} policy_loss=-10.7189 policy updated! \n",
      "train step 11510 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.9522 diff={max=08.0744, min=00.0038, mean=01.6245} policy_loss=-13.9770 policy updated! \n",
      "train step 11511 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.5926 diff={max=06.1585, min=00.0111, mean=01.0293} policy_loss=-11.2897 policy updated! \n",
      "train step 11512 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0549 diff={max=07.4098, min=00.0042, mean=01.3571} policy_loss=-11.8410 policy updated! \n",
      "train step 11513 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=11.2529 diff={max=11.9796, min=00.0076, mean=02.0250} policy_loss=-13.0323 policy updated! \n",
      "train step 11514 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.5820 diff={max=09.1671, min=00.0086, mean=01.7608} policy_loss=-9.8959 policy updated! \n",
      "train step 11515 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.9694 diff={max=07.7284, min=00.0080, mean=01.4473} policy_loss=-12.3066 policy updated! \n",
      "train step 11516 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.6325 diff={max=09.3441, min=00.0134, mean=01.4325} policy_loss=-12.2255 policy updated! \n",
      "train step 11517 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4519 diff={max=07.1052, min=00.0016, mean=01.1942} policy_loss=-11.9890 policy updated! \n",
      "train step 11518 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=21.1849 diff={max=20.3513, min=00.0417, mean=02.2710} policy_loss=-14.0646 policy updated! \n",
      "train step 11519 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.3882 diff={max=22.0719, min=00.0102, mean=01.5089} policy_loss=-15.3041 policy updated! \n",
      "train step 11520 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.9239 diff={max=08.2648, min=00.0187, mean=01.5646} policy_loss=-15.7097 policy updated! \n",
      "train step 11521 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.7358 diff={max=13.1727, min=00.0162, mean=01.6799} policy_loss=-11.8978 policy updated! \n",
      "train step 11522 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1935 diff={max=04.9754, min=00.0262, mean=00.9316} policy_loss=-12.7423 policy updated! \n",
      "train step 11523 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.8299 diff={max=11.0393, min=00.0096, mean=01.7710} policy_loss=-14.0718 policy updated! \n",
      "train step 11524 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.6371 diff={max=10.3207, min=00.0414, mean=01.6130} policy_loss=-12.9475 policy updated! \n",
      "train step 11525 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.3921 diff={max=05.6971, min=00.0238, mean=01.2170} policy_loss=-10.9313 policy updated! \n",
      "train step 11526 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6862 diff={max=07.8392, min=00.0394, mean=01.3389} policy_loss=-12.5222 policy updated! \n",
      "train step 11527 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.4105 diff={max=10.2898, min=00.0090, mean=01.8875} policy_loss=-12.2680 policy updated! \n",
      "train step 11528 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.8659 diff={max=14.2802, min=00.0068, mean=02.0778} policy_loss=-12.7146 policy updated! \n",
      "train step 11529 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3119 diff={max=07.3733, min=00.0240, mean=01.7120} policy_loss=-12.4467 policy updated! \n",
      "train step 11530 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=04.0848 diff={max=07.1989, min=00.0282, mean=01.3475} policy_loss=-13.4741 policy updated! \n",
      "train step 11531 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8080 diff={max=09.7572, min=00.0641, mean=01.5722} policy_loss=-11.4077 policy updated! \n",
      "train step 11532 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.4330 diff={max=10.9486, min=00.0114, mean=01.6817} policy_loss=-15.5981 policy updated! \n",
      "train step 11533 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.6811 diff={max=09.0223, min=00.0210, mean=01.3020} policy_loss=-9.8360 policy updated! \n",
      "train step 11534 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.9539 diff={max=05.7899, min=00.0155, mean=01.1774} policy_loss=-11.9239 policy updated! \n",
      "train step 11535 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.3967 diff={max=08.9862, min=00.0080, mean=01.8111} policy_loss=-13.3721 policy updated! \n",
      "train step 11536 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3304 diff={max=11.2670, min=00.0000, mean=01.3634} policy_loss=-12.7062 policy updated! \n",
      "train step 11537 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.1716 diff={max=11.9568, min=00.0114, mean=01.7735} policy_loss=-13.0526 policy updated! \n",
      "train step 11538 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.7854 diff={max=08.5182, min=00.0058, mean=01.4816} policy_loss=-13.0113 policy updated! \n",
      "train step 11539 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9407 diff={max=10.7056, min=00.0107, mean=01.4017} policy_loss=-12.1368 policy updated! \n",
      "train step 11540 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.1045 diff={max=09.2450, min=00.0057, mean=01.5353} policy_loss=-13.2845 policy updated! \n",
      "train step 11541 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.8784 diff={max=12.5786, min=00.0008, mean=01.4931} policy_loss=-11.7121 policy updated! \n",
      "train step 11542 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0288 diff={max=10.6133, min=00.0087, mean=01.5218} policy_loss=-12.3590 policy updated! \n",
      "train step 11543 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.5350 diff={max=11.0664, min=00.0084, mean=02.1885} policy_loss=-12.7834 policy updated! \n",
      "train step 11544 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=09.3706 diff={max=11.5713, min=00.0548, mean=01.7688} policy_loss=-10.0055 policy updated! \n",
      "train step 11545 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.3783 diff={max=13.5036, min=00.0214, mean=01.9074} policy_loss=-14.5094 policy updated! \n",
      "train step 11546 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=08.1943 diff={max=14.0365, min=00.0308, mean=01.5048} policy_loss=-13.1453 policy updated! \n",
      "train step 11547 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4295 diff={max=05.1153, min=00.0199, mean=01.3705} policy_loss=-13.9576 policy updated! \n",
      "train step 11548 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=08.0776 diff={max=16.4589, min=00.0012, mean=01.3968} policy_loss=-13.3296 policy updated! \n",
      "train step 11549 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4378 diff={max=09.3816, min=00.0022, mean=01.2958} policy_loss=-12.9153 policy updated! \n",
      "train step 11550 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.6232 diff={max=04.6806, min=00.0123, mean=01.0538} policy_loss=-12.4488 policy updated! \n",
      "train step 11551 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7904 diff={max=06.5806, min=00.0232, mean=01.5392} policy_loss=-15.7291 policy updated! \n",
      "train step 11552 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=04.5886 diff={max=09.0564, min=00.0055, mean=01.3222} policy_loss=-12.7405 policy updated! \n",
      "train step 11553 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5706 diff={max=04.4894, min=00.0056, mean=01.1158} policy_loss=-14.0328 policy updated! \n",
      "train step 11554 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0657 diff={max=07.0247, min=00.0434, mean=01.4018} policy_loss=-12.8074 policy updated! \n",
      "train step 11555 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.6562 diff={max=08.8119, min=00.0246, mean=01.6062} policy_loss=-13.1784 policy updated! \n",
      "train step 11556 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.7833 diff={max=08.2782, min=00.0604, mean=01.7648} policy_loss=-15.0543 policy updated! \n",
      "train step 11557 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1151 diff={max=06.3819, min=00.0199, mean=01.2421} policy_loss=-14.0623 policy updated! \n",
      "train step 11558 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6518 diff={max=07.8089, min=00.0015, mean=00.8888} policy_loss=-12.1366 policy updated! \n",
      "train step 11559 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6998 diff={max=08.8322, min=00.0316, mean=01.5487} policy_loss=-13.7718 policy updated! \n",
      "train step 11560 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.6458 diff={max=08.6506, min=00.0272, mean=01.4291} policy_loss=-11.5348 policy updated! \n",
      "train step 11561 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.4331 diff={max=08.8378, min=00.0432, mean=01.4230} policy_loss=-13.0147 policy updated! \n",
      "train step 11562 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.1374 diff={max=08.2927, min=00.1381, mean=01.9676} policy_loss=-13.1717 policy updated! \n",
      "train step 11563 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2460 diff={max=09.7196, min=00.0012, mean=01.4757} policy_loss=-15.7935 policy updated! \n",
      "train step 11564 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.0980 diff={max=16.9018, min=00.0032, mean=01.7833} policy_loss=-13.2403 policy updated! \n",
      "train step 11565 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.2245 diff={max=08.4980, min=00.0069, mean=01.6889} policy_loss=-11.9483 policy updated! \n",
      "train step 11566 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.7634 diff={max=06.5330, min=00.0119, mean=01.2854} policy_loss=-16.0323 policy updated! \n",
      "train step 11567 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=15.4249 diff={max=20.6017, min=00.0164, mean=01.6637} policy_loss=-11.1249 policy updated! \n",
      "train step 11568 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.0911 diff={max=06.9951, min=00.0024, mean=01.1550} policy_loss=-12.8131 policy updated! \n",
      "train step 11569 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.0408 diff={max=08.5328, min=00.0215, mean=01.1431} policy_loss=-11.7683 policy updated! \n",
      "train step 11570 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.1730 diff={max=04.4069, min=00.0298, mean=01.0177} policy_loss=-12.1777 policy updated! \n",
      "train step 11571 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=15.5961 diff={max=19.7530, min=00.0232, mean=02.0024} policy_loss=-10.9583 policy updated! \n",
      "train step 11572 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2512 diff={max=06.1245, min=00.0240, mean=01.2151} policy_loss=-11.3414 policy updated! \n",
      "train step 11573 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.7555 diff={max=09.6835, min=00.0018, mean=01.0407} policy_loss=-11.3878 policy updated! \n",
      "train step 11574 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6525 diff={max=12.1123, min=00.0026, mean=01.4127} policy_loss=-12.0649 policy updated! \n",
      "train step 11575 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.5763 diff={max=08.8357, min=00.0360, mean=01.6098} policy_loss=-13.3732 policy updated! \n",
      "train step 11576 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9498 diff={max=04.5047, min=00.0048, mean=00.9230} policy_loss=-10.0860 policy updated! \n",
      "train step 11577 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2097 diff={max=09.3536, min=00.0025, mean=01.3327} policy_loss=-10.7579 policy updated! \n",
      "train step 11578 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2069 diff={max=07.1794, min=00.0596, mean=01.2544} policy_loss=-11.5966 policy updated! \n",
      "train step 11579 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8231 diff={max=05.9557, min=00.0081, mean=01.1733} policy_loss=-13.1379 policy updated! \n",
      "train step 11580 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.3884 diff={max=07.8469, min=00.0232, mean=01.5317} policy_loss=-14.1618 policy updated! \n",
      "train step 11581 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2126 diff={max=07.1798, min=00.0105, mean=01.6545} policy_loss=-14.6143 policy updated! \n",
      "train step 11582 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4845 diff={max=11.4112, min=00.0336, mean=01.4349} policy_loss=-11.5613 policy updated! \n",
      "train step 11583 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=09.7453 diff={max=16.1847, min=00.0010, mean=01.6582} policy_loss=-12.2590 policy updated! \n",
      "train step 11584 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8732 diff={max=11.7025, min=00.0545, mean=01.5749} policy_loss=-13.6636 policy updated! \n",
      "train step 11585 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.4627 diff={max=07.4602, min=00.0007, mean=01.4166} policy_loss=-11.5144 policy updated! \n",
      "train step 11586 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=09.5872 diff={max=15.3454, min=00.0127, mean=01.5721} policy_loss=-13.0171 policy updated! \n",
      "train step 11587 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1609 diff={max=09.0113, min=00.0841, mean=01.6609} policy_loss=-10.7396 policy updated! \n",
      "train step 11588 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.4797 diff={max=13.7714, min=00.0227, mean=02.1334} policy_loss=-11.7065 policy updated! \n",
      "train step 11589 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0626 diff={max=10.1970, min=00.0433, mean=01.5782} policy_loss=-12.0734 policy updated! \n",
      "train step 11590 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.6673 diff={max=09.4745, min=00.0482, mean=01.0771} policy_loss=-10.3622 policy updated! \n",
      "train step 11591 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.6905 diff={max=11.6466, min=00.0474, mean=01.6924} policy_loss=-11.4431 policy updated! \n",
      "train step 11592 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.5253 diff={max=09.5325, min=00.0358, mean=02.0604} policy_loss=-11.8839 policy updated! \n",
      "train step 11593 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.1155 diff={max=13.1845, min=00.0103, mean=02.0685} policy_loss=-14.8515 policy updated! \n",
      "train step 11594 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=10.4923 diff={max=15.8308, min=00.0325, mean=01.7033} policy_loss=-10.8472 policy updated! \n",
      "train step 11595 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.7148 diff={max=07.5744, min=00.0245, mean=01.5180} policy_loss=-14.2437 policy updated! \n",
      "train step 11596 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.4897 diff={max=11.0325, min=00.0259, mean=02.2547} policy_loss=-11.3259 policy updated! \n",
      "train step 11597 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4904 diff={max=07.8898, min=00.0362, mean=01.3444} policy_loss=-14.0838 policy updated! \n",
      "train step 11598 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9748 diff={max=05.5050, min=00.0200, mean=01.0109} policy_loss=-10.5709 policy updated! \n",
      "train step 11599 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6387 diff={max=06.9084, min=00.0098, mean=01.4996} policy_loss=-11.9797 policy updated! \n",
      "train step 11600 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.3161 diff={max=07.4511, min=00.0213, mean=01.3308} policy_loss=-10.4242 policy updated! \n",
      "train step 11601 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.5035 diff={max=17.1887, min=00.0023, mean=01.7625} policy_loss=-14.9725 policy updated! \n",
      "train step 11602 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1911 diff={max=07.4764, min=00.0262, mean=01.3604} policy_loss=-10.5101 policy updated! \n",
      "train step 11603 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6513 diff={max=05.2147, min=00.0074, mean=01.0653} policy_loss=-13.2603 policy updated! \n",
      "train step 11604 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.7330 diff={max=11.6323, min=00.0111, mean=01.3169} policy_loss=-10.3186 policy updated! \n",
      "train step 11605 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.7991 diff={max=07.3861, min=00.0000, mean=01.0778} policy_loss=-12.8556 policy updated! \n",
      "train step 11606 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=17.1447 diff={max=16.8201, min=00.0295, mean=02.4299} policy_loss=-12.2495 policy updated! \n",
      "train step 11607 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0199 diff={max=06.9509, min=00.0016, mean=01.2583} policy_loss=-11.6680 policy updated! \n",
      "train step 11608 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5849 diff={max=08.6612, min=00.0147, mean=01.2226} policy_loss=-14.1633 policy updated! \n",
      "train step 11609 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9744 diff={max=06.2840, min=00.0428, mean=01.3629} policy_loss=-14.0998 policy updated! \n",
      "train step 11610 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.3290 diff={max=07.6778, min=00.0406, mean=01.3715} policy_loss=-13.1856 policy updated! \n",
      "train step 11611 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8034 diff={max=05.6631, min=00.0537, mean=01.6432} policy_loss=-15.7714 policy updated! \n",
      "train step 11612 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3850 diff={max=06.4946, min=00.0255, mean=01.2497} policy_loss=-13.0801 policy updated! \n",
      "train step 11613 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1075 diff={max=08.0262, min=00.0230, mean=01.5501} policy_loss=-14.6348 policy updated! \n",
      "train step 11614 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8902 diff={max=05.3670, min=00.0057, mean=01.4533} policy_loss=-12.7674 policy updated! \n",
      "train step 11615 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.3904 diff={max=10.6822, min=00.0078, mean=01.7126} policy_loss=-13.8535 policy updated! \n",
      "train step 11616 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.6813 diff={max=09.9378, min=00.0081, mean=01.3213} policy_loss=-10.1678 policy updated! \n",
      "train step 11617 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.4322 diff={max=17.4635, min=00.0056, mean=01.3763} policy_loss=-12.0866 policy updated! \n",
      "train step 11618 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0091 diff={max=08.1328, min=00.0056, mean=01.2270} policy_loss=-10.2404 policy updated! \n",
      "train step 11619 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0673 diff={max=05.5724, min=00.0526, mean=01.2989} policy_loss=-10.3598 policy updated! \n",
      "train step 11620 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.0321 diff={max=06.6273, min=00.0267, mean=01.4782} policy_loss=-12.4778 policy updated! \n",
      "train step 11621 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.9218 diff={max=06.9447, min=00.0233, mean=01.6803} policy_loss=-13.4228 policy updated! \n",
      "train step 11622 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5947 diff={max=06.4018, min=00.0367, mean=01.4089} policy_loss=-12.7636 policy updated! \n",
      "train step 11623 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.8683 diff={max=14.4064, min=00.0036, mean=01.6791} policy_loss=-14.4384 policy updated! \n",
      "train step 11624 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7052 diff={max=08.7102, min=00.0050, mean=01.5660} policy_loss=-13.2248 policy updated! \n",
      "train step 11625 reward={max=07.0000, min=00.0000, mean=02.6000} optimizing loss=02.9000 diff={max=05.8158, min=00.0339, mean=01.1541} policy_loss=-11.8681 policy updated! \n",
      "train step 11626 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4386 diff={max=06.9756, min=00.0093, mean=01.1123} policy_loss=-14.4945 policy updated! \n",
      "train step 11627 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0190 diff={max=07.6719, min=00.0181, mean=01.3899} policy_loss=-11.4279 policy updated! \n",
      "train step 11628 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=15.7107 diff={max=19.7091, min=00.0646, mean=02.1097} policy_loss=-12.5322 policy updated! \n",
      "train step 11629 reward={max=07.0000, min=00.0000, mean=05.4000} optimizing loss=03.9533 diff={max=08.0000, min=00.0016, mean=01.1869} policy_loss=-11.0143 policy updated! \n",
      "train step 11630 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.2454 diff={max=08.7372, min=00.0050, mean=01.7150} policy_loss=-13.4204 policy updated! \n",
      "train step 11631 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.2463 diff={max=09.9316, min=00.0010, mean=01.5498} policy_loss=-12.9309 policy updated! \n",
      "train step 11632 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=04.5389 diff={max=07.8845, min=00.0344, mean=01.2798} policy_loss=-13.1443 policy updated! \n",
      "train step 11633 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1240 diff={max=08.2202, min=00.0327, mean=01.3905} policy_loss=-14.5798 policy updated! \n",
      "train step 11634 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9538 diff={max=07.5858, min=00.0271, mean=01.5334} policy_loss=-12.5794 policy updated! \n",
      "train step 11635 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.3254 diff={max=12.3270, min=00.0190, mean=01.0306} policy_loss=-14.0239 policy updated! \n",
      "train step 11636 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1339 diff={max=10.7213, min=00.0091, mean=01.1419} policy_loss=-13.0211 policy updated! \n",
      "train step 11637 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9881 diff={max=05.1359, min=00.0214, mean=01.2500} policy_loss=-14.4176 policy updated! \n",
      "train step 11638 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.3384 diff={max=15.0080, min=00.0148, mean=01.9742} policy_loss=-13.2097 policy updated! \n",
      "train step 11639 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6764 diff={max=05.3397, min=00.0435, mean=01.1522} policy_loss=-13.3722 policy updated! \n",
      "train step 11640 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.6167 diff={max=08.8137, min=00.0029, mean=01.5806} policy_loss=-11.7976 policy updated! \n",
      "train step 11641 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.2899 diff={max=15.5074, min=00.0021, mean=01.5229} policy_loss=-14.0981 policy updated! \n",
      "train step 11642 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5753 diff={max=09.1278, min=00.0073, mean=01.3108} policy_loss=-12.4350 policy updated! \n",
      "train step 11643 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=21.7948 diff={max=22.5649, min=00.0337, mean=02.2826} policy_loss=-15.3976 policy updated! \n",
      "train step 11644 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5254 diff={max=06.9252, min=00.0028, mean=01.1489} policy_loss=-12.2993 policy updated! \n",
      "train step 11645 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.2894 diff={max=08.9255, min=00.0016, mean=01.2905} policy_loss=-11.8057 policy updated! \n",
      "train step 11646 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=07.3324 diff={max=13.3940, min=00.0227, mean=01.6627} policy_loss=-13.2247 policy updated! \n",
      "train step 11647 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.5971 diff={max=09.8712, min=00.0092, mean=01.6852} policy_loss=-13.3623 policy updated! \n",
      "train step 11648 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.2604 diff={max=10.1498, min=00.0211, mean=02.0798} policy_loss=-11.9050 policy updated! \n",
      "train step 11649 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.2848 diff={max=07.9476, min=00.0744, mean=01.0908} policy_loss=-13.0782 policy updated! \n",
      "train step 11650 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.3161 diff={max=08.0263, min=00.0081, mean=01.2832} policy_loss=-11.8275 policy updated! \n",
      "train step 11651 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=10.3953 diff={max=15.5060, min=00.0168, mean=01.7323} policy_loss=-12.2218 policy updated! \n",
      "train step 11652 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3874 diff={max=08.8772, min=00.0128, mean=01.4553} policy_loss=-14.6706 policy updated! \n",
      "train step 11653 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0848 diff={max=09.7315, min=00.0316, mean=01.5545} policy_loss=-12.0101 policy updated! \n",
      "train step 11654 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1634 diff={max=12.1902, min=00.0212, mean=01.5386} policy_loss=-13.3719 policy updated! \n",
      "train step 11655 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.2740 diff={max=13.4990, min=00.0348, mean=01.6726} policy_loss=-14.1949 policy updated! \n",
      "train step 11656 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.6738 diff={max=15.4468, min=00.0231, mean=01.5998} policy_loss=-12.6190 policy updated! \n",
      "train step 11657 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8125 diff={max=11.9416, min=00.0048, mean=01.4526} policy_loss=-12.2865 policy updated! \n",
      "train step 11658 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.9811 diff={max=11.8544, min=00.0040, mean=01.7042} policy_loss=-11.8807 policy updated! \n",
      "train step 11659 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.8871 diff={max=15.4538, min=00.0262, mean=01.8793} policy_loss=-13.1882 policy updated! \n",
      "train step 11660 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.9853 diff={max=08.6223, min=00.0159, mean=01.2244} policy_loss=-13.4169 policy updated! \n",
      "train step 11661 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=09.5527 diff={max=15.1654, min=00.0304, mean=01.6745} policy_loss=-11.9121 policy updated! \n",
      "train step 11662 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3346 diff={max=07.8159, min=00.0038, mean=01.5985} policy_loss=-11.4882 policy updated! \n",
      "train step 11663 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=06.8640 diff={max=08.9464, min=00.0036, mean=01.5251} policy_loss=-9.7665 policy updated! \n",
      "train step 11664 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.1157 diff={max=04.9081, min=00.0125, mean=01.6383} policy_loss=-12.5992 policy updated! \n",
      "train step 11665 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=06.3655 diff={max=09.2780, min=00.0206, mean=01.6719} policy_loss=-9.6789 policy updated! \n",
      "train step 11666 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6319 diff={max=08.6475, min=00.0152, mean=01.4210} policy_loss=-10.6502 policy updated! \n",
      "train step 11667 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7292 diff={max=05.3479, min=00.0295, mean=01.1606} policy_loss=-11.2790 policy updated! \n",
      "train step 11668 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.3607 diff={max=09.2936, min=00.0152, mean=01.9271} policy_loss=-12.5483 policy updated! \n",
      "train step 11669 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.0966 diff={max=17.3545, min=00.0466, mean=01.5317} policy_loss=-14.7556 policy updated! \n",
      "train step 11670 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.2455 diff={max=10.2353, min=00.0048, mean=01.2136} policy_loss=-13.0120 policy updated! \n",
      "train step 11671 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.4001 diff={max=13.9004, min=00.0976, mean=02.3449} policy_loss=-15.8622 policy updated! \n",
      "train step 11672 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.1260 diff={max=11.3012, min=00.0089, mean=02.1005} policy_loss=-13.2502 policy updated! \n",
      "train step 11673 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=07.1179 diff={max=11.8415, min=00.0104, mean=01.5228} policy_loss=-12.2436 policy updated! \n",
      "train step 11674 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8141 diff={max=08.6658, min=00.0262, mean=01.2818} policy_loss=-13.7749 policy updated! \n",
      "train step 11675 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.2378 diff={max=10.7752, min=00.0327, mean=01.4947} policy_loss=-11.3707 policy updated! \n",
      "train step 11676 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9511 diff={max=07.4743, min=00.0236, mean=01.5926} policy_loss=-12.5553 policy updated! \n",
      "train step 11677 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4974 diff={max=09.0925, min=00.0105, mean=01.4634} policy_loss=-11.9002 policy updated! \n",
      "train step 11678 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=11.5606 diff={max=15.9513, min=00.0898, mean=01.9299} policy_loss=-14.9586 policy updated! \n",
      "train step 11679 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.4373 diff={max=13.0288, min=00.0072, mean=01.7648} policy_loss=-11.9883 policy updated! \n",
      "train step 11680 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.8834 diff={max=07.3868, min=00.0199, mean=01.1708} policy_loss=-12.6968 policy updated! \n",
      "train step 11681 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=10.5227 diff={max=10.5982, min=00.0138, mean=02.0815} policy_loss=-10.8929 policy updated! \n",
      "train step 11682 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.2353 diff={max=13.5923, min=00.0076, mean=01.4070} policy_loss=-11.5814 policy updated! \n",
      "train step 11683 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0907 diff={max=08.1461, min=00.0003, mean=01.1752} policy_loss=-13.1038 policy updated! \n",
      "train step 11684 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.0284 diff={max=16.7621, min=00.0156, mean=02.1662} policy_loss=-13.8181 policy updated! \n",
      "train step 11685 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.8640 diff={max=05.7983, min=00.0765, mean=01.2324} policy_loss=-11.5850 policy updated! \n",
      "train step 11686 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7126 diff={max=06.9086, min=00.0437, mean=01.0923} policy_loss=-13.5853 policy updated! \n",
      "train step 11687 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0908 diff={max=07.1367, min=00.0313, mean=01.4998} policy_loss=-13.7189 policy updated! \n",
      "train step 11688 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.4556 diff={max=08.0120, min=00.0062, mean=01.8515} policy_loss=-11.9163 policy updated! \n",
      "train step 11689 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9056 diff={max=14.2407, min=00.0171, mean=01.4854} policy_loss=-12.5457 policy updated! \n",
      "train step 11690 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.4496 diff={max=07.1243, min=00.0558, mean=01.5613} policy_loss=-10.7406 policy updated! \n",
      "train step 11691 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.0506 diff={max=05.5774, min=00.0138, mean=01.1505} policy_loss=-12.2595 policy updated! \n",
      "train step 11692 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.5462 diff={max=07.5236, min=00.0092, mean=02.0948} policy_loss=-13.8004 policy updated! \n",
      "train step 11693 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.2026 diff={max=09.4813, min=00.0227, mean=01.4035} policy_loss=-12.4358 policy updated! \n",
      "train step 11694 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.1503 diff={max=09.8974, min=00.0264, mean=01.4242} policy_loss=-11.2968 policy updated! \n",
      "train step 11695 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.8359 diff={max=05.0570, min=00.0037, mean=01.1262} policy_loss=-11.0249 policy updated! \n",
      "train step 11696 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1748 diff={max=05.2951, min=00.0212, mean=00.9988} policy_loss=-10.8981 policy updated! \n",
      "train step 11697 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3276 diff={max=10.9227, min=00.0177, mean=01.5504} policy_loss=-14.5081 policy updated! \n",
      "train step 11698 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7622 diff={max=06.9970, min=00.0273, mean=01.5330} policy_loss=-13.0264 policy updated! \n",
      "train step 11699 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1694 diff={max=05.8527, min=00.0015, mean=01.1842} policy_loss=-12.6902 policy updated! \n",
      "train step 11700 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.5450 diff={max=08.1174, min=00.0003, mean=01.1849} policy_loss=-9.9873 policy updated! \n",
      "train step 11701 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.2895 diff={max=10.0317, min=00.0160, mean=02.0689} policy_loss=-14.2626 policy updated! \n",
      "train step 11702 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6230 diff={max=11.4310, min=00.0426, mean=01.4193} policy_loss=-12.5528 policy updated! \n",
      "train step 11703 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.7840 diff={max=08.2520, min=00.0251, mean=01.6642} policy_loss=-13.6318 policy updated! \n",
      "train step 11704 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6353 diff={max=04.8617, min=00.0114, mean=01.1713} policy_loss=-12.3421 policy updated! \n",
      "train step 11705 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.7344 diff={max=09.0278, min=00.0727, mean=01.7155} policy_loss=-13.8377 policy updated! \n",
      "train step 11706 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.1911 diff={max=14.9286, min=00.0035, mean=01.5756} policy_loss=-10.9537 policy updated! \n",
      "train step 11707 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0400 diff={max=08.9660, min=00.0006, mean=01.3334} policy_loss=-14.3462 policy updated! \n",
      "train step 11708 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=07.5698 diff={max=11.7899, min=00.0650, mean=01.5900} policy_loss=-11.5069 policy updated! \n",
      "train step 11709 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=14.5464 diff={max=22.7838, min=00.0083, mean=01.7276} policy_loss=-11.6136 policy updated! \n",
      "train step 11710 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.6892 diff={max=11.3129, min=00.0153, mean=01.5331} policy_loss=-10.7804 policy updated! \n",
      "train step 11711 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6996 diff={max=07.2413, min=00.0010, mean=01.2258} policy_loss=-12.4312 policy updated! \n",
      "train step 11712 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4913 diff={max=07.5484, min=00.0022, mean=01.3137} policy_loss=-15.0260 policy updated! \n",
      "train step 11713 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.4945 diff={max=07.3262, min=00.0098, mean=01.5659} policy_loss=-13.7396 policy updated! \n",
      "train step 11714 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2320 diff={max=05.8533, min=00.0049, mean=01.4718} policy_loss=-11.1941 policy updated! \n",
      "train step 11715 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.0733 diff={max=09.0071, min=00.0216, mean=01.5876} policy_loss=-13.3095 policy updated! \n",
      "train step 11716 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8074 diff={max=05.4872, min=00.0060, mean=01.1255} policy_loss=-13.6396 policy updated! \n",
      "train step 11717 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=16.5322 diff={max=24.5480, min=00.0722, mean=01.6285} policy_loss=-13.7929 policy updated! \n",
      "train step 11718 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=08.1407 diff={max=16.4764, min=00.0053, mean=01.3165} policy_loss=-11.0466 policy updated! \n",
      "train step 11719 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.7314 diff={max=13.2457, min=00.0310, mean=01.6327} policy_loss=-14.5040 policy updated! \n",
      "train step 11720 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.2292 diff={max=08.4147, min=00.0129, mean=01.0762} policy_loss=-10.0734 policy updated! \n",
      "train step 11721 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=07.0679 diff={max=10.9207, min=00.0546, mean=01.4532} policy_loss=-13.5959 policy updated! \n",
      "train step 11722 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3735 diff={max=08.4298, min=00.0024, mean=01.8591} policy_loss=-14.7480 policy updated! \n",
      "train step 11723 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0330 diff={max=07.2742, min=00.0217, mean=01.8391} policy_loss=-13.1587 policy updated! \n",
      "train step 11724 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.8976 diff={max=05.8172, min=00.0035, mean=01.1291} policy_loss=-11.6986 policy updated! \n",
      "train step 11725 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.3073 diff={max=09.4191, min=00.0096, mean=01.5355} policy_loss=-14.8104 policy updated! \n",
      "train step 11726 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=09.6413 diff={max=12.9352, min=00.0452, mean=01.9460} policy_loss=-16.7382 policy updated! \n",
      "train step 11727 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1457 diff={max=04.8883, min=00.0005, mean=01.1584} policy_loss=-11.9815 policy updated! \n",
      "train step 11728 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.9500 diff={max=08.1619, min=00.0238, mean=01.5181} policy_loss=-12.6348 policy updated! \n",
      "train step 11729 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=09.5444 diff={max=14.3335, min=00.0476, mean=01.8560} policy_loss=-14.6887 policy updated! \n",
      "train step 11730 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=10.3527 diff={max=14.4101, min=00.0121, mean=01.8606} policy_loss=-12.1444 policy updated! \n",
      "train step 11731 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.3833 diff={max=10.8099, min=00.0054, mean=01.5299} policy_loss=-14.2354 policy updated! \n",
      "train step 11732 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.8977 diff={max=12.7019, min=00.0080, mean=02.1156} policy_loss=-14.7986 policy updated! \n",
      "train step 11733 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9896 diff={max=09.3661, min=00.0058, mean=01.4994} policy_loss=-12.5801 policy updated! \n",
      "train step 11734 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.5635 diff={max=10.8458, min=00.0004, mean=01.8715} policy_loss=-12.9794 policy updated! \n",
      "train step 11735 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.9958 diff={max=05.7454, min=00.0042, mean=01.7757} policy_loss=-12.4867 policy updated! \n",
      "train step 11736 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9754 diff={max=07.3771, min=00.0155, mean=01.6730} policy_loss=-13.2859 policy updated! \n",
      "train step 11737 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=14.7266 diff={max=18.3318, min=00.0240, mean=02.0613} policy_loss=-13.6660 policy updated! \n",
      "train step 11738 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.7718 diff={max=15.6234, min=00.0251, mean=01.5583} policy_loss=-13.2900 policy updated! \n",
      "train step 11739 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.6104 diff={max=08.4092, min=00.0089, mean=01.7665} policy_loss=-12.4062 policy updated! \n",
      "train step 11740 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.4538 diff={max=11.6987, min=00.0198, mean=01.6145} policy_loss=-14.7218 policy updated! \n",
      "train step 11741 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0684 diff={max=06.9033, min=00.0428, mean=01.3211} policy_loss=-12.0245 policy updated! \n",
      "train step 11742 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3169 diff={max=07.7449, min=00.0327, mean=01.4724} policy_loss=-11.1078 policy updated! \n",
      "train step 11743 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.3590 diff={max=15.0421, min=00.0081, mean=01.8461} policy_loss=-11.8377 policy updated! \n",
      "train step 11744 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.1745 diff={max=09.2279, min=00.0282, mean=01.4007} policy_loss=-13.1251 policy updated! \n",
      "train step 11745 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=12.6456 diff={max=20.5242, min=00.0056, mean=01.7882} policy_loss=-16.2337 policy updated! \n",
      "train step 11746 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4380 diff={max=07.1109, min=00.0153, mean=01.2631} policy_loss=-10.4067 policy updated! \n",
      "train step 11747 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0334 diff={max=07.7852, min=00.0060, mean=01.6640} policy_loss=-11.6658 policy updated! \n",
      "train step 11748 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.9343 diff={max=11.6190, min=00.1003, mean=01.8494} policy_loss=-10.5643 policy updated! \n",
      "train step 11749 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.8005 diff={max=06.8432, min=00.0125, mean=01.5229} policy_loss=-14.2894 policy updated! \n",
      "train step 11750 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.9634 diff={max=07.3485, min=00.0315, mean=01.6894} policy_loss=-14.7671 policy updated! \n",
      "train step 11751 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6583 diff={max=04.4429, min=00.0168, mean=01.1636} policy_loss=-14.9707 policy updated! \n",
      "train step 11752 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9025 diff={max=13.0619, min=00.0071, mean=01.3210} policy_loss=-13.0903 policy updated! \n",
      "train step 11753 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4260 diff={max=07.6493, min=00.0232, mean=01.4575} policy_loss=-14.4040 policy updated! \n",
      "train step 11754 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8226 diff={max=05.2344, min=00.0267, mean=01.2793} policy_loss=-12.3073 policy updated! \n",
      "train step 11755 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=10.4922 diff={max=14.5639, min=00.1187, mean=02.0252} policy_loss=-15.0643 policy updated! \n",
      "train step 11756 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3190 diff={max=10.7590, min=00.0258, mean=01.5342} policy_loss=-13.4930 policy updated! \n",
      "train step 11757 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.1971 diff={max=11.5562, min=00.0253, mean=01.7809} policy_loss=-15.8364 policy updated! \n",
      "train step 11758 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.2100 diff={max=14.3754, min=00.0101, mean=01.7735} policy_loss=-14.8206 policy updated! \n",
      "train step 11759 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=09.5349 diff={max=11.4086, min=00.0059, mean=01.7652} policy_loss=-11.3896 policy updated! \n",
      "train step 11760 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.4707 diff={max=13.9802, min=00.0146, mean=01.5871} policy_loss=-11.7441 policy updated! \n",
      "train step 11761 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=14.6716 diff={max=16.8245, min=00.0769, mean=02.0513} policy_loss=-13.4274 policy updated! \n",
      "train step 11762 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6253 diff={max=07.1475, min=00.0753, mean=01.2227} policy_loss=-8.8981 policy updated! \n",
      "train step 11763 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.9160 diff={max=13.4750, min=00.0137, mean=01.5548} policy_loss=-8.5750 policy updated! \n",
      "train step 11764 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.1787 diff={max=09.4219, min=00.0341, mean=01.4242} policy_loss=-9.9541 policy updated! \n",
      "train step 11765 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.7486 diff={max=07.5041, min=00.0065, mean=01.7305} policy_loss=-12.4013 policy updated! \n",
      "train step 11766 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1046 diff={max=07.5472, min=00.0211, mean=01.6632} policy_loss=-12.9166 policy updated! \n",
      "train step 11767 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=04.4819 diff={max=10.7008, min=00.0085, mean=01.3090} policy_loss=-12.0285 policy updated! \n",
      "train step 11768 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7856 diff={max=06.0605, min=00.0283, mean=01.3796} policy_loss=-13.5801 policy updated! \n",
      "train step 11769 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.2276 diff={max=08.7205, min=00.0190, mean=01.3004} policy_loss=-12.2755 policy updated! \n",
      "train step 11770 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.9807 diff={max=08.7155, min=00.0463, mean=01.5982} policy_loss=-14.1268 policy updated! \n",
      "train step 11771 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9371 diff={max=13.1952, min=00.0102, mean=01.5007} policy_loss=-14.3721 policy updated! \n",
      "train step 11772 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7133 diff={max=09.2252, min=00.0508, mean=01.6221} policy_loss=-11.2123 policy updated! \n",
      "train step 11773 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8445 diff={max=06.1232, min=00.0276, mean=01.3191} policy_loss=-12.1194 policy updated! \n",
      "train step 11774 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9880 diff={max=08.8310, min=00.0061, mean=01.2646} policy_loss=-12.6396 policy updated! \n",
      "train step 11775 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.3026 diff={max=09.5338, min=00.0128, mean=01.3016} policy_loss=-11.1130 policy updated! \n",
      "train step 11776 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4432 diff={max=08.5934, min=00.0377, mean=01.0659} policy_loss=-11.5492 policy updated! \n",
      "train step 11777 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.3859 diff={max=09.9056, min=00.0027, mean=01.7976} policy_loss=-14.9306 policy updated! \n",
      "train step 11778 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.8674 diff={max=12.6362, min=00.0016, mean=01.2619} policy_loss=-12.5059 policy updated! \n",
      "train step 11779 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7163 diff={max=08.1410, min=00.0177, mean=01.3314} policy_loss=-12.3774 policy updated! \n",
      "train step 11780 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.6391 diff={max=07.9141, min=00.0082, mean=01.3574} policy_loss=-10.6966 policy updated! \n",
      "train step 11781 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3765 diff={max=09.4228, min=00.0080, mean=01.5140} policy_loss=-13.3863 policy updated! \n",
      "train step 11782 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.3639 diff={max=13.1206, min=00.0260, mean=01.7095} policy_loss=-10.5208 policy updated! \n",
      "train step 11783 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4640 diff={max=09.2819, min=00.0137, mean=01.4568} policy_loss=-13.8066 policy updated! \n",
      "train step 11784 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.6961 diff={max=08.8083, min=00.1052, mean=01.5318} policy_loss=-11.1755 policy updated! \n",
      "train step 11785 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.6669 diff={max=11.5307, min=00.0404, mean=01.4110} policy_loss=-10.9531 policy updated! \n",
      "train step 11786 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9466 diff={max=09.0539, min=00.0005, mean=01.2877} policy_loss=-15.4449 policy updated! \n",
      "train step 11787 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.0470 diff={max=08.9759, min=00.0027, mean=01.8116} policy_loss=-13.7560 policy updated! \n",
      "train step 11788 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9400 diff={max=08.2832, min=00.0114, mean=01.2845} policy_loss=-12.1064 policy updated! \n",
      "train step 11789 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0504 diff={max=08.6430, min=00.0237, mean=01.2636} policy_loss=-11.8198 policy updated! \n",
      "train step 11790 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=10.3885 diff={max=16.0858, min=00.0278, mean=01.7001} policy_loss=-12.9279 policy updated! \n",
      "train step 11791 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1595 diff={max=07.7257, min=00.0624, mean=01.1066} policy_loss=-11.8236 policy updated! \n",
      "train step 11792 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=18.3802 diff={max=19.2033, min=00.0194, mean=02.3573} policy_loss=-14.6120 policy updated! \n",
      "train step 11793 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.9809 diff={max=08.4514, min=00.0333, mean=01.4902} policy_loss=-12.6315 policy updated! \n",
      "train step 11794 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.3188 diff={max=18.2984, min=00.0295, mean=01.5362} policy_loss=-10.9754 policy updated! \n",
      "train step 11795 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.2477 diff={max=07.7277, min=00.0336, mean=01.7118} policy_loss=-13.3811 policy updated! \n",
      "train step 11796 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.2704 diff={max=07.8902, min=00.0111, mean=01.4297} policy_loss=-13.4524 policy updated! \n",
      "train step 11797 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8279 diff={max=09.9170, min=00.0079, mean=01.4871} policy_loss=-13.0005 policy updated! \n",
      "train step 11798 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=13.3262 diff={max=19.8391, min=00.0026, mean=01.9081} policy_loss=-12.8605 policy updated! \n",
      "train step 11799 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=08.0295 diff={max=07.5338, min=00.0289, mean=01.8781} policy_loss=-10.5620 policy updated! \n",
      "train step 11800 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=18.9045 diff={max=14.0858, min=00.0063, mean=02.7605} policy_loss=-10.8126 policy updated! \n",
      "train step 11801 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4385 diff={max=07.4418, min=00.0365, mean=01.3711} policy_loss=-13.9964 policy updated! \n",
      "train step 11802 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=17.2225 diff={max=17.6605, min=00.0267, mean=02.3281} policy_loss=-12.2394 policy updated! \n",
      "train step 11803 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=09.9341 diff={max=10.9594, min=00.0239, mean=01.9121} policy_loss=-14.3421 policy updated! \n",
      "train step 11804 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.8670 diff={max=09.6328, min=00.0018, mean=01.6924} policy_loss=-13.7475 policy updated! \n",
      "train step 11805 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.0651 diff={max=09.3855, min=00.0064, mean=01.9382} policy_loss=-11.9196 policy updated! \n",
      "train step 11806 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.4963 diff={max=14.9681, min=00.0871, mean=01.7305} policy_loss=-11.7744 policy updated! \n",
      "train step 11807 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.5850 diff={max=10.3190, min=00.0197, mean=01.8153} policy_loss=-15.6126 policy updated! \n",
      "train step 11808 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.3921 diff={max=14.9337, min=00.0217, mean=01.6106} policy_loss=-11.2510 policy updated! \n",
      "train step 11809 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1386 diff={max=06.8893, min=00.0686, mean=01.2587} policy_loss=-14.0385 policy updated! \n",
      "train step 11810 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.1145 diff={max=06.0279, min=00.0014, mean=01.1645} policy_loss=-10.9764 policy updated! \n",
      "train step 11811 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4531 diff={max=07.1709, min=00.0761, mean=01.3206} policy_loss=-12.3640 policy updated! \n",
      "train step 11812 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=18.6139 diff={max=22.4530, min=00.0123, mean=01.9768} policy_loss=-13.5849 policy updated! \n",
      "train step 11813 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=07.2801 diff={max=12.6287, min=00.0114, mean=01.6590} policy_loss=-15.2164 policy updated! \n",
      "train step 11814 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=15.0033 diff={max=17.1823, min=00.0149, mean=02.1658} policy_loss=-12.8706 policy updated! \n",
      "train step 11815 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.5541 diff={max=10.3868, min=00.0470, mean=01.3768} policy_loss=-15.2130 policy updated! \n",
      "train step 11816 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9290 diff={max=06.7986, min=00.0227, mean=01.0142} policy_loss=-11.8370 policy updated! \n",
      "train step 11817 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.2311 diff={max=12.2719, min=00.0014, mean=01.7018} policy_loss=-13.0097 policy updated! \n",
      "train step 11818 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8611 diff={max=12.1876, min=00.0031, mean=01.4762} policy_loss=-12.6851 policy updated! \n",
      "train step 11819 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=07.1731 diff={max=09.2421, min=00.0121, mean=01.7367} policy_loss=-12.2944 policy updated! \n",
      "train step 11820 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.5483 diff={max=08.9276, min=00.0056, mean=01.4956} policy_loss=-11.2695 policy updated! \n",
      "train step 11821 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=10.4820 diff={max=14.3426, min=00.0147, mean=01.7295} policy_loss=-13.5854 policy updated! \n",
      "train step 11822 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6410 diff={max=09.4300, min=00.0038, mean=01.0022} policy_loss=-9.4780 policy updated! \n",
      "train step 11823 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7071 diff={max=08.3684, min=00.0258, mean=01.7297} policy_loss=-12.9133 policy updated! \n",
      "train step 11824 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4155 diff={max=11.2486, min=00.0268, mean=01.1679} policy_loss=-11.9671 policy updated! \n",
      "train step 11825 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.2064 diff={max=08.2447, min=00.0001, mean=01.5436} policy_loss=-14.2108 policy updated! \n",
      "train step 11826 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.5865 diff={max=07.1054, min=00.0017, mean=01.2960} policy_loss=-13.9251 policy updated! \n",
      "train step 11827 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.8477 diff={max=10.9359, min=00.0087, mean=01.5715} policy_loss=-12.4460 policy updated! \n",
      "train step 11828 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1891 diff={max=12.1307, min=00.0132, mean=01.5574} policy_loss=-11.6803 policy updated! \n",
      "train step 11829 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5890 diff={max=08.8427, min=00.0046, mean=01.4320} policy_loss=-14.2031 policy updated! \n",
      "train step 11830 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.1993 diff={max=08.6129, min=00.0145, mean=01.3170} policy_loss=-12.3294 policy updated! \n",
      "train step 11831 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1985 diff={max=09.7197, min=00.0024, mean=01.6166} policy_loss=-12.7921 policy updated! \n",
      "train step 11832 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5559 diff={max=08.6555, min=00.0567, mean=01.2151} policy_loss=-12.3044 policy updated! \n",
      "train step 11833 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8522 diff={max=08.2917, min=00.0092, mean=01.2538} policy_loss=-12.1185 policy updated! \n",
      "train step 11834 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.1606 diff={max=07.2707, min=00.0116, mean=01.5822} policy_loss=-15.4562 policy updated! \n",
      "train step 11835 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.1926 diff={max=09.5875, min=00.0586, mean=01.4582} policy_loss=-11.3796 policy updated! \n",
      "train step 11836 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=12.0733 diff={max=14.3090, min=00.0201, mean=02.1966} policy_loss=-14.8440 policy updated! \n",
      "train step 11837 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8091 diff={max=07.6584, min=00.0128, mean=01.3476} policy_loss=-14.8421 policy updated! \n",
      "train step 11838 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0914 diff={max=07.4071, min=00.0193, mean=01.1554} policy_loss=-14.6775 policy updated! \n",
      "train step 11839 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.7904 diff={max=14.2373, min=00.0237, mean=01.7545} policy_loss=-10.8689 policy updated! \n",
      "train step 11840 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.0701 diff={max=09.6322, min=00.0002, mean=01.1111} policy_loss=-12.6366 policy updated! \n",
      "train step 11841 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1177 diff={max=07.5955, min=00.0095, mean=01.3776} policy_loss=-12.9783 policy updated! \n",
      "train step 11842 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.0191 diff={max=16.6841, min=00.0012, mean=02.1735} policy_loss=-12.4375 policy updated! \n",
      "train step 11843 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3962 diff={max=08.2056, min=00.0052, mean=01.1211} policy_loss=-14.7349 policy updated! \n",
      "train step 11844 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.8372 diff={max=06.5238, min=00.0030, mean=01.3724} policy_loss=-15.4361 policy updated! \n",
      "train step 11845 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.3220 diff={max=06.6226, min=00.0044, mean=01.4469} policy_loss=-13.9319 policy updated! \n",
      "train step 11846 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.5212 diff={max=10.4499, min=00.0089, mean=01.2532} policy_loss=-10.6743 policy updated! \n",
      "train step 11847 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.3614 diff={max=13.4224, min=00.0235, mean=01.8186} policy_loss=-13.7040 policy updated! \n",
      "train step 11848 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0194 diff={max=08.6068, min=00.0486, mean=01.2825} policy_loss=-11.5665 policy updated! \n",
      "train step 11849 reward={max=07.0000, min=00.0000, mean=05.4000} optimizing loss=04.8934 diff={max=08.1099, min=00.0110, mean=01.3579} policy_loss=-11.7029 policy updated! \n",
      "train step 11850 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.5273 diff={max=12.1397, min=00.0528, mean=01.5927} policy_loss=-11.2854 policy updated! \n",
      "train step 11851 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=17.5416 diff={max=18.9600, min=00.0761, mean=02.3237} policy_loss=-13.0429 policy updated! \n",
      "train step 11852 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4663 diff={max=06.0894, min=00.0193, mean=01.1515} policy_loss=-11.9448 policy updated! \n",
      "train step 11853 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1831 diff={max=11.3881, min=00.0225, mean=01.4974} policy_loss=-11.1909 policy updated! \n",
      "train step 11854 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.1138 diff={max=06.2581, min=00.0013, mean=01.3151} policy_loss=-10.0599 policy updated! \n",
      "train step 11855 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.7554 diff={max=09.0632, min=00.0106, mean=01.4645} policy_loss=-11.4013 policy updated! \n",
      "train step 11856 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.5017 diff={max=10.1213, min=00.0037, mean=01.8690} policy_loss=-12.6369 policy updated! \n",
      "train step 11857 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.4254 diff={max=12.1403, min=00.0120, mean=01.9447} policy_loss=-15.3089 policy updated! \n",
      "train step 11858 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=08.2382 diff={max=11.4782, min=00.0184, mean=01.7242} policy_loss=-13.1205 policy updated! \n",
      "train step 11859 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.5378 diff={max=10.6844, min=00.0093, mean=01.3817} policy_loss=-13.7048 policy updated! \n",
      "train step 11860 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.4497 diff={max=05.6212, min=00.0055, mean=01.2949} policy_loss=-12.9665 policy updated! \n",
      "train step 11861 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.7182 diff={max=10.4580, min=00.0346, mean=01.7747} policy_loss=-10.9826 policy updated! \n",
      "train step 11862 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2882 diff={max=09.8174, min=00.0173, mean=01.5943} policy_loss=-12.6664 policy updated! \n",
      "train step 11863 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.3434 diff={max=06.5633, min=00.0459, mean=01.3119} policy_loss=-9.7578 policy updated! \n",
      "train step 11864 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.4283 diff={max=07.4902, min=00.0167, mean=01.3670} policy_loss=-11.6400 policy updated! \n",
      "train step 11865 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.1521 diff={max=11.6266, min=00.0044, mean=01.7304} policy_loss=-13.4570 policy updated! \n",
      "train step 11866 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.0831 diff={max=16.2358, min=00.0246, mean=02.0038} policy_loss=-13.7836 policy updated! \n",
      "train step 11867 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.0910 diff={max=13.0590, min=00.0338, mean=01.9516} policy_loss=-14.4947 policy updated! \n",
      "train step 11868 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7745 diff={max=07.6123, min=00.0387, mean=01.3078} policy_loss=-13.1468 policy updated! \n",
      "train step 11869 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5532 diff={max=10.2286, min=00.0030, mean=01.5367} policy_loss=-12.6719 policy updated! \n",
      "train step 11870 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.1127 diff={max=06.1430, min=00.0161, mean=01.3685} policy_loss=-15.5004 policy updated! \n",
      "train step 11871 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.8673 diff={max=06.0880, min=00.0533, mean=01.3732} policy_loss=-13.9854 policy updated! \n",
      "train step 11872 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=05.7056 diff={max=06.5517, min=00.0084, mean=01.5988} policy_loss=-13.9208 policy updated! \n",
      "train step 11873 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.4598 diff={max=11.6607, min=00.0274, mean=01.5984} policy_loss=-11.7437 policy updated! \n",
      "train step 11874 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.8538 diff={max=13.9744, min=00.0049, mean=01.6236} policy_loss=-12.1885 policy updated! \n",
      "train step 11875 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.1029 diff={max=12.7481, min=00.0140, mean=01.3910} policy_loss=-11.6035 policy updated! \n",
      "train step 11876 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9321 diff={max=07.7801, min=00.0245, mean=01.2189} policy_loss=-12.7001 policy updated! \n",
      "train step 11877 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.8924 diff={max=09.3478, min=00.1212, mean=01.7167} policy_loss=-13.2125 policy updated! \n",
      "train step 11878 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.7641 diff={max=09.4313, min=00.0371, mean=01.8220} policy_loss=-12.4742 policy updated! \n",
      "train step 11879 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.1412 diff={max=08.2624, min=00.0147, mean=01.5420} policy_loss=-13.1161 policy updated! \n",
      "train step 11880 reward={max=07.0000, min=00.0000, mean=02.6000} optimizing loss=02.8711 diff={max=07.0842, min=00.0179, mean=01.0428} policy_loss=-14.8043 policy updated! \n",
      "train step 11881 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3198 diff={max=06.5357, min=00.0060, mean=01.2136} policy_loss=-12.1185 policy updated! \n",
      "train step 11882 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2107 diff={max=10.9065, min=00.0231, mean=01.1614} policy_loss=-11.1099 policy updated! \n",
      "train step 11883 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1602 diff={max=13.8927, min=00.0566, mean=01.4294} policy_loss=-11.5428 policy updated! \n",
      "train step 11884 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.4678 diff={max=09.9989, min=00.0164, mean=01.8019} policy_loss=-12.0339 policy updated! \n",
      "train step 11885 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.6805 diff={max=07.5050, min=00.0503, mean=01.4745} policy_loss=-12.3275 policy updated! \n",
      "train step 11886 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5590 diff={max=08.5678, min=00.0351, mean=01.5419} policy_loss=-15.2738 policy updated! \n",
      "train step 11887 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.7039 diff={max=08.8285, min=00.0431, mean=01.9200} policy_loss=-14.0695 policy updated! \n",
      "train step 11888 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9813 diff={max=08.4993, min=00.0211, mean=01.0655} policy_loss=-13.2559 policy updated! \n",
      "train step 11889 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8873 diff={max=05.7913, min=00.0090, mean=01.3693} policy_loss=-10.9263 policy updated! \n",
      "train step 11890 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=10.8562 diff={max=11.9330, min=00.0216, mean=01.7887} policy_loss=-13.2425 policy updated! \n",
      "train step 11891 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.2294 diff={max=06.4331, min=00.0968, mean=01.2615} policy_loss=-15.3023 policy updated! \n",
      "train step 11892 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4439 diff={max=08.6410, min=00.0007, mean=01.3266} policy_loss=-10.1999 policy updated! \n",
      "train step 11893 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5482 diff={max=09.2964, min=00.0100, mean=01.6739} policy_loss=-11.6097 policy updated! \n",
      "train step 11894 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4286 diff={max=05.8099, min=00.0585, mean=01.3963} policy_loss=-12.6533 policy updated! \n",
      "train step 11895 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.7576 diff={max=14.7518, min=00.0282, mean=01.6367} policy_loss=-12.1631 policy updated! \n",
      "train step 11896 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0516 diff={max=07.0688, min=00.0100, mean=01.5649} policy_loss=-12.4067 policy updated! \n",
      "train step 11897 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7759 diff={max=09.4155, min=00.0387, mean=01.4935} policy_loss=-13.8539 policy updated! \n",
      "train step 11898 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0220 diff={max=05.8781, min=00.0256, mean=00.8731} policy_loss=-9.8277 policy updated! \n",
      "train step 11899 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.6804 diff={max=07.2602, min=00.0009, mean=01.4773} policy_loss=-12.0279 policy updated! \n",
      "train step 11900 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.7442 diff={max=05.0158, min=00.0020, mean=01.1383} policy_loss=-12.4492 policy updated! \n",
      "train step 11901 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3351 diff={max=07.8992, min=00.0005, mean=01.2111} policy_loss=-15.3495 policy updated! \n",
      "train step 11902 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=15.4581 diff={max=19.1911, min=00.0334, mean=02.1519} policy_loss=-12.8198 policy updated! \n",
      "train step 11903 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8393 diff={max=09.6038, min=00.0593, mean=01.6120} policy_loss=-12.9031 policy updated! \n",
      "train step 11904 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.4008 diff={max=06.4711, min=00.0052, mean=01.2903} policy_loss=-13.6195 policy updated! \n",
      "train step 11905 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.0488 diff={max=05.6380, min=00.0046, mean=01.1286} policy_loss=-13.5538 policy updated! \n",
      "train step 11906 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.3079 diff={max=09.3589, min=00.0503, mean=01.4007} policy_loss=-14.1778 policy updated! \n",
      "train step 11907 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8898 diff={max=10.1529, min=00.0085, mean=01.3885} policy_loss=-13.3636 policy updated! \n",
      "train step 11908 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.2029 diff={max=17.1622, min=00.0028, mean=01.6267} policy_loss=-10.2208 policy updated! \n",
      "train step 11909 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=07.6605 diff={max=08.1347, min=00.0087, mean=01.5950} policy_loss=-14.0043 policy updated! \n",
      "train step 11910 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.4031 diff={max=09.2961, min=00.0014, mean=02.0307} policy_loss=-14.6224 policy updated! \n",
      "train step 11911 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=12.1276 diff={max=13.0643, min=00.0396, mean=02.1171} policy_loss=-14.8422 policy updated! \n",
      "train step 11912 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2713 diff={max=08.9331, min=00.0591, mean=01.4645} policy_loss=-14.3401 policy updated! \n",
      "train step 11913 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.2895 diff={max=08.4935, min=00.0155, mean=01.9947} policy_loss=-12.9519 policy updated! \n",
      "train step 11914 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0801 diff={max=08.6354, min=00.0119, mean=01.5804} policy_loss=-14.6962 policy updated! \n",
      "train step 11915 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.6747 diff={max=09.8685, min=00.0100, mean=01.4056} policy_loss=-13.7452 policy updated! \n",
      "train step 11916 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.1435 diff={max=14.0238, min=00.0035, mean=01.5077} policy_loss=-11.6360 policy updated! \n",
      "train step 11917 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.8592 diff={max=08.5814, min=00.0181, mean=01.9566} policy_loss=-16.0013 policy updated! \n",
      "train step 11918 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1304 diff={max=10.0956, min=00.0070, mean=01.5726} policy_loss=-10.5178 policy updated! \n",
      "train step 11919 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.3167 diff={max=13.1824, min=00.0271, mean=01.4036} policy_loss=-15.9400 policy updated! \n",
      "train step 11920 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.9766 diff={max=08.9314, min=00.0015, mean=01.9024} policy_loss=-13.8052 policy updated! \n",
      "train step 11921 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0095 diff={max=08.3422, min=00.0047, mean=01.3592} policy_loss=-11.4823 policy updated! \n",
      "train step 11922 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.7232 diff={max=21.2983, min=00.0634, mean=01.2976} policy_loss=-12.7353 policy updated! \n",
      "train step 11923 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.8851 diff={max=09.8373, min=00.0019, mean=01.1638} policy_loss=-13.9379 policy updated! \n",
      "train step 11924 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.6741 diff={max=07.5281, min=00.0328, mean=01.4325} policy_loss=-12.1493 policy updated! \n",
      "train step 11925 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.8356 diff={max=08.0128, min=00.0264, mean=01.2973} policy_loss=-11.3990 policy updated! \n",
      "train step 11926 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.6866 diff={max=11.4327, min=00.0149, mean=01.7381} policy_loss=-12.4799 policy updated! \n",
      "train step 11927 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2136 diff={max=11.2573, min=00.0125, mean=01.3698} policy_loss=-12.5972 policy updated! \n",
      "train step 11928 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=06.8702 diff={max=10.4647, min=00.0385, mean=01.8191} policy_loss=-13.9509 policy updated! \n",
      "train step 11929 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.7470 diff={max=05.3201, min=00.0261, mean=01.4744} policy_loss=-11.4139 policy updated! \n",
      "train step 11930 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.9243 diff={max=06.8941, min=00.0042, mean=01.1109} policy_loss=-11.7315 policy updated! \n",
      "train step 11931 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.8646 diff={max=10.6054, min=00.0432, mean=01.6394} policy_loss=-14.6149 policy updated! \n",
      "train step 11932 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7447 diff={max=06.4951, min=00.0313, mean=01.0809} policy_loss=-11.5929 policy updated! \n",
      "train step 11933 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=11.3428 diff={max=12.5160, min=00.0067, mean=02.0426} policy_loss=-15.6228 policy updated! \n",
      "train step 11934 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.1829 diff={max=08.2479, min=00.0081, mean=01.3441} policy_loss=-13.1725 policy updated! \n",
      "train step 11935 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.2848 diff={max=12.0502, min=00.0152, mean=01.7048} policy_loss=-10.5194 policy updated! \n",
      "train step 11936 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=07.4486 diff={max=12.0276, min=00.0093, mean=01.6361} policy_loss=-11.0259 policy updated! \n",
      "train step 11937 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1780 diff={max=08.5727, min=00.0387, mean=01.6674} policy_loss=-13.7284 policy updated! \n",
      "train step 11938 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1349 diff={max=05.8696, min=00.0016, mean=01.2023} policy_loss=-11.7298 policy updated! \n",
      "train step 11939 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2427 diff={max=06.3501, min=00.0115, mean=01.1328} policy_loss=-13.0429 policy updated! \n",
      "train step 11940 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.2424 diff={max=13.8063, min=00.0090, mean=01.3934} policy_loss=-15.0006 policy updated! \n",
      "train step 11941 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4600 diff={max=08.6011, min=00.0439, mean=01.2793} policy_loss=-10.8966 policy updated! \n",
      "train step 11942 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6252 diff={max=07.3101, min=00.0196, mean=01.2785} policy_loss=-12.0012 policy updated! \n",
      "train step 11943 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=13.0768 diff={max=12.1067, min=00.1157, mean=02.0609} policy_loss=-15.1294 policy updated! \n",
      "train step 11944 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6863 diff={max=06.5880, min=00.0247, mean=01.2958} policy_loss=-13.8990 policy updated! \n",
      "train step 11945 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.9020 diff={max=12.8410, min=00.0202, mean=01.2401} policy_loss=-11.6799 policy updated! \n",
      "train step 11946 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0006 diff={max=06.7585, min=00.0316, mean=00.9026} policy_loss=-12.2828 policy updated! \n",
      "train step 11947 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.4181 diff={max=12.6303, min=00.0069, mean=01.3716} policy_loss=-12.6750 policy updated! \n",
      "train step 11948 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.2304 diff={max=05.1088, min=00.0461, mean=01.2471} policy_loss=-15.6318 policy updated! \n",
      "train step 11949 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.5633 diff={max=05.9481, min=00.0010, mean=01.0617} policy_loss=-10.1971 policy updated! \n",
      "train step 11950 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.9323 diff={max=08.0447, min=00.0028, mean=01.3295} policy_loss=-14.4569 policy updated! \n",
      "train step 11951 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=07.4590 diff={max=12.7040, min=00.0250, mean=01.6994} policy_loss=-13.5677 policy updated! \n",
      "train step 11952 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.8475 diff={max=16.5697, min=00.0136, mean=01.6796} policy_loss=-16.5771 policy updated! \n",
      "train step 11953 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=08.8538 diff={max=13.9468, min=00.0058, mean=01.5845} policy_loss=-12.8330 policy updated! \n",
      "train step 11954 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1728 diff={max=11.1728, min=00.0032, mean=01.3157} policy_loss=-12.4738 policy updated! \n",
      "train step 11955 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.4733 diff={max=12.9972, min=00.0025, mean=01.6438} policy_loss=-12.3204 policy updated! \n",
      "train step 11956 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9795 diff={max=07.6975, min=00.0253, mean=01.8296} policy_loss=-15.1115 policy updated! \n",
      "train step 11957 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.3761 diff={max=07.1682, min=00.0282, mean=02.1022} policy_loss=-11.9146 policy updated! \n",
      "train step 11958 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.1710 diff={max=09.6232, min=00.1452, mean=02.0100} policy_loss=-14.4619 policy updated! \n",
      "train step 11959 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=14.5998 diff={max=20.6091, min=00.0220, mean=01.8808} policy_loss=-12.3724 policy updated! \n",
      "train step 11960 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.4186 diff={max=07.2570, min=00.0049, mean=01.1988} policy_loss=-9.0228 policy updated! \n",
      "train step 11961 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.6716 diff={max=08.1431, min=00.0037, mean=01.4189} policy_loss=-11.3927 policy updated! \n",
      "train step 11962 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0915 diff={max=08.7738, min=00.0050, mean=01.1360} policy_loss=-11.3861 policy updated! \n",
      "train step 11963 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.5778 diff={max=11.4226, min=00.0009, mean=01.7115} policy_loss=-12.1117 policy updated! \n",
      "train step 11964 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7633 diff={max=07.7675, min=00.0020, mean=01.6754} policy_loss=-11.8961 policy updated! \n",
      "train step 11965 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.2380 diff={max=09.2203, min=00.0081, mean=01.3579} policy_loss=-12.8311 policy updated! \n",
      "train step 11966 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5535 diff={max=06.6948, min=00.0032, mean=01.2263} policy_loss=-14.0709 policy updated! \n",
      "train step 11967 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1460 diff={max=07.2379, min=00.0019, mean=01.5633} policy_loss=-13.1560 policy updated! \n",
      "train step 11968 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.5814 diff={max=12.9489, min=00.0413, mean=01.6304} policy_loss=-11.9616 policy updated! \n",
      "train step 11969 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9211 diff={max=05.9277, min=00.0308, mean=01.1131} policy_loss=-12.2442 policy updated! \n",
      "train step 11970 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.0890 diff={max=10.1318, min=00.0057, mean=01.6868} policy_loss=-14.3659 policy updated! \n",
      "train step 11971 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.6110 diff={max=16.8238, min=00.0040, mean=01.7239} policy_loss=-14.0933 policy updated! \n",
      "train step 11972 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2918 diff={max=06.5788, min=00.0044, mean=01.3131} policy_loss=-12.8845 policy updated! \n",
      "train step 11973 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9505 diff={max=07.5146, min=00.0011, mean=01.4022} policy_loss=-12.3127 policy updated! \n",
      "train step 11974 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.7876 diff={max=12.7091, min=00.0056, mean=01.5128} policy_loss=-14.7657 policy updated! \n",
      "train step 11975 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.6317 diff={max=08.0929, min=00.0182, mean=01.3375} policy_loss=-12.2907 policy updated! \n",
      "train step 11976 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3900 diff={max=12.6069, min=00.0200, mean=01.4000} policy_loss=-13.7461 policy updated! \n",
      "train step 11977 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.8768 diff={max=14.2280, min=00.0173, mean=01.6683} policy_loss=-11.9734 policy updated! \n",
      "train step 11978 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.9523 diff={max=13.8452, min=00.0435, mean=01.6314} policy_loss=-12.7897 policy updated! \n",
      "train step 11979 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.7143 diff={max=18.2908, min=00.0138, mean=01.9515} policy_loss=-11.7497 policy updated! \n",
      "train step 11980 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.5885 diff={max=07.5341, min=00.0362, mean=01.1178} policy_loss=-11.9315 policy updated! \n",
      "train step 11981 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9880 diff={max=06.6865, min=00.0117, mean=01.2049} policy_loss=-14.1864 policy updated! \n",
      "train step 11982 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.1572 diff={max=13.5592, min=00.0273, mean=01.6481} policy_loss=-12.5293 policy updated! \n",
      "train step 11983 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0912 diff={max=05.1821, min=00.0098, mean=00.9473} policy_loss=-8.8784 policy updated! \n",
      "train step 11984 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.9646 diff={max=08.6116, min=00.0101, mean=01.4308} policy_loss=-11.0534 policy updated! \n",
      "train step 11985 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.4968 diff={max=07.0224, min=00.0134, mean=01.5556} policy_loss=-11.9777 policy updated! \n",
      "train step 11986 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9388 diff={max=07.0165, min=00.0252, mean=01.3168} policy_loss=-12.4982 policy updated! \n",
      "train step 11987 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5023 diff={max=12.4332, min=00.0121, mean=01.0836} policy_loss=-9.8556 policy updated! \n",
      "train step 11988 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.9119 diff={max=05.8944, min=00.0051, mean=00.9870} policy_loss=-10.0263 policy updated! \n",
      "train step 11989 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9830 diff={max=12.4078, min=00.0300, mean=01.1409} policy_loss=-14.5343 policy updated! \n",
      "train step 11990 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.9095 diff={max=12.4597, min=00.0212, mean=01.5433} policy_loss=-12.4913 policy updated! \n",
      "train step 11991 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=11.3512 diff={max=17.4816, min=00.0496, mean=01.7674} policy_loss=-14.1305 policy updated! \n",
      "train step 11992 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=03.5047 diff={max=08.4595, min=00.0348, mean=01.1406} policy_loss=-12.0987 policy updated! \n",
      "train step 11993 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.9983 diff={max=05.1759, min=00.0036, mean=01.2003} policy_loss=-12.1521 policy updated! \n",
      "train step 11994 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.7066 diff={max=12.0389, min=00.0168, mean=01.2530} policy_loss=-12.2685 policy updated! \n",
      "train step 11995 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.8686 diff={max=08.3302, min=00.0895, mean=01.3985} policy_loss=-13.5474 policy updated! \n",
      "train step 11996 reward={max=08.0000, min=00.0000, mean=04.4000} optimizing loss=05.6316 diff={max=08.2038, min=00.0028, mean=01.5864} policy_loss=-13.2885 policy updated! \n",
      "train step 11997 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1665 diff={max=06.6551, min=00.0009, mean=01.4185} policy_loss=-14.5885 policy updated! \n",
      "train step 11998 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.9163 diff={max=06.8161, min=00.0065, mean=01.5229} policy_loss=-13.9102 policy updated! \n",
      "train step 11999 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=07.0990 diff={max=12.8357, min=00.0351, mean=01.1840} policy_loss=-10.9645 policy updated! \n",
      "train step 12000 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.3774 diff={max=05.3419, min=00.0001, mean=01.0372} policy_loss=-15.4799 policy updated! \n",
      "train step 12001 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.4334 diff={max=11.1663, min=00.0943, mean=01.6878} policy_loss=-13.5399 policy updated! \n",
      "train step 12002 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2673 diff={max=08.4907, min=00.0345, mean=01.1580} policy_loss=-13.7621 policy updated! \n",
      "train step 12003 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.3233 diff={max=07.5193, min=00.0272, mean=00.9790} policy_loss=-13.0280 policy updated! \n",
      "train step 12004 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=16.1124 diff={max=20.6662, min=00.0117, mean=02.0625} policy_loss=-13.0084 policy updated! \n",
      "train step 12005 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.2407 diff={max=09.5447, min=00.0038, mean=01.7628} policy_loss=-11.2410 policy updated! \n",
      "train step 12006 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9517 diff={max=05.1152, min=00.0291, mean=01.2475} policy_loss=-13.3411 policy updated! \n",
      "train step 12007 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8987 diff={max=09.9144, min=00.0276, mean=01.3456} policy_loss=-11.8856 policy updated! \n",
      "train step 12008 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.8156 diff={max=11.2492, min=00.0488, mean=01.7029} policy_loss=-13.9673 policy updated! \n",
      "train step 12009 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3811 diff={max=06.4773, min=00.0008, mean=01.2555} policy_loss=-12.3894 policy updated! \n",
      "train step 12010 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.3501 diff={max=11.4482, min=00.0168, mean=01.4307} policy_loss=-12.7281 policy updated! \n",
      "train step 12011 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=11.9437 diff={max=14.4231, min=00.0115, mean=01.9390} policy_loss=-14.3646 policy updated! \n",
      "train step 12012 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7837 diff={max=07.1464, min=00.0845, mean=01.7328} policy_loss=-15.4566 policy updated! \n",
      "train step 12013 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3303 diff={max=07.6482, min=00.0611, mean=01.3829} policy_loss=-11.0121 policy updated! \n",
      "train step 12014 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9117 diff={max=10.5325, min=00.0000, mean=01.4502} policy_loss=-12.6705 policy updated! \n",
      "train step 12015 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.2008 diff={max=11.9203, min=00.0869, mean=01.3783} policy_loss=-12.8923 policy updated! \n",
      "train step 12016 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.9679 diff={max=08.9317, min=00.0053, mean=01.5435} policy_loss=-13.5654 policy updated! \n",
      "train step 12017 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.9765 diff={max=10.1313, min=00.0024, mean=02.0700} policy_loss=-14.0497 policy updated! \n",
      "train step 12018 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.0714 diff={max=15.6248, min=00.0347, mean=01.5469} policy_loss=-15.7275 policy updated! \n",
      "train step 12019 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.8959 diff={max=09.4416, min=00.0164, mean=01.7406} policy_loss=-12.9070 policy updated! \n",
      "train step 12020 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.9872 diff={max=09.5336, min=00.0302, mean=01.3880} policy_loss=-16.5888 policy updated! \n",
      "train step 12021 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8750 diff={max=07.3555, min=00.0089, mean=01.0895} policy_loss=-10.5972 policy updated! \n",
      "train step 12022 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9463 diff={max=10.5380, min=00.0317, mean=01.3433} policy_loss=-14.4497 policy updated! \n",
      "train step 12023 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.2290 diff={max=05.6878, min=00.0183, mean=01.2519} policy_loss=-13.3457 policy updated! \n",
      "train step 12024 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.5059 diff={max=07.7393, min=00.0225, mean=01.2333} policy_loss=-11.4436 policy updated! \n",
      "train step 12025 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.4360 diff={max=07.4043, min=00.0050, mean=01.3008} policy_loss=-13.2427 policy updated! \n",
      "train step 12026 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.2252 diff={max=08.4012, min=00.0479, mean=01.4255} policy_loss=-13.3014 policy updated! \n",
      "train step 12027 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3858 diff={max=09.3441, min=00.0129, mean=01.1241} policy_loss=-11.9535 policy updated! \n",
      "train step 12028 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6877 diff={max=10.9273, min=00.0161, mean=01.3093} policy_loss=-12.3244 policy updated! \n",
      "train step 12029 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=24.3004 diff={max=23.4595, min=00.0315, mean=02.5956} policy_loss=-11.9894 policy updated! \n",
      "train step 12030 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.1952 diff={max=06.4303, min=00.0526, mean=01.4157} policy_loss=-12.6011 policy updated! \n",
      "train step 12031 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=14.3150 diff={max=15.4841, min=00.0371, mean=02.2480} policy_loss=-12.9065 policy updated! \n",
      "train step 12032 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2120 diff={max=05.9116, min=00.0044, mean=01.1568} policy_loss=-13.9595 policy updated! \n",
      "train step 12033 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.5386 diff={max=12.0572, min=00.0333, mean=01.7954} policy_loss=-14.7115 policy updated! \n",
      "train step 12034 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.4871 diff={max=10.8992, min=00.0064, mean=01.6781} policy_loss=-10.9765 policy updated! \n",
      "train step 12035 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=10.0419 diff={max=12.3566, min=00.0102, mean=01.7105} policy_loss=-15.1186 policy updated! \n",
      "train step 12036 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.4006 diff={max=07.8378, min=00.0082, mean=01.4048} policy_loss=-12.3624 policy updated! \n",
      "train step 12037 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6227 diff={max=06.1122, min=00.0320, mean=01.3305} policy_loss=-13.8527 policy updated! \n",
      "train step 12038 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.8248 diff={max=14.0431, min=00.0235, mean=01.7739} policy_loss=-11.5028 policy updated! \n",
      "train step 12039 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=08.0905 diff={max=13.5559, min=00.0546, mean=01.5845} policy_loss=-12.4091 policy updated! \n",
      "train step 12040 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.9584 diff={max=06.2604, min=00.0019, mean=01.3285} policy_loss=-12.2267 policy updated! \n",
      "train step 12041 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.7125 diff={max=17.3673, min=00.0100, mean=01.9636} policy_loss=-15.6846 policy updated! \n",
      "train step 12042 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2496 diff={max=06.7801, min=00.0297, mean=01.3849} policy_loss=-13.9939 policy updated! \n",
      "train step 12043 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.2393 diff={max=12.2473, min=00.0305, mean=01.5176} policy_loss=-11.5687 policy updated! \n",
      "train step 12044 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.7741 diff={max=15.0195, min=00.0077, mean=02.2319} policy_loss=-13.7544 policy updated! \n",
      "train step 12045 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.3873 diff={max=07.5317, min=00.0147, mean=01.2547} policy_loss=-11.7653 policy updated! \n",
      "train step 12046 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1898 diff={max=07.8155, min=00.0260, mean=01.4435} policy_loss=-11.2322 policy updated! \n",
      "train step 12047 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4576 diff={max=07.2421, min=00.0008, mean=01.6143} policy_loss=-12.3233 policy updated! \n",
      "train step 12048 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5335 diff={max=09.0984, min=00.0346, mean=01.4650} policy_loss=-12.6064 policy updated! \n",
      "train step 12049 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.1961 diff={max=09.3287, min=00.0091, mean=02.0636} policy_loss=-13.5405 policy updated! \n",
      "train step 12050 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=24.2494 diff={max=26.2466, min=00.0027, mean=02.5050} policy_loss=-15.7573 policy updated! \n",
      "train step 12051 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.6520 diff={max=07.8811, min=00.0181, mean=01.6603} policy_loss=-13.9106 policy updated! \n",
      "train step 12052 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0508 diff={max=08.9228, min=00.0406, mean=01.7022} policy_loss=-13.9288 policy updated! \n",
      "train step 12053 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.9572 diff={max=12.6177, min=00.0525, mean=01.7638} policy_loss=-12.8456 policy updated! \n",
      "train step 12054 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.7962 diff={max=09.1602, min=00.0105, mean=01.5915} policy_loss=-14.0574 policy updated! \n",
      "train step 12055 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.1762 diff={max=10.2120, min=00.0044, mean=01.0897} policy_loss=-13.0929 policy updated! \n",
      "train step 12056 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=10.5617 diff={max=14.0812, min=00.0801, mean=01.7968} policy_loss=-14.4284 policy updated! \n",
      "train step 12057 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3455 diff={max=04.9964, min=00.0264, mean=01.0233} policy_loss=-12.2523 policy updated! \n",
      "train step 12058 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.1765 diff={max=08.7975, min=00.0216, mean=01.7668} policy_loss=-13.9327 policy updated! \n",
      "train step 12059 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3216 diff={max=09.3184, min=00.0012, mean=01.5345} policy_loss=-10.8497 policy updated! \n",
      "train step 12060 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.3925 diff={max=07.8494, min=00.0309, mean=01.5008} policy_loss=-13.2216 policy updated! \n",
      "train step 12061 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7127 diff={max=07.2263, min=00.0529, mean=01.3631} policy_loss=-12.5069 policy updated! \n",
      "train step 12062 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.6250 diff={max=10.0499, min=00.0336, mean=01.7827} policy_loss=-14.7260 policy updated! \n",
      "train step 12063 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9277 diff={max=06.8703, min=00.0004, mean=01.1050} policy_loss=-13.0871 policy updated! \n",
      "train step 12064 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8214 diff={max=07.1981, min=00.0089, mean=01.2947} policy_loss=-14.5895 policy updated! \n",
      "train step 12065 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.0165 diff={max=13.1903, min=00.0388, mean=01.7187} policy_loss=-16.0029 policy updated! \n",
      "train step 12066 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.6226 diff={max=14.9159, min=00.0086, mean=01.8255} policy_loss=-14.5310 policy updated! \n",
      "train step 12067 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.8338 diff={max=14.7985, min=00.0126, mean=01.6523} policy_loss=-11.6023 policy updated! \n",
      "train step 12068 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=07.1146 diff={max=09.2758, min=00.0023, mean=01.8361} policy_loss=-16.3433 policy updated! \n",
      "train step 12069 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1325 diff={max=12.5498, min=00.0493, mean=01.3678} policy_loss=-14.3430 policy updated! \n",
      "train step 12070 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.1608 diff={max=09.2274, min=00.0308, mean=01.3613} policy_loss=-13.7516 policy updated! \n",
      "train step 12071 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0560 diff={max=07.6059, min=00.0034, mean=01.6949} policy_loss=-13.5930 policy updated! \n",
      "train step 12072 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.4468 diff={max=11.0355, min=00.0035, mean=01.9539} policy_loss=-12.1330 policy updated! \n",
      "train step 12073 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5439 diff={max=07.5570, min=00.0516, mean=01.3922} policy_loss=-12.7847 policy updated! \n",
      "train step 12074 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1940 diff={max=05.3360, min=00.0251, mean=01.2509} policy_loss=-11.8299 policy updated! \n",
      "train step 12075 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.6818 diff={max=11.5369, min=00.0280, mean=01.8312} policy_loss=-14.9113 policy updated! \n",
      "train step 12076 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.5972 diff={max=07.6008, min=00.0176, mean=01.6864} policy_loss=-12.6995 policy updated! \n",
      "train step 12077 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.9201 diff={max=17.0699, min=00.0023, mean=01.3720} policy_loss=-11.4304 policy updated! \n",
      "train step 12078 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=06.6129 diff={max=09.4280, min=00.0006, mean=01.5360} policy_loss=-9.9150 policy updated! \n",
      "train step 12079 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7193 diff={max=08.1801, min=00.0335, mean=01.6870} policy_loss=-15.0813 policy updated! \n",
      "train step 12080 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.4257 diff={max=07.4257, min=00.0132, mean=01.8039} policy_loss=-13.8343 policy updated! \n",
      "train step 12081 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6314 diff={max=06.5100, min=00.0071, mean=01.4513} policy_loss=-13.1372 policy updated! \n",
      "train step 12082 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8852 diff={max=07.7732, min=00.0401, mean=01.3925} policy_loss=-9.9260 policy updated! \n",
      "train step 12083 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.6901 diff={max=18.7015, min=00.0501, mean=02.0021} policy_loss=-14.0795 policy updated! \n",
      "train step 12084 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=08.0548 diff={max=11.7605, min=00.0694, mean=01.7155} policy_loss=-12.7986 policy updated! \n",
      "train step 12085 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=10.3537 diff={max=12.8977, min=00.0715, mean=01.8482} policy_loss=-16.0519 policy updated! \n",
      "train step 12086 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.7745 diff={max=08.4817, min=00.0005, mean=01.0908} policy_loss=-11.6766 policy updated! \n",
      "train step 12087 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.8411 diff={max=14.3355, min=00.0024, mean=01.6544} policy_loss=-15.2911 policy updated! \n",
      "train step 12088 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2438 diff={max=09.6790, min=00.0475, mean=01.4702} policy_loss=-13.1387 policy updated! \n",
      "train step 12089 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1119 diff={max=10.4605, min=00.0030, mean=01.5673} policy_loss=-12.9696 policy updated! \n",
      "train step 12090 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.7787 diff={max=09.2024, min=00.0090, mean=01.2562} policy_loss=-12.0839 policy updated! \n",
      "train step 12091 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8380 diff={max=06.6061, min=00.0036, mean=01.3568} policy_loss=-13.2475 policy updated! \n",
      "train step 12092 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4673 diff={max=11.1932, min=00.0219, mean=01.4772} policy_loss=-11.4894 policy updated! \n",
      "train step 12093 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.1973 diff={max=08.1218, min=00.0214, mean=01.3556} policy_loss=-14.3089 policy updated! \n",
      "train step 12094 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.0344 diff={max=14.6233, min=00.0067, mean=01.9788} policy_loss=-11.5966 policy updated! \n",
      "train step 12095 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.4831 diff={max=16.9384, min=00.0092, mean=01.7156} policy_loss=-11.0471 policy updated! \n",
      "train step 12096 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2824 diff={max=08.3839, min=00.0111, mean=00.9870} policy_loss=-11.9449 policy updated! \n",
      "train step 12097 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6131 diff={max=08.3609, min=00.0510, mean=01.0636} policy_loss=-12.4055 policy updated! \n",
      "train step 12098 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6806 diff={max=09.0070, min=00.0229, mean=01.2494} policy_loss=-12.5347 policy updated! \n",
      "train step 12099 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7842 diff={max=10.6732, min=00.0252, mean=01.5561} policy_loss=-12.6714 policy updated! \n",
      "train step 12100 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=06.3260 diff={max=08.5709, min=00.0049, mean=01.5526} policy_loss=-11.1629 policy updated! \n",
      "train step 12101 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=08.4784 diff={max=09.9208, min=00.0398, mean=01.7642} policy_loss=-14.1778 policy updated! \n",
      "train step 12102 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.7199 diff={max=08.2349, min=00.0080, mean=01.7824} policy_loss=-12.3392 policy updated! \n",
      "train step 12103 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7118 diff={max=07.5283, min=00.0969, mean=01.2481} policy_loss=-11.4410 policy updated! \n",
      "train step 12104 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.6258 diff={max=07.7684, min=00.0004, mean=01.1680} policy_loss=-13.5472 policy updated! \n",
      "train step 12105 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.0132 diff={max=09.7438, min=00.0020, mean=01.7746} policy_loss=-13.2293 policy updated! \n",
      "train step 12106 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.4491 diff={max=13.1693, min=00.0210, mean=01.5353} policy_loss=-14.2738 policy updated! \n",
      "train step 12107 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.5228 diff={max=15.2562, min=00.0154, mean=01.8889} policy_loss=-12.6903 policy updated! \n",
      "train step 12108 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.0082 diff={max=06.2069, min=00.0594, mean=01.5490} policy_loss=-14.2003 policy updated! \n",
      "train step 12109 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2867 diff={max=07.5397, min=00.0379, mean=01.5379} policy_loss=-13.6520 policy updated! \n",
      "train step 12110 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=12.4536 diff={max=16.3975, min=00.0048, mean=01.8623} policy_loss=-14.5845 policy updated! \n",
      "train step 12111 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6889 diff={max=07.8756, min=00.0075, mean=01.0885} policy_loss=-12.7446 policy updated! \n",
      "train step 12112 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6392 diff={max=11.8161, min=00.0570, mean=01.4431} policy_loss=-10.9089 policy updated! \n",
      "train step 12113 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=17.1210 diff={max=22.3835, min=00.0025, mean=02.0229} policy_loss=-13.6365 policy updated! \n",
      "train step 12114 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7257 diff={max=10.7077, min=00.0003, mean=01.1359} policy_loss=-12.2916 policy updated! \n",
      "train step 12115 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.8815 diff={max=11.4651, min=00.0631, mean=01.8341} policy_loss=-13.2122 policy updated! \n",
      "train step 12116 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3503 diff={max=11.3818, min=00.0014, mean=01.4838} policy_loss=-8.3399 policy updated! \n",
      "train step 12117 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7150 diff={max=08.7375, min=00.0204, mean=01.6335} policy_loss=-12.8724 policy updated! \n",
      "train step 12118 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.3545 diff={max=08.9411, min=00.0419, mean=01.1329} policy_loss=-12.0764 policy updated! \n",
      "train step 12119 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.2123 diff={max=07.0941, min=00.0034, mean=01.0818} policy_loss=-13.2785 policy updated! \n",
      "train step 12120 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.6348 diff={max=08.4346, min=00.0373, mean=01.7545} policy_loss=-15.7650 policy updated! \n",
      "train step 12121 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5537 diff={max=07.8088, min=00.0080, mean=01.5378} policy_loss=-13.0612 policy updated! \n",
      "train step 12122 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1443 diff={max=06.8866, min=00.0010, mean=01.1437} policy_loss=-11.1650 policy updated! \n",
      "train step 12123 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9513 diff={max=08.0768, min=00.0209, mean=01.4472} policy_loss=-12.2150 policy updated! \n",
      "train step 12124 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.3123 diff={max=18.4344, min=00.0048, mean=01.5782} policy_loss=-14.7047 policy updated! \n",
      "train step 12125 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.9686 diff={max=07.1575, min=00.0295, mean=01.3413} policy_loss=-14.6073 policy updated! \n",
      "train step 12126 reward={max=08.0000, min=00.0000, mean=04.4000} optimizing loss=06.5041 diff={max=13.8167, min=00.0243, mean=01.2349} policy_loss=-12.1686 policy updated! \n",
      "train step 12127 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4836 diff={max=07.1844, min=00.0658, mean=01.7719} policy_loss=-13.9513 policy updated! \n",
      "train step 12128 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.3116 diff={max=03.9852, min=00.0194, mean=01.0829} policy_loss=-15.7921 policy updated! \n",
      "train step 12129 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.9266 diff={max=15.3578, min=00.0093, mean=02.0063} policy_loss=-11.8011 policy updated! \n",
      "train step 12130 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.9406 diff={max=05.8288, min=00.0180, mean=01.5258} policy_loss=-13.1095 policy updated! \n",
      "train step 12131 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.7098 diff={max=12.0254, min=00.0134, mean=01.8908} policy_loss=-11.6130 policy updated! \n",
      "train step 12132 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3297 diff={max=13.7508, min=00.0339, mean=01.2644} policy_loss=-13.0454 policy updated! \n",
      "train step 12133 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.2832 diff={max=13.2480, min=00.0194, mean=01.8117} policy_loss=-11.6044 policy updated! \n",
      "train step 12134 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=09.0859 diff={max=15.1037, min=00.0242, mean=01.6273} policy_loss=-15.2505 policy updated! \n",
      "train step 12135 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.6186 diff={max=17.6776, min=00.0173, mean=01.2979} policy_loss=-12.9576 policy updated! \n",
      "train step 12136 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6690 diff={max=05.1569, min=00.0052, mean=01.2691} policy_loss=-11.2585 policy updated! \n",
      "train step 12137 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.5630 diff={max=08.7431, min=00.0067, mean=01.6886} policy_loss=-13.1341 policy updated! \n",
      "train step 12138 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=12.6905 diff={max=16.5883, min=00.0012, mean=02.0828} policy_loss=-13.5586 policy updated! \n",
      "train step 12139 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6504 diff={max=14.2645, min=00.0329, mean=01.3167} policy_loss=-13.5100 policy updated! \n",
      "train step 12140 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.0714 diff={max=11.8360, min=00.0050, mean=01.4211} policy_loss=-12.6730 policy updated! \n",
      "train step 12141 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.5954 diff={max=04.0482, min=00.0091, mean=00.9510} policy_loss=-14.5140 policy updated! \n",
      "train step 12142 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8400 diff={max=07.5743, min=00.0302, mean=01.2908} policy_loss=-13.2892 policy updated! \n",
      "train step 12143 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=08.4942 diff={max=10.6469, min=00.0073, mean=01.8688} policy_loss=-11.8389 policy updated! \n",
      "train step 12144 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.4138 diff={max=11.0669, min=00.0127, mean=01.5794} policy_loss=-12.4945 policy updated! \n",
      "train step 12145 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.2970 diff={max=06.9403, min=00.0231, mean=01.3243} policy_loss=-12.9528 policy updated! \n",
      "train step 12146 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1105 diff={max=08.7818, min=00.0014, mean=01.3743} policy_loss=-11.7799 policy updated! \n",
      "train step 12147 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.9559 diff={max=12.4664, min=00.0038, mean=01.7425} policy_loss=-12.0851 policy updated! \n",
      "train step 12148 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0307 diff={max=06.1266, min=00.0494, mean=01.5022} policy_loss=-16.2236 policy updated! \n",
      "train step 12149 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8248 diff={max=08.6842, min=00.0154, mean=01.4787} policy_loss=-13.0327 policy updated! \n",
      "train step 12150 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.6236 diff={max=10.6928, min=00.0079, mean=01.7751} policy_loss=-12.4764 policy updated! \n",
      "train step 12151 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8941 diff={max=10.5432, min=00.0005, mean=01.3461} policy_loss=-11.3685 policy updated! \n",
      "train step 12152 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2507 diff={max=08.5451, min=00.0054, mean=01.2347} policy_loss=-10.9665 policy updated! \n",
      "train step 12153 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2674 diff={max=05.3763, min=00.0161, mean=01.2395} policy_loss=-15.3015 policy updated! \n",
      "train step 12154 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5835 diff={max=09.1067, min=00.0369, mean=01.7793} policy_loss=-16.1257 policy updated! \n",
      "train step 12155 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.6986 diff={max=08.7029, min=00.0043, mean=01.5895} policy_loss=-12.1623 policy updated! \n",
      "train step 12156 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5955 diff={max=11.3151, min=00.0001, mean=01.5424} policy_loss=-13.0283 policy updated! \n",
      "train step 12157 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.1806 diff={max=09.4788, min=00.0396, mean=01.7582} policy_loss=-13.5940 policy updated! \n",
      "train step 12158 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.7072 diff={max=10.9434, min=00.0359, mean=01.7197} policy_loss=-13.4798 policy updated! \n",
      "train step 12159 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=02.8534 diff={max=06.4915, min=00.0229, mean=01.1212} policy_loss=-11.7115 policy updated! \n",
      "train step 12160 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=20.3492 diff={max=18.4879, min=00.0196, mean=02.2935} policy_loss=-14.0894 policy updated! \n",
      "train step 12161 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=07.1310 diff={max=08.6639, min=00.0030, mean=01.5789} policy_loss=-14.2549 policy updated! \n",
      "train step 12162 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2455 diff={max=08.4952, min=00.0796, mean=01.4807} policy_loss=-12.6104 policy updated! \n",
      "train step 12163 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3121 diff={max=10.7932, min=00.0040, mean=01.5670} policy_loss=-16.4323 policy updated! \n",
      "train step 12164 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1306 diff={max=06.4830, min=00.0115, mean=01.8032} policy_loss=-15.0476 policy updated! \n",
      "train step 12165 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.6468 diff={max=06.5577, min=00.0068, mean=01.0371} policy_loss=-13.3135 policy updated! \n",
      "train step 12166 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=08.2935 diff={max=11.6505, min=00.0280, mean=01.8346} policy_loss=-18.3785 policy updated! \n",
      "train step 12167 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=04.4213 diff={max=06.3601, min=00.0089, mean=01.5264} policy_loss=-14.0627 policy updated! \n",
      "train step 12168 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=10.6124 diff={max=16.0943, min=00.0092, mean=01.8482} policy_loss=-12.5300 policy updated! \n",
      "train step 12169 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=08.3988 diff={max=09.5237, min=00.0586, mean=01.7120} policy_loss=-14.1950 policy updated! \n",
      "train step 12170 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.2541 diff={max=04.8750, min=00.0374, mean=01.3602} policy_loss=-14.0481 policy updated! \n",
      "train step 12171 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5753 diff={max=06.4421, min=00.0102, mean=01.6300} policy_loss=-13.1305 policy updated! \n",
      "train step 12172 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0588 diff={max=07.9902, min=00.0290, mean=01.5888} policy_loss=-14.3093 policy updated! \n",
      "train step 12173 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1016 diff={max=11.3326, min=00.0246, mean=01.5670} policy_loss=-13.8082 policy updated! \n",
      "train step 12174 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8936 diff={max=11.7143, min=00.0189, mean=01.2902} policy_loss=-10.4171 policy updated! \n",
      "train step 12175 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=06.5985 diff={max=09.1683, min=00.0030, mean=01.6689} policy_loss=-14.0828 policy updated! \n",
      "train step 12176 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=07.7739 diff={max=14.2484, min=00.0236, mean=01.4826} policy_loss=-10.9142 policy updated! \n",
      "train step 12177 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.9955 diff={max=10.2328, min=00.0119, mean=01.9668} policy_loss=-12.0994 policy updated! \n",
      "train step 12178 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=06.2576 diff={max=09.5599, min=00.0305, mean=01.6448} policy_loss=-13.2769 policy updated! \n",
      "train step 12179 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.7828 diff={max=10.1496, min=00.0151, mean=01.6139} policy_loss=-12.2076 policy updated! \n",
      "train step 12180 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=12.5807 diff={max=19.5404, min=00.0044, mean=01.7516} policy_loss=-13.0424 policy updated! \n",
      "train step 12181 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8266 diff={max=07.0932, min=00.0288, mean=01.0831} policy_loss=-10.2628 policy updated! \n",
      "train step 12182 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5151 diff={max=06.8470, min=00.0010, mean=01.1623} policy_loss=-13.5903 policy updated! \n",
      "train step 12183 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9829 diff={max=08.2749, min=00.0369, mean=01.6380} policy_loss=-15.3217 policy updated! \n",
      "train step 12184 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.8136 diff={max=08.9349, min=00.0299, mean=01.1865} policy_loss=-13.8853 policy updated! \n",
      "train step 12185 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.1558 diff={max=11.1044, min=00.0431, mean=01.4526} policy_loss=-10.3605 policy updated! \n",
      "train step 12186 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7532 diff={max=07.6328, min=00.0225, mean=01.2209} policy_loss=-13.0899 policy updated! \n",
      "train step 12187 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2396 diff={max=08.9061, min=00.0105, mean=01.4220} policy_loss=-14.4177 policy updated! \n",
      "train step 12188 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=07.3789 diff={max=09.4776, min=00.0373, mean=01.6661} policy_loss=-11.1603 policy updated! \n",
      "train step 12189 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3027 diff={max=06.5443, min=00.0334, mean=01.4322} policy_loss=-15.1321 policy updated! \n",
      "train step 12190 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.6097 diff={max=07.8559, min=00.0041, mean=01.4481} policy_loss=-13.9152 policy updated! \n",
      "train step 12191 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.7227 diff={max=10.8930, min=00.0013, mean=01.7643} policy_loss=-13.5240 policy updated! \n",
      "train step 12192 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7335 diff={max=09.2101, min=00.0128, mean=01.3101} policy_loss=-11.0619 policy updated! \n",
      "train step 12193 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.2685 diff={max=08.5255, min=00.0318, mean=01.7330} policy_loss=-12.6711 policy updated! \n",
      "train step 12194 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=07.6897 diff={max=09.1235, min=00.0326, mean=01.4894} policy_loss=-13.0384 policy updated! \n",
      "train step 12195 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.6011 diff={max=09.8245, min=00.0170, mean=01.6834} policy_loss=-14.4748 policy updated! \n",
      "train step 12196 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.5838 diff={max=18.4971, min=00.0033, mean=01.8992} policy_loss=-14.2432 policy updated! \n",
      "train step 12197 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1873 diff={max=08.2557, min=00.0290, mean=01.5087} policy_loss=-10.9918 policy updated! \n",
      "train step 12198 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7011 diff={max=10.6267, min=00.0302, mean=01.4424} policy_loss=-13.6849 policy updated! \n",
      "train step 12199 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2520 diff={max=07.2596, min=00.0902, mean=01.2859} policy_loss=-13.4249 policy updated! \n",
      "train step 12200 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.0627 diff={max=08.0576, min=00.0707, mean=01.5070} policy_loss=-15.7665 policy updated! \n",
      "train step 12201 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8381 diff={max=07.3280, min=00.0057, mean=01.1810} policy_loss=-13.2203 policy updated! \n",
      "train step 12202 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2008 diff={max=06.8038, min=00.0298, mean=01.6287} policy_loss=-12.3553 policy updated! \n",
      "train step 12203 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3389 diff={max=06.0076, min=00.0076, mean=00.9579} policy_loss=-12.9074 policy updated! \n",
      "train step 12204 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.4781 diff={max=04.3522, min=00.0063, mean=01.1393} policy_loss=-15.1014 policy updated! \n",
      "train step 12205 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.7791 diff={max=06.6163, min=00.0269, mean=01.4348} policy_loss=-13.0713 policy updated! \n",
      "train step 12206 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=12.2196 diff={max=19.8883, min=00.0102, mean=01.8004} policy_loss=-13.2438 policy updated! \n",
      "train step 12207 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4825 diff={max=08.5441, min=00.0162, mean=01.4905} policy_loss=-12.6773 policy updated! \n",
      "train step 12208 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=13.3877 diff={max=16.7889, min=00.0225, mean=01.9358} policy_loss=-11.3935 policy updated! \n",
      "train step 12209 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.7564 diff={max=11.9080, min=00.0518, mean=01.8691} policy_loss=-15.7253 policy updated! \n",
      "train step 12210 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.3801 diff={max=11.0722, min=00.0236, mean=01.3677} policy_loss=-12.0184 policy updated! \n",
      "train step 12211 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.8928 diff={max=13.1437, min=00.0056, mean=01.5772} policy_loss=-9.2143 policy updated! \n",
      "train step 12212 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.6644 diff={max=13.2274, min=00.0078, mean=01.6759} policy_loss=-12.5900 policy updated! \n",
      "train step 12213 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7634 diff={max=09.6976, min=00.0239, mean=01.4795} policy_loss=-11.0508 policy updated! \n",
      "train step 12214 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=18.2236 diff={max=19.6488, min=00.0302, mean=02.3675} policy_loss=-11.9061 policy updated! \n",
      "train step 12215 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.3478 diff={max=08.3870, min=00.0303, mean=01.6861} policy_loss=-14.6593 policy updated! \n",
      "train step 12216 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=32.1351 diff={max=35.9827, min=00.0095, mean=02.1688} policy_loss=-12.9706 policy updated! \n",
      "train step 12217 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6616 diff={max=06.8260, min=00.0484, mean=01.4258} policy_loss=-13.7208 policy updated! \n",
      "train step 12218 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=08.1147 diff={max=10.7851, min=00.0009, mean=01.6785} policy_loss=-11.7349 policy updated! \n",
      "train step 12219 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.3563 diff={max=10.7805, min=00.0018, mean=01.6506} policy_loss=-13.3218 policy updated! \n",
      "train step 12220 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.9934 diff={max=08.4554, min=00.0457, mean=01.1761} policy_loss=-15.5065 policy updated! \n",
      "train step 12221 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6729 diff={max=09.7100, min=00.0172, mean=01.4969} policy_loss=-14.2252 policy updated! \n",
      "train step 12222 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7698 diff={max=10.0467, min=00.0079, mean=01.4365} policy_loss=-12.1437 policy updated! \n",
      "train step 12223 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5700 diff={max=06.0735, min=00.0364, mean=01.4793} policy_loss=-13.3588 policy updated! \n",
      "train step 12224 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=07.1836 diff={max=13.5999, min=00.0138, mean=01.6664} policy_loss=-13.9795 policy updated! \n",
      "train step 12225 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.3577 diff={max=06.8549, min=00.0052, mean=01.2385} policy_loss=-13.2260 policy updated! \n",
      "train step 12226 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=07.2602 diff={max=09.3520, min=00.0181, mean=01.6489} policy_loss=-13.2212 policy updated! \n",
      "train step 12227 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5761 diff={max=07.1686, min=00.0102, mean=01.4729} policy_loss=-12.9122 policy updated! \n",
      "train step 12228 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6374 diff={max=06.5527, min=00.0127, mean=01.5425} policy_loss=-15.6080 policy updated! \n",
      "train step 12229 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3029 diff={max=08.4783, min=00.0013, mean=01.4331} policy_loss=-14.6042 policy updated! \n",
      "train step 12230 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.6363 diff={max=15.5551, min=00.0219, mean=01.8675} policy_loss=-14.7531 policy updated! \n",
      "train step 12231 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.6900 diff={max=12.6526, min=00.0123, mean=01.8282} policy_loss=-16.5816 policy updated! \n",
      "train step 12232 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=15.7386 diff={max=23.2310, min=00.0651, mean=01.8653} policy_loss=-14.7669 policy updated! \n",
      "train step 12233 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9910 diff={max=10.0328, min=00.0334, mean=01.6535} policy_loss=-12.8333 policy updated! \n",
      "train step 12234 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7481 diff={max=15.5763, min=00.0148, mean=01.2901} policy_loss=-14.9127 policy updated! \n",
      "train step 12235 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=12.1188 diff={max=17.2234, min=00.0260, mean=01.5007} policy_loss=-12.6753 policy updated! \n",
      "train step 12236 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=20.8133 diff={max=26.4111, min=00.0602, mean=01.9323} policy_loss=-14.2942 policy updated! \n",
      "train step 12237 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3275 diff={max=08.0092, min=00.0132, mean=01.6004} policy_loss=-12.9810 policy updated! \n",
      "train step 12238 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.0089 diff={max=09.7618, min=00.0034, mean=01.7944} policy_loss=-12.2111 policy updated! \n",
      "train step 12239 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.4807 diff={max=14.7256, min=00.0571, mean=02.0125} policy_loss=-12.8839 policy updated! \n",
      "train step 12240 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.1479 diff={max=08.0759, min=00.0304, mean=01.5418} policy_loss=-15.8968 policy updated! \n",
      "train step 12241 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=17.4799 diff={max=22.8071, min=00.0199, mean=02.0149} policy_loss=-16.4253 policy updated! \n",
      "train step 12242 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.6979 diff={max=09.2519, min=00.0347, mean=01.9854} policy_loss=-14.9060 policy updated! \n",
      "train step 12243 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.6761 diff={max=14.1672, min=00.0154, mean=01.6523} policy_loss=-11.9472 policy updated! \n",
      "train step 12244 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9781 diff={max=08.2340, min=00.0245, mean=01.4758} policy_loss=-13.6984 policy updated! \n",
      "train step 12245 reward={max=07.0000, min=00.0000, mean=02.6000} optimizing loss=06.4604 diff={max=11.0793, min=00.0477, mean=01.8322} policy_loss=-12.7857 policy updated! \n",
      "train step 12246 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1545 diff={max=05.9179, min=00.0826, mean=01.7344} policy_loss=-13.0564 policy updated! \n",
      "train step 12247 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.7147 diff={max=11.9185, min=00.0018, mean=02.2252} policy_loss=-14.1661 policy updated! \n",
      "train step 12248 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.4894 diff={max=17.8010, min=00.0137, mean=01.6726} policy_loss=-13.9199 policy updated! \n",
      "train step 12249 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=15.9572 diff={max=18.8643, min=00.0212, mean=02.0280} policy_loss=-12.7448 policy updated! \n",
      "train step 12250 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.2871 diff={max=11.4214, min=00.0446, mean=01.4774} policy_loss=-15.5440 policy updated! \n",
      "train step 12251 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1287 diff={max=10.5521, min=00.0511, mean=01.4611} policy_loss=-13.1300 policy updated! \n",
      "train step 12252 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.1423 diff={max=08.6509, min=00.0658, mean=01.8385} policy_loss=-13.7474 policy updated! \n",
      "train step 12253 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=20.2301 diff={max=19.1295, min=00.0315, mean=02.3078} policy_loss=-12.8399 policy updated! \n",
      "train step 12254 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2848 diff={max=10.0789, min=00.0218, mean=01.4164} policy_loss=-11.8034 policy updated! \n",
      "train step 12255 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.5150 diff={max=13.9197, min=00.0484, mean=01.6324} policy_loss=-14.9511 policy updated! \n",
      "train step 12256 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=09.4487 diff={max=16.5272, min=00.0195, mean=01.6242} policy_loss=-13.6884 policy updated! \n",
      "train step 12257 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.2858 diff={max=14.2997, min=00.0050, mean=01.7744} policy_loss=-13.4429 policy updated! \n",
      "train step 12258 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.4041 diff={max=17.0089, min=00.0015, mean=02.1212} policy_loss=-14.2170 policy updated! \n",
      "train step 12259 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9946 diff={max=06.2909, min=00.0152, mean=01.6227} policy_loss=-16.2592 policy updated! \n",
      "train step 12260 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.3271 diff={max=07.3675, min=00.0779, mean=01.7893} policy_loss=-14.8679 policy updated! \n",
      "train step 12261 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.5070 diff={max=12.4228, min=00.0088, mean=01.7792} policy_loss=-15.0690 policy updated! \n",
      "train step 12262 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8176 diff={max=05.7057, min=00.1162, mean=01.4271} policy_loss=-13.9343 policy updated! \n",
      "train step 12263 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6706 diff={max=03.8485, min=00.0183, mean=01.1678} policy_loss=-12.9454 policy updated! \n",
      "train step 12264 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0877 diff={max=08.0801, min=00.0138, mean=01.2547} policy_loss=-10.8472 policy updated! \n",
      "train step 12265 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.2415 diff={max=07.9010, min=00.0161, mean=01.5468} policy_loss=-12.8187 policy updated! \n",
      "train step 12266 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=08.8937 diff={max=14.0595, min=00.0161, mean=01.5947} policy_loss=-10.4458 policy updated! \n",
      "train step 12267 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4915 diff={max=09.0651, min=00.0313, mean=01.5735} policy_loss=-12.7457 policy updated! \n",
      "train step 12268 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.1104 diff={max=10.0345, min=00.0010, mean=01.6890} policy_loss=-15.2723 policy updated! \n",
      "train step 12269 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=15.4557 diff={max=10.8192, min=00.0092, mean=02.5440} policy_loss=-14.4971 policy updated! \n",
      "train step 12270 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.6963 diff={max=10.1927, min=00.0656, mean=01.7623} policy_loss=-15.0571 policy updated! \n",
      "train step 12271 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6238 diff={max=07.3144, min=00.0450, mean=01.4514} policy_loss=-13.7614 policy updated! \n",
      "train step 12272 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.6112 diff={max=10.4987, min=00.0034, mean=01.6526} policy_loss=-13.2092 policy updated! \n",
      "train step 12273 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5189 diff={max=04.2364, min=00.0168, mean=01.1333} policy_loss=-12.1381 policy updated! \n",
      "train step 12274 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.5789 diff={max=14.8573, min=00.0195, mean=01.5571} policy_loss=-13.8605 policy updated! \n",
      "train step 12275 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.3673 diff={max=06.9367, min=00.0034, mean=01.3510} policy_loss=-12.3454 policy updated! \n",
      "train step 12276 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9564 diff={max=08.1372, min=00.0202, mean=01.1987} policy_loss=-13.3542 policy updated! \n",
      "train step 12277 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9898 diff={max=06.2351, min=00.0326, mean=01.3332} policy_loss=-9.7725 policy updated! \n",
      "train step 12278 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.5583 diff={max=15.3096, min=00.0101, mean=01.4685} policy_loss=-13.2390 policy updated! \n",
      "train step 12279 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6845 diff={max=08.8723, min=00.0030, mean=01.4812} policy_loss=-9.3469 policy updated! \n",
      "train step 12280 reward={max=07.0000, min=00.0000, mean=02.6000} optimizing loss=07.0362 diff={max=09.2116, min=00.0064, mean=01.7191} policy_loss=-12.3261 policy updated! \n",
      "train step 12281 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=09.4262 diff={max=08.1800, min=00.0064, mean=02.1935} policy_loss=-14.8186 policy updated! \n",
      "train step 12282 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7240 diff={max=07.8171, min=00.0028, mean=01.4675} policy_loss=-13.7136 policy updated! \n",
      "train step 12283 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=09.6057 diff={max=12.9649, min=00.0241, mean=01.7693} policy_loss=-13.5398 policy updated! \n",
      "train step 12284 reward={max=07.0000, min=00.0000, mean=05.4000} optimizing loss=03.3863 diff={max=05.9704, min=00.0144, mean=01.1641} policy_loss=-14.3637 policy updated! \n",
      "train step 12285 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.7315 diff={max=08.2403, min=00.0005, mean=01.4372} policy_loss=-12.4495 policy updated! \n",
      "train step 12286 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6973 diff={max=04.9308, min=00.0287, mean=01.1593} policy_loss=-13.3439 policy updated! \n",
      "train step 12287 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8477 diff={max=08.4877, min=00.0014, mean=01.3978} policy_loss=-11.2452 policy updated! \n",
      "train step 12288 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=08.6075 diff={max=07.6362, min=00.0046, mean=02.0591} policy_loss=-12.8295 policy updated! \n",
      "train step 12289 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3878 diff={max=06.8239, min=00.0036, mean=01.0971} policy_loss=-10.5197 policy updated! \n",
      "train step 12290 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.8371 diff={max=05.8412, min=00.0315, mean=01.3596} policy_loss=-11.5225 policy updated! \n",
      "train step 12291 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1747 diff={max=07.6114, min=00.0378, mean=01.4099} policy_loss=-12.0236 policy updated! \n",
      "train step 12292 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9865 diff={max=07.1853, min=00.0208, mean=01.2734} policy_loss=-11.4392 policy updated! \n",
      "train step 12293 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0330 diff={max=09.2479, min=00.0103, mean=01.8030} policy_loss=-15.5510 policy updated! \n",
      "train step 12294 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0855 diff={max=10.9620, min=00.0365, mean=01.4843} policy_loss=-15.5304 policy updated! \n",
      "train step 12295 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.5741 diff={max=09.7379, min=00.0085, mean=01.8121} policy_loss=-13.4345 policy updated! \n",
      "train step 12296 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7139 diff={max=08.3314, min=00.0279, mean=01.5642} policy_loss=-14.8455 policy updated! \n",
      "train step 12297 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4721 diff={max=12.0647, min=00.0086, mean=01.3733} policy_loss=-13.6113 policy updated! \n",
      "train step 12298 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=17.4272 diff={max=19.9913, min=00.0147, mean=02.1786} policy_loss=-14.7013 policy updated! \n",
      "train step 12299 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3479 diff={max=11.3457, min=00.0338, mean=01.7002} policy_loss=-14.4312 policy updated! \n",
      "train step 12300 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.5882 diff={max=07.8450, min=00.0395, mean=01.4206} policy_loss=-12.7155 policy updated! \n",
      "train step 12301 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7105 diff={max=10.3161, min=00.0076, mean=01.4642} policy_loss=-13.1160 policy updated! \n",
      "train step 12302 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2276 diff={max=08.4159, min=00.1059, mean=01.6610} policy_loss=-12.3883 policy updated! \n",
      "train step 12303 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.0477 diff={max=14.8830, min=00.0113, mean=01.8473} policy_loss=-13.2006 policy updated! \n",
      "train step 12304 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.6025 diff={max=05.7821, min=00.0026, mean=01.4875} policy_loss=-13.1487 policy updated! \n",
      "train step 12305 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=13.1202 diff={max=16.3337, min=00.0089, mean=02.0395} policy_loss=-16.1173 policy updated! \n",
      "train step 12306 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8729 diff={max=08.5417, min=00.0088, mean=01.5408} policy_loss=-14.4226 policy updated! \n",
      "train step 12307 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4515 diff={max=07.5421, min=00.0135, mean=01.3732} policy_loss=-11.5925 policy updated! \n",
      "train step 12308 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.5195 diff={max=11.3063, min=00.0159, mean=01.2514} policy_loss=-11.9493 policy updated! \n",
      "train step 12309 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1950 diff={max=07.5797, min=00.0279, mean=01.5637} policy_loss=-13.0135 policy updated! \n",
      "train step 12310 reward={max=07.0000, min=00.0000, mean=02.6000} optimizing loss=07.1045 diff={max=08.0680, min=00.0026, mean=01.6998} policy_loss=-14.3187 policy updated! \n",
      "train step 12311 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0016 diff={max=06.8170, min=00.0096, mean=01.2379} policy_loss=-11.9525 policy updated! \n",
      "train step 12312 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.4273 diff={max=10.4375, min=00.0025, mean=01.5311} policy_loss=-10.0866 policy updated! \n",
      "train step 12313 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=06.5548 diff={max=07.8217, min=00.0282, mean=01.7529} policy_loss=-11.6742 policy updated! \n",
      "train step 12314 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9519 diff={max=07.3884, min=00.0670, mean=01.2139} policy_loss=-13.8589 policy updated! \n",
      "train step 12315 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.0752 diff={max=07.3762, min=00.0047, mean=01.1488} policy_loss=-12.7151 policy updated! \n",
      "train step 12316 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.4989 diff={max=09.7335, min=00.0338, mean=01.9343} policy_loss=-16.3437 policy updated! \n",
      "train step 12317 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2580 diff={max=11.1554, min=00.0185, mean=01.4005} policy_loss=-12.1094 policy updated! \n",
      "train step 12318 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6561 diff={max=06.4686, min=00.0071, mean=01.2756} policy_loss=-14.6920 policy updated! \n",
      "train step 12319 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8908 diff={max=08.0407, min=00.0131, mean=01.4721} policy_loss=-14.7090 policy updated! \n",
      "train step 12320 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.6613 diff={max=04.7279, min=00.0174, mean=00.9402} policy_loss=-13.9683 policy updated! \n",
      "train step 12321 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.5815 diff={max=14.5525, min=00.0016, mean=01.7805} policy_loss=-10.6958 policy updated! \n",
      "train step 12322 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.3652 diff={max=12.9953, min=00.0161, mean=01.6922} policy_loss=-15.4297 policy updated! \n",
      "train step 12323 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2943 diff={max=10.9470, min=00.0389, mean=01.5144} policy_loss=-15.1985 policy updated! \n",
      "train step 12324 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.6391 diff={max=09.2371, min=00.0100, mean=01.7347} policy_loss=-11.9529 policy updated! \n",
      "train step 12325 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.1792 diff={max=05.8860, min=00.0183, mean=01.3551} policy_loss=-14.1624 policy updated! \n",
      "train step 12326 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.5219 diff={max=08.3737, min=00.0044, mean=01.3234} policy_loss=-11.6515 policy updated! \n",
      "train step 12327 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.2080 diff={max=04.2659, min=00.0314, mean=01.0790} policy_loss=-15.5197 policy updated! \n",
      "train step 12328 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=06.1277 diff={max=06.4068, min=00.0043, mean=01.6457} policy_loss=-13.2758 policy updated! \n",
      "train step 12329 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3655 diff={max=11.4553, min=00.0015, mean=01.3655} policy_loss=-13.0942 policy updated! \n",
      "train step 12330 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=11.2642 diff={max=14.5893, min=00.0011, mean=01.6760} policy_loss=-12.0137 policy updated! \n",
      "train step 12331 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=08.9542 diff={max=15.0777, min=00.0457, mean=01.4903} policy_loss=-12.4409 policy updated! \n",
      "train step 12332 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1770 diff={max=06.7892, min=00.0525, mean=01.0146} policy_loss=-12.8218 policy updated! \n",
      "train step 12333 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.2842 diff={max=07.1318, min=00.0025, mean=01.3577} policy_loss=-12.9279 policy updated! \n",
      "train step 12334 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9656 diff={max=06.7775, min=00.0366, mean=01.0844} policy_loss=-11.1895 policy updated! \n",
      "train step 12335 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.7612 diff={max=06.4241, min=00.0099, mean=01.2772} policy_loss=-14.0932 policy updated! \n",
      "train step 12336 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.2422 diff={max=04.9711, min=00.0050, mean=01.0030} policy_loss=-11.6606 policy updated! \n",
      "train step 12337 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5128 diff={max=07.2698, min=00.0316, mean=01.4514} policy_loss=-12.8961 policy updated! \n",
      "train step 12338 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7443 diff={max=06.3192, min=00.0162, mean=01.4137} policy_loss=-11.6855 policy updated! \n",
      "train step 12339 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.8718 diff={max=14.2979, min=00.0160, mean=01.4667} policy_loss=-16.0704 policy updated! \n",
      "train step 12340 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.1821 diff={max=10.3275, min=00.0162, mean=01.5062} policy_loss=-12.5029 policy updated! \n",
      "train step 12341 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.9190 diff={max=06.6188, min=00.0317, mean=01.0747} policy_loss=-12.7184 policy updated! \n",
      "train step 12342 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0989 diff={max=11.4367, min=00.0120, mean=01.5316} policy_loss=-12.5177 policy updated! \n",
      "train step 12343 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=08.2609 diff={max=11.9974, min=00.0095, mean=01.5896} policy_loss=-11.2696 policy updated! \n",
      "train step 12344 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.6768 diff={max=12.7233, min=00.0124, mean=01.7234} policy_loss=-13.3043 policy updated! \n",
      "train step 12345 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.8951 diff={max=10.7092, min=00.0165, mean=01.2617} policy_loss=-13.2474 policy updated! \n",
      "train step 12346 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=17.9897 diff={max=18.3650, min=00.0052, mean=02.2754} policy_loss=-14.8292 policy updated! \n",
      "train step 12347 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1815 diff={max=07.7214, min=00.0007, mean=01.4134} policy_loss=-9.8702 policy updated! \n",
      "train step 12348 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7850 diff={max=10.4546, min=00.0036, mean=01.1124} policy_loss=-10.8832 policy updated! \n",
      "train step 12349 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.2513 diff={max=09.0642, min=00.0108, mean=01.7420} policy_loss=-12.4966 policy updated! \n",
      "train step 12350 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.0290 diff={max=09.1404, min=00.0046, mean=01.7328} policy_loss=-15.0052 policy updated! \n",
      "train step 12351 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.4074 diff={max=08.7098, min=00.0005, mean=01.2971} policy_loss=-13.2538 policy updated! \n",
      "train step 12352 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8919 diff={max=10.8952, min=00.0642, mean=01.4451} policy_loss=-14.8955 policy updated! \n",
      "train step 12353 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.5332 diff={max=06.2613, min=00.0791, mean=01.2518} policy_loss=-14.8712 policy updated! \n",
      "train step 12354 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.2154 diff={max=08.8217, min=00.0295, mean=01.6786} policy_loss=-16.8461 policy updated! \n",
      "train step 12355 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.2588 diff={max=12.9316, min=00.0194, mean=01.3833} policy_loss=-11.0032 policy updated! \n",
      "train step 12356 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=08.7651 diff={max=12.7604, min=00.0092, mean=01.6551} policy_loss=-14.5825 policy updated! \n",
      "train step 12357 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5807 diff={max=05.6862, min=00.0461, mean=01.1104} policy_loss=-12.3900 policy updated! \n",
      "train step 12358 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1638 diff={max=05.2040, min=00.0250, mean=01.3375} policy_loss=-11.8421 policy updated! \n",
      "train step 12359 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9210 diff={max=11.1868, min=00.0075, mean=01.1201} policy_loss=-9.8381 policy updated! \n",
      "train step 12360 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.4926 diff={max=09.7061, min=00.0446, mean=01.5747} policy_loss=-10.6224 policy updated! \n",
      "train step 12361 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=08.3353 diff={max=10.1541, min=00.0036, mean=01.7690} policy_loss=-14.0073 policy updated! \n",
      "train step 12362 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.8338 diff={max=12.0098, min=00.0027, mean=01.5872} policy_loss=-12.2417 policy updated! \n",
      "train step 12363 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.9106 diff={max=11.0649, min=00.0156, mean=01.9347} policy_loss=-13.5517 policy updated! \n",
      "train step 12364 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5714 diff={max=09.1267, min=00.0335, mean=01.5898} policy_loss=-14.4481 policy updated! \n",
      "train step 12365 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.2925 diff={max=08.1978, min=00.0266, mean=01.3685} policy_loss=-15.2600 policy updated! \n",
      "train step 12366 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=11.2471 diff={max=16.3232, min=00.1035, mean=01.7308} policy_loss=-15.2128 policy updated! \n",
      "train step 12367 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.0279 diff={max=13.5415, min=00.0303, mean=02.2781} policy_loss=-15.2536 policy updated! \n",
      "train step 12368 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3678 diff={max=11.2667, min=00.0020, mean=01.8291} policy_loss=-17.8242 policy updated! \n",
      "train step 12369 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6864 diff={max=06.7447, min=00.0028, mean=01.6760} policy_loss=-15.0736 policy updated! \n",
      "train step 12370 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.9063 diff={max=06.3449, min=00.0250, mean=01.5397} policy_loss=-14.0836 policy updated! \n",
      "train step 12371 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.8127 diff={max=06.0972, min=00.1503, mean=01.2795} policy_loss=-14.3361 policy updated! \n",
      "train step 12372 reward={max=08.0000, min=07.0000, mean=07.2000} optimizing loss=04.9578 diff={max=09.5627, min=00.0159, mean=01.3141} policy_loss=-16.6959 policy updated! \n",
      "train step 12373 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=09.5370 diff={max=16.2497, min=00.0558, mean=01.5621} policy_loss=-10.7001 policy updated! \n",
      "train step 12374 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.2263 diff={max=04.8657, min=00.0060, mean=01.0333} policy_loss=-14.3752 policy updated! \n",
      "train step 12375 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.7297 diff={max=12.8212, min=00.0003, mean=01.5535} policy_loss=-12.5311 policy updated! \n",
      "train step 12376 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3491 diff={max=08.8045, min=00.0009, mean=01.1874} policy_loss=-13.9915 policy updated! \n",
      "train step 12377 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.1213 diff={max=11.2090, min=00.0133, mean=01.6281} policy_loss=-13.4364 policy updated! \n",
      "train step 12378 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8506 diff={max=06.4952, min=00.0659, mean=01.2862} policy_loss=-14.7130 policy updated! \n",
      "train step 12379 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.6194 diff={max=07.1692, min=00.0312, mean=01.2912} policy_loss=-14.7327 policy updated! \n",
      "train step 12380 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=10.1865 diff={max=13.1830, min=00.0061, mean=01.9835} policy_loss=-12.2905 policy updated! \n",
      "train step 12381 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.2141 diff={max=11.9540, min=00.0653, mean=01.4022} policy_loss=-11.4288 policy updated! \n",
      "train step 12382 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1128 diff={max=07.3942, min=00.0100, mean=01.1795} policy_loss=-10.9791 policy updated! \n",
      "train step 12383 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2596 diff={max=07.1131, min=00.0572, mean=01.3418} policy_loss=-9.8168 policy updated! \n",
      "train step 12384 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=08.4004 diff={max=12.0162, min=00.0083, mean=01.7492} policy_loss=-12.6604 policy updated! \n",
      "train step 12385 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=12.0417 diff={max=17.8964, min=00.0416, mean=01.7640} policy_loss=-12.2244 policy updated! \n",
      "train step 12386 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.2292 diff={max=17.2431, min=00.0006, mean=01.6730} policy_loss=-15.9911 policy updated! \n",
      "train step 12387 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.9430 diff={max=14.7571, min=00.0041, mean=02.0259} policy_loss=-13.5475 policy updated! \n",
      "train step 12388 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=10.5272 diff={max=10.3022, min=00.0236, mean=02.0310} policy_loss=-15.2841 policy updated! \n",
      "train step 12389 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4795 diff={max=09.1118, min=00.0038, mean=01.6282} policy_loss=-13.2771 policy updated! \n",
      "train step 12390 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.8474 diff={max=08.3324, min=00.0274, mean=01.7141} policy_loss=-13.8260 policy updated! \n",
      "train step 12391 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.6156 diff={max=07.3170, min=00.0258, mean=01.6971} policy_loss=-13.5625 policy updated! \n",
      "train step 12392 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=24.0352 diff={max=31.9792, min=00.0086, mean=01.8294} policy_loss=-10.6492 policy updated! \n",
      "train step 12393 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.0972 diff={max=08.8753, min=00.0212, mean=01.9473} policy_loss=-11.3178 policy updated! \n",
      "train step 12394 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7563 diff={max=10.4405, min=00.0648, mean=01.5086} policy_loss=-11.3422 policy updated! \n",
      "train step 12395 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.5058 diff={max=07.7130, min=00.0091, mean=01.2090} policy_loss=-11.4187 policy updated! \n",
      "train step 12396 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.9188 diff={max=09.8514, min=00.1474, mean=01.8311} policy_loss=-12.3096 policy updated! \n",
      "train step 12397 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=15.0030 diff={max=12.6994, min=00.0041, mean=02.6927} policy_loss=-12.7141 policy updated! \n",
      "train step 12398 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5465 diff={max=11.0876, min=00.0043, mean=01.6504} policy_loss=-11.9382 policy updated! \n",
      "train step 12399 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6190 diff={max=05.3503, min=00.0164, mean=01.0666} policy_loss=-13.3190 policy updated! \n",
      "train step 12400 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.2706 diff={max=10.8067, min=00.0032, mean=01.5924} policy_loss=-12.9546 policy updated! \n",
      "train step 12401 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8785 diff={max=07.6750, min=00.0215, mean=01.3727} policy_loss=-14.3488 policy updated! \n",
      "train step 12402 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1531 diff={max=08.2370, min=00.1062, mean=01.3411} policy_loss=-11.7039 policy updated! \n",
      "train step 12403 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8245 diff={max=06.0921, min=00.0019, mean=01.1206} policy_loss=-15.5049 policy updated! \n",
      "train step 12404 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=08.8473 diff={max=10.3708, min=00.0188, mean=01.7084} policy_loss=-11.9939 policy updated! \n",
      "train step 12405 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.3517 diff={max=07.7695, min=00.0360, mean=01.6300} policy_loss=-13.4812 policy updated! \n",
      "train step 12406 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=07.5221 diff={max=10.3192, min=00.1286, mean=01.9440} policy_loss=-16.0711 policy updated! \n",
      "train step 12407 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.6919 diff={max=09.8088, min=00.0075, mean=02.0308} policy_loss=-13.5752 policy updated! \n",
      "train step 12408 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7858 diff={max=13.3698, min=00.0004, mean=01.4484} policy_loss=-12.1568 policy updated! \n",
      "train step 12409 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9383 diff={max=10.4898, min=00.0067, mean=01.5614} policy_loss=-10.5713 policy updated! \n",
      "train step 12410 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=10.3954 diff={max=14.2363, min=00.0114, mean=01.7864} policy_loss=-13.0513 policy updated! \n",
      "train step 12411 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=07.3600 diff={max=09.1638, min=00.0175, mean=01.7790} policy_loss=-16.0333 policy updated! \n",
      "train step 12412 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.4747 diff={max=11.1496, min=00.0114, mean=01.6633} policy_loss=-15.3218 policy updated! \n",
      "train step 12413 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3212 diff={max=08.1732, min=00.0229, mean=01.3982} policy_loss=-14.1693 policy updated! \n",
      "train step 12414 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=09.6510 diff={max=09.4446, min=00.0075, mean=01.8750} policy_loss=-15.3045 policy updated! \n",
      "train step 12415 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.0722 diff={max=07.0480, min=00.0627, mean=01.3235} policy_loss=-11.5153 policy updated! \n",
      "train step 12416 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.2825 diff={max=12.5606, min=00.0018, mean=02.0992} policy_loss=-12.5382 policy updated! \n",
      "train step 12417 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.9288 diff={max=09.4874, min=00.0363, mean=01.8439} policy_loss=-17.4943 policy updated! \n",
      "train step 12418 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2873 diff={max=07.2887, min=00.0511, mean=01.4898} policy_loss=-13.4429 policy updated! \n",
      "train step 12419 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.7712 diff={max=14.8423, min=00.0005, mean=01.8850} policy_loss=-12.7070 policy updated! \n",
      "train step 12420 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.8916 diff={max=06.9237, min=00.0000, mean=01.5404} policy_loss=-11.6496 policy updated! \n",
      "train step 12421 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.4424 diff={max=09.0587, min=00.0030, mean=01.8145} policy_loss=-15.5273 policy updated! \n",
      "train step 12422 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3335 diff={max=09.7433, min=00.0087, mean=01.6023} policy_loss=-12.5367 policy updated! \n",
      "train step 12423 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.5844 diff={max=05.8467, min=00.0006, mean=01.3031} policy_loss=-11.1407 policy updated! \n",
      "train step 12424 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.1566 diff={max=07.3622, min=00.0047, mean=01.3291} policy_loss=-12.9113 policy updated! \n",
      "train step 12425 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=10.2565 diff={max=15.0506, min=00.0197, mean=01.8323} policy_loss=-14.2909 policy updated! \n",
      "train step 12426 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.4517 diff={max=09.5474, min=00.1225, mean=01.3403} policy_loss=-11.2272 policy updated! \n",
      "train step 12427 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3183 diff={max=09.6322, min=00.0028, mean=01.4833} policy_loss=-12.8480 policy updated! \n",
      "train step 12428 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3717 diff={max=08.9850, min=00.0297, mean=01.3951} policy_loss=-13.0443 policy updated! \n",
      "train step 12429 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.4620 diff={max=08.7545, min=00.0050, mean=01.2805} policy_loss=-10.9458 policy updated! \n",
      "train step 12430 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=17.7870 diff={max=18.0141, min=00.0209, mean=01.8494} policy_loss=-11.0460 policy updated! \n",
      "train step 12431 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=08.1194 diff={max=09.2407, min=00.0009, mean=01.8123} policy_loss=-13.0029 policy updated! \n",
      "train step 12432 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.4927 diff={max=07.9583, min=00.0225, mean=01.8172} policy_loss=-12.3855 policy updated! \n",
      "train step 12433 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1795 diff={max=10.1740, min=00.0446, mean=01.3799} policy_loss=-13.6695 policy updated! \n",
      "train step 12434 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0136 diff={max=06.7429, min=00.0059, mean=01.3716} policy_loss=-11.9621 policy updated! \n",
      "train step 12435 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.4769 diff={max=09.2229, min=00.0214, mean=01.5913} policy_loss=-11.3529 policy updated! \n",
      "train step 12436 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5648 diff={max=07.1847, min=00.0334, mean=01.3656} policy_loss=-13.1195 policy updated! \n",
      "train step 12437 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.0764 diff={max=09.3590, min=00.0163, mean=01.9008} policy_loss=-12.5861 policy updated! \n",
      "train step 12438 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4945 diff={max=07.5825, min=00.0052, mean=01.5848} policy_loss=-11.1878 policy updated! \n",
      "train step 12439 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.4667 diff={max=06.7441, min=00.0064, mean=01.4173} policy_loss=-12.2507 policy updated! \n",
      "train step 12440 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.9387 diff={max=09.8945, min=00.0156, mean=01.4967} policy_loss=-12.1950 policy updated! \n",
      "train step 12441 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=13.8578 diff={max=13.5591, min=00.0137, mean=02.2179} policy_loss=-16.1637 policy updated! \n",
      "train step 12442 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1931 diff={max=09.5192, min=00.0052, mean=01.6972} policy_loss=-10.9772 policy updated! \n",
      "train step 12443 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8097 diff={max=07.8313, min=00.0177, mean=01.2775} policy_loss=-12.3003 policy updated! \n",
      "train step 12444 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6209 diff={max=06.3168, min=00.0430, mean=01.3078} policy_loss=-12.0548 policy updated! \n",
      "train step 12445 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=11.4963 diff={max=17.5751, min=00.0277, mean=01.8361} policy_loss=-15.4984 policy updated! \n",
      "train step 12446 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2798 diff={max=09.6492, min=00.0608, mean=01.3962} policy_loss=-14.8928 policy updated! \n",
      "train step 12447 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1243 diff={max=08.8087, min=00.0306, mean=01.4111} policy_loss=-13.6380 policy updated! \n",
      "train step 12448 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=06.3075 diff={max=13.3747, min=00.0189, mean=01.4173} policy_loss=-11.8953 policy updated! \n",
      "train step 12449 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8334 diff={max=05.9203, min=00.0039, mean=01.0873} policy_loss=-13.4589 policy updated! \n",
      "train step 12450 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.6661 diff={max=06.8783, min=00.0142, mean=01.2105} policy_loss=-12.1114 policy updated! \n",
      "train step 12451 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7178 diff={max=07.1249, min=00.0005, mean=01.3407} policy_loss=-11.6228 policy updated! \n",
      "train step 12452 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5807 diff={max=09.1119, min=00.0016, mean=01.0842} policy_loss=-14.1841 policy updated! \n",
      "train step 12453 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9525 diff={max=08.7006, min=00.0263, mean=01.6107} policy_loss=-13.3354 policy updated! \n",
      "train step 12454 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.7970 diff={max=13.6594, min=00.0060, mean=01.6995} policy_loss=-12.0672 policy updated! \n",
      "train step 12455 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.0464 diff={max=05.6179, min=00.0063, mean=01.1191} policy_loss=-12.5360 policy updated! \n",
      "train step 12456 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=09.3124 diff={max=14.2517, min=00.0249, mean=01.7081} policy_loss=-11.9568 policy updated! \n",
      "train step 12457 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.5498 diff={max=11.2884, min=00.0070, mean=02.1431} policy_loss=-16.5477 policy updated! \n",
      "train step 12458 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.1580 diff={max=04.6771, min=00.0085, mean=01.0092} policy_loss=-13.4467 policy updated! \n",
      "train step 12459 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1139 diff={max=07.4762, min=00.0373, mean=01.8525} policy_loss=-13.2527 policy updated! \n",
      "train step 12460 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=04.5816 diff={max=06.2735, min=00.0165, mean=01.3273} policy_loss=-12.2983 policy updated! \n",
      "train step 12461 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2264 diff={max=10.1040, min=00.0197, mean=01.3339} policy_loss=-13.0102 policy updated! \n",
      "train step 12462 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5223 diff={max=06.9453, min=00.0089, mean=01.2440} policy_loss=-14.3570 policy updated! \n",
      "train step 12463 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=07.5184 diff={max=07.1855, min=00.0819, mean=01.8966} policy_loss=-15.9414 policy updated! \n",
      "train step 12464 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6047 diff={max=06.7799, min=00.0542, mean=01.3189} policy_loss=-12.9573 policy updated! \n",
      "train step 12465 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.9869 diff={max=07.1254, min=00.0013, mean=01.3116} policy_loss=-15.0460 policy updated! \n",
      "train step 12466 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.3937 diff={max=06.4330, min=00.0348, mean=01.1939} policy_loss=-14.1847 policy updated! \n",
      "train step 12467 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6153 diff={max=07.3388, min=00.0521, mean=01.3903} policy_loss=-16.1357 policy updated! \n",
      "train step 12468 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7817 diff={max=07.9983, min=00.0273, mean=01.3368} policy_loss=-16.7060 policy updated! \n",
      "train step 12469 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=10.7137 diff={max=12.9804, min=00.0059, mean=01.8994} policy_loss=-12.3555 policy updated! \n",
      "train step 12470 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.6293 diff={max=09.9484, min=00.0155, mean=01.4107} policy_loss=-10.8323 policy updated! \n",
      "train step 12471 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9196 diff={max=06.1724, min=00.0219, mean=01.1413} policy_loss=-12.9232 policy updated! \n",
      "train step 12472 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0467 diff={max=06.0493, min=00.0348, mean=01.1396} policy_loss=-10.7572 policy updated! \n",
      "train step 12473 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9967 diff={max=11.4421, min=00.0460, mean=01.1796} policy_loss=-10.4348 policy updated! \n",
      "train step 12474 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3722 diff={max=06.4763, min=00.0568, mean=01.0484} policy_loss=-12.9617 policy updated! \n",
      "train step 12475 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.7326 diff={max=13.1465, min=00.0076, mean=01.5660} policy_loss=-16.0702 policy updated! \n",
      "train step 12476 reward={max=07.0000, min=00.0000, mean=04.0000} optimizing loss=08.3381 diff={max=10.9525, min=00.0338, mean=01.6295} policy_loss=-13.7806 policy updated! \n",
      "train step 12477 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8071 diff={max=05.8277, min=00.0033, mean=01.1076} policy_loss=-14.6517 policy updated! \n",
      "train step 12478 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4832 diff={max=07.2259, min=00.0396, mean=01.3305} policy_loss=-13.2581 policy updated! \n",
      "train step 12479 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2344 diff={max=10.4936, min=00.0056, mean=01.3993} policy_loss=-15.2874 policy updated! \n",
      "train step 12480 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.2920 diff={max=07.4428, min=00.0172, mean=01.2659} policy_loss=-12.7457 policy updated! \n",
      "train step 12481 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=10.0188 diff={max=14.2990, min=00.0621, mean=01.9225} policy_loss=-13.0406 policy updated! \n",
      "train step 12482 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9953 diff={max=09.8120, min=00.0334, mean=01.5366} policy_loss=-10.4986 policy updated! \n",
      "train step 12483 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.6807 diff={max=15.3066, min=00.0126, mean=01.6498} policy_loss=-15.1213 policy updated! \n",
      "train step 12484 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0144 diff={max=10.8851, min=00.0438, mean=01.3364} policy_loss=-10.5383 policy updated! \n",
      "train step 12485 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=10.5194 diff={max=10.1250, min=00.0057, mean=02.1900} policy_loss=-16.3695 policy updated! \n",
      "train step 12486 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.5938 diff={max=12.2671, min=00.0140, mean=01.4396} policy_loss=-13.4411 policy updated! \n",
      "train step 12487 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3523 diff={max=08.8792, min=00.0159, mean=01.3614} policy_loss=-16.2902 policy updated! \n",
      "train step 12488 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=06.8947 diff={max=10.9078, min=00.0124, mean=01.5377} policy_loss=-13.6854 policy updated! \n",
      "train step 12489 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9941 diff={max=08.0158, min=00.0412, mean=01.4289} policy_loss=-15.6810 policy updated! \n",
      "train step 12490 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.3367 diff={max=09.3790, min=00.0182, mean=01.8960} policy_loss=-14.9931 policy updated! \n",
      "train step 12491 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.3313 diff={max=14.9596, min=00.0448, mean=02.2485} policy_loss=-14.1243 policy updated! \n",
      "train step 12492 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4214 diff={max=08.1973, min=00.0328, mean=01.6167} policy_loss=-15.3679 policy updated! \n",
      "train step 12493 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.0338 diff={max=13.7338, min=00.0387, mean=01.8396} policy_loss=-15.9589 policy updated! \n",
      "train step 12494 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5191 diff={max=06.1535, min=00.0122, mean=01.4240} policy_loss=-13.8933 policy updated! \n",
      "train step 12495 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.1603 diff={max=07.3765, min=00.0166, mean=01.3668} policy_loss=-11.6384 policy updated! \n",
      "train step 12496 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.1942 diff={max=08.1263, min=00.0152, mean=01.4264} policy_loss=-11.7155 policy updated! \n",
      "train step 12497 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.2332 diff={max=16.9296, min=00.0078, mean=01.9587} policy_loss=-11.9177 policy updated! \n",
      "train step 12498 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9263 diff={max=09.1300, min=00.0072, mean=01.4113} policy_loss=-11.4095 policy updated! \n",
      "train step 12499 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.2791 diff={max=12.8975, min=00.0205, mean=01.6983} policy_loss=-16.0799 policy updated! \n",
      "train step 12500 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.1795 diff={max=11.3929, min=00.0101, mean=01.4406} policy_loss=-12.8857 policy updated! \n",
      "train step 12501 reward={max=07.0000, min=00.0000, mean=04.0000} optimizing loss=14.6633 diff={max=19.1649, min=00.0068, mean=02.1578} policy_loss=-15.2529 policy updated! \n",
      "train step 12502 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1629 diff={max=09.5405, min=00.0082, mean=01.6372} policy_loss=-16.2120 policy updated! \n",
      "train step 12503 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1065 diff={max=07.2562, min=00.0199, mean=01.4081} policy_loss=-11.5619 policy updated! \n",
      "train step 12504 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2921 diff={max=08.3252, min=00.0267, mean=01.7156} policy_loss=-12.0336 policy updated! \n",
      "train step 12505 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.8242 diff={max=09.4209, min=00.0080, mean=01.3697} policy_loss=-15.8693 policy updated! \n",
      "train step 12506 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2505 diff={max=07.9306, min=00.0056, mean=01.5444} policy_loss=-16.7495 policy updated! \n",
      "train step 12507 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7645 diff={max=10.2587, min=00.0125, mean=01.1388} policy_loss=-13.3555 policy updated! \n",
      "train step 12508 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3200 diff={max=07.0210, min=00.0053, mean=01.1456} policy_loss=-10.3411 policy updated! \n",
      "train step 12509 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=11.2903 diff={max=12.7206, min=00.0025, mean=02.1008} policy_loss=-14.4094 policy updated! \n",
      "train step 12510 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.8301 diff={max=11.2827, min=00.0355, mean=01.5549} policy_loss=-15.7250 policy updated! \n",
      "train step 12511 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=08.1332 diff={max=12.8771, min=00.0696, mean=01.5594} policy_loss=-12.3457 policy updated! \n",
      "train step 12512 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9767 diff={max=08.3961, min=00.0102, mean=01.5405} policy_loss=-14.7829 policy updated! \n",
      "train step 12513 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6706 diff={max=08.8711, min=00.0240, mean=01.3972} policy_loss=-13.0311 policy updated! \n",
      "train step 12514 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4700 diff={max=09.8434, min=00.0014, mean=01.5596} policy_loss=-12.5394 policy updated! \n",
      "train step 12515 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.4496 diff={max=06.6821, min=00.0334, mean=01.6504} policy_loss=-13.1499 policy updated! \n",
      "train step 12516 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.4856 diff={max=12.8139, min=00.0086, mean=02.0349} policy_loss=-14.0166 policy updated! \n",
      "train step 12517 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.2414 diff={max=11.0313, min=00.0279, mean=01.4506} policy_loss=-13.1758 policy updated! \n",
      "train step 12518 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6707 diff={max=06.0995, min=00.0300, mean=01.5287} policy_loss=-12.6774 policy updated! \n",
      "train step 12519 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1786 diff={max=07.8652, min=00.0102, mean=01.1129} policy_loss=-13.1698 policy updated! \n",
      "train step 12520 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.0942 diff={max=07.8209, min=00.0460, mean=01.2476} policy_loss=-10.0640 policy updated! \n",
      "train step 12521 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=08.4481 diff={max=11.9323, min=00.0140, mean=01.7812} policy_loss=-12.5443 policy updated! \n",
      "train step 12522 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.7072 diff={max=08.8205, min=00.0046, mean=02.1239} policy_loss=-14.6771 policy updated! \n",
      "train step 12523 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=09.2447 diff={max=12.7975, min=00.0073, mean=01.7131} policy_loss=-11.1276 policy updated! \n",
      "train step 12524 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=16.5573 diff={max=22.7022, min=00.0096, mean=01.8233} policy_loss=-14.3634 policy updated! \n",
      "train step 12525 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=21.1882 diff={max=23.4389, min=00.0231, mean=02.0821} policy_loss=-12.7494 policy updated! \n",
      "train step 12526 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=25.8801 diff={max=29.9628, min=00.1202, mean=02.3176} policy_loss=-15.0477 policy updated! \n",
      "train step 12527 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=14.4845 diff={max=15.6720, min=00.0030, mean=02.1579} policy_loss=-16.0558 policy updated! \n",
      "train step 12528 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6186 diff={max=10.5123, min=00.0231, mean=01.6103} policy_loss=-12.3147 policy updated! \n",
      "train step 12529 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=10.1737 diff={max=09.8487, min=00.0496, mean=02.1792} policy_loss=-15.9475 policy updated! \n",
      "train step 12530 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.7486 diff={max=12.9256, min=00.0251, mean=01.7466} policy_loss=-12.6477 policy updated! \n",
      "train step 12531 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.1923 diff={max=09.1098, min=00.0298, mean=01.3382} policy_loss=-14.6281 policy updated! \n",
      "train step 12532 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2310 diff={max=05.7628, min=00.0098, mean=01.2175} policy_loss=-12.3405 policy updated! \n",
      "train step 12533 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4257 diff={max=07.4984, min=00.0124, mean=01.5242} policy_loss=-13.1689 policy updated! \n",
      "train step 12534 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=08.4790 diff={max=09.7851, min=00.0874, mean=02.0635} policy_loss=-15.7000 policy updated! \n",
      "train step 12535 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=05.3353 diff={max=08.1747, min=00.0963, mean=01.5253} policy_loss=-11.8050 policy updated! \n",
      "train step 12536 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.9328 diff={max=07.0079, min=00.0104, mean=01.6405} policy_loss=-10.0711 policy updated! \n",
      "train step 12537 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.6458 diff={max=08.3804, min=00.0136, mean=01.9285} policy_loss=-17.2306 policy updated! \n",
      "train step 12538 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=10.0787 diff={max=11.0503, min=00.0110, mean=02.0539} policy_loss=-16.3350 policy updated! \n",
      "train step 12539 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=09.3321 diff={max=12.1595, min=00.0091, mean=01.8960} policy_loss=-13.5538 policy updated! \n",
      "train step 12540 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=04.4908 diff={max=08.7565, min=00.0162, mean=01.3084} policy_loss=-12.4199 policy updated! \n",
      "train step 12541 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9885 diff={max=07.4181, min=00.0260, mean=01.3310} policy_loss=-13.8032 policy updated! \n",
      "train step 12542 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1009 diff={max=11.4294, min=00.0256, mean=01.2581} policy_loss=-13.4559 policy updated! \n",
      "train step 12543 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.3171 diff={max=15.0111, min=00.0431, mean=01.9001} policy_loss=-13.3799 policy updated! \n",
      "train step 12544 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6099 diff={max=08.3744, min=00.0222, mean=01.3603} policy_loss=-14.1020 policy updated! \n",
      "train step 12545 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=13.8116 diff={max=21.3042, min=00.0004, mean=01.8087} policy_loss=-12.7229 policy updated! \n",
      "train step 12546 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.0880 diff={max=13.1745, min=00.0751, mean=02.1364} policy_loss=-14.3408 policy updated! \n",
      "train step 12547 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.1268 diff={max=17.0181, min=00.0050, mean=02.0209} policy_loss=-12.5228 policy updated! \n",
      "train step 12548 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5039 diff={max=09.5087, min=00.0026, mean=01.3955} policy_loss=-13.0130 policy updated! \n",
      "train step 12549 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.0773 diff={max=06.7834, min=00.0046, mean=01.3820} policy_loss=-15.9196 policy updated! \n",
      "train step 12550 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.3005 diff={max=09.8007, min=00.0062, mean=01.4534} policy_loss=-13.0469 policy updated! \n",
      "train step 12551 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.6835 diff={max=10.6356, min=00.0114, mean=01.8617} policy_loss=-13.8722 policy updated! \n",
      "train step 12552 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8944 diff={max=09.0098, min=00.0023, mean=01.5386} policy_loss=-13.6076 policy updated! \n",
      "train step 12553 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.5972 diff={max=08.7490, min=00.0237, mean=01.9028} policy_loss=-12.5181 policy updated! \n",
      "train step 12554 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.4569 diff={max=09.6680, min=00.0082, mean=01.9137} policy_loss=-16.3571 policy updated! \n",
      "train step 12555 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.3020 diff={max=07.7718, min=00.0002, mean=01.6407} policy_loss=-16.6508 policy updated! \n",
      "train step 12556 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.4873 diff={max=10.9571, min=00.0059, mean=01.7775} policy_loss=-14.4638 policy updated! \n",
      "train step 12557 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.2712 diff={max=11.8996, min=00.0052, mean=02.0599} policy_loss=-11.9645 policy updated! \n",
      "train step 12558 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=06.1579 diff={max=11.6977, min=00.0057, mean=01.3485} policy_loss=-11.1999 policy updated! \n",
      "train step 12559 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7983 diff={max=05.4576, min=00.0141, mean=01.3333} policy_loss=-12.8439 policy updated! \n",
      "train step 12560 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=15.5153 diff={max=13.3288, min=00.0001, mean=02.2282} policy_loss=-12.6918 policy updated! \n",
      "train step 12561 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7198 diff={max=09.7627, min=00.0560, mean=01.6581} policy_loss=-12.4796 policy updated! \n",
      "train step 12562 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.0926 diff={max=13.8094, min=00.0194, mean=01.9151} policy_loss=-12.3990 policy updated! \n",
      "train step 12563 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2060 diff={max=06.6279, min=00.0144, mean=01.1912} policy_loss=-11.9927 policy updated! \n",
      "train step 12564 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6231 diff={max=06.8851, min=00.0447, mean=01.5171} policy_loss=-15.8658 policy updated! \n",
      "train step 12565 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.3371 diff={max=08.0075, min=00.0349, mean=01.7766} policy_loss=-15.1709 policy updated! \n",
      "train step 12566 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.9104 diff={max=13.1893, min=00.0013, mean=01.6533} policy_loss=-13.0844 policy updated! \n",
      "train step 12567 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.3716 diff={max=11.2780, min=00.0069, mean=01.7523} policy_loss=-13.9978 policy updated! \n",
      "train step 12568 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.9005 diff={max=06.3762, min=00.0018, mean=01.3548} policy_loss=-11.7430 policy updated! \n",
      "train step 12569 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.2728 diff={max=08.2699, min=00.0025, mean=01.7537} policy_loss=-14.5389 policy updated! \n",
      "train step 12570 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.3810 diff={max=07.8564, min=00.0158, mean=01.4848} policy_loss=-14.1829 policy updated! \n",
      "train step 12571 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9410 diff={max=07.1609, min=00.0011, mean=01.5043} policy_loss=-15.1746 policy updated! \n",
      "train step 12572 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7426 diff={max=10.8691, min=00.0201, mean=01.4299} policy_loss=-11.2494 policy updated! \n",
      "train step 12573 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=25.1267 diff={max=33.7916, min=00.0048, mean=01.6849} policy_loss=-12.8783 policy updated! \n",
      "train step 12574 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0753 diff={max=05.5869, min=00.0074, mean=01.2362} policy_loss=-11.7694 policy updated! \n",
      "train step 12575 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.6165 diff={max=12.6894, min=00.0320, mean=01.6220} policy_loss=-15.9768 policy updated! \n",
      "train step 12576 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.8302 diff={max=08.1451, min=00.0677, mean=01.4673} policy_loss=-13.4294 policy updated! \n",
      "train step 12577 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2650 diff={max=06.2479, min=00.1013, mean=01.3197} policy_loss=-12.5749 policy updated! \n",
      "train step 12578 reward={max=07.0000, min=00.0000, mean=02.6000} optimizing loss=03.6661 diff={max=06.5784, min=00.0131, mean=01.1747} policy_loss=-10.7435 policy updated! \n",
      "train step 12579 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.5874 diff={max=17.2041, min=00.0139, mean=01.9510} policy_loss=-14.7304 policy updated! \n",
      "train step 12580 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=29.1405 diff={max=31.4043, min=00.0406, mean=02.2875} policy_loss=-13.7032 policy updated! \n",
      "train step 12581 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.0725 diff={max=11.7124, min=00.0104, mean=01.9732} policy_loss=-12.2714 policy updated! \n",
      "train step 12582 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.7351 diff={max=10.9978, min=00.0269, mean=01.8967} policy_loss=-13.3549 policy updated! \n",
      "train step 12583 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7938 diff={max=07.7425, min=00.1371, mean=01.6310} policy_loss=-12.0985 policy updated! \n",
      "train step 12584 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.4218 diff={max=08.4635, min=00.0333, mean=01.4031} policy_loss=-12.0198 policy updated! \n",
      "train step 12585 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.2059 diff={max=04.4913, min=00.0019, mean=01.1531} policy_loss=-13.4628 policy updated! \n",
      "train step 12586 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=19.8511 diff={max=21.8472, min=00.0038, mean=02.2413} policy_loss=-10.9682 policy updated! \n",
      "train step 12587 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1606 diff={max=08.2913, min=00.0159, mean=01.3555} policy_loss=-12.4474 policy updated! \n",
      "train step 12588 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4787 diff={max=06.9325, min=00.0343, mean=01.3642} policy_loss=-13.1655 policy updated! \n",
      "train step 12589 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9744 diff={max=06.6028, min=00.0181, mean=01.5177} policy_loss=-14.2389 policy updated! \n",
      "train step 12590 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.5245 diff={max=06.2102, min=00.0105, mean=01.2908} policy_loss=-13.3305 policy updated! \n",
      "train step 12591 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.0694 diff={max=13.5267, min=00.0193, mean=01.7894} policy_loss=-12.9133 policy updated! \n",
      "train step 12592 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.7716 diff={max=15.4061, min=00.0677, mean=01.4705} policy_loss=-13.9930 policy updated! \n",
      "train step 12593 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=10.3148 diff={max=14.2337, min=00.0040, mean=01.8102} policy_loss=-13.9485 policy updated! \n",
      "train step 12594 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.3032 diff={max=15.5372, min=00.0228, mean=01.5734} policy_loss=-12.8374 policy updated! \n",
      "train step 12595 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=10.4321 diff={max=12.5698, min=00.0267, mean=01.9276} policy_loss=-11.8282 policy updated! \n",
      "train step 12596 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0421 diff={max=05.3069, min=00.0240, mean=00.9778} policy_loss=-15.8867 policy updated! \n",
      "train step 12597 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0905 diff={max=10.6566, min=00.0051, mean=01.4630} policy_loss=-12.5264 policy updated! \n",
      "train step 12598 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.7833 diff={max=11.0175, min=00.0087, mean=01.3209} policy_loss=-15.3557 policy updated! \n",
      "train step 12599 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.9435 diff={max=07.2026, min=00.0093, mean=01.2849} policy_loss=-9.9596 policy updated! \n",
      "train step 12600 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.3463 diff={max=07.9096, min=00.0294, mean=01.6627} policy_loss=-12.5963 policy updated! \n",
      "train step 12601 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4327 diff={max=11.8653, min=00.0191, mean=01.3462} policy_loss=-12.7492 policy updated! \n",
      "train step 12602 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.7847 diff={max=12.8189, min=00.0193, mean=01.4533} policy_loss=-12.0352 policy updated! \n",
      "train step 12603 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2509 diff={max=05.9314, min=00.0169, mean=01.4825} policy_loss=-11.0222 policy updated! \n",
      "train step 12604 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.9618 diff={max=05.9900, min=00.0032, mean=01.2203} policy_loss=-12.1387 policy updated! \n",
      "train step 12605 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.7950 diff={max=13.0563, min=00.0286, mean=01.4705} policy_loss=-13.6785 policy updated! \n",
      "train step 12606 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=11.1695 diff={max=10.4854, min=00.0509, mean=02.0348} policy_loss=-16.1259 policy updated! \n",
      "train step 12607 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.4356 diff={max=12.1511, min=00.0058, mean=02.2903} policy_loss=-14.2160 policy updated! \n",
      "train step 12608 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7979 diff={max=07.2263, min=00.0271, mean=01.3440} policy_loss=-13.0327 policy updated! \n",
      "train step 12609 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.8782 diff={max=07.5847, min=00.0086, mean=01.2473} policy_loss=-14.4280 policy updated! \n",
      "train step 12610 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.6776 diff={max=05.6244, min=00.0529, mean=01.3206} policy_loss=-14.1551 policy updated! \n",
      "train step 12611 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6519 diff={max=06.5177, min=00.0420, mean=01.5579} policy_loss=-14.4934 policy updated! \n",
      "train step 12612 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.3410 diff={max=17.0210, min=00.0452, mean=01.8267} policy_loss=-11.5518 policy updated! \n",
      "train step 12613 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8185 diff={max=09.9280, min=00.0288, mean=01.3914} policy_loss=-10.9537 policy updated! \n",
      "train step 12614 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.2786 diff={max=17.9865, min=00.0161, mean=01.5702} policy_loss=-13.2250 policy updated! \n",
      "train step 12615 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.1231 diff={max=08.1927, min=00.0504, mean=01.4945} policy_loss=-13.9083 policy updated! \n",
      "train step 12616 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.0366 diff={max=13.9764, min=00.0603, mean=01.5761} policy_loss=-12.1793 policy updated! \n",
      "train step 12617 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4070 diff={max=05.3190, min=00.0038, mean=01.1362} policy_loss=-14.1529 policy updated! \n",
      "train step 12618 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=06.0117 diff={max=09.1857, min=00.0119, mean=01.6280} policy_loss=-15.6117 policy updated! \n",
      "train step 12619 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9067 diff={max=11.4935, min=00.0114, mean=00.9567} policy_loss=-10.8640 policy updated! \n",
      "train step 12620 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.3587 diff={max=06.9121, min=00.0087, mean=01.3424} policy_loss=-13.3188 policy updated! \n",
      "train step 12621 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.5057 diff={max=09.2608, min=00.0111, mean=01.5670} policy_loss=-13.1717 policy updated! \n",
      "train step 12622 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.1775 diff={max=08.7276, min=00.0269, mean=01.8629} policy_loss=-13.5676 policy updated! \n",
      "train step 12623 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=17.9779 diff={max=21.0538, min=00.0064, mean=02.0708} policy_loss=-13.1909 policy updated! \n",
      "train step 12624 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.7320 diff={max=11.2198, min=00.0493, mean=01.9586} policy_loss=-13.5002 policy updated! \n",
      "train step 12625 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.8243 diff={max=08.5350, min=00.0136, mean=01.2157} policy_loss=-11.5380 policy updated! \n",
      "train step 12626 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=07.8928 diff={max=08.5257, min=00.0066, mean=01.7830} policy_loss=-14.2074 policy updated! \n",
      "train step 12627 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5270 diff={max=07.3507, min=00.0012, mean=01.3732} policy_loss=-14.5140 policy updated! \n",
      "train step 12628 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5369 diff={max=06.0974, min=00.0007, mean=01.4371} policy_loss=-10.7746 policy updated! \n",
      "train step 12629 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3598 diff={max=08.1195, min=00.0061, mean=01.0942} policy_loss=-12.7999 policy updated! \n",
      "train step 12630 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.2315 diff={max=12.9919, min=00.0070, mean=01.7014} policy_loss=-15.2155 policy updated! \n",
      "train step 12631 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.2693 diff={max=10.9302, min=00.0022, mean=01.3114} policy_loss=-10.2515 policy updated! \n",
      "train step 12632 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.3345 diff={max=11.9384, min=00.0192, mean=01.9635} policy_loss=-13.8522 policy updated! \n",
      "train step 12633 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6629 diff={max=06.9318, min=00.0032, mean=01.0916} policy_loss=-12.8474 policy updated! \n",
      "train step 12634 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.6051 diff={max=09.1512, min=00.0356, mean=01.7819} policy_loss=-15.7603 policy updated! \n",
      "train step 12635 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.5737 diff={max=13.8926, min=00.0344, mean=01.5277} policy_loss=-14.2621 policy updated! \n",
      "train step 12636 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9626 diff={max=09.4701, min=00.0250, mean=00.9818} policy_loss=-11.4192 policy updated! \n",
      "train step 12637 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3267 diff={max=09.8197, min=00.0137, mean=01.6780} policy_loss=-11.8539 policy updated! \n",
      "train step 12638 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0890 diff={max=12.0194, min=00.0239, mean=01.5007} policy_loss=-13.2425 policy updated! \n",
      "train step 12639 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.4721 diff={max=14.8066, min=00.0043, mean=01.7787} policy_loss=-12.5648 policy updated! \n",
      "train step 12640 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.1155 diff={max=07.1907, min=00.0004, mean=01.0866} policy_loss=-13.7280 policy updated! \n",
      "train step 12641 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=16.3937 diff={max=14.9646, min=00.0354, mean=02.2760} policy_loss=-15.0142 policy updated! \n",
      "train step 12642 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4948 diff={max=08.5752, min=00.0151, mean=01.2700} policy_loss=-12.2232 policy updated! \n",
      "train step 12643 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1219 diff={max=05.7856, min=00.0034, mean=01.1464} policy_loss=-11.5416 policy updated! \n",
      "train step 12644 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.5425 diff={max=12.4822, min=00.0086, mean=01.7250} policy_loss=-13.3512 policy updated! \n",
      "train step 12645 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=10.9396 diff={max=17.4105, min=00.0278, mean=01.7872} policy_loss=-12.6816 policy updated! \n",
      "train step 12646 reward={max=07.0000, min=00.0000, mean=04.0000} optimizing loss=22.5673 diff={max=20.8673, min=00.0583, mean=02.3747} policy_loss=-16.9726 policy updated! \n",
      "train step 12647 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8669 diff={max=07.5439, min=00.0047, mean=01.2302} policy_loss=-12.6950 policy updated! \n",
      "train step 12648 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.3293 diff={max=10.7768, min=00.0022, mean=01.9249} policy_loss=-12.1420 policy updated! \n",
      "train step 12649 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6478 diff={max=13.7795, min=00.0389, mean=01.2564} policy_loss=-11.8532 policy updated! \n",
      "train step 12650 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.0914 diff={max=08.0230, min=00.0127, mean=01.2569} policy_loss=-10.3397 policy updated! \n",
      "train step 12651 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.3659 diff={max=07.0116, min=00.0070, mean=01.1224} policy_loss=-11.9414 policy updated! \n",
      "train step 12652 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.9030 diff={max=09.1159, min=00.0516, mean=01.9142} policy_loss=-16.3698 policy updated! \n",
      "train step 12653 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5727 diff={max=07.1758, min=00.0057, mean=01.4817} policy_loss=-11.9454 policy updated! \n",
      "train step 12654 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.5729 diff={max=10.7447, min=00.0311, mean=01.5669} policy_loss=-10.7671 policy updated! \n",
      "train step 12655 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=14.7607 diff={max=18.4383, min=00.0002, mean=02.1601} policy_loss=-13.3965 policy updated! \n",
      "train step 12656 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3309 diff={max=08.7677, min=00.0005, mean=01.3087} policy_loss=-10.7880 policy updated! \n",
      "train step 12657 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4722 diff={max=06.7251, min=00.0785, mean=01.1898} policy_loss=-10.8642 policy updated! \n",
      "train step 12658 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5197 diff={max=10.3280, min=00.0071, mean=01.4734} policy_loss=-13.1118 policy updated! \n",
      "train step 12659 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3723 diff={max=07.4822, min=00.0042, mean=01.5172} policy_loss=-13.3441 policy updated! \n",
      "train step 12660 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.4646 diff={max=06.5828, min=00.0169, mean=01.3429} policy_loss=-11.3013 policy updated! \n",
      "train step 12661 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=11.9353 diff={max=22.3302, min=00.0197, mean=01.4064} policy_loss=-12.6087 policy updated! \n",
      "train step 12662 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0766 diff={max=05.4185, min=00.0157, mean=01.2319} policy_loss=-12.7171 policy updated! \n",
      "train step 12663 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5586 diff={max=07.8026, min=00.0707, mean=01.2628} policy_loss=-12.6719 policy updated! \n",
      "train step 12664 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1447 diff={max=06.4327, min=00.0020, mean=00.9311} policy_loss=-12.0076 policy updated! \n",
      "train step 12665 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=15.9782 diff={max=19.4201, min=00.0079, mean=01.7423} policy_loss=-11.9411 policy updated! \n",
      "train step 12666 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.7761 diff={max=11.7427, min=00.0136, mean=01.7556} policy_loss=-15.1190 policy updated! \n",
      "train step 12667 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5001 diff={max=07.8291, min=00.0260, mean=01.7174} policy_loss=-17.6810 policy updated! \n",
      "train step 12668 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1751 diff={max=06.6470, min=00.0675, mean=01.5138} policy_loss=-12.0917 policy updated! \n",
      "train step 12669 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1551 diff={max=09.9498, min=00.0018, mean=00.9120} policy_loss=-10.4190 policy updated! \n",
      "train step 12670 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.9521 diff={max=10.1789, min=00.0495, mean=01.3485} policy_loss=-13.3498 policy updated! \n",
      "train step 12671 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.0907 diff={max=06.1487, min=00.0026, mean=01.3802} policy_loss=-12.8843 policy updated! \n",
      "train step 12672 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1766 diff={max=06.6054, min=00.0270, mean=01.4010} policy_loss=-12.7733 policy updated! \n",
      "train step 12673 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.3297 diff={max=14.6036, min=00.0113, mean=01.6302} policy_loss=-13.2364 policy updated! \n",
      "train step 12674 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9736 diff={max=10.8876, min=00.0117, mean=01.5901} policy_loss=-12.3295 policy updated! \n",
      "train step 12675 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=17.0454 diff={max=21.3722, min=00.0518, mean=01.9676} policy_loss=-14.3141 policy updated! \n",
      "train step 12676 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1576 diff={max=08.2659, min=00.0354, mean=01.2760} policy_loss=-14.9960 policy updated! \n",
      "train step 12677 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5999 diff={max=06.4524, min=00.0021, mean=01.2771} policy_loss=-12.8093 policy updated! \n",
      "train step 12678 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3611 diff={max=08.0550, min=00.0488, mean=01.6035} policy_loss=-13.5746 policy updated! \n",
      "train step 12679 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4213 diff={max=12.0395, min=00.0028, mean=01.4506} policy_loss=-11.5775 policy updated! \n",
      "train step 12680 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.0338 diff={max=09.8282, min=00.0027, mean=01.3689} policy_loss=-14.2388 policy updated! \n",
      "train step 12681 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9292 diff={max=05.9422, min=00.0347, mean=01.1210} policy_loss=-13.2220 policy updated! \n",
      "train step 12682 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6488 diff={max=08.5397, min=00.0137, mean=01.3499} policy_loss=-12.6523 policy updated! \n",
      "train step 12683 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.2608 diff={max=08.9830, min=00.0628, mean=01.8337} policy_loss=-13.1341 policy updated! \n",
      "train step 12684 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1667 diff={max=10.7951, min=00.0094, mean=01.6911} policy_loss=-11.9266 policy updated! \n",
      "train step 12685 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.9460 diff={max=09.3180, min=00.0229, mean=01.5247} policy_loss=-13.4399 policy updated! \n",
      "train step 12686 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=09.7021 diff={max=14.1636, min=00.0103, mean=01.7570} policy_loss=-13.6093 policy updated! \n",
      "train step 12687 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2316 diff={max=08.9809, min=00.0036, mean=01.2644} policy_loss=-10.6353 policy updated! \n",
      "train step 12688 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.5840 diff={max=08.1618, min=00.0124, mean=01.2094} policy_loss=-12.9259 policy updated! \n",
      "train step 12689 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=08.6718 diff={max=13.7027, min=00.0162, mean=01.5053} policy_loss=-14.4918 policy updated! \n",
      "train step 12690 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.6727 diff={max=06.8251, min=00.0166, mean=01.0332} policy_loss=-10.8870 policy updated! \n",
      "train step 12691 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.9365 diff={max=12.6495, min=00.0043, mean=01.6665} policy_loss=-11.0868 policy updated! \n",
      "train step 12692 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9951 diff={max=07.7706, min=00.0307, mean=01.6218} policy_loss=-14.3701 policy updated! \n",
      "train step 12693 reward={max=07.0000, min=00.0000, mean=02.6000} optimizing loss=07.6409 diff={max=11.8485, min=00.0053, mean=01.5130} policy_loss=-11.1726 policy updated! \n",
      "train step 12694 reward={max=07.0000, min=00.0000, mean=05.4000} optimizing loss=05.7807 diff={max=09.1489, min=00.0181, mean=01.4082} policy_loss=-10.4070 policy updated! \n",
      "train step 12695 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.2742 diff={max=09.0327, min=00.0008, mean=01.5303} policy_loss=-10.5717 policy updated! \n",
      "train step 12696 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=10.0009 diff={max=16.3241, min=00.1199, mean=01.5547} policy_loss=-14.0397 policy updated! \n",
      "train step 12697 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1787 diff={max=06.8171, min=00.0239, mean=01.1262} policy_loss=-12.2160 policy updated! \n",
      "train step 12698 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6377 diff={max=06.3913, min=00.0116, mean=01.4035} policy_loss=-13.2817 policy updated! \n",
      "train step 12699 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.3419 diff={max=15.4624, min=00.0027, mean=01.9710} policy_loss=-13.8481 policy updated! \n",
      "train step 12700 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.0285 diff={max=11.3638, min=00.0328, mean=01.3972} policy_loss=-14.5878 policy updated! \n",
      "train step 12701 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=14.1490 diff={max=18.0883, min=00.0209, mean=01.7458} policy_loss=-12.4102 policy updated! \n",
      "train step 12702 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.5457 diff={max=15.8288, min=00.0046, mean=01.7086} policy_loss=-15.1140 policy updated! \n",
      "train step 12703 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.9901 diff={max=10.5670, min=00.0017, mean=01.7245} policy_loss=-14.1052 policy updated! \n",
      "train step 12704 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=14.1550 diff={max=17.9668, min=00.0103, mean=02.0513} policy_loss=-16.0237 policy updated! \n",
      "train step 12705 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.9783 diff={max=05.9821, min=00.0271, mean=01.2931} policy_loss=-13.2115 policy updated! \n",
      "train step 12706 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.6828 diff={max=12.3346, min=00.0145, mean=01.6546} policy_loss=-12.3774 policy updated! \n",
      "train step 12707 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.8056 diff={max=12.5912, min=00.0400, mean=01.9083} policy_loss=-16.1206 policy updated! \n",
      "train step 12708 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.7194 diff={max=07.3019, min=00.0184, mean=01.4100} policy_loss=-14.8170 policy updated! \n",
      "train step 12709 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.7837 diff={max=15.0973, min=00.0252, mean=02.1567} policy_loss=-15.1634 policy updated! \n",
      "train step 12710 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.8234 diff={max=09.2442, min=00.0025, mean=01.4395} policy_loss=-12.5707 policy updated! \n",
      "train step 12711 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1491 diff={max=07.1603, min=00.0027, mean=01.3155} policy_loss=-11.3247 policy updated! \n",
      "train step 12712 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4434 diff={max=09.3058, min=00.0202, mean=01.6921} policy_loss=-13.8152 policy updated! \n",
      "train step 12713 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.0901 diff={max=06.1714, min=00.0016, mean=01.1645} policy_loss=-10.6296 policy updated! \n",
      "train step 12714 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5885 diff={max=07.9296, min=00.0030, mean=01.1101} policy_loss=-10.2518 policy updated! \n",
      "train step 12715 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.6589 diff={max=10.8141, min=00.0105, mean=01.8101} policy_loss=-13.6857 policy updated! \n",
      "train step 12716 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8462 diff={max=05.9702, min=00.0344, mean=01.4321} policy_loss=-10.8185 policy updated! \n",
      "train step 12717 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8394 diff={max=09.6138, min=00.0053, mean=01.5906} policy_loss=-16.0170 policy updated! \n",
      "train step 12718 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.7946 diff={max=09.5629, min=00.0044, mean=02.0268} policy_loss=-15.5437 policy updated! \n",
      "train step 12719 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.9369 diff={max=09.0333, min=00.0039, mean=01.8320} policy_loss=-12.4032 policy updated! \n",
      "train step 12720 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.7124 diff={max=13.7917, min=00.0062, mean=01.6359} policy_loss=-12.0710 policy updated! \n",
      "train step 12721 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.4517 diff={max=12.1173, min=00.0154, mean=01.9317} policy_loss=-10.8134 policy updated! \n",
      "train step 12722 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.4021 diff={max=13.9178, min=00.0708, mean=01.5278} policy_loss=-12.7455 policy updated! \n",
      "train step 12723 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2228 diff={max=07.4172, min=00.0301, mean=01.3189} policy_loss=-10.7151 policy updated! \n",
      "train step 12724 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5302 diff={max=07.7981, min=00.0102, mean=01.4592} policy_loss=-14.4467 policy updated! \n",
      "train step 12725 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.9920 diff={max=07.4139, min=00.0221, mean=01.2022} policy_loss=-10.3685 policy updated! \n",
      "train step 12726 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.6522 diff={max=13.0384, min=00.0016, mean=01.7230} policy_loss=-14.1971 policy updated! \n",
      "train step 12727 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.5313 diff={max=10.0163, min=00.0836, mean=01.7145} policy_loss=-13.4159 policy updated! \n",
      "train step 12728 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.1636 diff={max=15.2973, min=00.0135, mean=01.7619} policy_loss=-12.3815 policy updated! \n",
      "train step 12729 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8614 diff={max=09.4545, min=00.0057, mean=01.4676} policy_loss=-12.1731 policy updated! \n",
      "train step 12730 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.9534 diff={max=08.4092, min=00.0371, mean=01.4604} policy_loss=-11.2729 policy updated! \n",
      "train step 12731 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7161 diff={max=03.6302, min=00.0013, mean=00.9321} policy_loss=-12.4056 policy updated! \n",
      "train step 12732 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7431 diff={max=05.9692, min=00.0057, mean=01.3040} policy_loss=-11.9194 policy updated! \n",
      "train step 12733 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0158 diff={max=10.4859, min=00.0011, mean=01.1958} policy_loss=-12.3429 policy updated! \n",
      "train step 12734 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5346 diff={max=05.5738, min=00.0503, mean=01.1064} policy_loss=-13.0960 policy updated! \n",
      "train step 12735 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.8573 diff={max=08.3717, min=00.0127, mean=01.5533} policy_loss=-13.2195 policy updated! \n",
      "train step 12736 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=15.2504 diff={max=14.6624, min=00.0307, mean=02.1142} policy_loss=-13.2707 policy updated! \n",
      "train step 12737 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.9793 diff={max=12.0131, min=00.0210, mean=01.6107} policy_loss=-13.6280 policy updated! \n",
      "train step 12738 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.6832 diff={max=06.2333, min=00.0380, mean=01.4678} policy_loss=-13.4054 policy updated! \n",
      "train step 12739 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.9847 diff={max=09.1143, min=00.0065, mean=00.9504} policy_loss=-10.3557 policy updated! \n",
      "train step 12740 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=10.7023 diff={max=14.0552, min=00.0113, mean=01.7247} policy_loss=-12.6119 policy updated! \n",
      "train step 12741 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7365 diff={max=10.3153, min=00.0124, mean=01.4059} policy_loss=-14.2178 policy updated! \n",
      "train step 12742 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1272 diff={max=07.5793, min=00.0381, mean=01.5220} policy_loss=-15.0377 policy updated! \n",
      "train step 12743 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6179 diff={max=08.8385, min=00.0003, mean=01.5384} policy_loss=-13.8038 policy updated! \n",
      "train step 12744 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.3515 diff={max=12.9404, min=00.0515, mean=01.2970} policy_loss=-14.3101 policy updated! \n",
      "train step 12745 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=17.8439 diff={max=20.7606, min=00.0001, mean=01.8019} policy_loss=-14.8031 policy updated! \n",
      "train step 12746 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=08.3908 diff={max=10.0025, min=00.0046, mean=01.7743} policy_loss=-15.7209 policy updated! \n",
      "train step 12747 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7427 diff={max=06.4754, min=00.0213, mean=01.3099} policy_loss=-12.2260 policy updated! \n",
      "train step 12748 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.1914 diff={max=09.7923, min=00.0069, mean=01.4208} policy_loss=-15.1038 policy updated! \n",
      "train step 12749 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=07.6750 diff={max=11.1107, min=00.0246, mean=01.7330} policy_loss=-15.4858 policy updated! \n",
      "train step 12750 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.8264 diff={max=10.4540, min=00.0078, mean=01.7488} policy_loss=-14.9551 policy updated! \n",
      "train step 12751 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.9034 diff={max=08.5659, min=00.0073, mean=01.3495} policy_loss=-14.3109 policy updated! \n",
      "train step 12752 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7653 diff={max=11.4389, min=00.0157, mean=01.4152} policy_loss=-14.1515 policy updated! \n",
      "train step 12753 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4005 diff={max=08.3834, min=00.0295, mean=01.1623} policy_loss=-10.7637 policy updated! \n",
      "train step 12754 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.7398 diff={max=07.0189, min=00.0229, mean=01.5057} policy_loss=-12.9104 policy updated! \n",
      "train step 12755 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=07.9497 diff={max=10.3277, min=00.0127, mean=01.8312} policy_loss=-13.6574 policy updated! \n",
      "train step 12756 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7966 diff={max=07.3936, min=00.0313, mean=01.5105} policy_loss=-12.7170 policy updated! \n",
      "train step 12757 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.3684 diff={max=10.6129, min=00.0494, mean=01.8516} policy_loss=-13.6480 policy updated! \n",
      "train step 12758 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7691 diff={max=05.6649, min=00.0068, mean=01.2466} policy_loss=-15.4543 policy updated! \n",
      "train step 12759 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=17.3195 diff={max=15.5555, min=00.0261, mean=02.2857} policy_loss=-14.5480 policy updated! \n",
      "train step 12760 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.0309 diff={max=09.7357, min=00.1349, mean=01.4022} policy_loss=-13.2104 policy updated! \n",
      "train step 12761 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.2543 diff={max=11.3638, min=00.0114, mean=02.3401} policy_loss=-12.4315 policy updated! \n",
      "train step 12762 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1986 diff={max=07.8950, min=00.0653, mean=01.5198} policy_loss=-12.2314 policy updated! \n",
      "train step 12763 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9383 diff={max=12.3159, min=00.1197, mean=01.4304} policy_loss=-11.7337 policy updated! \n",
      "train step 12764 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.0760 diff={max=13.1387, min=00.0014, mean=01.6000} policy_loss=-13.0021 policy updated! \n",
      "train step 12765 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.9988 diff={max=05.5790, min=00.0034, mean=01.1703} policy_loss=-11.2268 policy updated! \n",
      "train step 12766 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=08.1893 diff={max=12.7712, min=00.0129, mean=01.6885} policy_loss=-12.7892 policy updated! \n",
      "train step 12767 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.8172 diff={max=20.8149, min=00.0023, mean=01.8211} policy_loss=-11.8297 policy updated! \n",
      "train step 12768 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=17.3827 diff={max=23.5578, min=00.0100, mean=02.0678} policy_loss=-10.5278 policy updated! \n",
      "train step 12769 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.8927 diff={max=14.0656, min=00.0369, mean=02.0695} policy_loss=-15.0817 policy updated! \n",
      "train step 12770 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.5448 diff={max=11.4991, min=00.0052, mean=01.7967} policy_loss=-13.1948 policy updated! \n",
      "train step 12771 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7325 diff={max=06.8474, min=00.0046, mean=01.6009} policy_loss=-14.3536 policy updated! \n",
      "train step 12772 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1008 diff={max=06.5391, min=00.0181, mean=01.2755} policy_loss=-12.0729 policy updated! \n",
      "train step 12773 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3161 diff={max=09.4021, min=00.0033, mean=01.5755} policy_loss=-14.1161 policy updated! \n",
      "train step 12774 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.4413 diff={max=07.2859, min=00.0133, mean=01.5986} policy_loss=-14.0716 policy updated! \n",
      "train step 12775 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=27.2437 diff={max=30.4966, min=00.0535, mean=02.0988} policy_loss=-12.3609 policy updated! \n",
      "train step 12776 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7864 diff={max=06.8323, min=00.0063, mean=01.4945} policy_loss=-11.8928 policy updated! \n",
      "train step 12777 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2049 diff={max=08.8130, min=00.0076, mean=01.1737} policy_loss=-11.7973 policy updated! \n",
      "train step 12778 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=08.7644 diff={max=10.8366, min=00.0305, mean=01.7729} policy_loss=-13.3063 policy updated! \n",
      "train step 12779 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8604 diff={max=08.0814, min=00.0421, mean=01.6192} policy_loss=-13.2425 policy updated! \n",
      "train step 12780 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.0769 diff={max=09.4022, min=00.0022, mean=01.6432} policy_loss=-14.2320 policy updated! \n",
      "train step 12781 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=11.9417 diff={max=10.7009, min=00.0204, mean=02.3477} policy_loss=-15.5706 policy updated! \n",
      "train step 12782 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4319 diff={max=08.2054, min=00.0142, mean=01.2784} policy_loss=-13.8061 policy updated! \n",
      "train step 12783 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=14.5070 diff={max=18.9063, min=00.0371, mean=01.9356} policy_loss=-15.4027 policy updated! \n",
      "train step 12784 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=14.1206 diff={max=11.8051, min=00.0087, mean=02.4310} policy_loss=-11.8065 policy updated! \n",
      "train step 12785 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.6313 diff={max=06.9114, min=00.0002, mean=01.1456} policy_loss=-12.3252 policy updated! \n",
      "train step 12786 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.6825 diff={max=09.4898, min=00.0665, mean=01.8558} policy_loss=-14.2094 policy updated! \n",
      "train step 12787 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2872 diff={max=10.6586, min=00.0200, mean=01.4743} policy_loss=-13.2186 policy updated! \n",
      "train step 12788 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0967 diff={max=07.6895, min=00.0137, mean=01.5030} policy_loss=-14.1365 policy updated! \n",
      "train step 12789 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7077 diff={max=08.4918, min=00.0513, mean=01.6437} policy_loss=-13.8401 policy updated! \n",
      "train step 12790 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.5823 diff={max=09.9447, min=00.0035, mean=01.7858} policy_loss=-11.9886 policy updated! \n",
      "train step 12791 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.2878 diff={max=12.3000, min=00.0239, mean=01.6542} policy_loss=-14.8439 policy updated! \n",
      "train step 12792 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.2520 diff={max=14.2595, min=00.0001, mean=01.3441} policy_loss=-14.2569 policy updated! \n",
      "train step 12793 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4274 diff={max=05.4519, min=00.0128, mean=01.0521} policy_loss=-12.2040 policy updated! \n",
      "train step 12794 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1733 diff={max=06.5521, min=00.0004, mean=01.2106} policy_loss=-12.0837 policy updated! \n",
      "train step 12795 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.7217 diff={max=08.5234, min=00.0560, mean=01.3818} policy_loss=-12.2890 policy updated! \n",
      "train step 12796 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1237 diff={max=08.9803, min=00.0064, mean=01.1776} policy_loss=-13.5919 policy updated! \n",
      "train step 12797 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.9460 diff={max=12.1298, min=00.0290, mean=01.8198} policy_loss=-15.0918 policy updated! \n",
      "train step 12798 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=07.0789 diff={max=13.2968, min=00.1023, mean=01.5095} policy_loss=-14.4447 policy updated! \n",
      "train step 12799 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.8793 diff={max=09.6315, min=00.0307, mean=01.8298} policy_loss=-13.8632 policy updated! \n",
      "train step 12800 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=14.2949 diff={max=18.3958, min=00.1235, mean=02.0607} policy_loss=-14.1465 policy updated! \n",
      "train step 12801 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.1717 diff={max=07.5275, min=00.0084, mean=01.9387} policy_loss=-12.9888 policy updated! \n",
      "train step 12802 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1943 diff={max=08.8983, min=00.0625, mean=01.9111} policy_loss=-13.6234 policy updated! \n",
      "train step 12803 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.8052 diff={max=22.9484, min=00.0489, mean=01.6207} policy_loss=-13.5574 policy updated! \n",
      "train step 12804 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5604 diff={max=08.6874, min=00.0146, mean=01.4272} policy_loss=-13.9277 policy updated! \n",
      "train step 12805 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.8881 diff={max=03.9895, min=00.0205, mean=01.3091} policy_loss=-13.7955 policy updated! \n",
      "train step 12806 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.1641 diff={max=08.7828, min=00.0149, mean=01.4764} policy_loss=-14.7429 policy updated! \n",
      "train step 12807 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.7407 diff={max=10.6125, min=00.0031, mean=01.9443} policy_loss=-12.3403 policy updated! \n",
      "train step 12808 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.4005 diff={max=10.0543, min=00.1070, mean=01.3525} policy_loss=-14.8455 policy updated! \n",
      "train step 12809 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8507 diff={max=08.7119, min=00.0122, mean=01.3960} policy_loss=-13.7403 policy updated! \n",
      "train step 12810 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.8889 diff={max=11.2761, min=00.0058, mean=01.8952} policy_loss=-14.5259 policy updated! \n",
      "train step 12811 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.3579 diff={max=05.0846, min=00.0156, mean=01.0961} policy_loss=-14.4180 policy updated! \n",
      "train step 12812 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3960 diff={max=11.2142, min=00.0113, mean=01.5610} policy_loss=-11.5779 policy updated! \n",
      "train step 12813 reward={max=08.0000, min=00.0000, mean=03.0000} optimizing loss=05.7319 diff={max=07.8805, min=00.0013, mean=01.4795} policy_loss=-12.2042 policy updated! \n",
      "train step 12814 reward={max=08.0000, min=00.0000, mean=05.8000} optimizing loss=11.0818 diff={max=12.4868, min=00.0347, mean=01.8681} policy_loss=-14.2936 policy updated! \n",
      "train step 12815 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.4596 diff={max=08.1211, min=00.0009, mean=01.2876} policy_loss=-16.7535 policy updated! \n",
      "train step 12816 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=15.1576 diff={max=16.1924, min=00.0103, mean=02.0215} policy_loss=-12.2623 policy updated! \n",
      "train step 12817 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8600 diff={max=08.7588, min=00.0352, mean=01.5062} policy_loss=-14.0642 policy updated! \n",
      "train step 12818 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4641 diff={max=08.7576, min=00.0525, mean=01.7099} policy_loss=-16.5164 policy updated! \n",
      "train step 12819 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=07.0481 diff={max=09.4570, min=00.0120, mean=01.8509} policy_loss=-12.8226 policy updated! \n",
      "train step 12820 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.1039 diff={max=07.6848, min=00.0463, mean=01.7306} policy_loss=-12.1629 policy updated! \n",
      "train step 12821 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3325 diff={max=09.9324, min=00.0174, mean=01.3095} policy_loss=-12.3080 policy updated! \n",
      "train step 12822 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0668 diff={max=11.6827, min=00.0351, mean=01.2566} policy_loss=-11.4809 policy updated! \n",
      "train step 12823 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=10.0191 diff={max=09.5010, min=00.0008, mean=01.9926} policy_loss=-12.4488 policy updated! \n",
      "train step 12824 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0192 diff={max=07.4716, min=00.0028, mean=01.2712} policy_loss=-11.3149 policy updated! \n",
      "train step 12825 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.4205 diff={max=08.2839, min=00.0131, mean=01.3446} policy_loss=-11.3387 policy updated! \n",
      "train step 12826 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=09.7142 diff={max=12.2009, min=00.0076, mean=01.7312} policy_loss=-12.9832 policy updated! \n",
      "train step 12827 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.6538 diff={max=16.7288, min=00.0066, mean=02.0483} policy_loss=-12.6048 policy updated! \n",
      "train step 12828 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7159 diff={max=07.4733, min=00.0264, mean=01.5221} policy_loss=-12.8916 policy updated! \n",
      "train step 12829 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1509 diff={max=11.9829, min=00.0007, mean=01.2267} policy_loss=-12.1872 policy updated! \n",
      "train step 12830 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.5884 diff={max=09.2215, min=00.0004, mean=01.4275} policy_loss=-16.2669 policy updated! \n",
      "train step 12831 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.2734 diff={max=07.8671, min=00.0115, mean=01.7637} policy_loss=-12.7577 policy updated! \n",
      "train step 12832 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0493 diff={max=07.0764, min=00.0223, mean=01.3440} policy_loss=-14.8745 policy updated! \n",
      "train step 12833 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=21.8469 diff={max=22.8913, min=00.0005, mean=02.2718} policy_loss=-14.1191 policy updated! \n",
      "train step 12834 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=07.0414 diff={max=09.2136, min=00.0067, mean=01.5556} policy_loss=-10.8482 policy updated! \n",
      "train step 12835 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.5026 diff={max=09.1394, min=00.0137, mean=01.6801} policy_loss=-14.6924 policy updated! \n",
      "train step 12836 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.5501 diff={max=09.1437, min=00.0028, mean=01.3594} policy_loss=-11.1227 policy updated! \n",
      "train step 12837 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0739 diff={max=08.8247, min=00.0151, mean=01.3226} policy_loss=-14.8782 policy updated! \n",
      "train step 12838 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2302 diff={max=06.2377, min=00.0215, mean=01.1331} policy_loss=-12.1346 policy updated! \n",
      "train step 12839 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9687 diff={max=08.9898, min=00.0096, mean=01.5786} policy_loss=-13.5202 policy updated! \n",
      "train step 12840 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.1982 diff={max=11.1664, min=00.0209, mean=01.6935} policy_loss=-14.6107 policy updated! \n",
      "train step 12841 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3951 diff={max=07.9303, min=00.0550, mean=01.4191} policy_loss=-11.7672 policy updated! \n",
      "train step 12842 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.2527 diff={max=13.0127, min=00.0146, mean=01.4727} policy_loss=-13.0899 policy updated! \n",
      "train step 12843 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3957 diff={max=06.7667, min=00.0061, mean=00.9558} policy_loss=-12.0058 policy updated! \n",
      "train step 12844 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=24.7984 diff={max=31.2717, min=00.0042, mean=01.9972} policy_loss=-12.1790 policy updated! \n",
      "train step 12845 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.5591 diff={max=10.0046, min=00.0226, mean=01.3835} policy_loss=-14.1350 policy updated! \n",
      "train step 12846 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.7052 diff={max=06.9135, min=00.0117, mean=01.2471} policy_loss=-15.8284 policy updated! \n",
      "train step 12847 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7256 diff={max=09.2721, min=00.0104, mean=01.4149} policy_loss=-15.3475 policy updated! \n",
      "train step 12848 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0818 diff={max=05.9352, min=00.0442, mean=01.5495} policy_loss=-14.6932 policy updated! \n",
      "train step 12849 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9881 diff={max=10.8219, min=00.0029, mean=01.3494} policy_loss=-12.1745 policy updated! \n",
      "train step 12850 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.1659 diff={max=10.0032, min=00.0061, mean=01.3447} policy_loss=-11.4526 policy updated! \n",
      "train step 12851 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.2422 diff={max=08.2729, min=00.0057, mean=01.5489} policy_loss=-16.1367 policy updated! \n",
      "train step 12852 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3134 diff={max=12.6807, min=00.0137, mean=01.5225} policy_loss=-14.9873 policy updated! \n",
      "train step 12853 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.0209 diff={max=06.6997, min=00.0074, mean=01.2977} policy_loss=-9.6598 policy updated! \n",
      "train step 12854 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9153 diff={max=09.6772, min=00.0079, mean=01.2320} policy_loss=-11.7039 policy updated! \n",
      "train step 12855 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.6156 diff={max=11.7438, min=00.0071, mean=01.4868} policy_loss=-14.7548 policy updated! \n",
      "train step 12856 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.0955 diff={max=08.4121, min=00.0000, mean=01.2855} policy_loss=-14.7071 policy updated! \n",
      "train step 12857 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.6035 diff={max=12.5155, min=00.0419, mean=01.7586} policy_loss=-10.9372 policy updated! \n",
      "train step 12858 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.3917 diff={max=15.2208, min=00.0080, mean=01.7376} policy_loss=-10.8453 policy updated! \n",
      "train step 12859 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1252 diff={max=07.7166, min=00.0322, mean=01.6573} policy_loss=-13.1472 policy updated! \n",
      "train step 12860 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.2521 diff={max=12.6798, min=00.0097, mean=01.8930} policy_loss=-11.4553 policy updated! \n",
      "train step 12861 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2573 diff={max=07.7583, min=00.0010, mean=01.3757} policy_loss=-13.1583 policy updated! \n",
      "train step 12862 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0901 diff={max=10.8104, min=00.0311, mean=01.5652} policy_loss=-12.4139 policy updated! \n",
      "train step 12863 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.5460 diff={max=11.2592, min=00.0178, mean=01.8251} policy_loss=-14.7921 policy updated! \n",
      "train step 12864 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.7901 diff={max=16.1098, min=00.0433, mean=01.6350} policy_loss=-13.4529 policy updated! \n",
      "train step 12865 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.9627 diff={max=10.9609, min=00.0227, mean=01.5098} policy_loss=-12.9469 policy updated! \n",
      "train step 12866 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1115 diff={max=11.1427, min=00.0153, mean=01.4944} policy_loss=-15.5940 policy updated! \n",
      "train step 12867 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.4889 diff={max=12.8220, min=00.0362, mean=01.5229} policy_loss=-11.9045 policy updated! \n",
      "train step 12868 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1602 diff={max=06.5703, min=00.0079, mean=01.2669} policy_loss=-14.4245 policy updated! \n",
      "train step 12869 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7629 diff={max=07.6574, min=00.0611, mean=01.4341} policy_loss=-12.8485 policy updated! \n",
      "train step 12870 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.1279 diff={max=09.3218, min=00.0064, mean=01.6012} policy_loss=-16.9088 policy updated! \n",
      "train step 12871 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4923 diff={max=05.3035, min=00.0596, mean=01.5560} policy_loss=-11.5042 policy updated! \n",
      "train step 12872 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.7978 diff={max=12.9643, min=00.0331, mean=01.6901} policy_loss=-15.9166 policy updated! \n",
      "train step 12873 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8236 diff={max=06.5406, min=00.0207, mean=01.0451} policy_loss=-11.7223 policy updated! \n",
      "train step 12874 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.6333 diff={max=13.0341, min=00.0177, mean=01.4568} policy_loss=-16.0632 policy updated! \n",
      "train step 12875 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.2132 diff={max=09.1179, min=00.0582, mean=01.7680} policy_loss=-11.9268 policy updated! \n",
      "train step 12876 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=11.0104 diff={max=16.3511, min=00.0005, mean=01.6679} policy_loss=-10.9000 policy updated! \n",
      "train step 12877 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6262 diff={max=07.0222, min=00.0231, mean=01.0081} policy_loss=-12.9560 policy updated! \n",
      "train step 12878 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.1031 diff={max=04.0161, min=00.0289, mean=00.9826} policy_loss=-12.2863 policy updated! \n",
      "train step 12879 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.0177 diff={max=09.1824, min=00.0043, mean=01.7533} policy_loss=-16.4571 policy updated! \n",
      "train step 12880 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.0379 diff={max=12.1919, min=00.0086, mean=01.5052} policy_loss=-11.9751 policy updated! \n",
      "train step 12881 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5498 diff={max=08.5939, min=00.0011, mean=01.3454} policy_loss=-11.8632 policy updated! \n",
      "train step 12882 reward={max=07.0000, min=06.0000, mean=06.8000} optimizing loss=06.6888 diff={max=10.5503, min=00.0151, mean=01.5161} policy_loss=-16.7164 policy updated! \n",
      "train step 12883 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.9610 diff={max=06.9224, min=00.0272, mean=01.2797} policy_loss=-11.6199 policy updated! \n",
      "train step 12884 reward={max=07.0000, min=00.0000, mean=05.2000} optimizing loss=09.4928 diff={max=08.6008, min=00.0071, mean=01.8989} policy_loss=-14.9810 policy updated! \n",
      "train step 12885 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.3629 diff={max=05.2946, min=00.0025, mean=01.2303} policy_loss=-14.0141 policy updated! \n",
      "train step 12886 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=05.4101 diff={max=10.3397, min=00.0118, mean=01.3135} policy_loss=-12.0743 policy updated! \n",
      "train step 12887 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.2686 diff={max=13.5209, min=00.0076, mean=01.8280} policy_loss=-12.4125 policy updated! \n",
      "train step 12888 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1651 diff={max=09.3398, min=00.0004, mean=01.5398} policy_loss=-13.4259 policy updated! \n",
      "train step 12889 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9377 diff={max=07.2424, min=00.0221, mean=01.2418} policy_loss=-9.9154 policy updated! \n",
      "train step 12890 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.5564 diff={max=07.1126, min=00.0010, mean=01.5065} policy_loss=-11.3474 policy updated! \n",
      "train step 12891 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.4539 diff={max=12.6969, min=00.0170, mean=02.0126} policy_loss=-12.6556 policy updated! \n",
      "train step 12892 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.4118 diff={max=13.5105, min=00.0073, mean=01.5861} policy_loss=-14.4417 policy updated! \n",
      "train step 12893 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2601 diff={max=06.1058, min=00.0149, mean=01.2857} policy_loss=-12.2067 policy updated! \n",
      "train step 12894 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.4022 diff={max=07.5565, min=00.0102, mean=02.0303} policy_loss=-13.5701 policy updated! \n",
      "train step 12895 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.9073 diff={max=08.2623, min=00.0177, mean=01.1715} policy_loss=-13.2707 policy updated! \n",
      "train step 12896 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.5659 diff={max=11.4727, min=00.0055, mean=01.8407} policy_loss=-13.4052 policy updated! \n",
      "train step 12897 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.4213 diff={max=20.2657, min=00.0019, mean=01.6116} policy_loss=-13.7501 policy updated! \n",
      "train step 12898 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=07.3957 diff={max=13.0369, min=00.0180, mean=01.6458} policy_loss=-14.0595 policy updated! \n",
      "train step 12899 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.1189 diff={max=14.7882, min=00.0111, mean=01.5116} policy_loss=-13.6001 policy updated! \n",
      "train step 12900 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.2657 diff={max=09.5469, min=00.0117, mean=01.6155} policy_loss=-16.1383 policy updated! \n",
      "train step 12901 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6721 diff={max=07.1098, min=00.0256, mean=01.2424} policy_loss=-12.1307 policy updated! \n",
      "train step 12902 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6942 diff={max=09.8934, min=00.0011, mean=01.3124} policy_loss=-11.8138 policy updated! \n",
      "train step 12903 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.0869 diff={max=21.2279, min=00.0153, mean=01.3304} policy_loss=-12.1101 policy updated! \n",
      "train step 12904 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.2949 diff={max=10.5779, min=00.0105, mean=01.8674} policy_loss=-16.4753 policy updated! \n",
      "train step 12905 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.1059 diff={max=07.8361, min=00.0183, mean=01.6355} policy_loss=-15.6547 policy updated! \n",
      "train step 12906 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3735 diff={max=07.2890, min=00.0705, mean=01.1817} policy_loss=-12.5649 policy updated! \n",
      "train step 12907 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5173 diff={max=07.9660, min=00.0012, mean=01.5609} policy_loss=-10.7108 policy updated! \n",
      "train step 12908 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=10.3084 diff={max=13.9193, min=00.0166, mean=01.8441} policy_loss=-14.0054 policy updated! \n",
      "train step 12909 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.2278 diff={max=12.9178, min=00.0135, mean=01.8669} policy_loss=-12.4870 policy updated! \n",
      "train step 12910 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=10.4482 diff={max=16.7533, min=00.0538, mean=01.6405} policy_loss=-14.4557 policy updated! \n",
      "train step 12911 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.8244 diff={max=11.3387, min=00.0269, mean=01.9456} policy_loss=-17.6484 policy updated! \n",
      "train step 12912 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4027 diff={max=05.5116, min=00.0235, mean=01.1333} policy_loss=-18.5904 policy updated! \n",
      "train step 12913 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.4704 diff={max=18.6524, min=00.0241, mean=02.1003} policy_loss=-17.4264 policy updated! \n",
      "train step 12914 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=09.4282 diff={max=10.3773, min=00.0088, mean=02.0574} policy_loss=-14.8711 policy updated! \n",
      "train step 12915 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.6753 diff={max=04.0681, min=00.0192, mean=01.2680} policy_loss=-15.2694 policy updated! \n",
      "train step 12916 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1602 diff={max=05.9635, min=00.0016, mean=01.2050} policy_loss=-14.0229 policy updated! \n",
      "train step 12917 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3715 diff={max=10.4269, min=00.0297, mean=01.5368} policy_loss=-14.2255 policy updated! \n",
      "train step 12918 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=15.1866 diff={max=25.4574, min=00.0185, mean=01.4701} policy_loss=-12.8509 policy updated! \n",
      "train step 12919 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7347 diff={max=07.5386, min=00.0519, mean=01.2792} policy_loss=-13.1785 policy updated! \n",
      "train step 12920 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.7307 diff={max=08.2458, min=00.0370, mean=01.4242} policy_loss=-11.7667 policy updated! \n",
      "train step 12921 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=15.7905 diff={max=16.7832, min=00.0195, mean=02.2549} policy_loss=-13.5252 policy updated! \n",
      "train step 12922 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7035 diff={max=05.6879, min=00.0313, mean=01.1164} policy_loss=-9.6777 policy updated! \n",
      "train step 12923 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.4364 diff={max=12.6279, min=00.0890, mean=02.4677} policy_loss=-13.6877 policy updated! \n",
      "train step 12924 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.4381 diff={max=14.6683, min=00.0116, mean=01.8221} policy_loss=-13.7490 policy updated! \n",
      "train step 12925 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.9023 diff={max=04.5432, min=00.0521, mean=01.2477} policy_loss=-12.7388 policy updated! \n",
      "train step 12926 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.6513 diff={max=14.8103, min=00.0030, mean=02.1595} policy_loss=-14.2854 policy updated! \n",
      "train step 12927 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4411 diff={max=09.7133, min=00.0523, mean=01.4459} policy_loss=-12.4869 policy updated! \n",
      "train step 12928 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.0849 diff={max=07.8818, min=00.0164, mean=01.0931} policy_loss=-11.5777 policy updated! \n",
      "train step 12929 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0583 diff={max=06.7707, min=00.0027, mean=01.5905} policy_loss=-16.2146 policy updated! \n",
      "train step 12930 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.9351 diff={max=08.8510, min=00.0120, mean=01.7505} policy_loss=-12.9028 policy updated! \n",
      "train step 12931 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=09.3916 diff={max=15.2376, min=00.0030, mean=01.6480} policy_loss=-15.7320 policy updated! \n",
      "train step 12932 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=18.5670 diff={max=20.6929, min=00.0553, mean=02.3522} policy_loss=-13.5811 policy updated! \n",
      "train step 12933 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.4982 diff={max=11.3743, min=00.0204, mean=01.7318} policy_loss=-14.5292 policy updated! \n",
      "train step 12934 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.4796 diff={max=03.1673, min=00.0548, mean=00.9415} policy_loss=-11.9151 policy updated! \n",
      "train step 12935 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.6670 diff={max=07.6284, min=00.0311, mean=01.9126} policy_loss=-13.1927 policy updated! \n",
      "train step 12936 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.5818 diff={max=12.4939, min=00.0034, mean=01.8648} policy_loss=-14.0168 policy updated! \n",
      "train step 12937 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.4250 diff={max=09.4022, min=00.0080, mean=02.1596} policy_loss=-12.1465 policy updated! \n",
      "train step 12938 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.1748 diff={max=07.9401, min=00.0352, mean=01.4128} policy_loss=-11.1578 policy updated! \n",
      "train step 12939 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3401 diff={max=08.8670, min=00.0009, mean=01.6486} policy_loss=-13.0216 policy updated! \n",
      "train step 12940 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.4365 diff={max=09.6995, min=00.0262, mean=01.9779} policy_loss=-11.1496 policy updated! \n",
      "train step 12941 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.4403 diff={max=08.8945, min=00.0095, mean=01.7063} policy_loss=-13.9917 policy updated! \n",
      "train step 12942 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5278 diff={max=05.0881, min=00.0568, mean=01.4852} policy_loss=-12.7221 policy updated! \n",
      "train step 12943 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9164 diff={max=06.8909, min=00.0055, mean=01.4279} policy_loss=-11.7329 policy updated! \n",
      "train step 12944 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.5698 diff={max=05.8878, min=00.0112, mean=01.2442} policy_loss=-11.9780 policy updated! \n",
      "train step 12945 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.5480 diff={max=06.9585, min=00.0162, mean=01.7380} policy_loss=-14.2983 policy updated! \n",
      "train step 12946 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9067 diff={max=05.9776, min=00.0117, mean=01.2781} policy_loss=-14.3236 policy updated! \n",
      "train step 12947 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1708 diff={max=09.7043, min=00.0144, mean=01.8347} policy_loss=-15.0969 policy updated! \n",
      "train step 12948 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.0072 diff={max=05.7406, min=00.0321, mean=01.4582} policy_loss=-13.7802 policy updated! \n",
      "train step 12949 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7213 diff={max=09.1733, min=00.0345, mean=01.7904} policy_loss=-14.7085 policy updated! \n",
      "train step 12950 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.6920 diff={max=09.3857, min=00.0080, mean=01.4020} policy_loss=-12.1675 policy updated! \n",
      "train step 12951 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.4199 diff={max=06.0912, min=00.0274, mean=01.2814} policy_loss=-15.0497 policy updated! \n",
      "train step 12952 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.7021 diff={max=12.3671, min=00.0101, mean=01.9747} policy_loss=-11.6253 policy updated! \n",
      "train step 12953 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.2511 diff={max=10.0023, min=00.0311, mean=01.8395} policy_loss=-14.8613 policy updated! \n",
      "train step 12954 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9948 diff={max=08.2324, min=00.0150, mean=01.7378} policy_loss=-13.0098 policy updated! \n",
      "train step 12955 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.5466 diff={max=05.7483, min=00.0017, mean=01.3268} policy_loss=-14.5222 policy updated! \n",
      "train step 12956 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=08.8686 diff={max=09.7464, min=00.0016, mean=02.0500} policy_loss=-14.2250 policy updated! \n",
      "train step 12957 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.2868 diff={max=08.3314, min=00.0127, mean=01.6653} policy_loss=-12.5624 policy updated! \n",
      "train step 12958 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=09.6593 diff={max=15.3959, min=00.0020, mean=01.7994} policy_loss=-13.1164 policy updated! \n",
      "train step 12959 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.8358 diff={max=06.3246, min=00.0312, mean=00.8273} policy_loss=-9.4141 policy updated! \n",
      "train step 12960 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.6831 diff={max=07.8591, min=00.0486, mean=01.3584} policy_loss=-13.2228 policy updated! \n",
      "train step 12961 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=09.9689 diff={max=12.6360, min=00.0046, mean=01.8862} policy_loss=-14.6919 policy updated! \n",
      "train step 12962 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.3052 diff={max=19.8979, min=00.0196, mean=01.6879} policy_loss=-14.9383 policy updated! \n",
      "train step 12963 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.7308 diff={max=06.2514, min=00.0019, mean=01.1087} policy_loss=-12.4420 policy updated! \n",
      "train step 12964 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9688 diff={max=12.0829, min=00.0803, mean=01.2161} policy_loss=-14.3861 policy updated! \n",
      "train step 12965 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.7074 diff={max=12.2165, min=00.0016, mean=01.4625} policy_loss=-12.9237 policy updated! \n",
      "train step 12966 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.6316 diff={max=13.8586, min=00.0177, mean=01.4055} policy_loss=-14.5817 policy updated! \n",
      "train step 12967 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8883 diff={max=08.4430, min=00.0036, mean=01.3960} policy_loss=-11.4340 policy updated! \n",
      "train step 12968 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.3432 diff={max=16.2112, min=00.0026, mean=01.8643} policy_loss=-11.3220 policy updated! \n",
      "train step 12969 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.8827 diff={max=12.3060, min=00.0318, mean=01.6062} policy_loss=-15.9829 policy updated! \n",
      "train step 12970 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.7581 diff={max=06.0218, min=00.0652, mean=01.3479} policy_loss=-10.9869 policy updated! \n",
      "train step 12971 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7127 diff={max=08.8580, min=00.0037, mean=01.3557} policy_loss=-12.0242 policy updated! \n",
      "train step 12972 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8185 diff={max=07.9035, min=00.0070, mean=01.6669} policy_loss=-14.7077 policy updated! \n",
      "train step 12973 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5144 diff={max=07.5910, min=00.1250, mean=01.7713} policy_loss=-16.0308 policy updated! \n",
      "train step 12974 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.4841 diff={max=07.2410, min=00.0008, mean=01.2395} policy_loss=-15.6510 policy updated! \n",
      "train step 12975 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.0409 diff={max=07.7936, min=00.0035, mean=01.3122} policy_loss=-14.8787 policy updated! \n",
      "train step 12976 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.1284 diff={max=06.0458, min=00.0149, mean=01.2107} policy_loss=-12.3740 policy updated! \n",
      "train step 12977 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5790 diff={max=08.6494, min=00.0078, mean=01.4734} policy_loss=-13.3855 policy updated! \n",
      "train step 12978 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=11.4288 diff={max=09.5984, min=00.0243, mean=02.0576} policy_loss=-13.9692 policy updated! \n",
      "train step 12979 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.0762 diff={max=18.4840, min=00.0170, mean=01.7394} policy_loss=-14.4894 policy updated! \n",
      "train step 12980 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=15.4532 diff={max=22.6357, min=00.0082, mean=02.0788} policy_loss=-14.2626 policy updated! \n",
      "train step 12981 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.6294 diff={max=05.7549, min=00.0031, mean=01.2584} policy_loss=-12.9987 policy updated! \n",
      "train step 12982 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.0545 diff={max=14.8594, min=00.0340, mean=01.8546} policy_loss=-14.0626 policy updated! \n",
      "train step 12983 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=02.4271 diff={max=07.4531, min=00.0109, mean=00.9629} policy_loss=-14.5423 policy updated! \n",
      "train step 12984 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.7203 diff={max=09.0521, min=00.0238, mean=01.7573} policy_loss=-14.4572 policy updated! \n",
      "train step 12985 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.5990 diff={max=06.3936, min=00.0070, mean=01.4683} policy_loss=-11.8068 policy updated! \n",
      "train step 12986 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3191 diff={max=11.8673, min=00.0781, mean=01.7100} policy_loss=-16.4367 policy updated! \n",
      "train step 12987 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9722 diff={max=04.3300, min=00.0211, mean=00.9134} policy_loss=-10.7587 policy updated! \n",
      "train step 12988 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.3739 diff={max=11.4418, min=00.0028, mean=02.1549} policy_loss=-14.8905 policy updated! \n",
      "train step 12989 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.6799 diff={max=12.1203, min=00.0606, mean=02.0348} policy_loss=-14.2949 policy updated! \n",
      "train step 12990 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=11.3491 diff={max=19.8250, min=00.0001, mean=01.5300} policy_loss=-14.4342 policy updated! \n",
      "train step 12991 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.4282 diff={max=06.2307, min=00.0206, mean=01.5461} policy_loss=-12.8243 policy updated! \n",
      "train step 12992 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4483 diff={max=07.2267, min=00.0175, mean=01.6547} policy_loss=-14.0001 policy updated! \n",
      "train step 12993 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.4197 diff={max=08.9507, min=00.0298, mean=01.7728} policy_loss=-15.7986 policy updated! \n",
      "train step 12994 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=18.4350 diff={max=16.7876, min=00.0194, mean=02.1787} policy_loss=-11.2635 policy updated! \n",
      "train step 12995 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.7898 diff={max=07.8421, min=00.0151, mean=01.3992} policy_loss=-14.8775 policy updated! \n",
      "train step 12996 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8279 diff={max=10.6141, min=00.0481, mean=01.5730} policy_loss=-11.8472 policy updated! \n",
      "train step 12997 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1655 diff={max=09.1442, min=00.0156, mean=01.4570} policy_loss=-13.0988 policy updated! \n",
      "train step 12998 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=08.5315 diff={max=12.6211, min=00.0147, mean=01.5792} policy_loss=-11.3622 policy updated! \n",
      "train step 12999 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9273 diff={max=05.2920, min=00.0091, mean=00.9528} policy_loss=-10.2394 policy updated! \n",
      "train step 13000 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=05.3159 diff={max=10.2457, min=00.0063, mean=01.2980} policy_loss=-10.8035 policy updated! \n",
      "train step 13001 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7880 diff={max=06.1647, min=00.0702, mean=01.0356} policy_loss=-12.3376 policy updated! \n",
      "train step 13002 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.6558 diff={max=15.1428, min=00.0012, mean=02.1539} policy_loss=-14.7950 policy updated! \n",
      "train step 13003 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8533 diff={max=06.7991, min=00.0052, mean=01.2969} policy_loss=-13.0108 policy updated! \n",
      "train step 13004 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=10.0743 diff={max=11.5039, min=00.0141, mean=01.8084} policy_loss=-12.6494 policy updated! \n",
      "train step 13005 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.5116 diff={max=05.8488, min=00.0318, mean=01.0858} policy_loss=-12.0352 policy updated! \n",
      "train step 13006 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.1008 diff={max=11.2421, min=00.0537, mean=01.9541} policy_loss=-14.7947 policy updated! \n",
      "train step 13007 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=14.9631 diff={max=22.1522, min=00.0336, mean=02.0741} policy_loss=-15.4389 policy updated! \n",
      "train step 13008 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8646 diff={max=13.4618, min=00.0153, mean=01.2818} policy_loss=-13.6032 policy updated! \n",
      "train step 13009 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1129 diff={max=12.1993, min=00.0045, mean=01.4341} policy_loss=-14.8003 policy updated! \n",
      "train step 13010 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.6997 diff={max=11.8570, min=00.0341, mean=01.7164} policy_loss=-12.3708 policy updated! \n",
      "train step 13011 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.6085 diff={max=10.9764, min=00.0307, mean=01.8948} policy_loss=-14.6262 policy updated! \n",
      "train step 13012 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.1286 diff={max=18.9551, min=00.0254, mean=01.9055} policy_loss=-13.1675 policy updated! \n",
      "train step 13013 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=06.3977 diff={max=07.7486, min=00.0116, mean=01.6734} policy_loss=-12.3262 policy updated! \n",
      "train step 13014 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.8667 diff={max=08.9677, min=00.0013, mean=01.4926} policy_loss=-14.5164 policy updated! \n",
      "train step 13015 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.9810 diff={max=07.3624, min=00.0055, mean=01.6625} policy_loss=-12.3498 policy updated! \n",
      "train step 13016 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.1062 diff={max=21.5233, min=00.0040, mean=01.6512} policy_loss=-12.4060 policy updated! \n",
      "train step 13017 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8170 diff={max=07.9974, min=00.0046, mean=01.1699} policy_loss=-11.0263 policy updated! \n",
      "train step 13018 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6438 diff={max=07.8892, min=00.0134, mean=01.6589} policy_loss=-12.5904 policy updated! \n",
      "train step 13019 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=09.6467 diff={max=14.1175, min=00.0124, mean=01.9170} policy_loss=-13.5791 policy updated! \n",
      "train step 13020 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.2101 diff={max=10.0329, min=00.0346, mean=01.4976} policy_loss=-15.3615 policy updated! \n",
      "train step 13021 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5723 diff={max=08.1428, min=00.0038, mean=01.5776} policy_loss=-15.1033 policy updated! \n",
      "train step 13022 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3247 diff={max=04.7897, min=00.0325, mean=01.0474} policy_loss=-15.5172 policy updated! \n",
      "train step 13023 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0995 diff={max=07.8848, min=00.0132, mean=01.4367} policy_loss=-11.2963 policy updated! \n",
      "train step 13024 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3059 diff={max=08.9621, min=00.0132, mean=01.5916} policy_loss=-13.8211 policy updated! \n",
      "train step 13025 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.0622 diff={max=07.7030, min=00.0337, mean=01.3766} policy_loss=-13.7356 policy updated! \n",
      "train step 13026 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1696 diff={max=09.9027, min=00.0136, mean=01.5120} policy_loss=-14.2806 policy updated! \n",
      "train step 13027 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=17.7732 diff={max=15.8139, min=00.0131, mean=02.5161} policy_loss=-15.6124 policy updated! \n",
      "train step 13028 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=10.4455 diff={max=12.4453, min=00.0092, mean=01.9221} policy_loss=-13.7743 policy updated! \n",
      "train step 13029 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1267 diff={max=08.2700, min=00.0350, mean=01.4372} policy_loss=-14.1358 policy updated! \n",
      "train step 13030 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.4222 diff={max=09.2659, min=00.0117, mean=01.6064} policy_loss=-13.3429 policy updated! \n",
      "train step 13031 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8839 diff={max=09.7285, min=00.0003, mean=01.5065} policy_loss=-12.7164 policy updated! \n",
      "train step 13032 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6394 diff={max=07.8132, min=00.0132, mean=01.2177} policy_loss=-13.1560 policy updated! \n",
      "train step 13033 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4803 diff={max=06.0314, min=00.0324, mean=01.2050} policy_loss=-13.4182 policy updated! \n",
      "train step 13034 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.7694 diff={max=10.3577, min=00.0431, mean=01.5801} policy_loss=-11.9453 policy updated! \n",
      "train step 13035 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.3430 diff={max=07.2226, min=00.0373, mean=01.3643} policy_loss=-13.5352 policy updated! \n",
      "train step 13036 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.5404 diff={max=10.0377, min=00.0002, mean=01.8656} policy_loss=-14.9211 policy updated! \n",
      "train step 13037 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.1875 diff={max=15.2381, min=00.0136, mean=01.9646} policy_loss=-16.5499 policy updated! \n",
      "train step 13038 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.9235 diff={max=10.2137, min=00.0166, mean=01.7817} policy_loss=-12.5555 policy updated! \n",
      "train step 13039 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9146 diff={max=04.7794, min=00.0256, mean=00.9367} policy_loss=-9.2965 policy updated! \n",
      "train step 13040 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=10.9004 diff={max=11.5433, min=00.0013, mean=01.9218} policy_loss=-13.7017 policy updated! \n",
      "train step 13041 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5402 diff={max=06.2289, min=00.0773, mean=01.4972} policy_loss=-13.0050 policy updated! \n",
      "train step 13042 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.5965 diff={max=09.7029, min=00.0557, mean=02.2645} policy_loss=-15.5150 policy updated! \n",
      "train step 13043 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.6490 diff={max=10.3044, min=00.0117, mean=01.6690} policy_loss=-15.0603 policy updated! \n",
      "train step 13044 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4278 diff={max=07.3779, min=00.0090, mean=01.2589} policy_loss=-13.8262 policy updated! \n",
      "train step 13045 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=15.1640 diff={max=21.9620, min=00.0111, mean=01.8039} policy_loss=-14.8485 policy updated! \n",
      "train step 13046 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.3222 diff={max=09.0107, min=00.0173, mean=01.5724} policy_loss=-12.5486 policy updated! \n",
      "train step 13047 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.5453 diff={max=18.5903, min=00.0005, mean=01.5788} policy_loss=-14.2037 policy updated! \n",
      "train step 13048 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3256 diff={max=12.8309, min=00.0261, mean=01.4046} policy_loss=-12.5391 policy updated! \n",
      "train step 13049 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.7115 diff={max=03.0883, min=00.0001, mean=00.9903} policy_loss=-12.9311 policy updated! \n",
      "train step 13050 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.2469 diff={max=10.1871, min=00.0080, mean=01.3189} policy_loss=-14.7549 policy updated! \n",
      "train step 13051 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=11.0564 diff={max=20.1550, min=00.0204, mean=01.4145} policy_loss=-13.1383 policy updated! \n",
      "train step 13052 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9317 diff={max=07.9496, min=00.0386, mean=01.1885} policy_loss=-13.6549 policy updated! \n",
      "train step 13053 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4213 diff={max=07.0838, min=00.0109, mean=01.5951} policy_loss=-13.4676 policy updated! \n",
      "train step 13054 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.4832 diff={max=08.5069, min=00.0530, mean=01.6837} policy_loss=-14.6523 policy updated! \n",
      "train step 13055 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=10.0912 diff={max=19.4071, min=00.0020, mean=01.4487} policy_loss=-13.5344 policy updated! \n",
      "train step 13056 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7389 diff={max=07.0725, min=00.0048, mean=01.0297} policy_loss=-14.1722 policy updated! \n",
      "train step 13057 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3108 diff={max=12.1569, min=00.0075, mean=01.4142} policy_loss=-11.6866 policy updated! \n",
      "train step 13058 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7730 diff={max=07.6717, min=00.0048, mean=01.0668} policy_loss=-12.1493 policy updated! \n",
      "train step 13059 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=09.6556 diff={max=13.8554, min=00.0013, mean=01.7334} policy_loss=-14.2540 policy updated! \n",
      "train step 13060 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.2781 diff={max=10.9873, min=00.0189, mean=01.5389} policy_loss=-11.1810 policy updated! \n",
      "train step 13061 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=09.7717 diff={max=14.2969, min=00.0012, mean=01.7954} policy_loss=-14.1084 policy updated! \n",
      "train step 13062 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=09.4340 diff={max=12.4919, min=00.0017, mean=01.6779} policy_loss=-12.5126 policy updated! \n",
      "train step 13063 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=06.7565 diff={max=09.4161, min=00.0127, mean=01.7358} policy_loss=-13.9107 policy updated! \n",
      "train step 13064 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=02.1910 diff={max=04.2529, min=00.0090, mean=01.0534} policy_loss=-10.8944 policy updated! \n",
      "train step 13065 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.6142 diff={max=05.6809, min=00.0140, mean=01.5585} policy_loss=-13.0099 policy updated! \n",
      "train step 13066 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.8834 diff={max=10.7182, min=00.0036, mean=01.5931} policy_loss=-12.0899 policy updated! \n",
      "train step 13067 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9741 diff={max=05.9942, min=00.0827, mean=01.8540} policy_loss=-14.5292 policy updated! \n",
      "train step 13068 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=08.2182 diff={max=15.6499, min=00.0204, mean=01.5400} policy_loss=-12.8659 policy updated! \n",
      "train step 13069 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.3955 diff={max=10.5314, min=00.0092, mean=02.2094} policy_loss=-14.1074 policy updated! \n",
      "train step 13070 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=14.6177 diff={max=15.3162, min=00.0783, mean=02.1777} policy_loss=-16.0974 policy updated! \n",
      "train step 13071 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9595 diff={max=08.1586, min=00.0079, mean=01.3390} policy_loss=-14.1552 policy updated! \n",
      "train step 13072 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.0965 diff={max=13.0782, min=00.0432, mean=01.5907} policy_loss=-14.4798 policy updated! \n",
      "train step 13073 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.0655 diff={max=05.3253, min=00.0078, mean=00.9716} policy_loss=-14.8132 policy updated! \n",
      "train step 13074 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0468 diff={max=12.0425, min=00.0203, mean=01.3273} policy_loss=-11.3306 policy updated! \n",
      "train step 13075 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.7987 diff={max=11.7149, min=00.0249, mean=01.6422} policy_loss=-12.7229 policy updated! \n",
      "train step 13076 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=07.3978 diff={max=06.3825, min=00.0165, mean=01.8943} policy_loss=-15.1280 policy updated! \n",
      "train step 13077 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5524 diff={max=08.5152, min=00.0050, mean=01.1047} policy_loss=-10.5691 policy updated! \n",
      "train step 13078 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=17.4455 diff={max=21.4486, min=00.0232, mean=01.9631} policy_loss=-13.1564 policy updated! \n",
      "train step 13079 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.6774 diff={max=06.9786, min=00.0014, mean=01.2294} policy_loss=-14.8447 policy updated! \n",
      "train step 13080 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=13.3769 diff={max=11.2886, min=00.0209, mean=02.3432} policy_loss=-16.0710 policy updated! \n",
      "train step 13081 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.9951 diff={max=08.0198, min=00.0221, mean=01.2280} policy_loss=-9.6546 policy updated! \n",
      "train step 13082 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2586 diff={max=10.8300, min=00.0187, mean=01.2379} policy_loss=-11.6312 policy updated! \n",
      "train step 13083 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=13.3051 diff={max=15.2514, min=00.0247, mean=02.0244} policy_loss=-14.9005 policy updated! \n",
      "train step 13084 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.7291 diff={max=08.2579, min=00.0376, mean=01.3637} policy_loss=-11.9735 policy updated! \n",
      "train step 13085 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.5426 diff={max=09.7122, min=00.0057, mean=01.5113} policy_loss=-13.5174 policy updated! \n",
      "train step 13086 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7210 diff={max=06.9194, min=00.0099, mean=01.3443} policy_loss=-13.2023 policy updated! \n",
      "train step 13087 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4268 diff={max=04.9912, min=00.0150, mean=01.3639} policy_loss=-12.3991 policy updated! \n",
      "train step 13088 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3224 diff={max=07.7880, min=00.0252, mean=01.2973} policy_loss=-13.5151 policy updated! \n",
      "train step 13089 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4580 diff={max=10.3696, min=00.0577, mean=01.5065} policy_loss=-13.3769 policy updated! \n",
      "train step 13090 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.6052 diff={max=09.0544, min=00.0006, mean=01.6488} policy_loss=-15.5935 policy updated! \n",
      "train step 13091 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.5673 diff={max=12.0995, min=00.0100, mean=01.4995} policy_loss=-16.5954 policy updated! \n",
      "train step 13092 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1011 diff={max=06.2355, min=00.0035, mean=01.4813} policy_loss=-12.9011 policy updated! \n",
      "train step 13093 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.2149 diff={max=13.8787, min=00.0017, mean=01.9524} policy_loss=-15.6155 policy updated! \n",
      "train step 13094 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4217 diff={max=07.6040, min=00.0049, mean=01.4093} policy_loss=-13.1056 policy updated! \n",
      "train step 13095 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.4301 diff={max=06.5894, min=00.0265, mean=01.5283} policy_loss=-14.4400 policy updated! \n",
      "train step 13096 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.3045 diff={max=09.7258, min=00.0479, mean=01.8347} policy_loss=-14.2960 policy updated! \n",
      "train step 13097 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6460 diff={max=10.0240, min=00.0484, mean=01.7006} policy_loss=-15.4543 policy updated! \n",
      "train step 13098 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9718 diff={max=05.9352, min=00.0383, mean=01.3978} policy_loss=-13.2097 policy updated! \n",
      "train step 13099 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.9201 diff={max=18.3142, min=00.0042, mean=02.0093} policy_loss=-12.7979 policy updated! \n",
      "train step 13100 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.4031 diff={max=11.9381, min=00.0148, mean=01.6846} policy_loss=-11.2407 policy updated! \n",
      "train step 13101 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.2274 diff={max=08.5225, min=00.0456, mean=01.5855} policy_loss=-13.9064 policy updated! \n",
      "train step 13102 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=24.9033 diff={max=19.3053, min=00.0045, mean=02.5127} policy_loss=-15.0597 policy updated! \n",
      "train step 13103 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4643 diff={max=06.7951, min=00.0102, mean=01.4086} policy_loss=-11.2800 policy updated! \n",
      "train step 13104 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3150 diff={max=11.2379, min=00.0345, mean=01.4244} policy_loss=-13.6603 policy updated! \n",
      "train step 13105 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.7890 diff={max=06.2945, min=00.0400, mean=01.3135} policy_loss=-14.2422 policy updated! \n",
      "train step 13106 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1130 diff={max=08.3864, min=00.0041, mean=01.4908} policy_loss=-15.5269 policy updated! \n",
      "train step 13107 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.8080 diff={max=09.8561, min=00.0398, mean=01.5535} policy_loss=-14.1354 policy updated! \n",
      "train step 13108 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5898 diff={max=06.3112, min=00.0002, mean=01.1992} policy_loss=-13.6950 policy updated! \n",
      "train step 13109 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.7547 diff={max=06.8107, min=00.0525, mean=01.3295} policy_loss=-14.5160 policy updated! \n",
      "train step 13110 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.5961 diff={max=12.8261, min=00.0072, mean=01.5298} policy_loss=-16.6977 policy updated! \n",
      "train step 13111 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5531 diff={max=13.1516, min=00.0103, mean=01.3875} policy_loss=-14.7923 policy updated! \n",
      "train step 13112 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.4537 diff={max=11.9067, min=00.0264, mean=01.9038} policy_loss=-14.4901 policy updated! \n",
      "train step 13113 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.4335 diff={max=07.9022, min=00.0028, mean=01.2239} policy_loss=-13.3725 policy updated! \n",
      "train step 13114 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.5923 diff={max=08.9501, min=00.0132, mean=01.5442} policy_loss=-13.7046 policy updated! \n",
      "train step 13115 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.9780 diff={max=06.2327, min=00.0025, mean=01.1147} policy_loss=-11.2652 policy updated! \n",
      "train step 13116 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5662 diff={max=09.7412, min=00.0129, mean=01.4067} policy_loss=-12.3590 policy updated! \n",
      "train step 13117 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.6247 diff={max=10.6854, min=00.0094, mean=01.8168} policy_loss=-14.7376 policy updated! \n",
      "train step 13118 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.6223 diff={max=11.6022, min=00.0123, mean=02.0501} policy_loss=-13.6418 policy updated! \n",
      "train step 13119 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=09.8091 diff={max=13.8198, min=00.0127, mean=01.6317} policy_loss=-13.0115 policy updated! \n",
      "train step 13120 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.2824 diff={max=09.7280, min=00.0076, mean=01.4792} policy_loss=-13.0721 policy updated! \n",
      "train step 13121 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.2928 diff={max=09.6400, min=00.0266, mean=01.4364} policy_loss=-11.6724 policy updated! \n",
      "train step 13122 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6573 diff={max=09.7909, min=00.0145, mean=01.6111} policy_loss=-12.8051 policy updated! \n",
      "train step 13123 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.8476 diff={max=14.0251, min=00.0009, mean=01.9833} policy_loss=-14.8215 policy updated! \n",
      "train step 13124 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0245 diff={max=09.4912, min=00.0390, mean=01.6789} policy_loss=-16.1126 policy updated! \n",
      "train step 13125 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=14.4426 diff={max=13.7957, min=00.0213, mean=02.1870} policy_loss=-14.5960 policy updated! \n",
      "train step 13126 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.2588 diff={max=14.9737, min=00.0091, mean=01.7357} policy_loss=-12.6967 policy updated! \n",
      "train step 13127 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1347 diff={max=06.6755, min=00.0018, mean=01.6737} policy_loss=-12.6661 policy updated! \n",
      "train step 13128 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=08.2978 diff={max=11.2201, min=00.0074, mean=01.7623} policy_loss=-13.1216 policy updated! \n",
      "train step 13129 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.3485 diff={max=13.1245, min=00.0292, mean=01.6950} policy_loss=-12.5681 policy updated! \n",
      "train step 13130 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.1716 diff={max=04.9726, min=00.0008, mean=00.9594} policy_loss=-12.2100 policy updated! \n",
      "train step 13131 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.0928 diff={max=17.0645, min=00.0245, mean=01.6442} policy_loss=-11.3453 policy updated! \n",
      "train step 13132 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0225 diff={max=05.9409, min=00.0284, mean=01.5275} policy_loss=-13.4566 policy updated! \n",
      "train step 13133 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.6930 diff={max=06.5531, min=00.0080, mean=01.4104} policy_loss=-12.8840 policy updated! \n",
      "train step 13134 reward={max=07.0000, min=00.0000, mean=05.4000} optimizing loss=04.6297 diff={max=07.6980, min=00.0191, mean=01.4005} policy_loss=-13.6372 policy updated! \n",
      "train step 13135 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.5720 diff={max=11.2948, min=00.0642, mean=01.8327} policy_loss=-13.8594 policy updated! \n",
      "train step 13136 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=02.9295 diff={max=06.5902, min=00.0166, mean=01.0325} policy_loss=-10.4635 policy updated! \n",
      "train step 13137 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1482 diff={max=07.1788, min=00.0292, mean=01.8707} policy_loss=-15.6986 policy updated! \n",
      "train step 13138 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=09.0090 diff={max=13.7911, min=00.0630, mean=01.9134} policy_loss=-14.8668 policy updated! \n",
      "train step 13139 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.5613 diff={max=07.6591, min=00.0713, mean=01.7994} policy_loss=-13.6229 policy updated! \n",
      "train step 13140 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.3013 diff={max=11.6468, min=00.0444, mean=01.8305} policy_loss=-14.6491 policy updated! \n",
      "train step 13141 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.4985 diff={max=10.0398, min=00.0268, mean=01.5869} policy_loss=-13.2309 policy updated! \n",
      "train step 13142 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5441 diff={max=06.3531, min=00.0184, mean=01.4883} policy_loss=-12.9204 policy updated! \n",
      "train step 13143 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=09.2202 diff={max=15.6237, min=00.0388, mean=01.6412} policy_loss=-14.6244 policy updated! \n",
      "train step 13144 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.5304 diff={max=17.1272, min=00.0302, mean=01.8359} policy_loss=-13.3097 policy updated! \n",
      "train step 13145 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=13.5448 diff={max=16.7633, min=00.0146, mean=01.9990} policy_loss=-13.3338 policy updated! \n",
      "train step 13146 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.8518 diff={max=16.5886, min=00.0015, mean=01.9222} policy_loss=-11.9444 policy updated! \n",
      "train step 13147 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=15.3957 diff={max=13.5152, min=00.0194, mean=02.7549} policy_loss=-15.5805 policy updated! \n",
      "train step 13148 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.2303 diff={max=15.6829, min=00.0176, mean=01.7894} policy_loss=-14.9702 policy updated! \n",
      "train step 13149 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.2384 diff={max=05.7311, min=00.0729, mean=01.2901} policy_loss=-11.4435 policy updated! \n",
      "train step 13150 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.5241 diff={max=06.4861, min=00.0159, mean=01.2921} policy_loss=-12.1733 policy updated! \n",
      "train step 13151 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=32.7708 diff={max=32.3458, min=00.0053, mean=02.6601} policy_loss=-14.1499 policy updated! \n",
      "train step 13152 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2488 diff={max=06.6184, min=00.0048, mean=01.3427} policy_loss=-15.0526 policy updated! \n",
      "train step 13153 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1430 diff={max=08.1480, min=00.0055, mean=01.3102} policy_loss=-14.2447 policy updated! \n",
      "train step 13154 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=09.2903 diff={max=12.4437, min=00.0120, mean=02.0960} policy_loss=-19.7532 policy updated! \n",
      "train step 13155 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=12.1677 diff={max=15.9720, min=00.0160, mean=02.2114} policy_loss=-14.1082 policy updated! \n",
      "train step 13156 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6658 diff={max=06.7240, min=00.0027, mean=01.8434} policy_loss=-14.8418 policy updated! \n",
      "train step 13157 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=14.2431 diff={max=13.2727, min=00.0340, mean=02.1074} policy_loss=-14.0755 policy updated! \n",
      "train step 13158 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5391 diff={max=06.5799, min=00.0155, mean=01.2718} policy_loss=-9.3976 policy updated! \n",
      "train step 13159 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.2293 diff={max=07.5688, min=00.0434, mean=02.0841} policy_loss=-14.2410 policy updated! \n",
      "train step 13160 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=13.4156 diff={max=15.7617, min=00.0099, mean=01.9669} policy_loss=-12.6295 policy updated! \n",
      "train step 13161 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.4912 diff={max=14.6561, min=00.0254, mean=01.8997} policy_loss=-11.3633 policy updated! \n",
      "train step 13162 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5104 diff={max=06.2794, min=00.0065, mean=01.3339} policy_loss=-13.9311 policy updated! \n",
      "train step 13163 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.4698 diff={max=10.2805, min=00.0188, mean=02.0612} policy_loss=-15.0423 policy updated! \n",
      "train step 13164 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4800 diff={max=08.7187, min=00.0012, mean=01.1682} policy_loss=-11.9463 policy updated! \n",
      "train step 13165 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.5676 diff={max=06.3866, min=00.0251, mean=01.5209} policy_loss=-16.2752 policy updated! \n",
      "train step 13166 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.2801 diff={max=14.0496, min=00.1236, mean=01.7925} policy_loss=-15.2438 policy updated! \n",
      "train step 13167 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8025 diff={max=09.5670, min=00.0114, mean=01.6263} policy_loss=-14.8852 policy updated! \n",
      "train step 13168 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.3217 diff={max=07.5698, min=00.0340, mean=01.5401} policy_loss=-11.5792 policy updated! \n",
      "train step 13169 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.5372 diff={max=09.1152, min=00.0022, mean=01.6148} policy_loss=-14.3786 policy updated! \n",
      "train step 13170 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.6131 diff={max=11.9963, min=00.0361, mean=01.6197} policy_loss=-12.2452 policy updated! \n",
      "train step 13171 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6887 diff={max=14.0278, min=00.0135, mean=01.1234} policy_loss=-11.8792 policy updated! \n",
      "train step 13172 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=15.9793 diff={max=19.8208, min=00.0110, mean=02.2787} policy_loss=-15.8691 policy updated! \n",
      "train step 13173 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1097 diff={max=06.4585, min=00.0079, mean=01.3046} policy_loss=-12.4902 policy updated! \n",
      "train step 13174 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.8078 diff={max=07.1188, min=00.0272, mean=01.7420} policy_loss=-13.9780 policy updated! \n",
      "train step 13175 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=11.8517 diff={max=13.2994, min=00.0052, mean=02.2396} policy_loss=-13.4777 policy updated! \n",
      "train step 13176 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.0118 diff={max=06.5213, min=00.0533, mean=01.1932} policy_loss=-11.1385 policy updated! \n",
      "train step 13177 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.7379 diff={max=15.6253, min=00.0123, mean=01.5974} policy_loss=-11.1043 policy updated! \n",
      "train step 13178 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8016 diff={max=07.9480, min=00.0316, mean=01.7690} policy_loss=-14.7407 policy updated! \n",
      "train step 13179 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.4266 diff={max=11.1525, min=00.0101, mean=01.9162} policy_loss=-19.1186 policy updated! \n",
      "train step 13180 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.9450 diff={max=10.4791, min=00.0092, mean=01.5483} policy_loss=-13.5382 policy updated! \n",
      "train step 13181 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1743 diff={max=11.7405, min=00.0035, mean=01.5868} policy_loss=-13.1953 policy updated! \n",
      "train step 13182 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2699 diff={max=10.1953, min=00.0297, mean=01.2413} policy_loss=-13.9717 policy updated! \n",
      "train step 13183 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.4682 diff={max=08.0042, min=00.0033, mean=01.9001} policy_loss=-14.0391 policy updated! \n",
      "train step 13184 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1867 diff={max=08.3739, min=00.0225, mean=01.0093} policy_loss=-14.3432 policy updated! \n",
      "train step 13185 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.6611 diff={max=05.7654, min=00.0304, mean=01.2667} policy_loss=-13.1595 policy updated! \n",
      "train step 13186 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.6498 diff={max=12.3629, min=00.0068, mean=01.6717} policy_loss=-12.2637 policy updated! \n",
      "train step 13187 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.0921 diff={max=12.3045, min=00.0071, mean=01.6805} policy_loss=-12.3091 policy updated! \n",
      "train step 13188 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1042 diff={max=10.4628, min=00.0083, mean=01.3105} policy_loss=-13.4673 policy updated! \n",
      "train step 13189 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.6436 diff={max=12.4777, min=00.0383, mean=01.6934} policy_loss=-13.9507 policy updated! \n",
      "train step 13190 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=11.2774 diff={max=13.4380, min=00.0186, mean=01.8735} policy_loss=-14.6958 policy updated! \n",
      "train step 13191 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=13.4888 diff={max=20.9010, min=00.0066, mean=01.9317} policy_loss=-11.7842 policy updated! \n",
      "train step 13192 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.5072 diff={max=14.6002, min=00.0004, mean=01.7320} policy_loss=-12.2126 policy updated! \n",
      "train step 13193 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=09.3343 diff={max=13.5014, min=00.0974, mean=01.7507} policy_loss=-11.3884 policy updated! \n",
      "train step 13194 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=06.7668 diff={max=09.1996, min=00.0088, mean=01.4924} policy_loss=-13.5345 policy updated! \n",
      "train step 13195 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.4687 diff={max=11.0943, min=00.0337, mean=01.6992} policy_loss=-11.1259 policy updated! \n",
      "train step 13196 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=15.9765 diff={max=10.9275, min=00.0223, mean=02.3937} policy_loss=-14.8644 policy updated! \n",
      "train step 13197 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0776 diff={max=08.6396, min=00.0271, mean=01.4816} policy_loss=-13.9332 policy updated! \n",
      "train step 13198 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=12.5794 diff={max=15.2702, min=00.0165, mean=01.9642} policy_loss=-14.0568 policy updated! \n",
      "train step 13199 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=07.3869 diff={max=09.6248, min=00.0082, mean=01.8084} policy_loss=-15.0536 policy updated! \n",
      "train step 13200 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.1640 diff={max=08.2013, min=00.0050, mean=01.8045} policy_loss=-15.6294 policy updated! \n",
      "train step 13201 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.8102 diff={max=07.2624, min=00.0026, mean=01.1983} policy_loss=-13.8652 policy updated! \n",
      "train step 13202 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3521 diff={max=05.2247, min=00.0095, mean=01.3217} policy_loss=-13.3893 policy updated! \n",
      "train step 13203 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=13.0202 diff={max=17.9723, min=00.0746, mean=02.3164} policy_loss=-15.1942 policy updated! \n",
      "train step 13204 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=16.0753 diff={max=23.6632, min=00.0413, mean=02.0495} policy_loss=-13.0577 policy updated! \n",
      "train step 13205 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.3645 diff={max=08.0063, min=00.0184, mean=01.4502} policy_loss=-11.0010 policy updated! \n",
      "train step 13206 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=09.6676 diff={max=13.7999, min=00.0652, mean=01.8556} policy_loss=-16.1009 policy updated! \n",
      "train step 13207 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=07.0056 diff={max=10.2078, min=00.0366, mean=01.6749} policy_loss=-11.5888 policy updated! \n",
      "train step 13208 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=08.4438 diff={max=12.5772, min=00.0005, mean=01.7053} policy_loss=-15.1956 policy updated! \n",
      "train step 13209 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.3073 diff={max=21.1502, min=00.0415, mean=01.8310} policy_loss=-15.2517 policy updated! \n",
      "train step 13210 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.6343 diff={max=15.2145, min=00.0348, mean=01.6554} policy_loss=-12.6892 policy updated! \n",
      "train step 13211 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.0750 diff={max=10.1304, min=00.0226, mean=01.7777} policy_loss=-11.5771 policy updated! \n",
      "train step 13212 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.5927 diff={max=05.5326, min=00.0221, mean=01.2933} policy_loss=-11.3730 policy updated! \n",
      "train step 13213 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.2230 diff={max=11.4224, min=00.0130, mean=01.7119} policy_loss=-11.9876 policy updated! \n",
      "train step 13214 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5553 diff={max=09.3305, min=00.0222, mean=01.6846} policy_loss=-11.4892 policy updated! \n",
      "train step 13215 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.9594 diff={max=12.6279, min=00.0030, mean=01.8268} policy_loss=-14.2849 policy updated! \n",
      "train step 13216 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3585 diff={max=06.5569, min=00.0338, mean=01.2126} policy_loss=-14.4429 policy updated! \n",
      "train step 13217 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9805 diff={max=05.5208, min=00.0083, mean=00.9994} policy_loss=-14.0910 policy updated! \n",
      "train step 13218 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=06.1166 diff={max=07.3066, min=00.0520, mean=01.8145} policy_loss=-17.3615 policy updated! \n",
      "train step 13219 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8230 diff={max=09.3842, min=00.0179, mean=01.3150} policy_loss=-14.8310 policy updated! \n",
      "train step 13220 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.9279 diff={max=07.0710, min=00.0065, mean=01.0499} policy_loss=-13.4915 policy updated! \n",
      "train step 13221 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9272 diff={max=08.4230, min=00.0052, mean=01.6010} policy_loss=-14.8980 policy updated! \n",
      "train step 13222 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5841 diff={max=09.0597, min=00.0024, mean=01.2955} policy_loss=-13.0542 policy updated! \n",
      "train step 13223 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.4689 diff={max=13.5834, min=00.0153, mean=02.2048} policy_loss=-15.6998 policy updated! \n",
      "train step 13224 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.0441 diff={max=15.3778, min=00.0202, mean=01.5343} policy_loss=-14.4483 policy updated! \n",
      "train step 13225 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.4883 diff={max=09.2518, min=00.0010, mean=01.3933} policy_loss=-14.4332 policy updated! \n",
      "train step 13226 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.5904 diff={max=16.3741, min=00.0123, mean=01.9219} policy_loss=-14.0793 policy updated! \n",
      "train step 13227 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=04.2699 diff={max=06.3638, min=00.0569, mean=01.4147} policy_loss=-11.8987 policy updated! \n",
      "train step 13228 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=15.8745 diff={max=16.1254, min=00.0067, mean=02.1162} policy_loss=-13.4532 policy updated! \n",
      "train step 13229 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.0660 diff={max=10.2672, min=00.0045, mean=02.0946} policy_loss=-14.1945 policy updated! \n",
      "train step 13230 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=11.6527 diff={max=13.9083, min=00.0333, mean=02.0412} policy_loss=-14.0755 policy updated! \n",
      "train step 13231 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.2996 diff={max=08.7587, min=00.0315, mean=01.8804} policy_loss=-13.2362 policy updated! \n",
      "train step 13232 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9843 diff={max=07.6220, min=00.0448, mean=01.5937} policy_loss=-13.5544 policy updated! \n",
      "train step 13233 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.0250 diff={max=09.8044, min=00.0049, mean=01.3013} policy_loss=-16.0793 policy updated! \n",
      "train step 13234 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.9082 diff={max=10.3052, min=00.1230, mean=01.9768} policy_loss=-11.5960 policy updated! \n",
      "train step 13235 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.1033 diff={max=09.9065, min=00.0750, mean=01.7540} policy_loss=-13.9800 policy updated! \n",
      "train step 13236 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0927 diff={max=06.8311, min=00.0024, mean=01.5577} policy_loss=-14.6541 policy updated! \n",
      "train step 13237 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7823 diff={max=10.5389, min=00.0474, mean=01.5687} policy_loss=-14.5945 policy updated! \n",
      "train step 13238 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=21.0079 diff={max=20.3400, min=00.0594, mean=02.5550} policy_loss=-15.6289 policy updated! \n",
      "train step 13239 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.7768 diff={max=09.9536, min=00.0641, mean=01.6932} policy_loss=-13.8284 policy updated! \n",
      "train step 13240 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.4630 diff={max=10.4217, min=00.0031, mean=01.5635} policy_loss=-12.6094 policy updated! \n",
      "train step 13241 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1906 diff={max=06.5799, min=00.0559, mean=01.8141} policy_loss=-12.0586 policy updated! \n",
      "train step 13242 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1593 diff={max=06.8658, min=00.0265, mean=01.4199} policy_loss=-11.6221 policy updated! \n",
      "train step 13243 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.2592 diff={max=10.8307, min=00.0199, mean=01.9737} policy_loss=-14.0390 policy updated! \n",
      "train step 13244 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.1314 diff={max=09.7294, min=00.0005, mean=01.6385} policy_loss=-12.8183 policy updated! \n",
      "train step 13245 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.4307 diff={max=04.3346, min=00.0043, mean=01.1579} policy_loss=-11.7730 policy updated! \n",
      "train step 13246 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8850 diff={max=08.0694, min=00.0007, mean=01.3819} policy_loss=-14.1486 policy updated! \n",
      "train step 13247 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=15.7993 diff={max=21.3682, min=00.0403, mean=02.2041} policy_loss=-14.9579 policy updated! \n",
      "train step 13248 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.0331 diff={max=06.5777, min=00.0159, mean=01.3722} policy_loss=-12.4112 policy updated! \n",
      "train step 13249 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=07.5293 diff={max=08.0925, min=00.0266, mean=01.8601} policy_loss=-13.1493 policy updated! \n",
      "train step 13250 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.8775 diff={max=05.4991, min=00.0252, mean=01.3962} policy_loss=-13.6094 policy updated! \n",
      "train step 13251 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4417 diff={max=07.6152, min=00.0067, mean=01.3695} policy_loss=-12.1794 policy updated! \n",
      "train step 13252 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3179 diff={max=07.5321, min=00.0031, mean=01.5864} policy_loss=-14.1101 policy updated! \n",
      "train step 13253 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=08.6517 diff={max=10.4418, min=00.0397, mean=01.8292} policy_loss=-11.1211 policy updated! \n",
      "train step 13254 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4166 diff={max=07.4546, min=00.0241, mean=01.4744} policy_loss=-11.9016 policy updated! \n",
      "train step 13255 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.9403 diff={max=10.0810, min=00.0226, mean=01.6457} policy_loss=-14.2351 policy updated! \n",
      "train step 13256 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.7082 diff={max=17.9819, min=00.0760, mean=01.5604} policy_loss=-11.5311 policy updated! \n",
      "train step 13257 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3604 diff={max=07.2249, min=00.0212, mean=01.8761} policy_loss=-14.1154 policy updated! \n",
      "train step 13258 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=06.6474 diff={max=08.8409, min=00.0002, mean=01.7613} policy_loss=-14.1183 policy updated! \n",
      "train step 13259 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2786 diff={max=05.8930, min=00.0192, mean=01.2180} policy_loss=-13.8647 policy updated! \n",
      "train step 13260 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.9833 diff={max=10.7672, min=00.0352, mean=01.3354} policy_loss=-13.2155 policy updated! \n",
      "train step 13261 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.8260 diff={max=09.7409, min=00.0200, mean=01.3386} policy_loss=-14.6380 policy updated! \n",
      "train step 13262 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.4924 diff={max=11.7713, min=00.0151, mean=02.2185} policy_loss=-15.3676 policy updated! \n",
      "train step 13263 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.9214 diff={max=09.7557, min=00.0131, mean=01.3652} policy_loss=-12.3174 policy updated! \n",
      "train step 13264 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=13.3918 diff={max=11.6829, min=00.0368, mean=02.1480} policy_loss=-16.3705 policy updated! \n",
      "train step 13265 reward={max=07.0000, min=00.0000, mean=02.6000} optimizing loss=02.0725 diff={max=04.7533, min=00.0174, mean=01.0281} policy_loss=-12.0785 policy updated! \n",
      "train step 13266 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7998 diff={max=05.5511, min=00.0214, mean=01.3556} policy_loss=-16.7204 policy updated! \n",
      "train step 13267 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1729 diff={max=07.0229, min=00.0286, mean=01.5703} policy_loss=-13.8695 policy updated! \n",
      "train step 13268 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.6853 diff={max=06.0874, min=00.0241, mean=01.3095} policy_loss=-15.7330 policy updated! \n",
      "train step 13269 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1273 diff={max=09.3774, min=00.0066, mean=01.6321} policy_loss=-13.3347 policy updated! \n",
      "train step 13270 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.5450 diff={max=06.9456, min=00.0162, mean=01.2866} policy_loss=-14.1955 policy updated! \n",
      "train step 13271 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=08.4581 diff={max=14.4373, min=00.0085, mean=01.6759} policy_loss=-12.2527 policy updated! \n",
      "train step 13272 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.9918 diff={max=13.8408, min=00.0135, mean=01.8096} policy_loss=-13.8539 policy updated! \n",
      "train step 13273 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5947 diff={max=09.1044, min=00.0015, mean=01.2730} policy_loss=-15.0263 policy updated! \n",
      "train step 13274 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=17.1855 diff={max=25.0438, min=00.0335, mean=01.8208} policy_loss=-14.6088 policy updated! \n",
      "train step 13275 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.1404 diff={max=06.8946, min=00.0030, mean=01.2812} policy_loss=-12.0636 policy updated! \n",
      "train step 13276 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8337 diff={max=05.0211, min=00.0492, mean=01.2426} policy_loss=-12.7792 policy updated! \n",
      "train step 13277 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.9673 diff={max=03.6810, min=00.0252, mean=01.0231} policy_loss=-15.4798 policy updated! \n",
      "train step 13278 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.2013 diff={max=05.8961, min=00.0042, mean=01.1290} policy_loss=-9.6577 policy updated! \n",
      "train step 13279 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=07.6243 diff={max=10.5382, min=00.0482, mean=01.5996} policy_loss=-10.9644 policy updated! \n",
      "train step 13280 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.9426 diff={max=12.1467, min=00.0242, mean=01.4275} policy_loss=-11.6147 policy updated! \n",
      "train step 13281 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.2690 diff={max=06.9495, min=00.0309, mean=01.7218} policy_loss=-14.8662 policy updated! \n",
      "train step 13282 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.9235 diff={max=18.6941, min=00.0070, mean=01.6524} policy_loss=-11.8576 policy updated! \n",
      "train step 13283 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.1331 diff={max=12.9590, min=00.0180, mean=01.9296} policy_loss=-12.4956 policy updated! \n",
      "train step 13284 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1063 diff={max=09.2554, min=00.0637, mean=01.7004} policy_loss=-15.0673 policy updated! \n",
      "train step 13285 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=12.2494 diff={max=16.7401, min=00.0334, mean=01.7792} policy_loss=-15.7199 policy updated! \n",
      "train step 13286 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.8901 diff={max=08.4204, min=00.0027, mean=02.0674} policy_loss=-14.9229 policy updated! \n",
      "train step 13287 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7776 diff={max=06.1244, min=00.0405, mean=01.4217} policy_loss=-15.4213 policy updated! \n",
      "train step 13288 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=14.3385 diff={max=17.6947, min=00.0258, mean=01.8251} policy_loss=-14.3345 policy updated! \n",
      "train step 13289 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.7732 diff={max=09.5734, min=00.0182, mean=01.4450} policy_loss=-11.6892 policy updated! \n",
      "train step 13290 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.4993 diff={max=16.0178, min=00.0041, mean=01.5429} policy_loss=-11.8491 policy updated! \n",
      "train step 13291 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.5770 diff={max=05.5471, min=00.0386, mean=01.0934} policy_loss=-13.2155 policy updated! \n",
      "train step 13292 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7198 diff={max=06.7806, min=00.0066, mean=00.9262} policy_loss=-10.9775 policy updated! \n",
      "train step 13293 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.9339 diff={max=19.9569, min=00.0182, mean=01.7121} policy_loss=-11.9661 policy updated! \n",
      "train step 13294 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.8765 diff={max=07.5419, min=00.0091, mean=01.5505} policy_loss=-13.8153 policy updated! \n",
      "train step 13295 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=10.7364 diff={max=12.7960, min=00.0059, mean=01.6262} policy_loss=-12.9633 policy updated! \n",
      "train step 13296 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8560 diff={max=06.3786, min=00.0103, mean=01.0557} policy_loss=-13.2143 policy updated! \n",
      "train step 13297 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0302 diff={max=07.3141, min=00.0944, mean=01.3080} policy_loss=-11.6257 policy updated! \n",
      "train step 13298 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.0623 diff={max=09.8399, min=00.0000, mean=01.8726} policy_loss=-15.2436 policy updated! \n",
      "train step 13299 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.5547 diff={max=14.8676, min=00.0043, mean=01.9428} policy_loss=-13.1126 policy updated! \n",
      "train step 13300 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=10.6400 diff={max=19.7064, min=00.0118, mean=01.4640} policy_loss=-18.2815 policy updated! \n",
      "train step 13301 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2846 diff={max=08.8961, min=00.0301, mean=01.5500} policy_loss=-15.3836 policy updated! \n",
      "train step 13302 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6448 diff={max=06.2025, min=00.0011, mean=01.0631} policy_loss=-10.7704 policy updated! \n",
      "train step 13303 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=06.2694 diff={max=07.9296, min=00.0179, mean=01.6496} policy_loss=-14.1346 policy updated! \n",
      "train step 13304 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3813 diff={max=08.0320, min=00.0491, mean=01.4787} policy_loss=-14.8192 policy updated! \n",
      "train step 13305 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.3460 diff={max=09.6703, min=00.0019, mean=01.5186} policy_loss=-11.2899 policy updated! \n",
      "train step 13306 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.1876 diff={max=08.1220, min=00.0013, mean=01.3089} policy_loss=-16.3063 policy updated! \n",
      "train step 13307 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7480 diff={max=09.7716, min=00.0193, mean=01.3744} policy_loss=-15.8348 policy updated! \n",
      "train step 13308 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=36.1425 diff={max=38.7294, min=00.0241, mean=02.1954} policy_loss=-14.2825 policy updated! \n",
      "train step 13309 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8844 diff={max=08.9354, min=00.0012, mean=01.6501} policy_loss=-14.2584 policy updated! \n",
      "train step 13310 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.2622 diff={max=06.5679, min=00.0003, mean=01.4566} policy_loss=-14.1895 policy updated! \n",
      "train step 13311 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8722 diff={max=06.6472, min=00.0165, mean=01.2819} policy_loss=-15.7617 policy updated! \n",
      "train step 13312 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.4068 diff={max=14.4576, min=00.0053, mean=01.8979} policy_loss=-17.6292 policy updated! \n",
      "train step 13313 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.1332 diff={max=15.9122, min=00.0159, mean=01.9029} policy_loss=-14.1648 policy updated! \n",
      "train step 13314 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2936 diff={max=10.1629, min=00.0143, mean=01.6675} policy_loss=-14.1406 policy updated! \n",
      "train step 13315 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.4229 diff={max=13.1960, min=00.0180, mean=01.7435} policy_loss=-16.8418 policy updated! \n",
      "train step 13316 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=09.6791 diff={max=14.2391, min=00.0049, mean=01.6843} policy_loss=-14.1799 policy updated! \n",
      "train step 13317 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7971 diff={max=05.2638, min=00.0063, mean=01.0973} policy_loss=-10.6473 policy updated! \n",
      "train step 13318 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.4587 diff={max=09.9746, min=00.0130, mean=01.9329} policy_loss=-13.9521 policy updated! \n",
      "train step 13319 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=10.2670 diff={max=18.3995, min=00.0245, mean=01.5818} policy_loss=-11.7550 policy updated! \n",
      "train step 13320 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=42.2140 diff={max=41.7966, min=00.0370, mean=02.3507} policy_loss=-14.6807 policy updated! \n",
      "train step 13321 reward={max=07.0000, min=00.0000, mean=04.0000} optimizing loss=02.1666 diff={max=03.7414, min=00.0645, mean=01.1288} policy_loss=-15.8241 policy updated! \n",
      "train step 13322 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.1620 diff={max=11.2451, min=00.0270, mean=01.8824} policy_loss=-17.3939 policy updated! \n",
      "train step 13323 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.5856 diff={max=10.6281, min=00.0269, mean=01.4403} policy_loss=-14.5308 policy updated! \n",
      "train step 13324 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7368 diff={max=08.8565, min=00.0156, mean=01.5202} policy_loss=-11.4134 policy updated! \n",
      "train step 13325 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.6439 diff={max=11.0533, min=00.0746, mean=01.8648} policy_loss=-16.1088 policy updated! \n",
      "train step 13326 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=25.6030 diff={max=25.5290, min=00.0418, mean=02.8116} policy_loss=-18.0627 policy updated! \n",
      "train step 13327 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4464 diff={max=05.3681, min=00.0120, mean=01.7830} policy_loss=-14.8123 policy updated! \n",
      "train step 13328 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5038 diff={max=06.9891, min=00.0099, mean=01.6701} policy_loss=-12.2199 policy updated! \n",
      "train step 13329 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.5140 diff={max=07.0979, min=00.0265, mean=01.3810} policy_loss=-14.2426 policy updated! \n",
      "train step 13330 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.2611 diff={max=07.8828, min=00.0571, mean=01.7919} policy_loss=-12.6568 policy updated! \n",
      "train step 13331 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=08.7511 diff={max=08.0871, min=00.0053, mean=02.0562} policy_loss=-15.0822 policy updated! \n",
      "train step 13332 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8208 diff={max=09.9568, min=00.0083, mean=01.6347} policy_loss=-11.6572 policy updated! \n",
      "train step 13333 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=06.3542 diff={max=10.1370, min=00.0432, mean=01.6020} policy_loss=-9.6292 policy updated! \n",
      "train step 13334 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=08.6076 diff={max=09.7656, min=00.0013, mean=01.9200} policy_loss=-12.1408 policy updated! \n",
      "train step 13335 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.5285 diff={max=10.2507, min=00.0181, mean=01.2602} policy_loss=-10.1038 policy updated! \n",
      "train step 13336 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.6763 diff={max=15.7757, min=00.0398, mean=01.6645} policy_loss=-11.4292 policy updated! \n",
      "train step 13337 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1268 diff={max=07.6700, min=00.0514, mean=01.6313} policy_loss=-14.9020 policy updated! \n",
      "train step 13338 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4728 diff={max=05.9482, min=00.0364, mean=01.2958} policy_loss=-12.4816 policy updated! \n",
      "train step 13339 reward={max=07.0000, min=00.0000, mean=05.4000} optimizing loss=07.7208 diff={max=09.6224, min=00.0039, mean=01.9333} policy_loss=-17.1479 policy updated! \n",
      "train step 13340 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.6228 diff={max=07.5250, min=00.0278, mean=01.8119} policy_loss=-14.5676 policy updated! \n",
      "train step 13341 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9275 diff={max=08.8960, min=00.0327, mean=01.5284} policy_loss=-13.7717 policy updated! \n",
      "train step 13342 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2886 diff={max=07.2850, min=00.0579, mean=01.7007} policy_loss=-15.8351 policy updated! \n",
      "train step 13343 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.5001 diff={max=14.6212, min=00.0221, mean=01.9995} policy_loss=-15.0145 policy updated! \n",
      "train step 13344 reward={max=07.0000, min=00.0000, mean=05.4000} optimizing loss=08.4511 diff={max=09.4605, min=00.0276, mean=01.8862} policy_loss=-14.4905 policy updated! \n",
      "train step 13345 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.5321 diff={max=10.6035, min=00.0212, mean=01.6645} policy_loss=-16.1402 policy updated! \n",
      "train step 13346 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6627 diff={max=11.6928, min=00.0009, mean=01.5745} policy_loss=-13.2012 policy updated! \n",
      "train step 13347 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.9047 diff={max=13.2267, min=00.0069, mean=01.5766} policy_loss=-14.7238 policy updated! \n",
      "train step 13348 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.8858 diff={max=06.0458, min=00.0013, mean=01.5382} policy_loss=-14.3725 policy updated! \n",
      "train step 13349 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=12.3478 diff={max=13.1844, min=00.0174, mean=02.0965} policy_loss=-14.8720 policy updated! \n",
      "train step 13350 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.8740 diff={max=07.0973, min=00.0005, mean=01.7836} policy_loss=-10.1983 policy updated! \n",
      "train step 13351 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6707 diff={max=06.1236, min=00.0008, mean=01.6781} policy_loss=-15.8986 policy updated! \n",
      "train step 13352 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.0084 diff={max=12.4384, min=00.0159, mean=01.8501} policy_loss=-12.5850 policy updated! \n",
      "train step 13353 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5998 diff={max=07.2730, min=00.0019, mean=01.4921} policy_loss=-11.5034 policy updated! \n",
      "train step 13354 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=07.4938 diff={max=12.1188, min=00.0006, mean=01.6868} policy_loss=-12.2608 policy updated! \n",
      "train step 13355 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.9771 diff={max=10.6508, min=00.0001, mean=01.7787} policy_loss=-13.1741 policy updated! \n",
      "train step 13356 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=15.9277 diff={max=21.7299, min=00.0101, mean=01.9023} policy_loss=-13.9342 policy updated! \n",
      "train step 13357 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.2534 diff={max=13.8441, min=00.0088, mean=01.8642} policy_loss=-14.5462 policy updated! \n",
      "train step 13358 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=07.5820 diff={max=11.4106, min=00.0346, mean=01.6465} policy_loss=-14.2089 policy updated! \n",
      "train step 13359 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1862 diff={max=08.9399, min=00.0263, mean=01.7261} policy_loss=-14.5742 policy updated! \n",
      "train step 13360 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.7558 diff={max=07.8984, min=00.0215, mean=01.2224} policy_loss=-13.4938 policy updated! \n",
      "train step 13361 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.8806 diff={max=12.1980, min=00.0004, mean=01.7560} policy_loss=-16.2450 policy updated! \n",
      "train step 13362 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9062 diff={max=07.7277, min=00.0011, mean=01.5878} policy_loss=-14.1425 policy updated! \n",
      "train step 13363 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=17.3180 diff={max=14.6725, min=00.0185, mean=02.6256} policy_loss=-14.1829 policy updated! \n",
      "train step 13364 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9642 diff={max=06.3298, min=00.0032, mean=01.3756} policy_loss=-13.9631 policy updated! \n",
      "train step 13365 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.8993 diff={max=09.3924, min=00.0080, mean=01.9119} policy_loss=-13.5716 policy updated! \n",
      "train step 13366 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.4805 diff={max=04.4688, min=00.0600, mean=01.0816} policy_loss=-15.2616 policy updated! \n",
      "train step 13367 reward={max=07.0000, min=06.0000, mean=06.8000} optimizing loss=05.1459 diff={max=06.8242, min=00.0113, mean=01.5069} policy_loss=-13.7199 policy updated! \n",
      "train step 13368 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9070 diff={max=09.9411, min=00.0031, mean=01.5028} policy_loss=-11.4521 policy updated! \n",
      "train step 13369 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6389 diff={max=09.1448, min=00.0728, mean=01.3153} policy_loss=-11.7368 policy updated! \n",
      "train step 13370 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=10.4230 diff={max=10.2663, min=00.0265, mean=01.9978} policy_loss=-15.1839 policy updated! \n",
      "train step 13371 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.6025 diff={max=08.2687, min=00.0615, mean=02.0195} policy_loss=-13.0273 policy updated! \n",
      "train step 13372 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.3909 diff={max=08.0475, min=00.0272, mean=01.1379} policy_loss=-9.3757 policy updated! \n",
      "train step 13373 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5182 diff={max=05.1569, min=00.0134, mean=01.0868} policy_loss=-15.8804 policy updated! \n",
      "train step 13374 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7232 diff={max=07.5668, min=00.0348, mean=01.5441} policy_loss=-10.8751 policy updated! \n",
      "train step 13375 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.7714 diff={max=07.6181, min=00.0137, mean=01.5211} policy_loss=-14.2657 policy updated! \n",
      "train step 13376 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3084 diff={max=07.5444, min=00.0119, mean=01.5396} policy_loss=-14.3887 policy updated! \n",
      "train step 13377 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.3194 diff={max=13.2592, min=00.0214, mean=01.9965} policy_loss=-15.7180 policy updated! \n",
      "train step 13378 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.8103 diff={max=09.0191, min=00.0652, mean=01.7886} policy_loss=-10.4911 policy updated! \n",
      "train step 13379 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=04.9943 diff={max=07.9942, min=00.0120, mean=01.4099} policy_loss=-13.7286 policy updated! \n",
      "train step 13380 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=11.3263 diff={max=09.5940, min=00.0126, mean=02.0587} policy_loss=-13.6820 policy updated! \n",
      "train step 13381 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=15.9548 diff={max=14.3522, min=00.0009, mean=02.2540} policy_loss=-13.7245 policy updated! \n",
      "train step 13382 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.5316 diff={max=10.5050, min=00.0782, mean=01.5805} policy_loss=-13.0553 policy updated! \n",
      "train step 13383 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=03.3270 diff={max=07.7782, min=00.0380, mean=01.1683} policy_loss=-12.2858 policy updated! \n",
      "train step 13384 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=01.7159 diff={max=03.7967, min=00.0316, mean=00.9625} policy_loss=-11.5222 policy updated! \n",
      "train step 13385 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.8820 diff={max=05.8341, min=00.0178, mean=01.3982} policy_loss=-11.5560 policy updated! \n",
      "train step 13386 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=17.4233 diff={max=20.8684, min=00.0003, mean=01.9979} policy_loss=-14.2123 policy updated! \n",
      "train step 13387 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0474 diff={max=08.5463, min=00.0194, mean=01.6737} policy_loss=-14.4067 policy updated! \n",
      "train step 13388 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.1445 diff={max=09.4882, min=00.0155, mean=01.9971} policy_loss=-15.1320 policy updated! \n",
      "train step 13389 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.6441 diff={max=10.4863, min=00.0027, mean=01.1915} policy_loss=-14.0787 policy updated! \n",
      "train step 13390 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=11.7360 diff={max=13.4334, min=00.0287, mean=02.1280} policy_loss=-15.4650 policy updated! \n",
      "train step 13391 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.6084 diff={max=06.1372, min=00.0222, mean=01.0471} policy_loss=-14.0263 policy updated! \n",
      "train step 13392 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1006 diff={max=07.9505, min=00.0462, mean=01.4634} policy_loss=-13.2293 policy updated! \n",
      "train step 13393 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=06.0983 diff={max=11.7061, min=00.0095, mean=01.3850} policy_loss=-15.5647 policy updated! \n",
      "train step 13394 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8572 diff={max=05.4350, min=00.0288, mean=01.3049} policy_loss=-11.4384 policy updated! \n",
      "train step 13395 reward={max=08.0000, min=00.0000, mean=01.6000} optimizing loss=07.8196 diff={max=09.3590, min=00.0021, mean=01.7937} policy_loss=-14.6853 policy updated! \n",
      "train step 13396 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5972 diff={max=08.9391, min=00.0088, mean=01.4982} policy_loss=-14.3795 policy updated! \n",
      "train step 13397 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9614 diff={max=06.8056, min=00.0018, mean=01.2286} policy_loss=-14.1385 policy updated! \n",
      "train step 13398 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.9557 diff={max=08.1100, min=00.0104, mean=01.4221} policy_loss=-13.5648 policy updated! \n",
      "train step 13399 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.2914 diff={max=14.9222, min=00.0207, mean=01.6092} policy_loss=-14.6718 policy updated! \n",
      "train step 13400 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.0527 diff={max=08.5317, min=00.0671, mean=01.7185} policy_loss=-14.5368 policy updated! \n",
      "train step 13401 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=08.4959 diff={max=12.4088, min=00.0005, mean=01.6241} policy_loss=-14.7103 policy updated! \n",
      "train step 13402 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5533 diff={max=09.5037, min=00.0228, mean=01.4344} policy_loss=-15.2233 policy updated! \n",
      "train step 13403 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=16.2360 diff={max=21.0531, min=00.0289, mean=01.9377} policy_loss=-9.8997 policy updated! \n",
      "train step 13404 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.8127 diff={max=07.0396, min=00.0054, mean=01.5905} policy_loss=-15.1774 policy updated! \n",
      "train step 13405 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.0258 diff={max=06.5147, min=00.0328, mean=01.6925} policy_loss=-15.0246 policy updated! \n",
      "train step 13406 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9370 diff={max=10.0442, min=00.0910, mean=01.2353} policy_loss=-12.0425 policy updated! \n",
      "train step 13407 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7081 diff={max=05.7042, min=00.0112, mean=01.6766} policy_loss=-13.6997 policy updated! \n",
      "train step 13408 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.2698 diff={max=04.7672, min=00.0566, mean=01.5225} policy_loss=-14.7218 policy updated! \n",
      "train step 13409 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=10.6253 diff={max=15.7054, min=00.0369, mean=01.7972} policy_loss=-13.3495 policy updated! \n",
      "train step 13410 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.5974 diff={max=07.9832, min=00.0027, mean=00.9857} policy_loss=-11.5977 policy updated! \n",
      "train step 13411 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.9778 diff={max=06.3981, min=00.0372, mean=01.1560} policy_loss=-10.1785 policy updated! \n",
      "train step 13412 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.3166 diff={max=11.4720, min=00.0070, mean=01.7849} policy_loss=-13.2909 policy updated! \n",
      "train step 13413 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2590 diff={max=08.8219, min=00.0477, mean=01.4214} policy_loss=-12.5642 policy updated! \n",
      "train step 13414 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.2095 diff={max=16.2849, min=00.0292, mean=01.9528} policy_loss=-11.6274 policy updated! \n",
      "train step 13415 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.3655 diff={max=15.2717, min=00.0386, mean=01.3951} policy_loss=-9.7243 policy updated! \n",
      "train step 13416 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.2301 diff={max=07.7254, min=00.0151, mean=01.2918} policy_loss=-11.0051 policy updated! \n",
      "train step 13417 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=08.7334 diff={max=13.0548, min=00.0294, mean=01.4889} policy_loss=-15.3491 policy updated! \n",
      "train step 13418 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.2347 diff={max=11.3550, min=00.0123, mean=01.8472} policy_loss=-13.1031 policy updated! \n",
      "train step 13419 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.9688 diff={max=10.2495, min=00.0056, mean=01.8312} policy_loss=-13.3142 policy updated! \n",
      "train step 13420 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=11.4812 diff={max=09.0171, min=00.0359, mean=02.1408} policy_loss=-12.0530 policy updated! \n",
      "train step 13421 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.2073 diff={max=09.3719, min=00.0382, mean=01.6406} policy_loss=-14.7745 policy updated! \n",
      "train step 13422 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.9827 diff={max=12.1518, min=00.0027, mean=01.8253} policy_loss=-14.6989 policy updated! \n",
      "train step 13423 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0056 diff={max=10.5078, min=00.0036, mean=01.4064} policy_loss=-15.9416 policy updated! \n",
      "train step 13424 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.2826 diff={max=06.0051, min=00.0118, mean=01.2062} policy_loss=-15.6121 policy updated! \n",
      "train step 13425 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.5515 diff={max=04.4717, min=00.0439, mean=01.2335} policy_loss=-17.2986 policy updated! \n",
      "train step 13426 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.4471 diff={max=08.0930, min=00.0106, mean=01.4070} policy_loss=-13.5957 policy updated! \n",
      "train step 13427 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.5169 diff={max=11.8677, min=00.0329, mean=01.9906} policy_loss=-12.5226 policy updated! \n",
      "train step 13428 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=14.8705 diff={max=19.7043, min=00.0048, mean=02.0314} policy_loss=-16.0776 policy updated! \n",
      "train step 13429 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.0773 diff={max=16.3760, min=00.0394, mean=01.8340} policy_loss=-14.0633 policy updated! \n",
      "train step 13430 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.2221 diff={max=04.8454, min=00.0248, mean=01.0882} policy_loss=-16.5135 policy updated! \n",
      "train step 13431 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=09.9390 diff={max=17.5798, min=00.0361, mean=01.6913} policy_loss=-14.1361 policy updated! \n",
      "train step 13432 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5985 diff={max=10.6429, min=00.0383, mean=01.5211} policy_loss=-10.5528 policy updated! \n",
      "train step 13433 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.8828 diff={max=16.7822, min=00.0037, mean=02.2926} policy_loss=-11.9429 policy updated! \n",
      "train step 13434 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6972 diff={max=08.9135, min=00.0217, mean=01.7671} policy_loss=-14.4060 policy updated! \n",
      "train step 13435 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.9271 diff={max=04.6052, min=00.0041, mean=01.2894} policy_loss=-10.8210 policy updated! \n",
      "train step 13436 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3941 diff={max=03.6538, min=00.0247, mean=01.1639} policy_loss=-13.9560 policy updated! \n",
      "train step 13437 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1637 diff={max=09.0564, min=00.0222, mean=01.6112} policy_loss=-12.6012 policy updated! \n",
      "train step 13438 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.5969 diff={max=17.3055, min=00.0038, mean=01.5746} policy_loss=-14.5864 policy updated! \n",
      "train step 13439 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.4271 diff={max=10.2541, min=00.0049, mean=01.4753} policy_loss=-14.7575 policy updated! \n",
      "train step 13440 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.3069 diff={max=09.4138, min=00.0201, mean=01.5876} policy_loss=-13.8873 policy updated! \n",
      "train step 13441 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.7105 diff={max=13.7732, min=00.0112, mean=01.7607} policy_loss=-13.2837 policy updated! \n",
      "train step 13442 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0450 diff={max=07.5440, min=00.1161, mean=01.3966} policy_loss=-11.8682 policy updated! \n",
      "train step 13443 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=15.0106 diff={max=16.2328, min=00.0015, mean=02.2604} policy_loss=-14.6966 policy updated! \n",
      "train step 13444 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3957 diff={max=09.6050, min=00.0139, mean=01.6376} policy_loss=-12.9485 policy updated! \n",
      "train step 13445 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.9088 diff={max=11.3517, min=00.0183, mean=01.5432} policy_loss=-11.8877 policy updated! \n",
      "train step 13446 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=30.0322 diff={max=35.7193, min=00.0007, mean=01.8437} policy_loss=-11.1349 policy updated! \n",
      "train step 13447 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.8011 diff={max=15.0185, min=00.0028, mean=01.4664} policy_loss=-13.6676 policy updated! \n",
      "train step 13448 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1344 diff={max=08.5969, min=00.0060, mean=01.4668} policy_loss=-10.8118 policy updated! \n",
      "train step 13449 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.7099 diff={max=13.7698, min=00.0075, mean=02.0889} policy_loss=-14.1391 policy updated! \n",
      "train step 13450 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=17.7814 diff={max=17.0790, min=00.0023, mean=02.4854} policy_loss=-15.5221 policy updated! \n",
      "train step 13451 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5696 diff={max=04.7375, min=00.0503, mean=01.2530} policy_loss=-14.6758 policy updated! \n",
      "train step 13452 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6009 diff={max=09.4962, min=00.0039, mean=01.7688} policy_loss=-12.6336 policy updated! \n",
      "train step 13453 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.7466 diff={max=04.9443, min=00.0084, mean=01.0388} policy_loss=-12.1480 policy updated! \n",
      "train step 13454 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0366 diff={max=05.4872, min=00.0331, mean=01.2853} policy_loss=-12.3332 policy updated! \n",
      "train step 13455 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=20.1204 diff={max=24.1129, min=00.0485, mean=01.8920} policy_loss=-15.7949 policy updated! \n",
      "train step 13456 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.2239 diff={max=10.9228, min=00.0016, mean=01.2878} policy_loss=-14.5874 policy updated! \n",
      "train step 13457 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.2896 diff={max=11.8507, min=00.0440, mean=01.8933} policy_loss=-12.0269 policy updated! \n",
      "train step 13458 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=06.2523 diff={max=10.4779, min=00.0262, mean=01.5221} policy_loss=-12.8745 policy updated! \n",
      "train step 13459 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.7644 diff={max=08.9718, min=00.0042, mean=01.5413} policy_loss=-12.1181 policy updated! \n",
      "train step 13460 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=02.3076 diff={max=04.8056, min=00.0527, mean=01.1500} policy_loss=-12.5668 policy updated! \n",
      "train step 13461 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0164 diff={max=07.5691, min=00.0324, mean=01.3716} policy_loss=-13.3499 policy updated! \n",
      "train step 13462 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.8739 diff={max=09.3397, min=00.0363, mean=02.2606} policy_loss=-15.8147 policy updated! \n",
      "train step 13463 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.0742 diff={max=06.3749, min=00.0134, mean=01.2666} policy_loss=-13.8129 policy updated! \n",
      "train step 13464 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0239 diff={max=09.7550, min=00.0461, mean=01.1379} policy_loss=-11.8044 policy updated! \n",
      "train step 13465 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.7892 diff={max=12.8594, min=00.0107, mean=01.5301} policy_loss=-15.7413 policy updated! \n",
      "train step 13466 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.2279 diff={max=08.4638, min=00.0087, mean=01.4728} policy_loss=-12.0905 policy updated! \n",
      "train step 13467 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.6494 diff={max=17.6093, min=00.0084, mean=01.5909} policy_loss=-13.9888 policy updated! \n",
      "train step 13468 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.5607 diff={max=15.4736, min=00.0092, mean=01.9830} policy_loss=-16.3308 policy updated! \n",
      "train step 13469 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2994 diff={max=07.2727, min=00.0029, mean=01.4651} policy_loss=-12.2880 policy updated! \n",
      "train step 13470 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.2249 diff={max=13.0745, min=00.0698, mean=01.6451} policy_loss=-14.6076 policy updated! \n",
      "train step 13471 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.0377 diff={max=10.0129, min=00.0115, mean=01.8466} policy_loss=-15.6489 policy updated! \n",
      "train step 13472 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6811 diff={max=07.8217, min=00.0172, mean=01.7530} policy_loss=-15.3771 policy updated! \n",
      "train step 13473 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7614 diff={max=07.6120, min=00.0043, mean=01.3239} policy_loss=-14.4252 policy updated! \n",
      "train step 13474 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.2338 diff={max=05.2426, min=00.0032, mean=01.1964} policy_loss=-15.9873 policy updated! \n",
      "train step 13475 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.9436 diff={max=09.1893, min=00.0016, mean=01.7437} policy_loss=-13.7403 policy updated! \n",
      "train step 13476 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2988 diff={max=10.6105, min=00.0064, mean=01.5349} policy_loss=-14.7222 policy updated! \n",
      "train step 13477 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.6912 diff={max=21.2895, min=00.0004, mean=01.8324} policy_loss=-11.1898 policy updated! \n",
      "train step 13478 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7825 diff={max=08.8975, min=00.0010, mean=01.1578} policy_loss=-14.0617 policy updated! \n",
      "train step 13479 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5606 diff={max=05.1402, min=00.0091, mean=01.1005} policy_loss=-11.1198 policy updated! \n",
      "train step 13480 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.6598 diff={max=05.7874, min=00.0128, mean=01.2466} policy_loss=-18.2107 policy updated! \n",
      "train step 13481 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.4024 diff={max=12.7968, min=00.0040, mean=01.6829} policy_loss=-15.5366 policy updated! \n",
      "train step 13482 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4538 diff={max=08.8092, min=00.0105, mean=01.4193} policy_loss=-10.8308 policy updated! \n",
      "train step 13483 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1115 diff={max=09.2072, min=00.0238, mean=01.4542} policy_loss=-14.3466 policy updated! \n",
      "train step 13484 reward={max=07.0000, min=00.0000, mean=05.4000} optimizing loss=19.6011 diff={max=25.6873, min=00.0158, mean=02.0044} policy_loss=-12.3748 policy updated! \n",
      "train step 13485 reward={max=08.0000, min=07.0000, mean=07.2000} optimizing loss=02.6373 diff={max=05.0141, min=00.0089, mean=01.0643} policy_loss=-12.9226 policy updated! \n",
      "train step 13486 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.2029 diff={max=06.6156, min=00.0317, mean=01.1589} policy_loss=-12.3648 policy updated! \n",
      "train step 13487 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1623 diff={max=07.8571, min=00.0185, mean=01.0158} policy_loss=-12.4691 policy updated! \n",
      "train step 13488 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=04.4311 diff={max=07.4484, min=00.0484, mean=01.3990} policy_loss=-13.2811 policy updated! \n",
      "train step 13489 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0162 diff={max=09.9012, min=00.0238, mean=01.4469} policy_loss=-13.6807 policy updated! \n",
      "train step 13490 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.3337 diff={max=10.3064, min=00.0693, mean=01.5515} policy_loss=-14.1506 policy updated! \n",
      "train step 13491 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.5556 diff={max=10.0352, min=00.0213, mean=01.7294} policy_loss=-16.1373 policy updated! \n",
      "train step 13492 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5358 diff={max=10.2626, min=00.0010, mean=01.4823} policy_loss=-13.1888 policy updated! \n",
      "train step 13493 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=15.2067 diff={max=25.5157, min=00.0391, mean=01.5728} policy_loss=-12.6392 policy updated! \n",
      "train step 13494 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5061 diff={max=08.0881, min=00.0475, mean=01.5330} policy_loss=-15.9454 policy updated! \n",
      "train step 13495 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.2971 diff={max=09.4205, min=00.0327, mean=01.2819} policy_loss=-10.8019 policy updated! \n",
      "train step 13496 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.4361 diff={max=07.8330, min=00.0035, mean=01.6939} policy_loss=-14.6978 policy updated! \n",
      "train step 13497 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.9099 diff={max=11.0191, min=00.0229, mean=01.7555} policy_loss=-16.5234 policy updated! \n",
      "train step 13498 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4043 diff={max=06.8239, min=00.0083, mean=01.3956} policy_loss=-13.7554 policy updated! \n",
      "train step 13499 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.6224 diff={max=09.5952, min=00.0090, mean=01.1118} policy_loss=-11.5296 policy updated! \n",
      "train step 13500 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.9898 diff={max=06.9493, min=00.0046, mean=01.5166} policy_loss=-12.6427 policy updated! \n",
      "train step 13501 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=14.6025 diff={max=14.0851, min=00.0169, mean=02.0127} policy_loss=-14.3815 policy updated! \n",
      "train step 13502 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1689 diff={max=07.6276, min=00.0645, mean=01.7544} policy_loss=-14.6799 policy updated! \n",
      "train step 13503 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.1315 diff={max=10.2227, min=00.0026, mean=01.6283} policy_loss=-13.9765 policy updated! \n",
      "train step 13504 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.4264 diff={max=09.3586, min=00.0029, mean=01.6641} policy_loss=-14.3034 policy updated! \n",
      "train step 13505 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.0309 diff={max=10.0517, min=00.0147, mean=01.7118} policy_loss=-10.9260 policy updated! \n",
      "train step 13506 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5089 diff={max=10.4618, min=00.0043, mean=01.1808} policy_loss=-14.2504 policy updated! \n",
      "train step 13507 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.8673 diff={max=14.8311, min=00.0054, mean=01.5888} policy_loss=-12.4356 policy updated! \n",
      "train step 13508 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.9652 diff={max=09.2507, min=00.0190, mean=01.6293} policy_loss=-13.6231 policy updated! \n",
      "train step 13509 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=07.5525 diff={max=07.8254, min=00.0083, mean=01.7586} policy_loss=-12.7436 policy updated! \n",
      "train step 13510 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.4137 diff={max=08.2494, min=00.0075, mean=01.0982} policy_loss=-14.2670 policy updated! \n",
      "train step 13511 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.4201 diff={max=14.3061, min=00.0701, mean=01.9261} policy_loss=-17.9298 policy updated! \n",
      "train step 13512 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3943 diff={max=08.3290, min=00.0570, mean=01.7609} policy_loss=-13.4882 policy updated! \n",
      "train step 13513 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.9598 diff={max=08.5465, min=00.0286, mean=01.8951} policy_loss=-13.7157 policy updated! \n",
      "train step 13514 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.2171 diff={max=08.8830, min=00.0062, mean=01.7813} policy_loss=-11.4337 policy updated! \n",
      "train step 13515 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.5851 diff={max=06.1343, min=00.0066, mean=01.2359} policy_loss=-13.6689 policy updated! \n",
      "train step 13516 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=06.1800 diff={max=07.2122, min=00.0401, mean=01.6673} policy_loss=-14.3134 policy updated! \n",
      "train step 13517 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3060 diff={max=09.5605, min=00.0242, mean=01.5638} policy_loss=-11.3137 policy updated! \n",
      "train step 13518 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.9512 diff={max=10.8620, min=00.0094, mean=01.4622} policy_loss=-13.4928 policy updated! \n",
      "train step 13519 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=14.8769 diff={max=15.6413, min=00.0398, mean=01.9348} policy_loss=-13.4296 policy updated! \n",
      "train step 13520 reward={max=07.0000, min=00.0000, mean=02.6000} optimizing loss=06.9087 diff={max=10.1376, min=00.0423, mean=01.6162} policy_loss=-12.0075 policy updated! \n",
      "train step 13521 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.0307 diff={max=08.3633, min=00.0123, mean=01.0983} policy_loss=-12.5194 policy updated! \n",
      "train step 13522 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2227 diff={max=10.2314, min=00.0053, mean=01.4777} policy_loss=-13.5425 policy updated! \n",
      "train step 13523 reward={max=07.0000, min=00.0000, mean=02.6000} optimizing loss=04.9161 diff={max=09.0870, min=00.0243, mean=01.3289} policy_loss=-15.0386 policy updated! \n",
      "train step 13524 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9234 diff={max=06.4172, min=00.0114, mean=01.4095} policy_loss=-12.9311 policy updated! \n",
      "train step 13525 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.2641 diff={max=10.7310, min=00.0154, mean=01.8388} policy_loss=-14.0300 policy updated! \n",
      "train step 13526 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3129 diff={max=10.1557, min=00.0119, mean=01.3505} policy_loss=-13.3101 policy updated! \n",
      "train step 13527 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.1364 diff={max=13.4201, min=00.0680, mean=01.7031} policy_loss=-10.8514 policy updated! \n",
      "train step 13528 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.8015 diff={max=11.7757, min=00.0497, mean=01.7891} policy_loss=-16.8506 policy updated! \n",
      "train step 13529 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=03.8549 diff={max=05.7689, min=00.0194, mean=01.3365} policy_loss=-17.4942 policy updated! \n",
      "train step 13530 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.1562 diff={max=13.1126, min=00.0174, mean=01.3495} policy_loss=-16.8380 policy updated! \n",
      "train step 13531 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.2467 diff={max=08.8456, min=00.0424, mean=02.2867} policy_loss=-15.8365 policy updated! \n",
      "train step 13532 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4931 diff={max=07.5398, min=00.0347, mean=01.5818} policy_loss=-13.1387 policy updated! \n",
      "train step 13533 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9257 diff={max=06.2240, min=00.0058, mean=01.5149} policy_loss=-19.1944 policy updated! \n",
      "train step 13534 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=01.9318 diff={max=03.8964, min=00.0163, mean=01.0201} policy_loss=-12.4421 policy updated! \n",
      "train step 13535 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.4895 diff={max=11.6491, min=00.0203, mean=01.4630} policy_loss=-13.6125 policy updated! \n",
      "train step 13536 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7069 diff={max=07.9007, min=00.0119, mean=01.4653} policy_loss=-13.4051 policy updated! \n",
      "train step 13537 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4915 diff={max=06.4453, min=00.0220, mean=01.3419} policy_loss=-14.7516 policy updated! \n",
      "train step 13538 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.7661 diff={max=09.4881, min=00.0197, mean=01.7693} policy_loss=-14.8860 policy updated! \n",
      "train step 13539 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.0689 diff={max=07.4681, min=00.0408, mean=01.3212} policy_loss=-15.6040 policy updated! \n",
      "train step 13540 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.7488 diff={max=13.6412, min=00.0043, mean=01.6914} policy_loss=-15.5582 policy updated! \n",
      "train step 13541 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.4587 diff={max=07.1506, min=00.0124, mean=01.3707} policy_loss=-14.6835 policy updated! \n",
      "train step 13542 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.8204 diff={max=06.9956, min=00.0186, mean=01.7742} policy_loss=-13.1164 policy updated! \n",
      "train step 13543 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.6103 diff={max=11.7171, min=00.0123, mean=01.2607} policy_loss=-12.7594 policy updated! \n",
      "train step 13544 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=19.9946 diff={max=15.1230, min=00.0279, mean=02.4248} policy_loss=-13.7971 policy updated! \n",
      "train step 13545 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=09.0265 diff={max=12.5772, min=00.0082, mean=01.8304} policy_loss=-15.0870 policy updated! \n",
      "train step 13546 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=03.6891 diff={max=09.7302, min=00.0118, mean=01.1478} policy_loss=-12.4678 policy updated! \n",
      "train step 13547 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4389 diff={max=06.3859, min=00.0035, mean=01.4527} policy_loss=-13.3301 policy updated! \n",
      "train step 13548 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.9867 diff={max=09.2601, min=00.0128, mean=01.6171} policy_loss=-13.5277 policy updated! \n",
      "train step 13549 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=05.8212 diff={max=09.4899, min=00.0166, mean=01.5093} policy_loss=-14.5072 policy updated! \n",
      "train step 13550 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.6015 diff={max=10.6353, min=00.0425, mean=01.0607} policy_loss=-11.4755 policy updated! \n",
      "train step 13551 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=02.4050 diff={max=04.6655, min=00.0758, mean=01.0979} policy_loss=-12.7638 policy updated! \n",
      "train step 13552 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.1385 diff={max=05.9040, min=00.0145, mean=01.1599} policy_loss=-13.0756 policy updated! \n",
      "train step 13553 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.7921 diff={max=08.5998, min=00.0266, mean=01.4786} policy_loss=-13.5678 policy updated! \n",
      "train step 13554 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0644 diff={max=10.4918, min=00.0472, mean=01.3393} policy_loss=-11.5898 policy updated! \n",
      "train step 13555 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=08.0585 diff={max=11.3578, min=00.0109, mean=01.6655} policy_loss=-13.5039 policy updated! \n",
      "train step 13556 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.3626 diff={max=07.3637, min=00.0462, mean=01.3329} policy_loss=-14.2678 policy updated! \n",
      "train step 13557 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=03.5649 diff={max=06.0881, min=00.0356, mean=01.2731} policy_loss=-11.3118 policy updated! \n",
      "train step 13558 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=05.0094 diff={max=07.0061, min=00.0122, mean=01.4586} policy_loss=-13.8546 policy updated! \n",
      "train step 13559 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.6202 diff={max=09.1423, min=00.0095, mean=01.7251} policy_loss=-15.4470 policy updated! \n",
      "train step 13560 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.3799 diff={max=09.7290, min=00.0025, mean=01.2578} policy_loss=-12.8382 policy updated! \n",
      "train step 13561 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=05.0408 diff={max=08.7582, min=00.0856, mean=01.4350} policy_loss=-14.6147 policy updated! \n",
      "train step 13562 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.2616 diff={max=14.1876, min=00.0012, mean=02.0427} policy_loss=-13.5959 policy updated! \n",
      "train step 13563 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2824 diff={max=11.9518, min=00.0345, mean=01.4245} policy_loss=-12.8955 policy updated! \n",
      "train step 13564 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=15.6215 diff={max=20.0892, min=00.0332, mean=01.8767} policy_loss=-13.7122 policy updated! \n",
      "train step 13565 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.7883 diff={max=09.3520, min=00.0260, mean=01.7380} policy_loss=-15.2341 policy updated! \n",
      "train step 13566 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.5742 diff={max=10.9058, min=00.0142, mean=02.1137} policy_loss=-13.9669 policy updated! \n",
      "train step 13567 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.5269 diff={max=07.2688, min=00.0280, mean=01.3342} policy_loss=-13.6076 policy updated! \n",
      "train step 13568 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.4887 diff={max=08.4454, min=00.0547, mean=01.3752} policy_loss=-14.1429 policy updated! \n",
      "train step 13569 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9780 diff={max=07.6180, min=00.0603, mean=01.7986} policy_loss=-19.0840 policy updated! \n",
      "train step 13570 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.0135 diff={max=07.1364, min=00.0282, mean=01.5003} policy_loss=-16.1636 policy updated! \n",
      "train step 13571 reward={max=07.0000, min=07.0000, mean=07.0000} optimizing loss=11.3570 diff={max=14.6011, min=00.0047, mean=02.0302} policy_loss=-14.5238 policy updated! \n",
      "train step 13572 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.5402 diff={max=10.4224, min=00.0122, mean=01.7266} policy_loss=-14.4806 policy updated! \n",
      "train step 13573 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0708 diff={max=13.0859, min=00.0054, mean=01.3368} policy_loss=-12.8762 policy updated! \n",
      "train step 13574 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.7891 diff={max=13.1703, min=00.0005, mean=01.7860} policy_loss=-14.1194 policy updated! \n",
      "train step 13575 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.8582 diff={max=07.8394, min=00.0342, mean=01.5367} policy_loss=-14.1851 policy updated! \n",
      "train step 13576 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.1235 diff={max=07.9793, min=00.0056, mean=01.5098} policy_loss=-14.5205 policy updated! \n",
      "train step 13577 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.6153 diff={max=11.7341, min=00.0067, mean=01.6835} policy_loss=-14.0494 policy updated! \n",
      "train step 13578 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8407 diff={max=06.9594, min=00.0688, mean=01.0468} policy_loss=-12.0056 policy updated! \n",
      "train step 13579 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.3752 diff={max=05.0049, min=00.0032, mean=01.1261} policy_loss=-14.3025 policy updated! \n",
      "train step 13580 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.6264 diff={max=04.6412, min=00.0187, mean=01.3534} policy_loss=-13.1817 policy updated! \n",
      "train step 13581 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.8672 diff={max=05.7176, min=00.0018, mean=01.1780} policy_loss=-13.7845 policy updated! \n",
      "train step 13582 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.0317 diff={max=11.4152, min=00.0637, mean=02.0596} policy_loss=-15.3709 policy updated! \n",
      "train step 13583 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.1238 diff={max=06.2995, min=00.0036, mean=01.3181} policy_loss=-13.2935 policy updated! \n",
      "train step 13584 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.9004 diff={max=08.5874, min=00.0109, mean=01.3927} policy_loss=-15.0274 policy updated! \n",
      "train step 13585 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=01.5813 diff={max=04.4543, min=00.0017, mean=00.8637} policy_loss=-11.4456 policy updated! \n",
      "train step 13586 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.6285 diff={max=11.7357, min=00.0593, mean=01.4517} policy_loss=-13.3908 policy updated! \n",
      "train step 13587 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5907 diff={max=05.2363, min=00.0066, mean=01.0344} policy_loss=-15.8618 policy updated! \n",
      "train step 13588 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.8838 diff={max=07.8271, min=00.0193, mean=01.1288} policy_loss=-11.3113 policy updated! \n",
      "train step 13589 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0419 diff={max=08.9600, min=00.0172, mean=01.5497} policy_loss=-16.2275 policy updated! \n",
      "train step 13590 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.6916 diff={max=09.0383, min=00.0358, mean=01.3954} policy_loss=-16.8998 policy updated! \n",
      "train step 13591 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.2132 diff={max=07.0608, min=00.0003, mean=01.3876} policy_loss=-15.0304 policy updated! \n",
      "train step 13592 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.0219 diff={max=06.5579, min=00.0139, mean=01.7942} policy_loss=-12.7912 policy updated! \n",
      "train step 13593 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=09.6701 diff={max=15.7916, min=00.0348, mean=01.6113} policy_loss=-14.8615 policy updated! \n",
      "train step 13594 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=25.1099 diff={max=19.7396, min=00.0037, mean=02.6234} policy_loss=-13.1689 policy updated! \n",
      "train step 13595 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.9457 diff={max=11.2251, min=00.0292, mean=01.5368} policy_loss=-16.3023 policy updated! \n",
      "train step 13596 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.2542 diff={max=07.5375, min=00.0153, mean=01.3688} policy_loss=-14.0697 policy updated! \n",
      "train step 13597 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4092 diff={max=06.1264, min=00.0229, mean=01.4269} policy_loss=-15.3882 policy updated! \n",
      "train step 13598 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9086 diff={max=09.1393, min=00.0053, mean=01.4557} policy_loss=-14.2607 policy updated! \n",
      "train step 13599 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4832 diff={max=06.2908, min=00.0038, mean=01.1614} policy_loss=-13.4243 policy updated! \n",
      "train step 13600 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.8163 diff={max=07.4267, min=00.0164, mean=01.0774} policy_loss=-14.9331 policy updated! \n",
      "train step 13601 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.0816 diff={max=08.7672, min=00.0422, mean=01.2896} policy_loss=-10.8406 policy updated! \n",
      "train step 13602 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4119 diff={max=06.0091, min=00.0015, mean=01.2619} policy_loss=-13.8750 policy updated! \n",
      "train step 13603 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.5233 diff={max=11.8961, min=00.0323, mean=01.1554} policy_loss=-15.9440 policy updated! \n",
      "train step 13604 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=02.5273 diff={max=04.0647, min=00.0015, mean=01.2223} policy_loss=-14.9127 policy updated! \n",
      "train step 13605 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.8436 diff={max=07.1854, min=00.1126, mean=01.5730} policy_loss=-17.9979 policy updated! \n",
      "train step 13606 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2511 diff={max=08.0657, min=00.0185, mean=01.5769} policy_loss=-11.9543 policy updated! \n",
      "train step 13607 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.5175 diff={max=16.3083, min=00.0317, mean=01.8034} policy_loss=-14.5565 policy updated! \n",
      "train step 13608 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.9407 diff={max=06.8096, min=00.0120, mean=01.4268} policy_loss=-17.5626 policy updated! \n",
      "train step 13609 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=16.9291 diff={max=25.5140, min=00.0541, mean=01.8834} policy_loss=-13.7662 policy updated! \n",
      "train step 13610 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=07.5624 diff={max=09.1994, min=00.0026, mean=01.6385} policy_loss=-14.3336 policy updated! \n",
      "train step 13611 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.5453 diff={max=09.2654, min=00.0050, mean=01.8246} policy_loss=-14.3342 policy updated! \n",
      "train step 13612 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.4589 diff={max=07.5281, min=00.0675, mean=01.4988} policy_loss=-13.1433 policy updated! \n",
      "train step 13613 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=07.5207 diff={max=08.9079, min=00.0570, mean=01.8251} policy_loss=-12.3759 policy updated! \n",
      "train step 13614 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=11.4316 diff={max=11.1563, min=00.1677, mean=02.5070} policy_loss=-14.4708 policy updated! \n",
      "train step 13615 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.4573 diff={max=06.7865, min=00.0496, mean=01.5421} policy_loss=-10.9738 policy updated! \n",
      "train step 13616 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=09.3813 diff={max=09.9399, min=00.0188, mean=01.9477} policy_loss=-12.0568 policy updated! \n",
      "train step 13617 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=10.5399 diff={max=14.0616, min=00.0168, mean=01.8621} policy_loss=-13.2237 policy updated! \n",
      "train step 13618 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.2224 diff={max=09.7567, min=00.0177, mean=01.8877} policy_loss=-13.6970 policy updated! \n",
      "train step 13619 reward={max=07.0000, min=00.0000, mean=05.6000} optimizing loss=06.9659 diff={max=10.9734, min=00.0071, mean=01.6250} policy_loss=-16.1414 policy updated! \n",
      "train step 13620 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=10.2359 diff={max=16.0179, min=00.0077, mean=01.5988} policy_loss=-12.1644 policy updated! \n",
      "train step 13621 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2652 diff={max=10.1270, min=00.0095, mean=01.6256} policy_loss=-14.1122 policy updated! \n",
      "train step 13622 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.4270 diff={max=07.8702, min=00.0157, mean=01.1663} policy_loss=-13.4979 policy updated! \n",
      "train step 13623 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=12.9667 diff={max=20.8216, min=00.0661, mean=01.8301} policy_loss=-14.5460 policy updated! \n",
      "train step 13624 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.5529 diff={max=08.4674, min=00.0512, mean=01.6804} policy_loss=-13.5840 policy updated! \n",
      "train step 13625 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.7734 diff={max=09.7847, min=00.0277, mean=01.0747} policy_loss=-14.1928 policy updated! \n",
      "train step 13626 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=09.0367 diff={max=09.9987, min=00.0111, mean=01.6945} policy_loss=-13.3078 policy updated! \n",
      "train step 13627 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.3517 diff={max=08.4123, min=00.0168, mean=01.5859} policy_loss=-13.7909 policy updated! \n",
      "train step 13628 reward={max=07.0000, min=00.0000, mean=02.8000} optimizing loss=08.2121 diff={max=13.3402, min=00.0023, mean=01.7198} policy_loss=-15.5624 policy updated! \n",
      "train step 13629 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=09.7412 diff={max=16.9837, min=00.0048, mean=01.5546} policy_loss=-10.6562 policy updated! \n",
      "train step 13630 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=06.8875 diff={max=08.3280, min=00.0119, mean=01.6261} policy_loss=-17.9112 policy updated! \n",
      "train step 13631 reward={max=06.0000, min=00.0000, mean=01.2000} optimizing loss=18.2475 diff={max=20.1337, min=00.0083, mean=01.6875} policy_loss=-11.4161 policy updated! \n",
      "train step 13632 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=13.4737 diff={max=20.0559, min=00.0343, mean=01.8572} policy_loss=-13.6227 policy updated! \n",
      "train step 13633 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.3773 diff={max=11.9202, min=00.0031, mean=01.6947} policy_loss=-13.6760 policy updated! \n",
      "train step 13634 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=16.3942 diff={max=21.7374, min=00.0080, mean=02.0096} policy_loss=-14.2566 policy updated! \n",
      "train step 13635 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=05.6209 diff={max=10.7185, min=00.0388, mean=01.3968} policy_loss=-14.0619 policy updated! \n",
      "train step 13636 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.3296 diff={max=10.3721, min=00.0118, mean=01.2864} policy_loss=-16.0937 policy updated! \n",
      "train step 13637 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9871 diff={max=07.8525, min=00.0144, mean=01.5330} policy_loss=-13.3076 policy updated! \n",
      "train step 13638 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.7243 diff={max=06.8274, min=00.0026, mean=01.5624} policy_loss=-13.4726 policy updated! \n",
      "train step 13639 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=03.7330 diff={max=07.1678, min=00.0072, mean=01.2539} policy_loss=-11.9149 policy updated! \n",
      "train step 13640 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=04.0150 diff={max=05.8151, min=00.0096, mean=01.4316} policy_loss=-14.0335 policy updated! \n",
      "train step 13641 reward={max=07.0000, min=00.0000, mean=04.2000} optimizing loss=04.5966 diff={max=06.5047, min=00.0166, mean=01.5419} policy_loss=-15.1085 policy updated! \n",
      "train step 13642 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=06.2379 diff={max=06.0967, min=00.0195, mean=01.7469} policy_loss=-16.0878 policy updated! \n",
      "train step 13643 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=08.5024 diff={max=12.8767, min=00.0060, mean=01.7354} policy_loss=-14.3152 policy updated! \n",
      "train step 13644 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=05.1230 diff={max=07.5007, min=00.0019, mean=01.4901} policy_loss=-14.0610 policy updated! \n",
      "train step 13645 reward={max=07.0000, min=00.0000, mean=01.4000} optimizing loss=03.4367 diff={max=06.8102, min=00.0002, mean=01.2846} policy_loss=-14.5764 policy updated! \n",
      "train step 13646 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=04.9899 diff={max=09.9026, min=00.0319, mean=01.4831} policy_loss=-15.6671 policy updated! \n",
      "train step 13647 reward={max=00.0000, min=00.0000, mean=00.0000} optimizing loss=07.0019 diff={max=08.6431, min=00.0244, mean=01.7897} policy_loss=-15.4300 policy updated! \n",
      "train step 13648 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11948/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\Repos\\School\\Vehicular-Game-Theory\\platoongame\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m             \u001b[1;31m# exploration between optimization steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m             rewards = [\n\u001b[0m\u001b[0;32m    187\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake_explore_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexploration_per_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Repos\\School\\Vehicular-Game-Theory\\platoongame\\training.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    185\u001b[0m             \u001b[1;31m# exploration between optimization steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m             rewards = [\n\u001b[1;32m--> 187\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake_explore_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexploration_per_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m             ]\n",
      "\u001b[1;32md:\\Repos\\School\\Vehicular-Game-Theory\\platoongame\\training.py\u001b[0m in \u001b[0;36mtake_explore_step\u001b[1;34m(self, random)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtake_explore_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# returns reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[0mprev_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         _, defender_action = self.game.take_step(\n\u001b[0m\u001b[0;32m    115\u001b[0m             \u001b[0mattacker_agent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattacker_agent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[0mdefender_agent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefender_agent\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mrandom\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_defender_agent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Repos\\School\\Vehicular-Game-Theory\\platoongame\\game.py\u001b[0m in \u001b[0;36mtake_step\u001b[1;34m(self, attacker_agent, defender_agent)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"defender turn begin\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[0mdefender_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefender_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefender_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefender_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"defender turn end\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\subgame\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Repos\\School\\Vehicular-Game-Theory\\platoongame\\agents.py\u001b[0m in \u001b[0;36mget_action\u001b[1;34m(self, state_obj)\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[1;31m# grade the acctions using the critic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m         \u001b[0maction_q_values\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\subgame\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Repos\\School\\Vehicular-Game-Theory\\platoongame\\models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, state, actions)\u001b[0m\n\u001b[0;32m    201\u001b[0m         ), dim=1)\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\subgame\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\subgame\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\subgame\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: predict a distribution instead of single actions if attacker behaviour is nondeterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "attempt_gpu_mem_flush = False #@param {type:\"boolean\"}\n",
    "if attempt_gpu_mem_flush:\n",
    "    import torch\n",
    "    torch.cuda.empty_cache()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No stats to plot below loss threshold 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tracker.plot()\n",
    "tracker.plot(loss_threshold=100)\n",
    "tracker.plot(loss_threshold=20)\n",
    "tracker.plot(loss_threshold=5)\n",
    "tracker.plot(loss_threshold=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x1440 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_episode_metrics(\n",
    "    defender_agent=defender,\n",
    "    attacker_agent=attacker,\n",
    "    game_config=game_config,\n",
    "    vehicle_provider=vehicle_provider,\n",
    "    num_turns=100 #@param {type:\"integer\"}\n",
    ").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action.members tensor([-10., -10.,  10.,  10.,  10.,  10.,  10.,  10.,  10., -10.],\n",
      "       grad_fn=<SumBackward1>)\n",
      "q_pred tensor([4.1732, 1.7117, 4.2848, 3.5527, 2.1747, 5.3937, 3.7552, 4.2848, 3.5265,\n",
      "        3.0028], grad_fn=<ReshapeAliasBackward0>)\n",
      "batch.reward tensor([4., 2., 7., 3., 5., 5., 7., 7., 4., 1.])\n",
      "pred reward err tensor([ 0.1732, -0.2883, -2.7152,  0.5527, -2.8253,  0.3937, -3.2448, -2.7152,\n",
      "        -0.4735,  2.0028], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from evaluation import sample_model_outputs\n",
    "sample_model_outputs(\n",
    "    defender_agent=defender,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually checkpoint model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-23 0015-31 350720\n"
     ]
    }
   ],
   "source": [
    "from utils import get_prefix\n",
    "\n",
    "manual_checkpoint = False #@param {type:\"boolean\"}\n",
    "if manual_checkpoint:\n",
    "    prefix = get_prefix()\n",
    "    defender.save(dir=checkpoints_dir, prefix=prefix)\n",
    "    trainer.config.dump(dir=checkpoints_dir, prefix=prefix)\n",
    "    print(prefix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('subgame')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09b31e4957d63d79e1e76c5537c345194b9f565583fac53cbbb105281d72baf4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
