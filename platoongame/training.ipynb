{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from importlib import reload\n",
    "# import vehicles\n",
    "# import game\n",
    "# import agents\n",
    "# import evaluation\n",
    "# import models\n",
    "# reload(vehicles)\n",
    "# reload(game)\n",
    "# reload(agents)\n",
    "# reload(evaluation)\n",
    "# reload(models)\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x14cef76ddf0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "import torch\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TeamD\\.conda\\envs\\subgame\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "from agents import BasicAttackerAgent, WolpertingerDefenderAgent\n",
    "from evaluation import Evaluator\n",
    "from game import GameConfig\n",
    "from models import StateShapeData\n",
    "from vehicles import JsonVehicleProvider, Vehicle, Vulnerability\n",
    "\n",
    "vehicle_provider=JsonVehicleProvider(\"../subgame/python/solutions.json\")\n",
    "game_config=GameConfig(\n",
    "    max_vehicles=30,\n",
    "    cycle_every=3,\n",
    "    cycle_num=5,\n",
    "    cycle_allow_platoon=False\n",
    ")\n",
    "\n",
    "attacker=BasicAttackerAgent(1)\n",
    "defender=WolpertingerDefenderAgent(\n",
    "    state_shape_data=StateShapeData(\n",
    "        num_vehicles=game_config.max_vehicles,\n",
    "        num_vehicle_features=Vehicle.get_shape()[0],\n",
    "        num_vulns=vehicle_provider.max_vulns,\n",
    "        num_vuln_features=Vulnerability.get_shape()[0]\n",
    "    )\n",
    ")\n",
    "\n",
    "engine = Evaluator(\n",
    "    vehicle_provider=vehicle_provider,\n",
    "    game_config=game_config,\n",
    "    num_rounds=1000\n",
    ")\n",
    "engine.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import WolpertingerDefenderAgentTrainer\n",
    "trainer = WolpertingerDefenderAgentTrainer(\n",
    "    batch_size=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 step 0 \n",
      "episode 0 step 1 \n",
      "episode 0 step 2 \n",
      "episode 0 step 3 \n",
      "episode 0 step 4 \n",
      "episode 0 step 5 \n",
      "episode 0 step 6 \n",
      "episode 0 step 7 \n",
      "episode 0 step 8 \n",
      "episode 0 step 9 \n",
      "episode 0 step 10 \n",
      "episode 0 step 11 \n",
      "episode 0 step 12 \n",
      "episode 0 step 13 \n",
      "episode 0 step 14 \n",
      "episode 0 step 15 \n",
      "episode 0 step 16 \n",
      "episode 0 step 17 \n",
      "episode 0 step 18 \n",
      "episode 0 step 19 \n",
      "episode 0 step 20 \n",
      "episode 0 step 21 \n",
      "episode 0 step 22 \n",
      "episode 0 step 23 \n",
      "episode 0 step 24 \n",
      "episode 0 step 25 optimizing loss=173.01840209960938\n",
      "episode 0 step 26 optimizing loss=208.2753143310547\n",
      "episode 0 step 27 optimizing loss=62.3670654296875\n",
      "episode 0 step 28 optimizing loss=184.3648223876953\n",
      "episode 0 step 29 optimizing "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TeamD\\.conda\\envs\\subgame\\lib\\site-packages\\torch\\nn\\functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=144.7010498046875\n",
      "episode 0 step 30 optimizing loss=124.83601379394531\n",
      "episode 0 step 31 optimizing loss=63.59955596923828\n",
      "episode 0 step 32 optimizing loss=22.754636764526367\n",
      "episode 0 step 33 optimizing loss=115.17305755615234\n",
      "episode 0 step 34 optimizing loss=108.84249114990234\n",
      "episode 0 step 35 optimizing loss=64.13493347167969\n",
      "episode 0 step 36 optimizing loss=172.11105346679688\n",
      "episode 0 step 37 optimizing loss=138.52151489257812\n",
      "episode 0 step 38 optimizing loss=107.93299865722656\n",
      "episode 0 step 39 optimizing loss=59.597633361816406\n",
      "episode 0 step 40 optimizing loss=31.236764907836914\n",
      "episode 0 step 41 optimizing loss=146.6715087890625\n",
      "episode 0 step 42 optimizing loss=118.53995513916016\n",
      "episode 0 step 43 optimizing loss=133.22569274902344\n",
      "episode 0 step 44 optimizing loss=87.16039276123047\n",
      "episode 0 step 45 optimizing loss=48.62333679199219\n",
      "episode 0 step 46 optimizing loss=55.86482620239258\n",
      "episode 0 step 47 optimizing loss=216.17410278320312\n",
      "episode 0 step 48 optimizing loss=107.2816162109375\n",
      "episode 0 step 49 optimizing loss=37.80487823486328\n",
      "episode 0 step 50 optimizing loss=71.13654327392578\n",
      "episode 0 step 51 optimizing loss=58.600975036621094\n",
      "episode 0 step 52 optimizing loss=139.81118774414062\n",
      "episode 0 step 53 optimizing loss=137.56858825683594\n",
      "episode 0 step 54 optimizing loss=196.1944122314453\n",
      "episode 0 step 55 optimizing loss=137.82217407226562\n",
      "episode 0 step 56 optimizing loss=113.6370620727539\n",
      "episode 0 step 57 optimizing loss=240.8352813720703\n",
      "episode 0 step 58 optimizing loss=158.8153076171875\n",
      "episode 0 step 59 optimizing loss=157.66470336914062\n",
      "episode 0 step 60 optimizing loss=149.8312530517578\n",
      "episode 0 step 61 optimizing loss=131.34927368164062\n",
      "episode 0 step 62 optimizing loss=172.1736297607422\n",
      "episode 0 step 63 optimizing loss=305.6465759277344\n",
      "episode 0 step 64 optimizing loss=87.26519012451172\n",
      "episode 0 step 65 optimizing loss=90.59501647949219\n",
      "episode 0 step 66 optimizing loss=202.5459747314453\n",
      "episode 0 step 67 optimizing loss=111.93634033203125\n",
      "episode 0 step 68 optimizing loss=173.6905517578125\n",
      "episode 0 step 69 optimizing loss=93.86070251464844\n",
      "episode 0 step 70 optimizing loss=70.81461334228516\n",
      "episode 0 step 71 optimizing loss=165.2796173095703\n",
      "episode 0 step 72 optimizing loss=123.19526672363281\n",
      "episode 0 step 73 optimizing loss=133.95614624023438\n",
      "episode 0 step 74 optimizing loss=184.4591827392578\n",
      "episode 0 step 75 optimizing loss=191.06008911132812\n",
      "episode 0 step 76 optimizing loss=214.5756072998047\n",
      "episode 0 step 77 optimizing loss=133.2583465576172\n",
      "episode 0 step 78 optimizing loss=77.55412292480469\n",
      "episode 0 step 79 optimizing loss=28.331645965576172\n",
      "episode 0 step 80 optimizing loss=113.81781005859375\n",
      "episode 0 step 81 optimizing loss=38.568443298339844\n",
      "episode 0 step 82 optimizing loss=59.710304260253906\n",
      "episode 0 step 83 optimizing loss=53.9046745300293\n",
      "episode 0 step 84 optimizing loss=7.531919002532959\n",
      "episode 0 step 85 optimizing loss=97.40327453613281\n",
      "episode 0 step 86 optimizing loss=106.3351821899414\n",
      "episode 0 step 87 optimizing loss=65.0624008178711\n",
      "episode 0 step 88 optimizing loss=199.86109924316406\n",
      "episode 0 step 89 optimizing loss=107.99357604980469\n",
      "episode 0 step 90 optimizing loss=57.632537841796875\n",
      "episode 0 step 91 optimizing loss=96.62667083740234\n",
      "episode 0 step 92 optimizing loss=123.05769348144531\n",
      "episode 0 step 93 optimizing loss=133.58460998535156\n",
      "episode 0 step 94 optimizing loss=285.2040710449219\n",
      "episode 0 step 95 optimizing loss=103.8346939086914\n",
      "episode 0 step 96 optimizing loss=88.62974548339844\n",
      "episode 0 step 97 optimizing loss=122.62403869628906\n",
      "episode 0 step 98 optimizing loss=144.58395385742188\n",
      "episode 0 step 99 optimizing loss=308.8766174316406\n",
      "episode 0 step 100 optimizing loss=256.9977111816406\n",
      "episode 0 step 101 optimizing loss=217.16586303710938\n",
      "episode 0 step 102 optimizing loss=61.6239128112793\n",
      "episode 0 step 103 optimizing loss=120.04801940917969\n",
      "episode 0 step 104 optimizing loss=81.2110824584961\n",
      "episode 0 step 105 optimizing loss=175.05990600585938\n",
      "episode 0 step 106 optimizing loss=97.50822448730469\n",
      "episode 0 step 107 optimizing loss=62.378334045410156\n",
      "episode 0 step 108 optimizing loss=86.77386474609375\n",
      "episode 0 step 109 optimizing loss=75.24483489990234\n",
      "episode 0 step 110 optimizing loss=78.32423400878906\n",
      "episode 0 step 111 optimizing loss=92.2560043334961\n",
      "episode 0 step 112 optimizing loss=151.9235076904297\n",
      "episode 0 step 113 optimizing loss=39.00470733642578\n",
      "episode 0 step 114 optimizing loss=63.0128288269043\n",
      "episode 0 step 115 optimizing loss=60.57365798950195\n",
      "episode 0 step 116 optimizing loss=139.0472412109375\n",
      "episode 0 step 117 optimizing loss=66.58709716796875\n",
      "episode 0 step 118 optimizing loss=163.4219970703125\n",
      "episode 0 step 119 optimizing loss=32.53661346435547\n",
      "episode 0 step 120 optimizing loss=233.88150024414062\n",
      "episode 0 step 121 optimizing loss=138.9249725341797\n",
      "episode 0 step 122 optimizing loss=202.52012634277344\n",
      "episode 0 step 123 optimizing loss=71.4412841796875\n",
      "episode 0 step 124 optimizing loss=636.7423095703125\n",
      "episode 0 step 125 optimizing loss=87.02824401855469\n",
      "episode 0 step 126 optimizing loss=116.2925796508789\n",
      "episode 0 step 127 optimizing loss=347.80657958984375\n",
      "episode 0 step 128 optimizing loss=22.63475227355957\n",
      "episode 0 step 129 optimizing loss=219.4032745361328\n",
      "episode 0 step 130 optimizing loss=58.954864501953125\n",
      "episode 0 step 131 optimizing loss=37.467185974121094\n",
      "episode 0 step 132 optimizing loss=61.2193489074707\n",
      "episode 0 step 133 optimizing loss=237.2296600341797\n",
      "episode 0 step 134 optimizing loss=133.77914428710938\n",
      "episode 0 step 135 optimizing loss=84.1225814819336\n",
      "episode 0 step 136 optimizing loss=151.79165649414062\n",
      "episode 0 step 137 optimizing loss=28.398996353149414\n",
      "episode 0 step 138 optimizing loss=260.94696044921875\n",
      "episode 0 step 139 optimizing loss=266.8648376464844\n",
      "episode 0 step 140 optimizing loss=384.86248779296875\n",
      "episode 0 step 141 optimizing loss=281.8085632324219\n",
      "episode 0 step 142 optimizing loss=93.27662658691406\n",
      "episode 0 step 143 optimizing loss=207.2362518310547\n",
      "episode 0 step 144 optimizing loss=60.825645446777344\n",
      "episode 0 step 145 optimizing loss=61.10295486450195\n",
      "episode 0 step 146 optimizing loss=468.94329833984375\n",
      "episode 0 step 147 optimizing loss=103.4612045288086\n",
      "episode 0 step 148 optimizing loss=333.9970703125\n",
      "episode 0 step 149 optimizing loss=69.8509521484375\n",
      "episode 0 step 150 optimizing loss=220.31295776367188\n",
      "episode 0 step 151 optimizing loss=289.7578125\n",
      "episode 0 step 152 optimizing loss=533.0535278320312\n",
      "episode 0 step 153 optimizing loss=133.7696075439453\n",
      "episode 0 step 154 optimizing loss=56.75407028198242\n",
      "episode 0 step 155 optimizing loss=362.2588195800781\n",
      "episode 0 step 156 optimizing loss=229.35830688476562\n",
      "episode 0 step 157 optimizing loss=270.6600341796875\n",
      "episode 0 step 158 optimizing loss=22.505352020263672\n",
      "episode 0 step 159 optimizing loss=398.7549133300781\n",
      "episode 0 step 160 optimizing loss=145.52566528320312\n",
      "episode 0 step 161 optimizing loss=556.0238037109375\n",
      "episode 0 step 162 optimizing loss=133.1182403564453\n",
      "episode 0 step 163 optimizing loss=236.8429718017578\n",
      "episode 0 step 164 optimizing loss=325.6483459472656\n",
      "episode 0 step 165 optimizing loss=431.5892028808594\n",
      "episode 0 step 166 optimizing loss=704.4091186523438\n",
      "episode 0 step 167 optimizing loss=121.13322448730469\n",
      "episode 0 step 168 optimizing loss=295.09979248046875\n",
      "episode 0 step 169 optimizing loss=519.3258666992188\n",
      "episode 0 step 170 optimizing loss=784.9080200195312\n",
      "episode 0 step 171 optimizing loss=91.75599670410156\n",
      "episode 0 step 172 optimizing loss=82.63267517089844\n",
      "episode 0 step 173 optimizing loss=192.52151489257812\n",
      "episode 0 step 174 optimizing loss=358.2518310546875\n",
      "episode 0 step 175 optimizing loss=138.9986114501953\n",
      "episode 0 step 176 optimizing loss=247.0519256591797\n",
      "episode 0 step 177 optimizing loss=459.96453857421875\n",
      "episode 0 step 178 optimizing loss=593.7107543945312\n",
      "episode 0 step 179 optimizing loss=621.9926147460938\n",
      "episode 0 step 180 optimizing loss=234.1563262939453\n",
      "episode 0 step 181 optimizing loss=342.2921447753906\n",
      "episode 0 step 182 optimizing loss=614.1486206054688\n",
      "episode 0 step 183 optimizing loss=626.7413330078125\n",
      "episode 0 step 184 optimizing loss=429.12548828125\n",
      "episode 0 step 185 optimizing loss=254.70584106445312\n",
      "episode 0 step 186 optimizing loss=274.8459167480469\n",
      "episode 0 step 187 optimizing loss=142.31625366210938\n",
      "episode 0 step 188 optimizing loss=74.50138854980469\n",
      "episode 0 step 189 optimizing loss=218.9730682373047\n",
      "episode 0 step 190 optimizing loss=518.7479248046875\n",
      "episode 0 step 191 optimizing loss=68.8493881225586\n",
      "episode 0 step 192 optimizing loss=671.705078125\n",
      "episode 0 step 193 optimizing loss=62.652732849121094\n",
      "episode 0 step 194 optimizing loss=231.8346405029297\n",
      "episode 0 step 195 optimizing loss=491.09344482421875\n",
      "episode 0 step 196 optimizing loss=297.5005187988281\n",
      "episode 0 step 197 optimizing loss=633.4217529296875\n",
      "episode 0 step 198 optimizing loss=194.86489868164062\n",
      "episode 0 step 199 optimizing loss=393.04315185546875\n",
      "episode 1 step 0 optimizing loss=161.91329956054688\n",
      "episode 1 step 1 optimizing loss=197.25352478027344\n",
      "episode 1 step 2 optimizing loss=168.27430725097656\n",
      "episode 1 step 3 optimizing loss=267.054931640625\n",
      "episode 1 step 4 optimizing loss=294.7348937988281\n",
      "episode 1 step 5 optimizing loss=84.87007141113281\n",
      "episode 1 step 6 optimizing loss=640.0126953125\n",
      "episode 1 step 7 optimizing loss=755.2702026367188\n",
      "episode 1 step 8 optimizing loss=252.3966827392578\n",
      "episode 1 step 9 optimizing loss=773.2384643554688\n",
      "episode 1 step 10 optimizing loss=106.8758544921875\n",
      "episode 1 step 11 optimizing loss=161.27188110351562\n",
      "episode 1 step 12 optimizing loss=384.07293701171875\n",
      "episode 1 step 13 optimizing loss=534.5499267578125\n",
      "episode 1 step 14 optimizing loss=365.3270568847656\n",
      "episode 1 step 15 optimizing loss=366.0655212402344\n",
      "episode 1 step 16 optimizing loss=65.1030044555664\n",
      "episode 1 step 17 optimizing loss=52.07337188720703\n",
      "episode 1 step 18 optimizing loss=491.0458984375\n",
      "episode 1 step 19 optimizing loss=84.04939270019531\n",
      "episode 1 step 20 optimizing loss=124.3194351196289\n",
      "episode 1 step 21 optimizing loss=94.38801574707031\n",
      "episode 1 step 22 optimizing loss=101.84476470947266\n",
      "episode 1 step 23 optimizing loss=406.6840515136719\n",
      "episode 1 step 24 optimizing loss=25.273345947265625\n",
      "episode 1 step 25 optimizing loss=86.32882690429688\n",
      "episode 1 step 26 optimizing loss=170.5303497314453\n",
      "episode 1 step 27 optimizing loss=84.75943756103516\n",
      "episode 1 step 28 optimizing loss=239.03329467773438\n",
      "episode 1 step 29 optimizing loss=590.7862548828125\n",
      "episode 1 step 30 optimizing loss=388.327392578125\n",
      "episode 1 step 31 optimizing loss=208.51986694335938\n",
      "episode 1 step 32 optimizing loss=292.8539123535156\n",
      "episode 1 step 33 optimizing loss=57.41957473754883\n",
      "episode 1 step 34 optimizing loss=153.875244140625\n",
      "episode 1 step 35 optimizing loss=72.00623321533203\n",
      "episode 1 step 36 optimizing loss=68.96717071533203\n",
      "episode 1 step 37 optimizing loss=294.9444580078125\n",
      "episode 1 step 38 optimizing loss=490.767822265625\n",
      "episode 1 step 39 optimizing loss=208.4595947265625\n",
      "episode 1 step 40 optimizing loss=291.839599609375\n",
      "episode 1 step 41 optimizing loss=405.2994384765625\n",
      "episode 1 step 42 optimizing loss=687.1785888671875\n",
      "episode 1 step 43 optimizing loss=257.5552673339844\n",
      "episode 1 step 44 optimizing loss=386.0211486816406\n",
      "episode 1 step 45 optimizing loss=60.53057098388672\n",
      "episode 1 step 46 optimizing loss=738.9191284179688\n",
      "episode 1 step 47 optimizing loss=54.55727005004883\n",
      "episode 1 step 48 optimizing loss=105.9297866821289\n",
      "episode 1 step 49 optimizing loss=41.76240158081055\n",
      "episode 1 step 50 optimizing loss=594.8493041992188\n",
      "episode 1 step 51 optimizing loss=61.79216766357422\n",
      "episode 1 step 52 optimizing loss=294.5185546875\n",
      "episode 1 step 53 optimizing loss=52.932594299316406\n",
      "episode 1 step 54 optimizing loss=82.4190673828125\n",
      "episode 1 step 55 optimizing loss=254.70309448242188\n",
      "episode 1 step 56 optimizing loss=281.6644592285156\n",
      "episode 1 step 57 optimizing loss=112.49201965332031\n",
      "episode 1 step 58 optimizing loss=84.78065490722656\n",
      "episode 1 step 59 optimizing loss=111.17909240722656\n",
      "episode 1 step 60 optimizing loss=407.43035888671875\n",
      "episode 1 step 61 optimizing loss=316.1080017089844\n",
      "episode 1 step 62 optimizing loss=418.31475830078125\n",
      "episode 1 step 63 optimizing loss=136.5602264404297\n",
      "episode 1 step 64 optimizing loss=77.65863800048828\n",
      "episode 1 step 65 optimizing loss=29.12750816345215\n",
      "episode 1 step 66 optimizing loss=177.75497436523438\n",
      "episode 1 step 67 optimizing loss=316.86328125\n",
      "episode 1 step 68 optimizing loss=55.80397415161133\n",
      "episode 1 step 69 optimizing loss=685.3079833984375\n",
      "episode 1 step 70 optimizing loss=386.6416931152344\n",
      "episode 1 step 71 optimizing loss=430.83624267578125\n",
      "episode 1 step 72 optimizing loss=307.7513732910156\n",
      "episode 1 step 73 optimizing loss=124.53984069824219\n",
      "episode 1 step 74 optimizing loss=268.2615051269531\n",
      "episode 1 step 75 optimizing loss=458.79022216796875\n",
      "episode 1 step 76 optimizing loss=91.66358184814453\n",
      "episode 1 step 77 optimizing loss=376.60174560546875\n",
      "episode 1 step 78 optimizing loss=184.38424682617188\n",
      "episode 1 step 79 optimizing loss=19.878467559814453\n",
      "episode 1 step 80 optimizing loss=135.98248291015625\n",
      "episode 1 step 81 optimizing loss=91.3454360961914\n",
      "episode 1 step 82 optimizing loss=102.41761779785156\n",
      "episode 1 step 83 optimizing loss=124.34934997558594\n",
      "episode 1 step 84 optimizing loss=209.00210571289062\n",
      "episode 1 step 85 optimizing loss=73.0490951538086\n",
      "episode 1 step 86 optimizing loss=454.905517578125\n",
      "episode 1 step 87 optimizing loss=287.30078125\n",
      "episode 1 step 88 optimizing loss=410.26025390625\n",
      "episode 1 step 89 optimizing loss=42.394737243652344\n",
      "episode 1 step 90 optimizing loss=57.5357551574707\n",
      "episode 1 step 91 optimizing loss=902.2571411132812\n",
      "episode 1 step 92 optimizing loss=278.011474609375\n",
      "episode 1 step 93 optimizing loss=619.6069946289062\n",
      "episode 1 step 94 optimizing loss=34.765464782714844\n",
      "episode 1 step 95 optimizing loss=361.50775146484375\n",
      "episode 1 step 96 optimizing loss=302.05621337890625\n",
      "episode 1 step 97 optimizing loss=107.83119201660156\n",
      "episode 1 step 98 optimizing loss=126.63648986816406\n",
      "episode 1 step 99 optimizing loss=200.6897430419922\n",
      "episode 1 step 100 optimizing loss=56.76782989501953\n",
      "episode 1 step 101 optimizing loss=41.03937530517578\n",
      "episode 1 step 102 optimizing loss=13.610389709472656\n",
      "episode 1 step 103 optimizing loss=449.4931640625\n",
      "episode 1 step 104 optimizing loss=590.8892822265625\n",
      "episode 1 step 105 optimizing loss=336.65924072265625\n",
      "episode 1 step 106 optimizing loss=685.1748046875\n",
      "episode 1 step 107 optimizing loss=151.96499633789062\n",
      "episode 1 step 108 optimizing loss=261.13507080078125\n",
      "episode 1 step 109 optimizing loss=254.380126953125\n",
      "episode 1 step 110 optimizing loss=153.40518188476562\n",
      "episode 1 step 111 optimizing loss=162.91427612304688\n",
      "episode 1 step 112 optimizing loss=505.794677734375\n",
      "episode 1 step 113 optimizing loss=666.7984619140625\n",
      "episode 1 step 114 optimizing loss=246.11227416992188\n",
      "episode 1 step 115 optimizing loss=185.9614715576172\n",
      "episode 1 step 116 optimizing loss=202.82887268066406\n",
      "episode 1 step 117 optimizing loss=207.7078857421875\n",
      "episode 1 step 118 optimizing loss=315.1128845214844\n",
      "episode 1 step 119 optimizing loss=5.291250705718994\n",
      "episode 1 step 120 optimizing loss=52.85551071166992\n",
      "episode 1 step 121 optimizing loss=271.31005859375\n",
      "episode 1 step 122 optimizing loss=76.1850814819336\n",
      "episode 1 step 123 optimizing loss=364.25836181640625\n",
      "episode 1 step 124 optimizing loss=176.42417907714844\n",
      "episode 1 step 125 optimizing loss=138.42001342773438\n",
      "episode 1 step 126 optimizing loss=225.4696044921875\n",
      "episode 1 step 127 optimizing loss=52.99150466918945\n",
      "episode 1 step 128 optimizing loss=152.5308074951172\n",
      "episode 1 step 129 optimizing loss=380.9409484863281\n",
      "episode 1 step 130 optimizing loss=329.3885803222656\n",
      "episode 1 step 131 optimizing loss=134.288818359375\n",
      "episode 1 step 132 optimizing loss=65.45968627929688\n",
      "episode 1 step 133 optimizing loss=100.9502182006836\n",
      "episode 1 step 134 optimizing loss=375.33380126953125\n",
      "episode 1 step 135 optimizing loss=242.07675170898438\n",
      "episode 1 step 136 optimizing loss=344.72137451171875\n",
      "episode 1 step 137 optimizing loss=403.8152770996094\n",
      "episode 1 step 138 optimizing loss=95.40504455566406\n",
      "episode 1 step 139 optimizing loss=222.1289825439453\n",
      "episode 1 step 140 optimizing loss=245.02059936523438\n",
      "episode 1 step 141 optimizing loss=48.006256103515625\n",
      "episode 1 step 142 optimizing loss=200.67837524414062\n",
      "episode 1 step 143 optimizing loss=74.02628326416016\n",
      "episode 1 step 144 optimizing loss=223.8974151611328\n",
      "episode 1 step 145 optimizing loss=433.9593811035156\n",
      "episode 1 step 146 optimizing loss=122.2220458984375\n",
      "episode 1 step 147 optimizing loss=425.70684814453125\n",
      "episode 1 step 148 optimizing loss=474.35137939453125\n",
      "episode 1 step 149 optimizing loss=315.94342041015625\n",
      "episode 1 step 150 optimizing loss=552.27734375\n",
      "episode 1 step 151 optimizing loss=339.00579833984375\n",
      "episode 1 step 152 optimizing loss=89.94304656982422\n",
      "episode 1 step 153 optimizing loss=102.67488098144531\n",
      "episode 1 step 154 optimizing loss=138.28616333007812\n",
      "episode 1 step 155 optimizing loss=711.6505737304688\n",
      "episode 1 step 156 optimizing loss=250.11300659179688\n",
      "episode 1 step 157 optimizing loss=35.14252853393555\n",
      "episode 1 step 158 optimizing loss=386.41900634765625\n",
      "episode 1 step 159 optimizing loss=189.13485717773438\n",
      "episode 1 step 160 optimizing loss=97.24122619628906\n",
      "episode 1 step 161 optimizing loss=218.813720703125\n",
      "episode 1 step 162 optimizing loss=193.1363067626953\n",
      "episode 1 step 163 optimizing loss=57.0161247253418\n",
      "episode 1 step 164 optimizing loss=7.355450630187988\n",
      "episode 1 step 165 optimizing loss=133.6297607421875\n",
      "episode 1 step 166 optimizing loss=220.0223846435547\n",
      "episode 1 step 167 optimizing loss=166.11375427246094\n",
      "episode 1 step 168 optimizing loss=264.6656799316406\n",
      "episode 1 step 169 optimizing loss=185.05142211914062\n",
      "episode 1 step 170 optimizing loss=201.84048461914062\n",
      "episode 1 step 171 optimizing loss=384.5984802246094\n",
      "episode 1 step 172 optimizing loss=293.81719970703125\n",
      "episode 1 step 173 optimizing loss=53.93377685546875\n",
      "episode 1 step 174 optimizing loss=29.524005889892578\n",
      "episode 1 step 175 optimizing loss=317.9914245605469\n",
      "episode 1 step 176 optimizing loss=40.69641876220703\n",
      "episode 1 step 177 optimizing loss=283.2076110839844\n",
      "episode 1 step 178 optimizing loss=140.84521484375\n",
      "episode 1 step 179 optimizing loss=64.94098663330078\n",
      "episode 1 step 180 optimizing loss=601.4025268554688\n",
      "episode 1 step 181 optimizing loss=79.65164947509766\n",
      "episode 1 step 182 optimizing loss=548.1887817382812\n",
      "episode 1 step 183 optimizing loss=74.44889831542969\n",
      "episode 1 step 184 optimizing loss=66.26679229736328\n",
      "episode 1 step 185 optimizing loss=14.962068557739258\n",
      "episode 1 step 186 optimizing loss=165.25840759277344\n",
      "episode 1 step 187 optimizing loss=77.68159484863281\n",
      "episode 1 step 188 optimizing loss=321.22650146484375\n",
      "episode 1 step 189 optimizing loss=677.2852783203125\n",
      "episode 1 step 190 optimizing loss=959.3883666992188\n",
      "episode 1 step 191 optimizing loss=178.2380828857422\n",
      "episode 1 step 192 optimizing loss=162.11227416992188\n",
      "episode 1 step 193 optimizing loss=341.101806640625\n",
      "episode 1 step 194 optimizing loss=615.1968383789062\n",
      "episode 1 step 195 optimizing loss=42.679054260253906\n",
      "episode 1 step 196 optimizing loss=122.28129577636719\n",
      "episode 1 step 197 optimizing loss=377.9977111816406\n",
      "episode 1 step 198 optimizing loss=285.86541748046875\n",
      "episode 1 step 199 optimizing loss=237.16455078125\n",
      "episode 2 step 0 optimizing loss=72.50634765625\n",
      "episode 2 step 1 optimizing loss=103.6063461303711\n",
      "episode 2 step 2 optimizing loss=245.1002655029297\n",
      "episode 2 step 3 optimizing loss=18.650272369384766\n",
      "episode 2 step 4 optimizing loss=52.2304801940918\n",
      "episode 2 step 5 optimizing loss=101.62525939941406\n",
      "episode 2 step 6 optimizing loss=84.12544250488281\n",
      "episode 2 step 7 optimizing loss=79.00212097167969\n",
      "episode 2 step 8 optimizing loss=334.04217529296875\n",
      "episode 2 step 9 optimizing loss=139.7122039794922\n",
      "episode 2 step 10 optimizing loss=131.27328491210938\n",
      "episode 2 step 11 optimizing loss=164.53555297851562\n",
      "episode 2 step 12 optimizing loss=405.8003234863281\n",
      "episode 2 step 13 optimizing loss=22.931982040405273\n",
      "episode 2 step 14 optimizing loss=35.66073989868164\n",
      "episode 2 step 15 optimizing loss=268.5391845703125\n",
      "episode 2 step 16 optimizing loss=238.8869171142578\n",
      "episode 2 step 17 optimizing loss=78.67988586425781\n",
      "episode 2 step 18 optimizing loss=277.6316833496094\n",
      "episode 2 step 19 optimizing loss=216.0102081298828\n",
      "episode 2 step 20 optimizing loss=479.75872802734375\n",
      "episode 2 step 21 optimizing loss=337.08154296875\n",
      "episode 2 step 22 optimizing loss=15.979641914367676\n",
      "episode 2 step 23 optimizing loss=151.79458618164062\n",
      "episode 2 step 24 optimizing loss=43.06135940551758\n",
      "episode 2 step 25 optimizing loss=45.35657501220703\n",
      "episode 2 step 26 optimizing loss=115.04000091552734\n",
      "episode 2 step 27 optimizing loss=128.5254364013672\n",
      "episode 2 step 28 optimizing loss=464.2109375\n",
      "episode 2 step 29 optimizing loss=119.40702819824219\n",
      "episode 2 step 30 optimizing loss=262.6954345703125\n",
      "episode 2 step 31 optimizing loss=410.072021484375\n",
      "episode 2 step 32 optimizing loss=531.2399291992188\n",
      "episode 2 step 33 optimizing loss=442.0643615722656\n",
      "episode 2 step 34 optimizing loss=72.63093566894531\n",
      "episode 2 step 35 optimizing loss=68.78157806396484\n",
      "episode 2 step 36 optimizing loss=37.62261199951172\n",
      "episode 2 step 37 optimizing loss=119.5064697265625\n",
      "episode 2 step 38 optimizing loss=150.599853515625\n",
      "episode 2 step 39 optimizing loss=504.97662353515625\n",
      "episode 2 step 40 optimizing loss=271.0530700683594\n",
      "episode 2 step 41 optimizing loss=299.22503662109375\n",
      "episode 2 step 42 optimizing loss=302.12347412109375\n",
      "episode 2 step 43 optimizing loss=315.9080505371094\n",
      "episode 2 step 44 optimizing loss=528.4444580078125\n",
      "episode 2 step 45 optimizing loss=158.81272888183594\n",
      "episode 2 step 46 optimizing loss=179.2587432861328\n",
      "episode 2 step 47 optimizing loss=600.3477783203125\n",
      "episode 2 step 48 optimizing loss=272.44842529296875\n",
      "episode 2 step 49 optimizing loss=272.2289123535156\n",
      "episode 2 step 50 optimizing loss=581.946533203125\n",
      "episode 2 step 51 optimizing loss=198.9213104248047\n",
      "episode 2 step 52 optimizing loss=344.0329284667969\n",
      "episode 2 step 53 optimizing loss=18.963863372802734\n",
      "episode 2 step 54 optimizing loss=218.124755859375\n",
      "episode 2 step 55 optimizing loss=263.7191162109375\n",
      "episode 2 step 56 optimizing loss=374.9496765136719\n",
      "episode 2 step 57 optimizing loss=47.964996337890625\n",
      "episode 2 step 58 optimizing loss=325.93743896484375\n",
      "episode 2 step 59 optimizing loss=209.80252075195312\n",
      "episode 2 step 60 optimizing loss=87.47758483886719\n",
      "episode 2 step 61 optimizing loss=84.66075897216797\n",
      "episode 2 step 62 optimizing loss=83.26914978027344\n",
      "episode 2 step 63 optimizing loss=153.4391326904297\n",
      "episode 2 step 64 optimizing loss=71.2936019897461\n",
      "episode 2 step 65 optimizing loss=64.97267150878906\n",
      "episode 2 step 66 optimizing loss=115.74894714355469\n",
      "episode 2 step 67 optimizing loss=128.52574157714844\n",
      "episode 2 step 68 optimizing loss=495.21240234375\n",
      "episode 2 step 69 optimizing loss=141.90582275390625\n",
      "episode 2 step 70 optimizing loss=476.5918884277344\n",
      "episode 2 step 71 optimizing loss=317.0080871582031\n",
      "episode 2 step 72 optimizing loss=70.96064758300781\n",
      "episode 2 step 73 optimizing loss=61.36981201171875\n",
      "episode 2 step 74 optimizing loss=10.034650802612305\n",
      "episode 2 step 75 optimizing loss=152.73358154296875\n",
      "episode 2 step 76 optimizing loss=35.84955978393555\n",
      "episode 2 step 77 optimizing loss=37.26392364501953\n",
      "episode 2 step 78 optimizing loss=76.78178405761719\n",
      "episode 2 step 79 optimizing loss=94.71928405761719\n",
      "episode 2 step 80 optimizing loss=287.23590087890625\n",
      "episode 2 step 81 optimizing loss=131.31092834472656\n",
      "episode 2 step 82 optimizing loss=110.31976318359375\n",
      "episode 2 step 83 optimizing loss=136.53875732421875\n",
      "episode 2 step 84 optimizing loss=158.44076538085938\n",
      "episode 2 step 85 optimizing loss=82.17527770996094\n",
      "episode 2 step 86 optimizing loss=225.2429656982422\n",
      "episode 2 step 87 optimizing loss=155.83154296875\n",
      "episode 2 step 88 optimizing loss=319.780029296875\n",
      "episode 2 step 89 optimizing loss=602.9003295898438\n",
      "episode 2 step 90 optimizing loss=299.9867858886719\n",
      "episode 2 step 91 optimizing loss=204.23069763183594\n",
      "episode 2 step 92 optimizing loss=192.24600219726562\n",
      "episode 2 step 93 optimizing loss=69.15203857421875\n",
      "episode 2 step 94 optimizing loss=156.38717651367188\n",
      "episode 2 step 95 optimizing loss=320.5587158203125\n",
      "episode 2 step 96 optimizing loss=85.40767669677734\n",
      "episode 2 step 97 optimizing loss=84.62553405761719\n",
      "episode 2 step 98 optimizing loss=34.563087463378906\n",
      "episode 2 step 99 optimizing loss=13.136505126953125\n",
      "episode 2 step 100 optimizing loss=81.8892822265625\n",
      "episode 2 step 101 optimizing loss=385.4978332519531\n",
      "episode 2 step 102 optimizing loss=30.027685165405273\n",
      "episode 2 step 103 optimizing loss=59.2332649230957\n",
      "episode 2 step 104 optimizing loss=16.00148582458496\n",
      "episode 2 step 105 optimizing loss=62.925697326660156\n",
      "episode 2 step 106 optimizing loss=374.8744812011719\n",
      "episode 2 step 107 optimizing loss=18.512624740600586\n",
      "episode 2 step 108 optimizing loss=178.26181030273438\n",
      "episode 2 step 109 optimizing loss=645.7471923828125\n",
      "episode 2 step 110 optimizing loss=49.20026397705078\n",
      "episode 2 step 111 optimizing loss=170.43470764160156\n",
      "episode 2 step 112 optimizing loss=556.7526245117188\n",
      "episode 2 step 113 optimizing loss=335.86932373046875\n",
      "episode 2 step 114 optimizing loss=84.58735656738281\n",
      "episode 2 step 115 optimizing loss=52.127281188964844\n",
      "episode 2 step 116 optimizing loss=197.88018798828125\n",
      "episode 2 step 117 optimizing loss=108.2591552734375\n",
      "episode 2 step 118 optimizing loss=412.49420166015625\n",
      "episode 2 step 119 optimizing loss=407.0833435058594\n",
      "episode 2 step 120 optimizing loss=234.9194793701172\n",
      "episode 2 step 121 optimizing loss=285.1583557128906\n",
      "episode 2 step 122 optimizing loss=355.956298828125\n",
      "episode 2 step 123 optimizing loss=209.49014282226562\n",
      "episode 2 step 124 optimizing loss=193.81475830078125\n",
      "episode 2 step 125 optimizing loss=155.67471313476562\n",
      "episode 2 step 126 optimizing loss=168.0753173828125\n",
      "episode 2 step 127 optimizing loss=78.86219787597656\n",
      "episode 2 step 128 optimizing loss=132.08163452148438\n",
      "episode 2 step 129 optimizing loss=44.480735778808594\n",
      "episode 2 step 130 optimizing loss=46.84288787841797\n",
      "episode 2 step 131 optimizing loss=72.64320373535156\n",
      "episode 2 step 132 optimizing loss=110.6816177368164\n",
      "episode 2 step 133 optimizing loss=253.2530517578125\n",
      "episode 2 step 134 optimizing loss=220.68478393554688\n",
      "episode 2 step 135 optimizing loss=203.04161071777344\n",
      "episode 2 step 136 optimizing loss=96.0015869140625\n",
      "episode 2 step 137 optimizing loss=85.51158905029297\n",
      "episode 2 step 138 optimizing loss=420.9205017089844\n",
      "episode 2 step 139 optimizing loss=64.61650085449219\n",
      "episode 2 step 140 optimizing loss=29.883419036865234\n",
      "episode 2 step 141 optimizing loss=318.5892028808594\n",
      "episode 2 step 142 optimizing loss=171.04786682128906\n",
      "episode 2 step 143 optimizing loss=72.61665344238281\n",
      "episode 2 step 144 optimizing loss=154.1094512939453\n",
      "episode 2 step 145 optimizing loss=122.03593444824219\n",
      "episode 2 step 146 optimizing loss=189.80186462402344\n",
      "episode 2 step 147 optimizing loss=220.71267700195312\n",
      "episode 2 step 148 optimizing loss=78.4764633178711\n",
      "episode 2 step 149 optimizing loss=257.69781494140625\n",
      "episode 2 step 150 optimizing loss=192.53610229492188\n",
      "episode 2 step 151 optimizing loss=345.69427490234375\n",
      "episode 2 step 152 optimizing loss=535.6297607421875\n",
      "episode 2 step 153 optimizing loss=145.21096801757812\n",
      "episode 2 step 154 optimizing loss=118.0180435180664\n",
      "episode 2 step 155 optimizing loss=202.64007568359375\n",
      "episode 2 step 156 optimizing loss=64.35993957519531\n",
      "episode 2 step 157 optimizing loss=160.76754760742188\n",
      "episode 2 step 158 optimizing loss=349.64874267578125\n",
      "episode 2 step 159 optimizing loss=100.23347473144531\n",
      "episode 2 step 160 optimizing loss=483.3758850097656\n",
      "episode 2 step 161 optimizing loss=174.65945434570312\n",
      "episode 2 step 162 optimizing loss=334.56707763671875\n",
      "episode 2 step 163 optimizing loss=42.99492645263672\n",
      "episode 2 step 164 optimizing loss=59.341468811035156\n",
      "episode 2 step 165 optimizing loss=184.4252166748047\n",
      "episode 2 step 166 optimizing loss=220.8892059326172\n",
      "episode 2 step 167 optimizing loss=313.54095458984375\n",
      "episode 2 step 168 optimizing loss=628.4246826171875\n",
      "episode 2 step 169 optimizing loss=37.833641052246094\n",
      "episode 2 step 170 optimizing loss=286.44122314453125\n",
      "episode 2 step 171 optimizing loss=117.1034927368164\n",
      "episode 2 step 172 optimizing loss=99.71629333496094\n",
      "episode 2 step 173 optimizing loss=153.14584350585938\n",
      "episode 2 step 174 optimizing loss=72.69541931152344\n",
      "episode 2 step 175 optimizing loss=18.522491455078125\n",
      "episode 2 step 176 optimizing loss=56.5599250793457\n",
      "episode 2 step 177 optimizing loss=226.94406127929688\n",
      "episode 2 step 178 optimizing loss=313.62615966796875\n",
      "episode 2 step 179 optimizing loss=224.38943481445312\n",
      "episode 2 step 180 optimizing loss=23.51816177368164\n",
      "episode 2 step 181 optimizing loss=90.80245208740234\n",
      "episode 2 step 182 optimizing loss=128.5323028564453\n",
      "episode 2 step 183 optimizing loss=92.05281066894531\n",
      "episode 2 step 184 optimizing loss=63.3769645690918\n",
      "episode 2 step 185 optimizing loss=111.64625549316406\n",
      "episode 2 step 186 optimizing loss=59.60441970825195\n",
      "episode 2 step 187 optimizing loss=222.6142578125\n",
      "episode 2 step 188 optimizing loss=195.43045043945312\n",
      "episode 2 step 189 optimizing loss=434.68267822265625\n",
      "episode 2 step 190 optimizing loss=55.7249755859375\n",
      "episode 2 step 191 optimizing loss=343.3480224609375\n",
      "episode 2 step 192 optimizing loss=175.53179931640625\n",
      "episode 2 step 193 optimizing loss=277.6250305175781\n",
      "episode 2 step 194 optimizing loss=63.30663299560547\n",
      "episode 2 step 195 optimizing loss=95.45718383789062\n",
      "episode 2 step 196 optimizing loss=220.18930053710938\n",
      "episode 2 step 197 optimizing loss=92.98126220703125\n",
      "episode 2 step 198 optimizing loss=160.91458129882812\n",
      "episode 2 step 199 optimizing loss=124.4717025756836\n"
     ]
    }
   ],
   "source": [
    "stats_history = trainer.train(\n",
    "    episodes=25,\n",
    "    max_steps_per_episode=500,\n",
    "    defender_agent=defender,\n",
    "    attacker_agent=attacker,\n",
    "    warmup=24,\n",
    "    evaluator=engine,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.optimize(engine.defender, trainer.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.plot(stats_history[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.plot(stats_history[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('subgame')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09b31e4957d63d79e1e76c5537c345194b9f565583fac53cbbb105281d72baf4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
