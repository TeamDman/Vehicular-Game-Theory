{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ray\n",
    "from ray import air, tune\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "from ray.tune.registry import get_trainable_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.0.dev0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.__version__\n",
    "# using nightly 3.0.0 build since gymnasium isn't supported in 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage stats collection is enabled by default for nightly wheels. To disable this, run the following command: `ray disable-usage-stats` before starting Ray. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-07 19:53:32,504\tINFO worker.py:1536 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.8.15</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 3.0.0.dev0</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://127.0.0.1:8266\" target=\"_blank\">http://127.0.0.1:8266</a></b></td>\n",
       "</tr>\n",
       "\n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8266', python_version='3.8.15', ray_version='3.0.0.dev0', ray_commit='3e18487d7427a3ffcea9fecd5c79ca3e81fb92d0', address_info={'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': 'tcp://127.0.0.1:32330', 'raylet_socket_name': 'tcp://127.0.0.1:49374', 'webui_url': '127.0.0.1:8266', 'session_dir': 'C:\\\\Users\\\\TeamD\\\\AppData\\\\Local\\\\Temp\\\\ray\\\\session_2023-01-07_19-53-28_113951_23948', 'metrics_export_port': 48835, 'gcs_address': '127.0.0.1:33988', 'address': '127.0.0.1:33988', 'dashboard_agent_listen_port': 52365, 'node_id': '9c221376744fb3e1c01117c4f1d36155187ce303ca852d02a1396cc6'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(num_cpus=None, num_gpus=1, local_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platoonenv\n",
    "env_cls = platoonenv.InOutDangerEnv\n",
    "env=env_cls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-07 19:53:53,233\tWARNING env.py:156 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "2023-01-07 19:53:53,234\tWARNING env.py:166 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n"
     ]
    }
   ],
   "source": [
    "ray.rllib.utils.check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_name = \"CartPole-v1\"\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "train_class = \"PPO\"\n",
    "param_space = (\n",
    "    PPOConfig()\n",
    "    .environment(env_cls)\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(num_rollout_workers=10)\n",
    "    .resources(num_gpus=1)\n",
    "    \n",
    ")\n",
    "# train_class=\"DQN\"\n",
    "# config = (\n",
    "#     DQNConfig()\n",
    "#     .environment(env_cls)\n",
    "#     .framework(\"torch\")\n",
    "#     .rollouts(num_rollout_workers=10)\n",
    "#     .resources(num_gpus=1)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-48\n"
     ]
    }
   ],
   "source": [
    "stop = {\n",
    "    \"training_iteration\": 50,\n",
    "    \"timesteps_total\": 100_000,\n",
    "    \"episode_reward_mean\": env.metadata[\"reward_threshold\"],\n",
    "}\n",
    "print(stop[\"episode_reward_mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config=air.RunConfig(\n",
    "    stop=stop,\n",
    "    verbose=2,\n",
    "    checkpoint_config=air.CheckpointConfig(\n",
    "        checkpoint_frequency=1, checkpoint_at_end=True\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunConfig(stop={'training_iteration': 50, 'timesteps_total': 100000, 'episode_reward_mean': -48}, checkpoint_config=CheckpointConfig(checkpoint_frequency=1, checkpoint_at_end=True), verbose=2)\n"
     ]
    }
   ],
   "source": [
    "print(run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = False\n",
    "resume_path = \"C:/Users/TeamD/ray_results/PPO\"\n",
    "if resume:\n",
    "    tuner = tune.Tuner.restore(resume_path)\n",
    "else:\n",
    "    tuner = tune.Tuner(\n",
    "        train_class,\n",
    "        param_space=param_space.to_dict(),\n",
    "        run_config=run_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-01-07 19:59:08</td></tr>\n",
       "<tr><td>Running for: </td><td>00:05:04.88        </td></tr>\n",
       "<tr><td>Memory:      </td><td>23.1/31.9 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/1 GPUs, 0.0/9.33 GiB heap, 0.0/4.66 GiB objects (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status    </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InOutDangerEnv_f252c_00000</td><td>TERMINATED</td><td>127.0.0.1:4524</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         272.949</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">  -73.08</td><td style=\"text-align: right;\">                 -51</td><td style=\"text-align: right;\">                -174</td><td style=\"text-align: right;\">                10</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=4524)\u001b[0m 2023-01-07 19:54:13,401\tWARNING algorithm_config.py:614 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m 2023-01-07 19:54:29,989\tWARNING env.py:156 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m 2023-01-07 19:54:29,989\tWARNING env.py:166 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m 2023-01-07 19:54:30,043\tINFO policy.py:1196 -- Policy (worker=1) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m 2023-01-07 19:54:30,043\tINFO torch_policy_v2.py:110 -- Found 1 visible cuda devices.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24644)\u001b[0m 2023-01-07 19:54:30,189\tINFO policy.py:1196 -- Policy (worker=6) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24644)\u001b[0m 2023-01-07 19:54:30,189\tINFO torch_policy_v2.py:110 -- Found 1 visible cuda devices.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10400)\u001b[0m 2023-01-07 19:54:30,187\tINFO policy.py:1196 -- Policy (worker=10) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10400)\u001b[0m 2023-01-07 19:54:30,188\tINFO torch_policy_v2.py:110 -- Found 1 visible cuda devices.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5496)\u001b[0m 2023-01-07 19:54:30,272\tINFO policy.py:1196 -- Policy (worker=3) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5496)\u001b[0m 2023-01-07 19:54:30,272\tINFO torch_policy_v2.py:110 -- Found 1 visible cuda devices.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4988)\u001b[0m 2023-01-07 19:54:30,273\tINFO policy.py:1196 -- Policy (worker=8) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4988)\u001b[0m 2023-01-07 19:54:30,273\tINFO torch_policy_v2.py:110 -- Found 1 visible cuda devices.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11496)\u001b[0m 2023-01-07 19:54:30,242\tINFO policy.py:1196 -- Policy (worker=2) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11496)\u001b[0m 2023-01-07 19:54:30,242\tINFO torch_policy_v2.py:110 -- Found 1 visible cuda devices.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12020)\u001b[0m 2023-01-07 19:54:30,271\tINFO policy.py:1196 -- Policy (worker=5) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12020)\u001b[0m 2023-01-07 19:54:30,271\tINFO torch_policy_v2.py:110 -- Found 1 visible cuda devices.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25440)\u001b[0m 2023-01-07 19:54:30,464\tINFO policy.py:1196 -- Policy (worker=9) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25440)\u001b[0m 2023-01-07 19:54:30,464\tINFO torch_policy_v2.py:110 -- Found 1 visible cuda devices.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12532)\u001b[0m 2023-01-07 19:54:30,478\tINFO policy.py:1196 -- Policy (worker=4) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12532)\u001b[0m 2023-01-07 19:54:30,479\tINFO torch_policy_v2.py:110 -- Found 1 visible cuda devices.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29360)\u001b[0m 2023-01-07 19:54:30,486\tINFO policy.py:1196 -- Policy (worker=7) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29360)\u001b[0m 2023-01-07 19:54:30,486\tINFO torch_policy_v2.py:110 -- Found 1 visible cuda devices.\n",
      "\u001b[2m\u001b[36m(PPO pid=4524)\u001b[0m 2023-01-07 19:54:30,509\tINFO worker_set.py:309 -- Inferred observation/action spaces from remote worker (local worker has no env): {'default_policy': (Box(-10.0, 1.0, (20,), float32), Discrete(11)), '__env__': (Box(-10.0, 1.0, (20,), float32), Discrete(11))}\n",
      "\u001b[2m\u001b[36m(PPO pid=4524)\u001b[0m 2023-01-07 19:54:30,551\tINFO policy.py:1196 -- Policy (worker=local) running on 1 GPUs.\n",
      "\u001b[2m\u001b[36m(PPO pid=4524)\u001b[0m 2023-01-07 19:54:30,553\tINFO torch_policy_v2.py:110 -- Found 1 visible cuda devices.\n",
      "\u001b[2m\u001b[36m(PPO pid=4524)\u001b[0m 2023-01-07 19:54:32,962\tINFO rollout_worker.py:2037 -- Built policy map: <PolicyMap lru-caching-capacity=100 policy-IDs=['default_policy']>\n",
      "\u001b[2m\u001b[36m(PPO pid=4524)\u001b[0m 2023-01-07 19:54:32,962\tINFO rollout_worker.py:2038 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x000002D159041460>}\n",
      "\u001b[2m\u001b[36m(PPO pid=4524)\u001b[0m 2023-01-07 19:54:32,962\tINFO rollout_worker.py:757 -- Built filter map: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x000002D15906F490>})\n",
      "\u001b[2m\u001b[36m(PPO pid=4524)\u001b[0m 2023-01-07 19:54:32,982\tINFO trainable.py:172 -- Trainable.setup took 19.224 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m 2023-01-07 19:54:32,999\tINFO rollout_worker.py:905 -- Generating sample batch of size 400\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m 2023-01-07 19:54:32,999\tINFO sampler.py:610 -- Raw obs from env: { 0: { 'agent0': np.ndarray((20,), dtype=float32, min=-10.0, max=0.0, mean=-1.0)}}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m 2023-01-07 19:54:32,999\tINFO sampler.py:611 -- Info return from env: {0: {'agent0': {}}}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m 2023-01-07 19:54:33,000\tINFO sampler.py:853 -- Preprocessed obs: np.ndarray((20,), dtype=float32, min=-10.0, max=0.0, mean=-1.0)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m 2023-01-07 19:54:33,000\tINFO sampler.py:858 -- Filtered obs: np.ndarray((20,), dtype=float32, min=-10.0, max=0.0, mean=-1.0)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m 2023-01-07 19:54:33,002\tINFO sampler.py:1144 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m                                   'info': {},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m                                   'obs': np.ndarray((20,), dtype=float32, min=-10.0, max=0.0, mean=-1.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m                                   'prev_action': None,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m                                   'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m                         'type': '_PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m 2023-01-07 19:54:33,005\tINFO sampler.py:1171 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 11), dtype=float32, min=-0.008, max=0.009, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-2.389, max=-2.389, mean=-2.389),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.092, max=0.092, mean=0.092),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.002, max=-0.002, mean=-0.002)})}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m 2023-01-07 19:54:33,062\tINFO simple_list_collector.py:520 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((10, 11), dtype=float32, min=-0.008, max=0.012, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m               'action_logp': np.ndarray((10,), dtype=float32, min=-2.405, max=-2.389, mean=-2.397),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m               'actions': np.ndarray((10,), dtype=int64, min=0.0, max=10.0, mean=5.4),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m               'advantages': np.ndarray((10,), dtype=float32, min=-149.176, max=-14.999, mean=-83.158),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m               'agent_index': np.ndarray((10,), dtype=int32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m               'eps_id': np.ndarray((10,), dtype=int64, min=8.486672085042604e+17, max=8.486672085042604e+17, mean=8.486672085042604e+17),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m               'infos': np.ndarray((10,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m               'new_obs': np.ndarray((10, 20), dtype=float32, min=-10.0, max=1.0, mean=-0.83),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m               'obs': np.ndarray((10, 20), dtype=float32, min=-10.0, max=1.0, mean=-0.855),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m               'rewards': np.ndarray((10,), dtype=float32, min=-25.0, max=-7.0, mean=-15.6),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m               't': np.ndarray((10,), dtype=int32, min=0.0, max=9.0, mean=4.5),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m               'terminateds': np.ndarray((10,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m               'truncateds': np.ndarray((10,), dtype=bool, min=0.0, max=1.0, mean=0.1),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m               'unroll_id': np.ndarray((10,), dtype=int32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m               'value_targets': np.ndarray((10,), dtype=float32, min=-149.178, max=-15.002, mean=-83.161),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m               'vf_preds': np.ndarray((10,), dtype=float32, min=-0.003, max=-0.002, mean=-0.003)}}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m 2023-01-07 19:54:34,289\tINFO rollout_worker.py:946 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m { 'action_dist_inputs': np.ndarray((400, 11), dtype=float32, min=-0.019, max=0.016, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m   'action_logp': np.ndarray((400,), dtype=float32, min=-2.416, max=-2.386, mean=-2.398),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m   'actions': np.ndarray((400,), dtype=int64, min=0.0, max=10.0, mean=5.095),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m   'advantages': np.ndarray((400,), dtype=float32, min=-236.363, max=-4.0, mean=-71.529),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m   'agent_index': np.ndarray((400,), dtype=int32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m   'eps_id': np.ndarray((400,), dtype=int64, min=1.188860607044784e+16, max=9.736800603096809e+17, mean=5.2816955622095014e+17),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m   'infos': np.ndarray((400,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m   'new_obs': np.ndarray((400, 20), dtype=float32, min=-10.0, max=1.0, mean=-0.86),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m   'obs': np.ndarray((400, 20), dtype=float32, min=-10.0, max=1.0, mean=-0.879),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m   'rewards': np.ndarray((400,), dtype=float32, min=-28.0, max=-2.0, mean=-13.073),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m   't': np.ndarray((400,), dtype=int32, min=0.0, max=9.0, mean=4.5),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m   'terminateds': np.ndarray((400,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m   'truncateds': np.ndarray((400,), dtype=bool, min=0.0, max=1.0, mean=0.1),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m   'unroll_id': np.ndarray((400,), dtype=int32, min=0.0, max=39.0, mean=19.5),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m   'value_targets': np.ndarray((400,), dtype=float32, min=-236.365, max=-4.001, mean=-71.53),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m   'vf_preds': np.ndarray((400,), dtype=float32, min=-0.009, max=0.006, mean=-0.001)}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22036)\u001b[0m \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>counters                                                                                                                                </th><th>custom_metrics  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_reqs</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                                                                                                                               </th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf                                                                                                                                                                                                    </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </th><th>timers                                                                                                                                                                                 </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InOutDangerEnv_f252c_00000</td><td style=\"text-align: right;\">                 100000</td><td>{&#x27;num_env_steps_sampled&#x27;: 100000, &#x27;num_env_steps_trained&#x27;: 100000, &#x27;num_agent_steps_sampled&#x27;: 100000, &#x27;num_agent_steps_trained&#x27;: 100000}</td><td>{}              </td><td style=\"text-align: right;\">                10</td><td>{}             </td><td style=\"text-align: right;\">                 -51</td><td style=\"text-align: right;\">               -73.08</td><td style=\"text-align: right;\">                -174</td><td style=\"text-align: right;\">                 400</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;custom_metrics&#x27;: {}, &#x27;learner_stats&#x27;: {&#x27;cur_kl_coeff&#x27;: 0.3, &#x27;cur_lr&#x27;: 5.0000000000000016e-05, &#x27;total_loss&#x27;: 9.17215512388496, &#x27;policy_loss&#x27;: -0.044242910889568186, &#x27;vf_loss&#x27;: 9.21086962915236, &#x27;vf_explained_var&#x27;: -4.8132352931525116e-08, &#x27;kl&#x27;: 0.018428052802423323, &#x27;entropy&#x27;: 1.9827778520122652, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;model&#x27;: {}, &#x27;num_grad_updates_lifetime&#x27;: 22785.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 464.5}}, &#x27;num_env_steps_sampled&#x27;: 100000, &#x27;num_env_steps_trained&#x27;: 100000, &#x27;num_agent_steps_sampled&#x27;: 100000, &#x27;num_agent_steps_trained&#x27;: 100000}</td><td style=\"text-align: right;\">                   100000</td><td style=\"text-align: right;\">                   100000</td><td style=\"text-align: right;\">                 100000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                 100000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                   10</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                         4000</td><td>{&#x27;cpu_util_percent&#x27;: 29.915384615384614, &#x27;ram_util_percent&#x27;: 72.37692307692306, &#x27;gpu_util_percent0&#x27;: 0.3123076923076923, &#x27;vram_util_percent0&#x27;: 0.15997770345596432}</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.8466101922865881, &#x27;mean_inference_ms&#x27;: 1.6149268676228383, &#x27;mean_action_processing_ms&#x27;: 0.1555506032343543, &#x27;mean_env_wait_ms&#x27;: 0.12382752727763723, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: -51.0, &#x27;episode_reward_min&#x27;: -174.0, &#x27;episode_reward_mean&#x27;: -73.08, &#x27;episode_len_mean&#x27;: 10.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 400, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [-75.0, -71.0, -69.0, -65.0, -73.0, -61.0, -83.0, -67.0, -87.0, -71.0, -81.0, -68.0, -79.0, -74.0, -69.0, -67.0, -77.0, -73.0, -67.0, -72.0, -65.0, -71.0, -79.0, -76.0, -174.0, -67.0, -73.0, -69.0, -71.0, -63.0, -73.0, -88.0, -71.0, -73.0, -65.0, -61.0, -74.0, -81.0, -73.0, -77.0, -71.0, -72.0, -73.0, -69.0, -51.0, -79.0, -77.0, -71.0, -73.0, -73.0, -73.0, -63.0, -61.0, -71.0, -81.0, -77.0, -63.0, -79.0, -73.0, -82.0, -81.0, -76.0, -77.0, -81.0, -73.0, -73.0, -63.0, -71.0, -71.0, -78.0, -67.0, -69.0, -81.0, -77.0, -74.0, -66.0, -59.0, -73.0, -73.0, -81.0, -85.0, -73.0, -77.0, -75.0, -73.0, -69.0, -63.0, -81.0, -69.0, -79.0, -73.0, -79.0, -79.0, -67.0, -65.0, -75.0, -59.0, -77.0, -69.0, -77.0, -73.0, -61.0, -75.0, -62.0, -73.0, -89.0, -67.0, -67.0, -81.0, -63.0, -77.0, -73.0, -77.0, -78.0, -68.0, -71.0, -67.0, -65.0, -69.0, -67.0, -71.0, -75.0, -81.0, -67.0, -69.0, -65.0, -75.0, -61.0, -73.0, -73.0, -61.0, -76.0, -57.0, -71.0, -151.0, -87.0, -70.0, -77.0, -71.0, -71.0, -62.0, -69.0, -83.0, -85.0, -77.0, -66.0, -87.0, -75.0, -64.0, -69.0, -69.0, -81.0, -63.0, -73.0, -81.0, -67.0, -85.0, -79.0, -55.0, -85.0, -81.0, -69.0, -81.0, -65.0, -57.0, -65.0, -59.0, -74.0, -62.0, -67.0, -70.0, -83.0, -81.0, -71.0, -69.0, -68.0, -65.0, -75.0, -81.0, -79.0, -67.0, -79.0, -72.0, -63.0, -61.0, -65.0, -67.0, -69.0, -69.0, -73.0, -72.0, -71.0, -59.0, -83.0, -70.0, -71.0, -63.0, -66.0, -71.0, -73.0, -79.0, -79.0, -73.0, -67.0, -77.0, -69.0, -65.0, -75.0, -73.0, -73.0, -73.0, -75.0, -71.0, -65.0, -77.0, -61.0, -75.0, -85.0, -79.0, -54.0, -71.0, -77.0, -69.0, -78.0, -73.0, -81.0, -77.0, -62.0, -77.0, -71.0, -67.0, -69.0, -83.0, -73.0, -81.0, -81.0, -85.0, -77.0, -81.0, -71.0, -96.0, -64.0, -73.0, -67.0, -83.0, -61.0, -69.0, -77.0, -67.0, -83.0, -79.0, -69.0, -59.0, -85.0, -63.0, -59.0, -81.0, -87.0, -69.0, -71.0, -63.0, -75.0, -76.0, -69.0, -84.0, -71.0, -75.0, -69.0, -67.0, -80.0, -76.0, -87.0, -71.0, -79.0, -69.0, -79.0, -67.0, -67.0, -73.0, -65.0, -63.0, -85.0, -73.0, -71.0, -65.0, -64.0, -85.0, -77.0, -76.0, -78.0, -74.0, -57.0, -63.0, -76.0, -63.0, -71.0, -62.0, -75.0, -78.0, -63.0, -75.0, -55.0, -69.0, -63.0, -73.0, -90.0, -57.0, -80.0, -61.0, -79.0, -59.0, -71.0, -63.0, -71.0, -75.0, -81.0, -85.0, -81.0, -65.0, -71.0, -65.0, -75.0, -65.0, -69.0, -59.0, -70.0, -64.0, -77.0, -69.0, -89.0, -79.0, -79.0, -65.0, -74.0, -63.0, -85.0, -75.0, -83.0, -81.0, -69.0, -63.0, -73.0, -65.0, -75.0, -75.0, -151.0, -59.0, -67.0, -63.0, -81.0, -80.0, -81.0, -87.0, -69.0, -63.0, -73.0, -77.0, -81.0, -73.0, -111.0, -75.0, -81.0, -73.0, -77.0, -59.0, -76.0, -63.0, -68.0, -79.0, -75.0, -78.0, -72.0, -81.0, -75.0, -63.0, -79.0, -61.0, -69.0, -73.0, -71.0, -77.0, -73.0, -69.0, -80.0, -63.0, -85.0, -74.0, -71.0, -69.0, -77.0, -73.0, -81.0, -72.0, -75.0, -71.0, -81.0, -64.0, -69.0, -75.0, -88.0], &#x27;episode_lengths&#x27;: [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.8466101922865881, &#x27;mean_inference_ms&#x27;: 1.6149268676228383, &#x27;mean_action_processing_ms&#x27;: 0.1555506032343543, &#x27;mean_env_wait_ms&#x27;: 0.12382752727763723, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0}</td><td>{&#x27;training_iteration_time_ms&#x27;: 11162.518, &#x27;load_time_ms&#x27;: 2.7, &#x27;load_throughput&#x27;: 1481641.203, &#x27;learn_time_ms&#x27;: 10001.82, &#x27;learn_throughput&#x27;: 399.927, &#x27;synch_weights_time_ms&#x27;: 13.402}</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-07 19:59:08,710\tINFO tune.py:774 -- Total run time: 305.27 seconds (304.74 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='titleData jp-RenderedHTMLCommon'>\n",
       "  <h3>RunConfig</h3>\n",
       "  <div class=\"runConfig\">\n",
       "  <div class=\"generalSettings\">\n",
       "    <div class=\"scrollableTable jp-RenderedHTMLCommon\">\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Setting                </th><th style=\"text-align: right;\">  Value</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>Name                   </td><td style=\"text-align: right;\">       </td></tr>\n",
       "<tr><td>Local results directory</td><td style=\"text-align: right;\">       </td></tr>\n",
       "<tr><td>Verbosity              </td><td style=\"text-align: right;\">      2</td></tr>\n",
       "<tr><td>Log to file            </td><td style=\"text-align: right;\">  False</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".scrollableTable {\n",
       "  overflow-y: auto;\n",
       "  max-height: 300px;\n",
       "}\n",
       ".scrollableTable table {\n",
       "  width: 100%;\n",
       "}\n",
       ".scrollableTable table :is(th,td) {\n",
       "  text-align: left !important;\n",
       "}\n",
       ".scrollableTable th {\n",
       "  background: var(--jp-layout-color1);\n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"sideBySide\">\n",
       "    <div class='miniTitleData jp-RenderedHTMLCommon'>\n",
       "  <h4><b>Failure Config</b></h4>\n",
       "  <div class=\"scrollableTable jp-RenderedHTMLCommon\">\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Setting     </th><th style=\"text-align: right;\">  Value</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>Max failures</td><td style=\"text-align: right;\">      0</td></tr>\n",
       "<tr><td>Fail fast   </td><td style=\"text-align: right;\">  False</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".scrollableTable {\n",
       "  overflow-y: auto;\n",
       "  max-height: none;\n",
       "}\n",
       ".scrollableTable table {\n",
       "  width: 100%;\n",
       "}\n",
       ".scrollableTable table :is(th,td) {\n",
       "  text-align: left !important;\n",
       "}\n",
       ".scrollableTable th {\n",
       "  background: var(--jp-layout-color1);\n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "}\n",
       "</style>\n",
       "\n",
       "</div>\n",
       "<div class=\"vDivider\"></div>\n",
       "<style>\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "<div class='miniTitleData jp-RenderedHTMLCommon'>\n",
       "  <h4><b>Sync Config</b></h4>\n",
       "  <div class=\"scrollableTable jp-RenderedHTMLCommon\">\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Setting           </th><th style=\"text-align: right;\">  Value</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>Upload directory  </td><td style=\"text-align: right;\">       </td></tr>\n",
       "<tr><td>Sync on checkpoint</td><td style=\"text-align: right;\">   True</td></tr>\n",
       "<tr><td>Sync period       </td><td style=\"text-align: right;\">    300</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".scrollableTable {\n",
       "  overflow-y: auto;\n",
       "  max-height: none;\n",
       "}\n",
       ".scrollableTable table {\n",
       "  width: 100%;\n",
       "}\n",
       ".scrollableTable table :is(th,td) {\n",
       "  text-align: left !important;\n",
       "}\n",
       ".scrollableTable th {\n",
       "  background: var(--jp-layout-color1);\n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "}\n",
       "</style>\n",
       "\n",
       "</div>\n",
       "<div class=\"vDivider\"></div>\n",
       "<style>\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "<div class='miniTitleData jp-RenderedHTMLCommon'>\n",
       "  <h4><b>Checkpoint Config</b></h4>\n",
       "  <div class=\"scrollableTable jp-RenderedHTMLCommon\">\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Setting                      </th><th>Value      </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>Number of checkpoints to keep</td><td>All        </td></tr>\n",
       "<tr><td>Checkpoint score attribute   </td><td>Most recent</td></tr>\n",
       "<tr><td>Checkpoint score order       </td><td>max        </td></tr>\n",
       "<tr><td>Checkpoint frequency         </td><td>1          </td></tr>\n",
       "<tr><td>Checkpoint at end            </td><td>True       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".scrollableTable {\n",
       "  overflow-y: auto;\n",
       "  max-height: none;\n",
       "}\n",
       ".scrollableTable table {\n",
       "  width: 100%;\n",
       "}\n",
       ".scrollableTable table :is(th,td) {\n",
       "  text-align: left !important;\n",
       "}\n",
       ".scrollableTable th {\n",
       "  background: var(--jp-layout-color1);\n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "}\n",
       "</style>\n",
       "\n",
       "</div>\n",
       "\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".sideBySide {\n",
       "  display: flex;\n",
       "  flex-direction: row;\n",
       "  gap: 1em;\n",
       "}\n",
       ".generalSettings {\n",
       "  border-bottom: var(--jp-border-width) solid var(--jp-border-color0);\n",
       "}\n",
       "</style>\n",
       "\n",
       "</div>\n",
       "<style>\n",
       ".titleData h3 {\n",
       "    border-bottom-width: var(--jp-border-width);\n",
       "    border-bottom-color: var(--jp-border-color0);\n",
       "    border-bottom-style: solid;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "RunConfig(stop={'training_iteration': 50, 'timesteps_total': 100000, 'episode_reward_mean': -60}, checkpoint_config=CheckpointConfig(checkpoint_frequency=1, checkpoint_at_end=True), verbose=2)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not find Tuner state in restore directory. Did you passthe correct path (including experiment directory?) Got: Checkpoint(local_path=C:\\Users\\TeamD\\ray_results\\PPO\\PPO_InOutDangerLongEnv_1b8f1_00000_0_2023-01-04_00-53-26\\checkpoint_000025)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tuner2 \u001b[39m=\u001b[39m tune\u001b[39m.\u001b[39;49mTuner\u001b[39m.\u001b[39;49mrestore(\u001b[39mstr\u001b[39;49m(checkpoint))\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\rllib\\lib\\site-packages\\ray\\tune\\tuner.py:219\u001b[0m, in \u001b[0;36mTuner.restore\u001b[1;34m(cls, path, resume_unfinished, resume_errored, restart_errored, overwrite_trainable)\u001b[0m\n\u001b[0;32m    212\u001b[0m resume_config \u001b[39m=\u001b[39m _ResumeConfig(\n\u001b[0;32m    213\u001b[0m     resume_unfinished\u001b[39m=\u001b[39mresume_unfinished,\n\u001b[0;32m    214\u001b[0m     resume_errored\u001b[39m=\u001b[39mresume_errored,\n\u001b[0;32m    215\u001b[0m     restart_errored\u001b[39m=\u001b[39mrestart_errored,\n\u001b[0;32m    216\u001b[0m )\n\u001b[0;32m    218\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ray\u001b[39m.\u001b[39mutil\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mray\u001b[39m.\u001b[39mis_connected():\n\u001b[1;32m--> 219\u001b[0m     tuner_internal \u001b[39m=\u001b[39m TunerInternal(\n\u001b[0;32m    220\u001b[0m         restore_path\u001b[39m=\u001b[39;49mpath,\n\u001b[0;32m    221\u001b[0m         resume_config\u001b[39m=\u001b[39;49mresume_config,\n\u001b[0;32m    222\u001b[0m         trainable\u001b[39m=\u001b[39;49moverwrite_trainable,\n\u001b[0;32m    223\u001b[0m     )\n\u001b[0;32m    224\u001b[0m     \u001b[39mreturn\u001b[39;00m Tuner(_tuner_internal\u001b[39m=\u001b[39mtuner_internal)\n\u001b[0;32m    225\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\rllib\\lib\\site-packages\\ray\\tune\\impl\\tuner_internal.py:95\u001b[0m, in \u001b[0;36mTunerInternal.__init__\u001b[1;34m(self, restore_path, resume_config, trainable, param_space, tune_config, run_config, _tuner_kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39m# Restore from Tuner checkpoint.\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[39mif\u001b[39;00m restore_path:\n\u001b[1;32m---> 95\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_restore_from_path_or_uri(\n\u001b[0;32m     96\u001b[0m         path_or_uri\u001b[39m=\u001b[39;49mrestore_path,\n\u001b[0;32m     97\u001b[0m         resume_config\u001b[39m=\u001b[39;49mresume_config,\n\u001b[0;32m     98\u001b[0m         overwrite_trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[0;32m     99\u001b[0m     )\n\u001b[0;32m    100\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[39m# Start from fresh\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\rllib\\lib\\site-packages\\ray\\tune\\impl\\tuner_internal.py:303\u001b[0m, in \u001b[0;36mTunerInternal._restore_from_path_or_uri\u001b[1;34m(self, path_or_uri, resume_config, overwrite_trainable)\u001b[0m\n\u001b[0;32m    297\u001b[0m experiment_checkpoint_path \u001b[39m=\u001b[39m Path(experiment_checkpoint_dir)\n\u001b[0;32m    299\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    300\u001b[0m     \u001b[39mnot\u001b[39;00m (experiment_checkpoint_path \u001b[39m/\u001b[39m _TRAINABLE_PKL)\u001b[39m.\u001b[39mexists()\n\u001b[0;32m    301\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m (experiment_checkpoint_path \u001b[39m/\u001b[39m _TUNER_PKL)\u001b[39m.\u001b[39mexists()\n\u001b[0;32m    302\u001b[0m ):\n\u001b[1;32m--> 303\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    304\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not find Tuner state in restore directory. Did you pass\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    305\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthe correct path (including experiment directory?) Got: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_uri\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m     )\n\u001b[0;32m    309\u001b[0m \u001b[39m# Load trainable and tuner state\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(experiment_checkpoint_path \u001b[39m/\u001b[39m _TRAINABLE_PKL, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m fp:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Could not find Tuner state in restore directory. Did you passthe correct path (including experiment directory?) Got: Checkpoint(local_path=C:\\Users\\TeamD\\ray_results\\PPO\\PPO_InOutDangerLongEnv_1b8f1_00000_0_2023-01-04_00-53-26\\checkpoint_000025)"
     ]
    }
   ],
   "source": [
    "tuner2 = tune.Tuner.restore(str(checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-07 19:59:31,562\tWARNING algorithm_config.py:614 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint(local_path=C:\\Users\\TeamD\\ray_results\\PPO\\PPO_InOutDangerEnv_f252c_00000_0_2023-01-07_19-54-03\\checkpoint_000025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=1876)\u001b[0m 2023-01-07 19:59:43,960\tINFO policy.py:1196 -- Policy (worker=2) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1876)\u001b[0m 2023-01-07 19:59:43,960\tINFO torch_policy_v2.py:110 -- Found 1 visible cuda devices.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20400)\u001b[0m 2023-01-07 19:59:44,005\tINFO policy.py:1196 -- Policy (worker=4) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20400)\u001b[0m 2023-01-07 19:59:44,005\tINFO torch_policy_v2.py:110 -- Found 1 visible cuda devices.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29404)\u001b[0m 2023-01-07 19:59:44,064\tINFO policy.py:1196 -- Policy (worker=10) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29404)\u001b[0m 2023-01-07 19:59:44,065\tINFO torch_policy_v2.py:110 -- Found 1 visible cuda devices.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16532)\u001b[0m 2023-01-07 19:59:44,172\tWARNING env.py:156 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16532)\u001b[0m 2023-01-07 19:59:44,172\tWARNING env.py:166 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19492)\u001b[0m 2023-01-07 19:59:44,211\tINFO policy.py:1196 -- Policy (worker=6) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19492)\u001b[0m 2023-01-07 19:59:44,211\tINFO torch_policy_v2.py:110 -- Found 1 visible cuda devices.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21988)\u001b[0m 2023-01-07 19:59:44,234\tINFO policy.py:1196 -- Policy (worker=7) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21988)\u001b[0m 2023-01-07 19:59:44,234\tINFO torch_policy_v2.py:110 -- Found 1 visible cuda devices.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16532)\u001b[0m 2023-01-07 19:59:44,246\tINFO policy.py:1196 -- Policy (worker=1) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16532)\u001b[0m 2023-01-07 19:59:44,246\tINFO torch_policy_v2.py:110 -- Found 1 visible cuda devices.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24988)\u001b[0m 2023-01-07 19:59:44,229\tINFO policy.py:1196 -- Policy (worker=8) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24988)\u001b[0m 2023-01-07 19:59:44,229\tINFO torch_policy_v2.py:110 -- Found 1 visible cuda devices.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24860)\u001b[0m 2023-01-07 19:59:44,260\tINFO policy.py:1196 -- Policy (worker=3) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24860)\u001b[0m 2023-01-07 19:59:44,260\tINFO torch_policy_v2.py:110 -- Found 1 visible cuda devices.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26300)\u001b[0m 2023-01-07 19:59:44,285\tINFO policy.py:1196 -- Policy (worker=5) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26300)\u001b[0m 2023-01-07 19:59:44,286\tINFO torch_policy_v2.py:110 -- Found 1 visible cuda devices.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7832)\u001b[0m 2023-01-07 19:59:44,320\tINFO policy.py:1196 -- Policy (worker=9) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7832)\u001b[0m 2023-01-07 19:59:44,321\tINFO torch_policy_v2.py:110 -- Found 1 visible cuda devices.\n",
      "2023-01-07 19:59:44,342\tINFO worker_set.py:309 -- Inferred observation/action spaces from remote worker (local worker has no env): {'default_policy': (Box(-10.0, 1.0, (20,), float32), Discrete(11)), '__env__': (Box(-10.0, 1.0, (20,), float32), Discrete(11))}\n",
      "2023-01-07 19:59:44,387\tINFO policy.py:1196 -- Policy (worker=local) running on 1 GPUs.\n",
      "2023-01-07 19:59:44,388\tINFO torch_policy_v2.py:110 -- Found 1 visible cuda devices.\n",
      "2023-01-07 19:59:46,466\tINFO rollout_worker.py:2037 -- Built policy map: <PolicyMap lru-caching-capacity=100 policy-IDs=['default_policy']>\n",
      "2023-01-07 19:59:46,467\tINFO rollout_worker.py:2038 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x00000288109BB0D0>}\n",
      "2023-01-07 19:59:46,467\tINFO rollout_worker.py:757 -- Built filter map: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x00000288109BBA90>})\n",
      "2023-01-07 19:59:46,485\tINFO trainable.py:172 -- Trainable.setup took 14.881 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = results.get_best_result().checkpoint\n",
    "# Checkpoint(local_path=C:\\Users\\TeamD\\ray_results\\PPO\\PPO_PlatoonEnvV1_56ad8_00000_0_2022-12-31_15-04-31\\checkpoint_000200)\n",
    "# Checkpoint(local_path=C:\\Users\\TeamD\\ray_results\\PPO\\PPO_PlatoonEnvV1_05ce9_00000_0_2023-01-02_23-56-45\\checkpoint_000200)\n",
    "assert checkpoint is not None, \"checkpoint doesn't exist, training failed?\"\n",
    "print(checkpoint)\n",
    "algo = Algorithm.from_checkpoint(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done: Total reward = -77.0, steps = 10\n",
      "Episode done: Total reward = -95.0, steps = 10\n",
      "Episode done: Total reward = -85.0, steps = 10\n",
      "Episode done: Total reward = -85.0, steps = 10\n",
      "Episode done: Total reward = -95.0, steps = 10\n",
      "Episode done: Total reward = -95.0, steps = 10\n",
      "Episode done: Total reward = -95.0, steps = 10\n",
      "Episode done: Total reward = -95.0, steps = 10\n",
      "Episode done: Total reward = -77.0, steps = 10\n",
      "Episode done: Total reward = -95.0, steps = 10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "num_episodes = 0\n",
    "episode_reward = 0.0\n",
    "episode_steps = 0\n",
    "truncated_already = False\n",
    "actions = []\n",
    "rewards = []\n",
    "obs, info = env.reset()\n",
    "data = []\n",
    "while num_episodes < 10:\n",
    "    # Compute an action (`a`).\n",
    "    a = algo.compute_single_action(\n",
    "        observation=obs,\n",
    "        explore=False,\n",
    "        policy_id=\"default_policy\",  # <- default value\n",
    "    )\n",
    "    # Send the computed action `a` to the env.\n",
    "    obs, reward, done, truncated, _ = env.step(a)\n",
    "    data.append([obs, a, reward, done or truncated])\n",
    "    episode_reward += reward\n",
    "    episode_steps += 1\n",
    "\n",
    "    if done or truncated:\n",
    "        print(f\"Episode done: Total reward = {episode_reward}, steps = {episode_steps}\")\n",
    "        obs, info = env.reset()\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        num_episodes += 1\n",
    "        episode_steps = 0\n",
    "        episode_reward = 0.0\n",
    "        truncated_already = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 -9.0\n",
      "10 -8.0\n",
      "4 -7.0\n",
      "10 -8.0\n",
      "10 -7.0\n",
      "10 -8.0\n",
      "10 -7.0\n",
      "10 -8.0\n",
      "10 -7.0\n",
      "10 -8.0\n"
     ]
    }
   ],
   "source": [
    "obs, info = env.reset()\n",
    "from itertools import count\n",
    "for i in count():\n",
    "    a = algo.compute_single_action(\n",
    "        observation=obs,\n",
    "        explore=False,\n",
    "        policy_id=\"default_policy\",  # <- default value\n",
    "    )\n",
    "    # Send the computed action `a` to the env.\n",
    "    obs, reward, done, truncated, _ = env.step(a)\n",
    "    print(a, reward)\n",
    "    if done or truncated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0., -10., -10.,   0.],\n",
       "       dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0., -10., -10.,   0.],\n",
       "       dtype=float32),\n",
       " -10.0,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([913., 914., 919., 892., 923., 911., 945., 877., 874., 887., 945.]),\n",
       " array([ 0.        ,  0.90909091,  1.81818182,  2.72727273,  3.63636364,\n",
       "         4.54545455,  5.45454545,  6.36363636,  7.27272727,  8.18181818,\n",
       "         9.09090909, 10.        ]),\n",
       " <BarContainer object of 11 artists>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "plt.hist([env.action_space.sample() for _ in range(10000)], bins=env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algo.stop()\n",
    "# ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "524f818cd743d7abf57cb48d668c781df3b45c3f425c53d80f3bb0fcd0df7a52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
