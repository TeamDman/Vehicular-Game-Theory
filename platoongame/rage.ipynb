{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import random\n",
    "import math\n",
    "from typing import List, Tuple, Union, Iterable, Callable, Optional\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = Tensor # vehicle, vuln, feature\n",
    "StateBatch = Tensor\n",
    "DefenderAction = Tensor # vehicles\n",
    "DefenderActionBatch = Tensor\n",
    "AttackerAction = Tensor # vehicles\n",
    "AttackerActionBatch = Tensor\n",
    "Reward = Tensor\n",
    "RewardBatch = Tensor\n",
    "Terminal = Tensor\n",
    "TerminalBatch = Tensor\n",
    "\n",
    "MAX_VEHICLES                        = 10     #@param {type:\"integer\"}\n",
    "MAX_VULNS                           = 3      #@param {type:\"integer\"}\n",
    "MAX_ATTACK                          = 2      #@param {type:\"integer\"}\n",
    "\n",
    "BATCH_SIZE                          = 500    #@param {type:\"integer\"}\n",
    "GPU_BATCH_SIZE                      = 1000    #@param {type:\"integer\"}\n",
    "if torch.cuda.is_available():\n",
    "    BATCH_SIZE = GPU_BATCH_SIZE\n",
    "\n",
    "MEMORY_SIZE                         = 100000  #@param {type:\"integer\"}\n",
    "MEMORY_WARMUP_STEPS                 = 5000     #@param {type:\"integer\"} # scaled by batch size\n",
    "\n",
    "TRAIN_STEPS                         = 250     #@param {type:\"integer\"}\n",
    "EXPLORATION_STEPS_PER_TRAIN_STEP    = 1      #@param {type:\"integer\"} # scaled by batch size\n",
    "\n",
    "LEARNING_RATE                       = 0.001  #@param {type:\"number\"}\n",
    "LEARNING_RATE_GAMMA                 = 0.9    #@param {type:\"number\"}\n",
    "LEARNING_RATE_GAMMA_FREQUENCY       = 100    #@param {type:\"integer\"}\n",
    "\n",
    "REWARD_GAMMA                        = 0.99   #@param {type:\"number\"}\n",
    "\n",
    "EPSILON_DECAY_STEPS                 = 10000  #@param {type:\"integer\"}\n",
    "EPSILON_THETA                       = 0.0    #@param {type:\"number\"}\n",
    "EPSILON_MU                          = 0.0    #@param {type:\"number\"}\n",
    "EPSILON_SIGMA                       = 3      #@param {type:\"number\"}\n",
    "\n",
    "SOFT_UPDATE_TAU                     = 0.001  #@param {type:\"number\"}\n",
    "\n",
    "EVAL_STEPS                          = 100    #@param {type:\"integer\"}\n",
    "\n",
    "DEVICE = torch.cuda.is_available() and torch.device('cuda') or torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get random starting state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_random_starting_state() -> State:\n",
    "    prob_dist = torch.distributions.Normal(\n",
    "        loc=torch.as_tensor(0.5, dtype=torch.float32),\n",
    "        scale=torch.as_tensor(0.25, dtype=torch.float32),\n",
    "    )\n",
    "    sev_dist = torch.distributions.Normal(\n",
    "        loc=torch.as_tensor(2, dtype=torch.float32),\n",
    "        scale=torch.as_tensor(1, dtype=torch.float32),\n",
    "    )\n",
    "    state = torch.zeros((MAX_VEHICLES, MAX_VULNS, 4), dtype=torch.float32)\n",
    "    for i in range(MAX_VEHICLES):\n",
    "        for j in range(random.randint(0, MAX_VULNS)):\n",
    "            state[i,j,0] = float(prob_dist.sample().clamp(0.05,1)) # prob\n",
    "            state[i,j,1] = int(sev_dist.sample().clamp(1,5)) ** 2 # sev\n",
    "            state[i,j,2] = 0 # compromised\n",
    "            state[i,j,3] = 0 # membership\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get empty state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_empty_state() -> State:\n",
    "    return torch.zeros((MAX_VEHICLES, MAX_VULNS, 4), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### batch states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_states(states: Union[Tuple[State, ...], List[State]]) -> StateBatch:\n",
    "    return torch.stack(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get random starting state batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_starting_state_batch(num_batches: int) -> StateBatch:\n",
    "    return batch_states([get_random_starting_state() for _ in range(num_batches)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get attacker actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attacker_actions(states: StateBatch) -> AttackerActionBatch:\n",
    "    priority = (states[:,:,:,0] * states[:,:,:,1] * (1-states[:,:,:,2])).sum(dim=-1)\n",
    "    # find indices of vehicles to attack\n",
    "    attack = priority.topk(MAX_ATTACK).indices\n",
    "    # return mask of vehicles to attack\n",
    "    return torch.zeros((states.shape[0], MAX_VEHICLES), dtype=torch.float32).to(states.device).scatter_(1, attack, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apply attacker actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_attacker_actions(states: StateBatch, actions: AttackerActionBatch) -> StateBatch:\n",
    "    assert states.shape[0] == actions.shape[0]\n",
    "    assert states.device == actions.device\n",
    "\n",
    "    # create a copy so we don't modify the original\n",
    "    states = states.clone()\n",
    "\n",
    "    # roll probability for each vulnerability\n",
    "    probs = torch.rand((states.shape[0], MAX_VEHICLES, MAX_VULNS), dtype=torch.float32, device=states.device)\n",
    "\n",
    "    # only keep vulns for vehicles that are being attacked\n",
    "    for i in range(states.shape[0]):\n",
    "        probs[i, actions[i]!=1, :] = 0 \n",
    "\n",
    "    # set the vulnerability compromised flag to 1 for each successful attack\n",
    "    states[:,:,:,2] += (probs > 1-states[:,:,:,0]).float()\n",
    "    states[:,:,:,2] = states[:,:,:,2].clamp(0, 1)\n",
    "    return states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get exploring defender actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exploring_defender_actions(states: StateBatch) -> DefenderActionBatch:\n",
    "    assert states.shape[0] >= MAX_VEHICLES * 2 # we want to give diagonal and random actions a chance\n",
    "\n",
    "    diag = torch.eye(MAX_VEHICLES, dtype=torch.float32, device=states.device)\n",
    "    rand = (torch.rand((states.shape[0] - MAX_VEHICLES, MAX_VEHICLES), dtype=torch.float32, device=states.device) > 0.5).float()\n",
    "    return torch.cat((diag, rand))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apply defender actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_defender_actions(states: StateBatch, actions: DefenderActionBatch) -> StateBatch:\n",
    "    assert states.shape[0] == actions.shape[0]\n",
    "    assert states.device == actions.device\n",
    "\n",
    "    # create a copy so we don't modify the original\n",
    "    states = states.clone()\n",
    "\n",
    "    # set the membership flag to 1 for each vuln in each vehicle that is chosen\n",
    "    states[:,:,:,3] = 0\n",
    "    for i in range(states.shape[0]):\n",
    "        states[i,actions[i,:]==1,:,3] = 1\n",
    "    return states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get defender utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_defender_utilities(states: StateBatch) -> RewardBatch:\n",
    "    # identify which platoons contain compromised vehicles\n",
    "    compromise_free_platoons = (states[:,:,:,2] * states[:,:,:,3]).sum(dim=[-1,-2]) == 0\n",
    "    # identify size of each platoon\n",
    "    members = states[:,:,:,3].max(dim=-1).values.sum(dim=-1)\n",
    "    # return 0 if platoon is compromised, size of platoon otherwise\n",
    "    return members * compromise_free_platoons.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefenderActor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.LazyConv2d(out_channels=5, kernel_size=2, stride=1)\n",
    "        self.fc1 = nn.LazyLinear(256)\n",
    "        self.fc2 = nn.LazyLinear(128)\n",
    "        self.fc3 = nn.LazyLinear(MAX_VEHICLES)\n",
    "\n",
    "    def forward(self, x: StateBatch) -> DefenderActionBatch:\n",
    "        x = torch.hstack((\n",
    "            F.gelu(self.conv1(x)).flatten(start_dim=1),\n",
    "            x.flatten(start_dim=1), # skip connection after conv\n",
    "        ))\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = F.gelu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefenderCritic(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.LazyConv2d(out_channels=5, kernel_size=2, stride=1)\n",
    "        self.fc1 = nn.LazyLinear(256)\n",
    "        self.fc2 = nn.LazyLinear(128)\n",
    "        self.fc3 = nn.LazyLinear(1)\n",
    "    def forward(self, x1: StateBatch, x2: DefenderActionBatch) -> Reward:\n",
    "        x = torch.hstack((\n",
    "            F.gelu(self.conv1(x1)).flatten(start_dim=1),\n",
    "            x1.flatten(start_dim=1), # skip connection after conv\n",
    "            x2,\n",
    "        ))\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = F.gelu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Transition:\n",
    "    state: State\n",
    "    action: DefenderAction\n",
    "    reward: Reward\n",
    "    next_state: State\n",
    "    terminal: Terminal\n",
    "\n",
    "    def to(self, device: torch.device) -> 'Transition':\n",
    "        return Transition(\n",
    "            state=self.state.to(device),\n",
    "            action=self.action.to(device),\n",
    "            reward=self.reward.to(device),\n",
    "            next_state=self.next_state.to(device),\n",
    "            terminal=self.terminal.to(device),\n",
    "        )\n",
    "\n",
    "@dataclass\n",
    "class TransitionBatch:\n",
    "    states: StateBatch\n",
    "    actions: DefenderActionBatch\n",
    "    rewards: RewardBatch\n",
    "    next_states: StateBatch\n",
    "    terminals: TerminalBatch\n",
    "\n",
    "    \n",
    "    def to(self, device: torch.device) -> 'TransitionBatch':\n",
    "        return TransitionBatch(\n",
    "            states=self.states.to(device),\n",
    "            actions=self.actions.to(device),\n",
    "            rewards=self.rewards.to(device),\n",
    "            next_states=self.next_states.to(device),\n",
    "            terminals=self.terminals.to(device),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_memory(memory: deque, batch_size: int) -> TransitionBatch:\n",
    "    samples = random.sample(memory, batch_size)\n",
    "    return TransitionBatch(\n",
    "        states=torch.stack([s.state for s in samples]),\n",
    "        actions=torch.stack([s.action for s in samples]),\n",
    "        rewards=torch.stack([s.reward for s in samples]),\n",
    "        next_states=torch.stack([s.next_state for s in samples]),\n",
    "        terminals=torch.stack([s.terminal for s in samples]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/ghliu/pytorch-ddpg/blob/master/util.py#L26\n",
    "\n",
    "def soft_update(target, source, tau):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            ## shouldn't be necessary since we use target networks to calculate loss\n",
    "            # if isinstance(target_param, torch.nn.parameter.UninitializedParameter):\n",
    "            #     # target model uninitialize, hard update\n",
    "            #     target_param.data.copy_(param.data)\n",
    "            # else:\n",
    "            target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "\n",
    "def hard_update(target, source):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "# from original deepRL author code\n",
    "class RandomProcess(object):\n",
    "    def reset_states(self):\n",
    "        pass\n",
    "class AnnealedGaussianProcess(RandomProcess):\n",
    "    def __init__(self, mu, sigma, sigma_min, n_steps_annealing):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.n_steps = 0\n",
    "\n",
    "        if sigma_min is not None:\n",
    "            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)\n",
    "            self.c = sigma\n",
    "            self.sigma_min = sigma_min\n",
    "        else:\n",
    "            self.m = 0.\n",
    "            self.c = sigma\n",
    "            self.sigma_min = sigma\n",
    "\n",
    "    @property\n",
    "    def current_sigma(self):\n",
    "        sigma = max(self.sigma_min, self.m * float(self.n_steps) + self.c)\n",
    "        return sigma\n",
    "\n",
    "# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
    "class OrnsteinUhlenbeckProcess(AnnealedGaussianProcess):\n",
    "    def __init__(self, theta, mu=0., sigma=1., dt=1e-2, x0=None, size=1, sigma_min=None, n_steps_annealing=1000):\n",
    "        super(OrnsteinUhlenbeckProcess, self).__init__(mu=mu, sigma=sigma, sigma_min=sigma_min, n_steps_annealing=n_steps_annealing)\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.size = size\n",
    "        self.normal = torch.distributions.Normal(torch.as_tensor(0, dtype=torch.float32), torch.as_tensor(1, dtype=torch.float32))\n",
    "        self.reset_states()\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * torch.sqrt(torch.as_tensor(self.dt, dtype=torch.float32)) * self.normal.sample((self.size,))  # type: ignore\n",
    "        self.x_prev = x\n",
    "        self.n_steps += 1\n",
    "        return x\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else torch.zeros(self.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/ghliu/pytorch-ddpg/blob/master/util.py#L26\n",
    "\n",
    "def soft_update(target, source, tau):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            ## shouldn't be necessary since we use target networks to calculate loss\n",
    "            # if isinstance(target_param, torch.nn.parameter.UninitializedParameter):\n",
    "            #     # target model uninitialize, hard update\n",
    "            #     target_param.data.copy_(param.data)\n",
    "            # else:\n",
    "            target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "\n",
    "def hard_update(target, source):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from original deepRL author code\n",
    "class RandomProcess(object):\n",
    "    def reset_states(self):\n",
    "        pass\n",
    "class AnnealedGaussianProcess(RandomProcess):\n",
    "    def __init__(self, mu, sigma, sigma_min, n_steps_annealing):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.n_steps = 0\n",
    "\n",
    "        if sigma_min is not None:\n",
    "            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)\n",
    "            self.c = sigma\n",
    "            self.sigma_min = sigma_min\n",
    "        else:\n",
    "            self.m = 0.\n",
    "            self.c = sigma\n",
    "            self.sigma_min = sigma\n",
    "\n",
    "    @property\n",
    "    def current_sigma(self):\n",
    "        sigma = max(self.sigma_min, self.m * float(self.n_steps) + self.c)\n",
    "        return sigma\n",
    "\n",
    "# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
    "class OrnsteinUhlenbeckProcess(AnnealedGaussianProcess):\n",
    "    def __init__(self, theta, mu=0., sigma=1., dt=1e-2, x0=None, size=1, sigma_min=None, n_steps_annealing=1000):\n",
    "        super(OrnsteinUhlenbeckProcess, self).__init__(mu=mu, sigma=sigma, sigma_min=sigma_min, n_steps_annealing=n_steps_annealing)\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.size = size\n",
    "        self.normal = torch.distributions.Normal(torch.as_tensor(0, dtype=torch.float32), torch.as_tensor(1, dtype=torch.float32))\n",
    "        self.reset_states()\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * torch.sqrt(torch.as_tensor(self.dt, dtype=torch.float32)) * self.normal.sample((self.size,))  # type: ignore\n",
    "        self.x_prev = x\n",
    "        self.n_steps += 1\n",
    "        return x\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else torch.zeros(self.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory: deque[Transition] = deque(maxlen=MEMORY_SIZE) # colab uses old python version that doesn't support this :P\n",
    "memory = deque(maxlen=MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TeamD\\.conda\\envs\\subgame\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "actor = DefenderActor().to(DEVICE)\n",
    "actor_target = DefenderActor().to(DEVICE)\n",
    "critic = DefenderCritic().to(DEVICE)\n",
    "critic_target = DefenderCritic().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure weights are initialized\n",
    "states = get_random_starting_state_batch(1).to(DEVICE)\n",
    "critic(states, actor(states))\n",
    "critic_target(states, actor_target(states))\n",
    "\n",
    "# hard update weights\n",
    "hard_update(actor_target, actor)\n",
    "hard_update(critic_target, critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optimizer = torch.optim.Adam(actor.parameters(), lr=LEARNING_RATE)\n",
    "actor_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    actor_optimizer,\n",
    "    step_size=LEARNING_RATE_GAMMA_FREQUENCY,\n",
    "    gamma=LEARNING_RATE_GAMMA,\n",
    ")\n",
    "critic_optimizer = torch.optim.Adam(critic.parameters(), lr=LEARNING_RATE)\n",
    "critic_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    critic_optimizer,\n",
    "    step_size=LEARNING_RATE_GAMMA_FREQUENCY,\n",
    "    gamma=LEARNING_RATE_GAMMA,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0\n",
    "epsilon_decay = epsilon / EPSILON_DECAY_STEPS\n",
    "epsilon_noise = OrnsteinUhlenbeckProcess(\n",
    "    size=MAX_VEHICLES,\n",
    "    theta=EPSILON_SIGMA,\n",
    "    mu=EPSILON_MU,\n",
    "    sigma=EPSILON_SIGMA,\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "digits = math.ceil(math.log10(TRAIN_STEPS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6bad2c439ca4e8895db22a794870526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16764/594314443.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;31m# reset environment for terminal states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mterminals\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_random_starting_state_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterminals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16764/2075312446.py\u001b[0m in \u001b[0;36mget_random_starting_state_batch\u001b[1;34m(num_batches)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_random_starting_state_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mStateBatch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mbatch_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mget_random_starting_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16764/2075312446.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_random_starting_state_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mStateBatch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mbatch_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mget_random_starting_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16764/2783752317.py\u001b[0m in \u001b[0;36mget_random_starting_state\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMAX_VEHICLES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_VULNS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob_dist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# prob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msev_dist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;31m# sev\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;31m# compromised\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\subgame\\lib\\site-packages\\torch\\distributions\\normal.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, sample_shape)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extended_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrsample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "states = get_random_starting_state_batch(BATCH_SIZE)\n",
    "streaks = torch.zeros((BATCH_SIZE,))\n",
    "\n",
    "# perform random exploration for one set of state sequences\n",
    "for _ in tqdm(range(MEMORY_WARMUP_STEPS // 2)):\n",
    "    assert len(memory) < MEMORY_SIZE\n",
    "    # random exploration\n",
    "    defender_actions = get_exploring_defender_actions(states)\n",
    "    next_states = apply_defender_actions(states, defender_actions)\n",
    "    attacker_actions = get_attacker_actions(next_states)\n",
    "    next_states = apply_attacker_actions(next_states, attacker_actions)\n",
    "    rewards = get_defender_utilities(next_states)\n",
    "    terminals = rewards == 0\n",
    "\n",
    "    # print(f\"terminal states found with streaks {streaks[terminals]}\")\n",
    "    # streaks[terminals] = 0\n",
    "    # streaks[~terminals] += 1\n",
    "\n",
    "\n",
    "    # track next states as empty if terminal\n",
    "    next_states[terminals] = get_empty_state()\n",
    "\n",
    "    # save each transition in the batch to memory\n",
    "    for i in range(BATCH_SIZE):\n",
    "        memory.append(Transition(\n",
    "            state=states[i],\n",
    "            action=defender_actions[i],\n",
    "            reward=rewards[i],\n",
    "            next_state=next_states[i],\n",
    "            terminal=torch.as_tensor(rewards[i] == 0),\n",
    "        ))\n",
    "\n",
    "    # reset environment for terminal states\n",
    "    states = next_states.clone()\n",
    "    states[terminals] = get_random_starting_state_batch(int(terminals.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternate paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using random states from the memory, see what would happen if we took different actions at those states\n",
    "for _ in tqdm(range(MEMORY_WARMUP_STEPS // 2)):\n",
    "    assert len(memory) < MEMORY_SIZE\n",
    "\n",
    "    states = sample_memory(memory, BATCH_SIZE).states\n",
    "\n",
    "    # random exploration\n",
    "    defender_actions = get_exploring_defender_actions(states)\n",
    "    next_states = apply_defender_actions(states, defender_actions)\n",
    "    attacker_actions = get_attacker_actions(next_states)\n",
    "    next_states = apply_attacker_actions(next_states, attacker_actions)\n",
    "    rewards = get_defender_utilities(next_states)\n",
    "    terminals = rewards == 0\n",
    "\n",
    "    # track next states as empty if terminal\n",
    "    next_states[terminals] = get_empty_state()\n",
    "\n",
    "    # save each transition in the batch to memory\n",
    "    for i in range(BATCH_SIZE):\n",
    "        memory.append(Transition(\n",
    "            state=states[i],\n",
    "            action=defender_actions[i],\n",
    "            reward=rewards[i],\n",
    "            next_state=next_states[i],\n",
    "            terminal=torch.as_tensor(rewards[i] == 0),\n",
    "        ))\n",
    "\n",
    "    # reset environment for terminal states\n",
    "    states = next_states.clone()\n",
    "    states[terminals] = get_random_starting_state_batch(int(terminals.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preflight checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(memory)/MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sample_memory(memory, 1000).rewards.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sample_memory(memory, 1000).rewards.count_nonzero()\n",
    "print(a, a/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sample_memory(memory, 1000).rewards.cumsum(0).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_loss_history = []\n",
    "actor_loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = get_random_starting_state_batch(BATCH_SIZE).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7626096e064848648de51d9285d9dd9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "with tqdm(total=TRAIN_STEPS) as pbar:\n",
    "    for i in range(TRAIN_STEPS):\n",
    "        # epsilon-noise exploration\n",
    "        for j in range(EXPLORATION_STEPS_PER_TRAIN_STEP):\n",
    "            proto_defender_actions = actor(states)\n",
    "            for i in range(BATCH_SIZE):\n",
    "                proto_defender_actions[i] += epsilon * epsilon_noise.sample().to(proto_defender_actions.device)\n",
    "            epsilon = max(0.0, epsilon - epsilon_decay)\n",
    "            defender_actions: DefenderActionBatch = (proto_defender_actions > 0.5).float() # convert to binary\n",
    "            next_states = apply_defender_actions(states, defender_actions)\n",
    "            attacker_actions = get_attacker_actions(next_states)\n",
    "            next_states = apply_attacker_actions(next_states, attacker_actions)\n",
    "            rewards = get_defender_utilities(next_states)\n",
    "            terminals = rewards == 0\n",
    "\n",
    "            # track next states as empty if terminal\n",
    "            next_states[terminals] = get_empty_state().to(DEVICE)\n",
    "            \n",
    "            # save each transition in the batch to memory\n",
    "            for i in range(BATCH_SIZE):\n",
    "                memory.append(Transition(\n",
    "                    state=states[i],\n",
    "                    action=defender_actions[i],\n",
    "                    reward=rewards[i],\n",
    "                    next_state=next_states[i],\n",
    "                    terminal=torch.as_tensor(rewards[i] == 0, dtype=torch.bool),\n",
    "                ).to(torch.device(\"cpu\")))\n",
    "\n",
    "            # reset environment for terminal states\n",
    "            states = next_states.clone()\n",
    "            num_terminal = int(terminals.sum() > 0)\n",
    "            if num_terminal > 0:\n",
    "                states[terminals] = get_random_starting_state_batch(num_terminal).to(DEVICE)\n",
    "\n",
    "        # train\n",
    "        batch = sample_memory(memory, BATCH_SIZE).to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_next: Tensor = critic_target(\n",
    "                batch.next_states,\n",
    "                actor_target(batch.next_states),\n",
    "            )\n",
    "        q_next.requires_grad_()\n",
    "        q_target = q_next * REWARD_GAMMA * (1-batch.terminals.float().unsqueeze(1)) + batch.rewards.unsqueeze(1)\n",
    "\n",
    "        critic.zero_grad()\n",
    "        q_pred = critic(batch.states, batch.actions)\n",
    "        critic_loss = criterion(q_pred, q_target)\n",
    "        critic_loss.backward()\n",
    "        critic_optimizer.step()\n",
    "        critic_scheduler.step()\n",
    "        \n",
    "        critic_loss_history.append(critic_loss.item())\n",
    "\n",
    "\n",
    "        actor.zero_grad()\n",
    "        actor_loss = -critic(batch.states, actor(batch.states)).mean()\n",
    "        actor_loss.backward()\n",
    "        actor_optimizer.step()\n",
    "        actor_scheduler.step()\n",
    "        actor_loss_history.append(actor_loss.item())\n",
    "\n",
    "        soft_update(actor_target, actor, SOFT_UPDATE_TAU)\n",
    "        soft_update(critic_target, critic, SOFT_UPDATE_TAU)\n",
    "\n",
    "        pbar.set_description(f\"criterion: {critic_loss.item():.3f}, actor: {actor_loss.item():.3f}\")\n",
    "\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1d4086b4220>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(critic_loss_history, label=\"critic_loss\")\n",
    "plt.plot(actor_loss_history, label=\"actor_loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6098e77cb2f4669934619d7d872166f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 2., 0., 1., 0., 0., 0., 1., 0.])\n",
      "terminal states found with streaks tensor([2., 0., 2., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0.,\n",
      "        0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 1., 5., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 1., 0., 0., 0., 0., 0., 1., 2., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 2., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 2.])\n",
      "terminal states found with streaks tensor([0., 0., 2., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "terminal states found with streaks tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        1.])\n",
      "terminal states found with streaks tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0.])\n",
      "terminal states found with streaks tensor([0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 2.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 2., 0., 0., 1., 0., 1.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 1., 0., 0., 0., 1., 0., 2., 1., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 6., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "terminal states found with streaks tensor([0., 0., 1., 0., 2., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 1., 0., 0.,\n",
      "        0.])\n",
      "terminal states found with streaks tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 2., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        1.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([1., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 2., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 1., 2., 0., 0., 2., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 2., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([2., 0., 0., 0., 0., 2., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 2., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([2., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 2., 1., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
      "        0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 3., 0., 1., 0., 0., 0., 0., 0., 1., 0.])\n",
      "terminal states found with streaks tensor([0., 4., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 2., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 5., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 1.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 1., 1., 2., 0., 0., 0., 0., 2., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 2., 0., 3., 1., 0., 0., 0., 0.,\n",
      "        0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 2., 1., 0.,\n",
      "        0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "terminal states found with streaks tensor([0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 2., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 2., 1., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 2., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 4., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([1., 0., 0., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 1., 1., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([1., 0., 0., 0., 1., 0., 0., 3., 0., 0., 0., 0., 0., 0., 1.])\n",
      "terminal states found with streaks tensor([1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 4., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([2., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 2., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([2., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 2., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.])\n",
      "terminal states found with streaks tensor([0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "states = get_random_starting_state_batch(BATCH_SIZE).to(DEVICE)\n",
    "streaks = torch.zeros((BATCH_SIZE,))\n",
    "\n",
    "for _ in tqdm(range(EVAL_STEPS)):\n",
    "    proto_defender_actions = actor(states)\n",
    "    defender_actions: DefenderActionBatch = (proto_defender_actions > 0.5).float() # convert to binary\n",
    "    next_states = apply_defender_actions(states, defender_actions)\n",
    "    attacker_actions = get_attacker_actions(next_states)\n",
    "    next_states = apply_attacker_actions(next_states, attacker_actions)\n",
    "    rewards = get_defender_utilities(next_states)\n",
    "    terminals = rewards == 0\n",
    "\n",
    "    print(f\"terminal states found with streaks {streaks[terminals]}\")\n",
    "    streaks[terminals] = 0\n",
    "    streaks[~terminals] += 1\n",
    "\n",
    "    # track next states as empty if terminal\n",
    "    next_states[terminals] = get_empty_state().to(DEVICE)\n",
    "\n",
    "    eval_rewards.append(rewards)\n",
    "\n",
    "    # reset environment for terminal states\n",
    "    states = next_states.clone()\n",
    "    states[terminals] = get_random_starting_state_batch(int(terminals.sum())).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f52c88b3c01c491da4a95d10caf323f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='i', max=19), Output()), _dom_classes=('widget-interact',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "@widgets.interact(i=(0, BATCH_SIZE-1))\n",
    "def asd(i=0):\n",
    "    plt.plot(torch.vstack(eval_rewards)[:,i].cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('subgame')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09b31e4957d63d79e1e76c5537c345194b9f565583fac53cbbb105281d72baf4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
