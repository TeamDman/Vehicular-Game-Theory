{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import random\n",
    "import math\n",
    "from typing import List, Tuple, Union, Iterable\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = Tensor # vehicle, vuln, feature\n",
    "StateBatch = Tensor\n",
    "DefenderAction = Tensor # vehicles\n",
    "DefenderActionBatch = Tensor\n",
    "AttackerAction = Tensor # vehicles\n",
    "AttackerActionBatch = Tensor\n",
    "Reward = Tensor\n",
    "RewardBatch = Tensor\n",
    "Terminal = Tensor\n",
    "TerminalBatch = Tensor\n",
    "\n",
    "MAX_VEHICLES = 10\n",
    "MAX_VULNS = 3\n",
    "MAX_ATTACK= 2\n",
    "MEMORY_SIZE = 10000\n",
    "MEMORY_WARMUP = 2000\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.001\n",
    "DEVICE = torch.cuda.is_available() and torch.device('cuda') or torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_starting_state() -> State:\n",
    "    prob_dist = torch.distributions.Normal(\n",
    "        loc=torch.as_tensor(0.5, dtype=torch.float32),\n",
    "        scale=torch.as_tensor(0.25, dtype=torch.float32),\n",
    "    )\n",
    "    sev_dist = torch.distributions.Normal(\n",
    "        loc=torch.as_tensor(2, dtype=torch.float32),\n",
    "        scale=torch.as_tensor(1, dtype=torch.float32),\n",
    "    )\n",
    "    state = torch.zeros((MAX_VEHICLES, MAX_VULNS, 4), dtype=torch.float32)\n",
    "    for i in range(MAX_VEHICLES):\n",
    "        for j in range(random.randint(0, MAX_VULNS)):\n",
    "            state[i,j,0] = float(prob_dist.sample().clamp(0.05,1)) # prob\n",
    "            state[i,j,1] = int(sev_dist.sample().clamp(1,5)) ** 2 # sev\n",
    "            state[i,j,2] = 0 # compromised\n",
    "            state[i,j,3] = 0 # membership\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_empty_state() -> State:\n",
    "    return torch.zeros((MAX_VEHICLES, MAX_VULNS, 4), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_states(states: Union[Tuple[State, ...], List[State]]) -> StateBatch:\n",
    "    return torch.stack(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_state_batch(num_batches: int) -> StateBatch:\n",
    "    return batch_states([get_random_starting_state() for _ in range(num_batches)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attacker_actions(states: StateBatch) -> AttackerActionBatch:\n",
    "    priority = (states[:,:,:,0] * states[:,:,:,1] * (1-states[:,:,:,2])).sum(dim=-1)\n",
    "    # find indices of vehicles to attack\n",
    "    attack = priority.topk(MAX_ATTACK).indices\n",
    "    # return mask of vehicles to attack\n",
    "    return torch.zeros((states.shape[0], MAX_VEHICLES), dtype=torch.float32).scatter_(1, attack, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_attacker_actions(states: StateBatch, actions: AttackerActionBatch) -> StateBatch:\n",
    "    # batch size must be the same\n",
    "    assert states.shape[0] == actions.shape[0]\n",
    "\n",
    "    # create a copy so we don't modify the original\n",
    "    states = states.clone()\n",
    "\n",
    "    # roll probability for each vulnerability\n",
    "    probs = torch.rand((states.shape[0], MAX_VEHICLES, MAX_VULNS), dtype=torch.float32)\n",
    "\n",
    "    # only keep vulns for vehicles that are being attacked\n",
    "    for i in range(states.shape[0]):\n",
    "        probs[i, actions[i]!=1, :] = 0 \n",
    "\n",
    "    # set the vulnerability compromised flag to 1 for each successful attack\n",
    "    states[:,:,:,2] += (probs > 1-states[:,:,:,0]).float()\n",
    "    states[:,:,:,2] = states[:,:,:,2].clamp(0, 1)\n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_defender_actions(states: StateBatch) -> DefenderActionBatch:\n",
    "    return (torch.rand((states.shape[0], MAX_VEHICLES), dtype=torch.float32) > 0.5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_defender_actions(states: StateBatch, actions: DefenderActionBatch) -> StateBatch:\n",
    "    # batch size must be the same\n",
    "    assert states.shape[0] == actions.shape[0]\n",
    "\n",
    "    # create a copy so we don't modify the original\n",
    "    states = states.clone()\n",
    "\n",
    "    # set the membership flag to 1 for each vuln in each vehicle that is chosen\n",
    "    states[:,:,:,3] = 0\n",
    "    for i in range(states.shape[0]):\n",
    "        states[i,actions[i,:]==1,:,3] = 1\n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_defender_utilities(states: StateBatch) -> RewardBatch:\n",
    "    # identify which platoons contain compromised vehicles\n",
    "    compromise_free_platoons = (states[:,:,:,2] * states[:,:,:,3]).sum(dim=[-1,-2]) == 0\n",
    "    # identify size of each platoon\n",
    "    members = states[:,:,:,3].max(dim=-1).values.sum(dim=-1)\n",
    "    # return 0 if platoon is compromised, size of platoon otherwise\n",
    "    return members * compromise_free_platoons.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefenderActor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.LazyConv2d(out_channels=5, kernel_size=2, stride=1)\n",
    "        self.fc1 = nn.LazyLinear(256)\n",
    "        self.fc2 = nn.LazyLinear(128)\n",
    "        self.fc3 = nn.LazyLinear(MAX_VEHICLES)\n",
    "\n",
    "    def forward(self, x: StateBatch) -> DefenderActionBatch:\n",
    "        x = torch.cat((\n",
    "            F.gelu(self.conv1(x)).flatten(start_dim=1),\n",
    "            x.flatten(start_dim=1), # skip connection after conv\n",
    "        ))\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = F.gelu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefenderCritic(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.LazyConv2d(out_channels=5, kernel_size=2, stride=1)\n",
    "        self.fc1 = nn.LazyLinear(256)\n",
    "        self.fc2 = nn.LazyLinear(128)\n",
    "        self.fc3 = nn.LazyLinear(1)\n",
    "    def forward(self, x1: StateBatch, x2: DefenderActionBatch) -> Reward:\n",
    "        x = torch.hstack((\n",
    "            F.gelu(self.conv1(x1)).flatten(start_dim=1),\n",
    "            x1.flatten(start_dim=1), # skip connection after conv\n",
    "            x2,\n",
    "        ))\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = F.gelu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor = DefenderActor()\n",
    "actor_target = DefenderActor()\n",
    "actor_target.load_state_dict(actor.state_dict())\n",
    "critic = DefenderCritic()\n",
    "critic_target = DefenderCritic()\n",
    "critic_target.load_state_dict(critic.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optimizer = torch.optim.Adam(actor.parameters(), lr=LEARNING_RATE)\n",
    "actor_scheduler = torch.optim.lr_scheduler.StepLR(actor_optimizer, step_size=100, gamma=0.9)\n",
    "critic_optimizer = torch.optim.Adam(critic.parameters(), lr=LEARNING_RATE)\n",
    "critic_scheduler = torch.optim.lr_scheduler.StepLR(critic_optimizer, step_size=100, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Transition:\n",
    "    state: State\n",
    "    action: DefenderAction\n",
    "    reward: Reward\n",
    "    next_state: State\n",
    "    terminal: Terminal\n",
    "\n",
    "@dataclass\n",
    "class TransitionBatch:\n",
    "    states: StateBatch\n",
    "    actions: DefenderActionBatch\n",
    "    rewards: RewardBatch\n",
    "    next_states: StateBatch\n",
    "    terminals: TerminalBatch\n",
    "\n",
    "memory: deque[Transition] = deque(maxlen=MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_memory(batch_size: int) -> TransitionBatch:\n",
    "    samples = random.sample(memory, batch_size)\n",
    "    return TransitionBatch(\n",
    "        states=torch.stack([s.state for s in samples]),\n",
    "        actions=torch.stack([s.action for s in samples]),\n",
    "        rewards=torch.stack([s.reward for s in samples]),\n",
    "        next_states=torch.stack([s.next_state for s in samples]),\n",
    "        terminals=torch.stack([s.terminal for s in samples]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warmup memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d19aa4b097824f979bf7232ae091f56b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "states = get_random_state_batch(BATCH_SIZE)\n",
    "\n",
    "for _ in tqdm(range(MEMORY_WARMUP // BATCH_SIZE)):\n",
    "    defender_actions = get_random_defender_actions(states)\n",
    "    next_states = apply_defender_actions(states, defender_actions)\n",
    "    attacker_actions = get_attacker_actions(next_states)\n",
    "    next_states = apply_attacker_actions(next_states, attacker_actions)\n",
    "    rewards = get_defender_utilities(next_states)\n",
    "    terminals = rewards == 0\n",
    "\n",
    "    # track next states as empty if terminal\n",
    "    next_states[terminals] = get_empty_state()\n",
    "    \n",
    "    for i in range(BATCH_SIZE):\n",
    "        memory.append(Transition(\n",
    "            state=states[i],\n",
    "            action=defender_actions[i],\n",
    "            reward=rewards[i],\n",
    "            next_state=next_states[i],\n",
    "            terminal=torch.as_tensor(rewards[i] == 0),\n",
    "        ))\n",
    "\n",
    "    # reset environment for terminal states\n",
    "    states = next_states.clone()\n",
    "    states[terminals] = get_random_state_batch(int(terminals.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1178.)"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_memory(1000).rewards.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('subgame')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09b31e4957d63d79e1e76c5537c345194b9f565583fac53cbbb105281d72baf4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
