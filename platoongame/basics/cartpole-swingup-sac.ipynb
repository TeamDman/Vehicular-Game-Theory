{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/sac_continuous_action.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.wrappers.monitor\n",
    "import gym_cartpole_swingup\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import count\n",
    "from IPython.display import Video, Image\n",
    "from collections import deque\n",
    "from typing import Tuple, List\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "LOG_STD_MAX = 2\n",
    "LOG_STD_MIN = -5\n",
    "Q_LEARNING_RATE = 0.001\n",
    "MEMORY_SIZE = 100000\n",
    "MEMORY_WARMUP = 500\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "POLICY_UPDATE_FREQUENCY = 2\n",
    "TARGET_UPDATE_FREQUENCY = 1\n",
    "SOFT_UPDATE_TAU = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPoleSwingUp-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(env.action_space, gym.spaces.Box), \"only continuous action space is supported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"cartpole-swingup-random.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview env\n",
    "fname = \"cartpole-swingup-random.mp4\"\n",
    "if not os.path.exists(fname):\n",
    "    recorder = gym.wrappers.monitor.video_recorder.VideoRecorder(env, fname, enabled=True)\n",
    "    env.reset()\n",
    "    recorder.capture_frame()\n",
    "    for i in tqdm(count()):\n",
    "        action = env.action_space.sample()\n",
    "        _, _, done, _ = env.step(action)\n",
    "        recorder.capture_frame()\n",
    "        if done:\n",
    "            break\n",
    "    recorder.close()\n",
    "Video(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.shape)\n",
    "print(env.action_space.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Transition:\n",
    "    observation: torch.Tensor\n",
    "    action: torch.Tensor\n",
    "    reward: float\n",
    "    next_observation: torch.Tensor\n",
    "    terminal: bool\n",
    "\n",
    "@dataclass\n",
    "class TransitionBatch:\n",
    "    observations: torch.Tensor\n",
    "    actions: torch.Tensor\n",
    "    rewards: torch.Tensor\n",
    "    next_observations: torch.Tensor\n",
    "    terminals: torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = deque(maxlen=MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_memory(batch_size: int) -> TransitionBatch:\n",
    "    global memory\n",
    "    samples: List[Transition] = random.sample(memory, batch_size)\n",
    "    return TransitionBatch(\n",
    "        observations=torch.stack([s.observation for s in samples]),\n",
    "        actions=torch.as_tensor([s.action for s in samples], dtype=torch.float32),\n",
    "        rewards=torch.as_tensor([s.reward for s in samples], dtype=torch.float32),\n",
    "        next_observations=torch.stack([s.next_observation for s in samples]),\n",
    "        terminals=torch.as_tensor([s.terminal for s in samples], dtype=torch.bool),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftQNetwork(nn.Module):\n",
    "    def __init__(self, env: gym.Env) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(math.prod(env.observation_space.shape) + math.prod(env.action_space.shape), 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "    \n",
    "    def forward(self, x, a):\n",
    "        x = torch.cat((x, a), dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, env: gym.Env) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(math.prod(env.observation_space.shape), 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc_mean = nn.Linear(256, math.prod(env.action_space.shape))\n",
    "        self.fc_logstd = nn.Linear(256, math.prod(env.action_space.shape))\n",
    "        # action rescaling\n",
    "        self.register_buffer(\"action_scale\", torch.tensor((env.action_space.high - env.action_space.low) / 2.))\n",
    "        self.register_buffer(\"action_bias\", torch.tensor((env.action_space.high + env.action_space.low) / 2.))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mean = self.fc_mean(x)\n",
    "        logstd = self.fc_logstd(x)\n",
    "        logstd = torch.tanh(logstd)\n",
    "        logstd = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (logstd + 1)\n",
    "        return mean, logstd\n",
    "\n",
    "    def get_action(self, x: torch.Tensor):\n",
    "        mean, logstd = self(x)\n",
    "        std = logstd.exp()\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        x_t = normal.rsample()\n",
    "        y_t = torch.tanh(x_t)\n",
    "        action = y_t * self.action_scale + self.action_bias\n",
    "        log_prob = normal.log_prob(x_t)\n",
    "        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)\n",
    "        log_prob = log_prob.sum(1, keepdim=True)\n",
    "        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n",
    "        return action, log_prob, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight update functions\n",
    "# from https://github.com/ghliu/pytorch-ddpg/blob/master/util.py#L26\n",
    "def soft_update(target, source, tau):\n",
    "    # using .parameters() doesn't include all params for batchnorm causing issues\n",
    "    for (k1, v1), (k2, v2) in zip(target.state_dict().items(), source.state_dict().items()):\n",
    "        assert k1 == k2\n",
    "        v1.copy_(v1 * (1.0 - tau) + v2 * tau)\n",
    "        # target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "\n",
    "def hard_update(target, source):\n",
    "    target.load_state_dict(source.state_dict())\n",
    "    # for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "    #         target_param.data.copy_(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(env)\n",
    "qf1 = SoftQNetwork(env)\n",
    "qf2 = SoftQNetwork(env)\n",
    "qf1_target = SoftQNetwork(env)\n",
    "qf2_target = SoftQNetwork(env)\n",
    "qf1_target.load_state_dict(qf1.state_dict())\n",
    "qf2_target.load_state_dict(qf2.state_dict())\n",
    "q_optimizer = torch.optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=Q_LEARNING_RATE)\n",
    "actor_optimizer = torch.optim.Adam(actor.parameters(), lr=Q_LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup autotune\n",
    "target_entropy = -torch.prod(torch.Tensor(env.action_space.shape)).item()\n",
    "log_alpha = torch.zeros(1, requires_grad=True)\n",
    "alpha = log_alpha.exp().item()\n",
    "alpha_optimizer = torch.optim.Adam([log_alpha], lr=Q_LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c8127a3ed34070a45a4f45ec58505f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10464/3755331334.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                     \u001b[0mactor_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                     \u001b[0mactor_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m                     \u001b[0mactor_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\subgame\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             )\n\u001b[1;32m--> 487\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\subgame\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "obs = torch.as_tensor(env.reset(), dtype=torch.float32)\n",
    "actor_loss=0\n",
    "with tqdm() as pbar:\n",
    "    for step in count():\n",
    "        if len(memory) < MEMORY_WARMUP:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action, _, _ = actor.get_action(obs.unsqueeze(0))\n",
    "            action = action.detach().cpu().numpy()\n",
    "\n",
    "        env.render()\n",
    "        \n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        # cartpoleswingupv1 giving weird next obs shape, manually fix for now\n",
    "        next_obs = torch.as_tensor(next_obs, dtype=torch.float32).reshape(env.observation_space.shape)\n",
    "\n",
    "        memory.append(Transition(\n",
    "            observation=obs,\n",
    "            action=action.item(),\n",
    "            reward=reward.item(),\n",
    "            next_observation=next_obs,\n",
    "            terminal=done,\n",
    "        ))\n",
    "\n",
    "        if done:\n",
    "            obs = torch.as_tensor(env.reset(), dtype=torch.float32)\n",
    "        else:\n",
    "            obs = next_obs\n",
    "\n",
    "        if len(memory) < MEMORY_WARMUP:\n",
    "            pbar.set_description(f\"warmup\")\n",
    "            pbar.total = MEMORY_WARMUP\n",
    "        else:\n",
    "            pbar.total = None\n",
    "            batch = sample_memory(BATCH_SIZE)\n",
    "            with torch.no_grad():\n",
    "                next_state_actions, next_state_log_pi, _ = actor.get_action(batch.next_observations)\n",
    "                qf1_next_target = qf1_target(batch.next_observations, next_state_actions)\n",
    "                qf2_next_target = qf2_target(batch.next_observations, next_state_actions)\n",
    "                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi\n",
    "                next_q_value = batch.rewards.unsqueeze(1) + (((1. - batch.terminals.float()) * GAMMA).unsqueeze(1) * min_qf_next_target)\n",
    "            qf1_a_values = qf1(batch.observations, batch.actions.unsqueeze(1))\n",
    "            qf2_a_values = qf2(batch.observations, batch.actions.unsqueeze(1))\n",
    "            qf1_loss = F.mse_loss(qf1_a_values, next_q_value)\n",
    "            qf2_loss = F.mse_loss(qf2_a_values, next_q_value)\n",
    "            qf_loss = qf1_loss + qf2_loss\n",
    "            \n",
    "            q_optimizer.zero_grad()\n",
    "            qf_loss.backward()\n",
    "            q_optimizer.step()\n",
    "\n",
    "            if step % POLICY_UPDATE_FREQUENCY == 0:\n",
    "                for _ in range(POLICY_UPDATE_FREQUENCY):\n",
    "                    pi, log_pi, _ = actor.get_action(batch.observations)\n",
    "                    qf1_pi = qf1(batch.observations, pi)\n",
    "                    qf2_pi = qf2(batch.observations, pi)\n",
    "                    min_qf_pi = torch.min(qf1_pi, qf2_pi).view(-1)\n",
    "                    actor_loss = (alpha * log_pi - min_qf_pi).mean()\n",
    "\n",
    "                    actor_optimizer.zero_grad()\n",
    "                    actor_loss.backward()\n",
    "                    actor_optimizer.step()\n",
    "\n",
    "                    # autotune\n",
    "                    with torch.no_grad():\n",
    "                        _, log_pi, _ = actor.get_action(batch.observations)\n",
    "                    alpha_loss = (-log_alpha * (log_pi + target_entropy)).mean()\n",
    "\n",
    "                    alpha_optimizer.zero_grad()\n",
    "                    alpha_loss.backward()\n",
    "                    alpha_optimizer.step()\n",
    "                    alpha = log_alpha.exp().item()\n",
    "\n",
    "            if step % TARGET_UPDATE_FREQUENCY == 0:\n",
    "                soft_update(qf1_target, qf1, SOFT_UPDATE_TAU)\n",
    "                soft_update(qf2_target, qf2, SOFT_UPDATE_TAU)\n",
    "\n",
    "            pbar.set_description(f\"reward: {reward.item():.3f}, actor_loss: {actor_loss:.3f}, qf1_loss: {qf1_loss:.3f}, qf2_loss: {qf2_loss:.3f}, alpha: {alpha:.3f}\")\n",
    "        pbar.update()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('subgame')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09b31e4957d63d79e1e76c5537c345194b9f565583fac53cbbb105281d72baf4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
