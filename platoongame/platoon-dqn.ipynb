{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also: [pytorch cartpole DQN](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "from itertools import count\n",
    "import random\n",
    "import math\n",
    "from typing import List, Tuple, Dict, Any, Optional, Callable, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make env\n",
    "import platoonenv\n",
    "from gym.wrappers import TimeLimit # type: ignore\n",
    "\n",
    "# envName = \"CartPole-v0\"\n",
    "# envName = \"MountainCar-v0\"\n",
    "# envName = \"Acrobot-v1\"\n",
    "\n",
    "envName = \"Platoon-v0\"\n",
    "envName = \"Platoon-v1\"\n",
    "# envName = \"Platoon-v2\"\n",
    "\n",
    "def make_env(render_mode: Union[str, None] = None) -> gym.Env[np.ndarray, int]:\n",
    "    env = gym.make(envName, render_mode=render_mode)\n",
    "    return env\n",
    "\n",
    "env = make_env(render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.reset(seed=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = env.reset(seed=1)[0]\n",
    "b = env.reset(seed=1)[0]\n",
    "c = env.reset(seed=2)[0]\n",
    "assert np.array_equal(a.flatten(),b.flatten()), \"env should respect reset seed\"\n",
    "assert envName == \"Platoon-v0\" or not np.array_equal(b,c), \"env should respect reset seed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.action_space)\n",
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.spec.reward_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_shape: Tuple[int,...], hidden_shapes: Tuple[int,...], num_actions: int):\n",
    "        super().__init__()\n",
    "        assert len(hidden_shapes) > 0\n",
    "        net = []\n",
    "        shapes = (math.prod(obs_shape), ) + hidden_shapes\n",
    "        for i in range(1, len(shapes)):\n",
    "            net.append(nn.Linear(shapes[i-1], shapes[i]))\n",
    "            net.append(nn.BatchNorm1d(shapes[i]))\n",
    "            net.append(nn.ReLU())\n",
    "        net.append(nn.Linear(hidden_shapes[-1], num_actions))\n",
    "        self.net = nn.Sequential(*net)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate models\n",
    "assert env.observation_space.shape is not None\n",
    "assert env.action_space.n is not None # type: ignore\n",
    "# HIDDEN_SHAPES = (128,128,128,128)\n",
    "HIDDEN_SHAPES = (512,512,512,512)\n",
    "policy = DQN(\n",
    "    obs_shape = env.observation_space.shape,\n",
    "    hidden_shapes = HIDDEN_SHAPES,\n",
    "    num_actions = env.action_space.n, # type: ignore\n",
    ")\n",
    "policy_target = DQN(\n",
    "    obs_shape = env.observation_space.shape,\n",
    "    hidden_shapes = HIDDEN_SHAPES,\n",
    "    num_actions = env.action_space.n, # type: ignore\n",
    ")\n",
    "policy_target.eval()\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight update functions\n",
    "# from https://github.com/ghliu/pytorch-ddpg/blob/master/util.py#L26\n",
    "def soft_update(target, source, tau):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            ## shouldn't be necessary since we use target networks to calculate loss\n",
    "            # if isinstance(target_param, torch.nn.parameter.UninitializedParameter):\n",
    "            #     # target model uninitialized, hard update\n",
    "            #     target_param.data.copy_(param.data)\n",
    "            # else:\n",
    "            target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "\n",
    "def hard_update(target, source):\n",
    "    target.load_state_dict(source.state_dict())\n",
    "    # for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "    #         target_param.data.copy_(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_update(policy_target, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Transition:\n",
    "    observation: Tensor\n",
    "    action: int\n",
    "    reward: float\n",
    "    next_state: Tensor\n",
    "    finished: bool\n",
    "\n",
    "@dataclass\n",
    "class TransitionBatch:\n",
    "    observations: Tensor\n",
    "    actions: Tensor\n",
    "    rewards: Tensor\n",
    "    next_observations: Tensor\n",
    "    finished: Tensor\n",
    "    indices: Tensor\n",
    "    priorities: Tensor\n",
    "    weights: Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        capacity: int,\n",
    "        alpha: float,\n",
    "        beta: float,\n",
    "        epsilon: float,\n",
    "    ):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon\n",
    "        from sumtree import SumTree\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.transitions: List[Union[None, Transition]] = [None for _ in range(capacity)]\n",
    "        self.current_index = 0\n",
    "        self.len = 0\n",
    "    \n",
    "    def append(self, transition: Transition):\n",
    "        # from the paper: New transitions arrive without a known TD-error,\n",
    "        # so we put them at maximal priority in order to guarantee that \n",
    "        # all experience is seen at least once.\n",
    "        priority = 1000 + self.epsilon\n",
    "        self.tree.update(self.current_index, priority) # type: ignore\n",
    "        self.transitions[self.current_index] = transition\n",
    "        self.current_index = (self.current_index + 1) % self.capacity\n",
    "        self.len = min(self.len + 1, self.capacity)\n",
    "\n",
    "    def update(self, indices: List[int], priorities: List[float]):\n",
    "        # from the paper: The TD-error is updated after each minibatch \n",
    "        # update, and the priorities are updated accordingly.\n",
    "        for i, p in zip(indices, priorities):\n",
    "            self.tree.update(i, (p + self.epsilon) ** self.alpha) # type: ignore\n",
    "\n",
    "    def sample(self, batch_size: int) -> TransitionBatch:\n",
    "        # from the paper:\n",
    "        # The ‘sum-tree’ data structure used here is very similar in spirit to the \n",
    "        # array representation of a binary heap. This provides a efficient way of\n",
    "        # calculating the cumulative sum of priorities, allowing O(logN) \n",
    "        # updates and sampling. To sample a minibatch of size k, the \n",
    "        # range [0; ptotal] is divided equally into k ranges. Next, a \n",
    "        # value is uniformly sampled from each range. Finally the transitions \n",
    "        # that correspond to each of these sampled values are retrieved from \n",
    "        # the tree. Overhead is similar to rank-based prioritization.\n",
    "        indices = []\n",
    "        priorities = []\n",
    "        transitions = []\n",
    "        # split the tree into batch_size+1 segments\n",
    "        space = torch.linspace(0, self.tree.total, batch_size+1).tolist()\n",
    "        # treat the segments as ranges\n",
    "        ranges = torch.as_tensor(list(zip(space[:-1], space[1:])))\n",
    "        # sample a random number in each range\n",
    "        rand = torch.rand(batch_size)\n",
    "        cumsums = (ranges[:,0] + (ranges[:,1]-ranges[:,0])*rand).tolist()\n",
    "        # get the transition corresponding to each random number\n",
    "        for x in cumsums:\n",
    "            index, priority = self.tree.get(x)\n",
    "            trans = self.transitions[index]\n",
    "            if trans is None:\n",
    "                index, priority, trans = indices[0], priorities[0], transitions[0]\n",
    "            indices.append(index)\n",
    "            priorities.append(priority)\n",
    "            # use assert to silence python type warnings\n",
    "            transitions.append(trans)\n",
    "\n",
    "        # from the paper:\n",
    "        # P(i) = p_i^alpha / sum_k{p_k^alpha}\n",
    "        # where p_i = |TD-error| + epsilon\n",
    "        priorities = torch.as_tensor(priorities, dtype=torch.float32)\n",
    "        probabilities = priorities / (self.tree.total)\n",
    "        # Computes the importance sampling (IS) weights for each transition in the batch based on the priorities.\n",
    "        weights = (self.capacity * probabilities) ** (-self.beta)\n",
    "        weights /= weights.max()\n",
    "\n",
    "        return TransitionBatch(\n",
    "            observations=torch.stack([x.observation for x in transitions]),\n",
    "            actions=torch.as_tensor([x.action for x in transitions], dtype=torch.int64),\n",
    "            rewards=torch.as_tensor([x.reward for x in transitions], dtype=torch.float32),\n",
    "            next_observations=torch.stack([x.next_state for x in transitions]),\n",
    "            finished=torch.as_tensor([x.finished for x in transitions], dtype=torch.bool),\n",
    "            indices=torch.as_tensor(indices, dtype=torch.int64),\n",
    "            priorities=torch.as_tensor(priorities, dtype=torch.float32),\n",
    "            weights=weights,\n",
    "        )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DequeReplayBuffer:\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def append(self, transition: Transition):\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    def sample(self, batch_size: int) -> TransitionBatch:\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        return TransitionBatch(\n",
    "            observations=torch.stack([x.observation for x in transitions]),\n",
    "            actions=torch.as_tensor([x.action for x in transitions], dtype=torch.int64),\n",
    "            rewards=torch.as_tensor([x.reward for x in transitions], dtype=torch.float32),\n",
    "            next_observations=torch.stack([x.next_state for x in transitions]),\n",
    "            finished=torch.as_tensor([x.finished for x in transitions], dtype=torch.bool),\n",
    "            indices=None,\n",
    "            priorities=None,\n",
    "            weights=None,\n",
    "        )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEMORY_SIZE = 10000\n",
    "MEMORY_SIZE = 10_000_000\n",
    "# MEMORY_TYPE = \"prioritized\"\n",
    "MEMORY_TYPE = \"deque\"\n",
    "if MEMORY_TYPE == \"prioritized\":\n",
    "    memory = PrioritizedReplayBuffer(\n",
    "        capacity=MEMORY_SIZE,\n",
    "        alpha=0.6,\n",
    "        beta=0.4,\n",
    "        epsilon=0.01,\n",
    "    )\n",
    "elif MEMORY_TYPE == \"deque\":\n",
    "    memory = DequeReplayBuffer(MEMORY_SIZE)\n",
    "else:\n",
    "    raise Exception(\"bad type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exploration_epsilon(steps_done: int) -> float:\n",
    "    EPSILON_START = 0.9\n",
    "    EPSILON_END = 0.05\n",
    "    EPSILON_DECAY = 10000\n",
    "    return EPSILON_END + (EPSILON_START - EPSILON_END) * math.exp(-1. * steps_done / EPSILON_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot epsilon\n",
    "# %matplotlib ipympl\n",
    "%matplotlib inline\n",
    "plt.figure()\n",
    "plt.style.use('dark_background')\n",
    "plt.title(\"exploration epsilon\")\n",
    "plt.plot([get_exploration_epsilon(i) for i in range(1000000) if get_exploration_epsilon(i) >= 0.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(obs: Tensor, epsilon: float) -> int:\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    state_batch = obs.unsqueeze(0)\n",
    "    policy.eval()\n",
    "    with torch.no_grad():\n",
    "        q_values = policy(state_batch)\n",
    "    return int(q_values.argmax(dim=1).squeeze()) # return action with highest q-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(render=False, seed:int=42) -> Tuple[bool, float]:\n",
    "    if render:\n",
    "        env = make_env(render_mode=\"human\")\n",
    "    else:\n",
    "        env = make_env()\n",
    "    assert env.spec.reward_threshold is not None\n",
    "    obs, info = env.reset(seed=seed)\n",
    "    obs = torch.as_tensor(obs, dtype=torch.float32)\n",
    "    episode_reward = 0\n",
    "    for i in count():\n",
    "        action = get_action(obs, 0)\n",
    "        obs, reward, done, trunc, info = env.step(action)\n",
    "        obs = torch.as_tensor(obs, dtype=torch.float32)\n",
    "        episode_reward += reward\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done or trunc:\n",
    "            break\n",
    "    return episode_reward >= env.spec.reward_threshold, episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not test()[0], \"untrained model should probably not be able to solve the environment\"\n",
    "assert np.array_equal(test()[1], test()[1]), \"tests should be performed with the same seed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step metrics\n",
    "loss_history: List[float] = []\n",
    "learning_rate_history: List[float] = []\n",
    "reward_history: List[float] = []\n",
    "duration_history: List[int] = []\n",
    "action_history: List[int] = []\n",
    "terminal_history: List[bool] = []\n",
    "# other metrics\n",
    "test_reward_history: List[Tuple[int, float, float]] = []\n",
    "last_episode_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(policy.parameters(), lr=0.001)\n",
    "# optimizer = torch.optim.RMSprop(policy.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    # mode=\"max\",\n",
    "    mode=\"min\",\n",
    "    factor=0.9,\n",
    "    patience=5000,\n",
    "    cooldown=5000,\n",
    "    min_lr=0.00001,\n",
    "    verbose=True,\n",
    ")\n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext scalene\n",
    "# use %%scalene to profile a cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop\n",
    "TRAIN_EPISODES = 200000\n",
    "# TRAIN_EPISODES = 100000\n",
    "solved = False\n",
    "with tqdm(total=TRAIN_EPISODES, dynamic_ncols=True, ascii=True) as pbar:\n",
    "    for episode in range(TRAIN_EPISODES):\n",
    "        if solved: break\n",
    "        state = torch.as_tensor(env.reset(seed=random.randint(0,100000))[0], dtype=torch.float32)\n",
    "        episode_reward = 0\n",
    "        for episode_step in count():\n",
    "            epsilon = get_exploration_epsilon(steps_done)\n",
    "            #region exploration\n",
    "            action = get_action(state, epsilon)\n",
    "            action_history.append(action)\n",
    "            next_state, reward, done, trunc, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            reward_history.append(reward)\n",
    "            terminal_history.append(done)\n",
    "            if done or trunc:\n",
    "                duration_history.append(episode_step)\n",
    "                next_state = torch.zeros(state.shape)\n",
    "            else:\n",
    "                next_state = torch.as_tensor(next_state, dtype=torch.float32)                \n",
    "            memory.append(Transition(\n",
    "                observation=state,\n",
    "                action=action,\n",
    "                reward=reward,\n",
    "                next_state=next_state,\n",
    "                finished=done or trunc,\n",
    "            ))\n",
    "            state = next_state\n",
    "\n",
    "            PREVIEW_EPISODE_INTERVAL = 30\n",
    "            if episode % PREVIEW_EPISODE_INTERVAL == 0:\n",
    "                env.render()\n",
    "            #endregion exploration\n",
    "\n",
    "            #region training\n",
    "            BATCH_SIZE = 128\n",
    "            if len(memory) < BATCH_SIZE+4:\n",
    "                pbar.set_description(f\"warmup\")\n",
    "            else:\n",
    "                policy.train()\n",
    "                batch = memory.sample(BATCH_SIZE)\n",
    "\n",
    "                # calculate q values for the actions that were taken\n",
    "                q_pred = policy(batch.observations).gather(1, batch.actions.unsqueeze(1))\n",
    "\n",
    "                # calculate q values for next state\n",
    "                q_next = torch.zeros(BATCH_SIZE)\n",
    "                non_final = ~batch.finished\n",
    "                q_next[non_final] = policy_target(batch.next_observations[non_final]).max(dim=1).values.detach()\n",
    "\n",
    "                # calculate expected q values\n",
    "                REWARD_GAMMA = 0.99\n",
    "                q_expected = ((q_next * REWARD_GAMMA) + batch.rewards).unsqueeze(1)\n",
    "\n",
    "                # calculate loss\n",
    "                # criterion = torch.nn.SmoothL1Loss()\n",
    "                # criterion = torch.nn.MSELoss()\n",
    "                policy_loss = F.mse_loss(q_pred, q_expected)\n",
    "\n",
    "                if isinstance(memory, PrioritizedReplayBuffer):\n",
    "                    td_error = (q_expected - q_pred).abs().detach().flatten().tolist()\n",
    "                    indices = batch.indices.tolist()\n",
    "                    # update replay buffer priorities\n",
    "                    memory.update(indices, td_error)\n",
    "                    # scale loss by weights\n",
    "                    policy_loss = (batch.weights * policy_loss).mean()\n",
    "\n",
    "\n",
    "                loss_history.append(policy_loss.item())\n",
    "\n",
    "                # apply weight update\n",
    "                optimizer.zero_grad()\n",
    "                policy_loss.backward()\n",
    "                for param in policy.parameters():\n",
    "                    assert param.grad is not None\n",
    "                    param.grad.data.clamp_(-1, 1) \n",
    "                optimizer.step()\n",
    "                \n",
    "                # update learning rate\n",
    "                lr = optimizer.param_groups[0][\"lr\"]\n",
    "                learning_rate_history.append(lr)\n",
    "                scheduler.step(policy_loss)\n",
    "\n",
    "                ## update target network\n",
    "                \n",
    "                SOFT_UPDATE_TAU = 0.001  \n",
    "                # soft_update(policy_target, policy, SOFT_UPDATE_TAU)\n",
    "                POLICY_TARGET_UPDATE_INTERVAL = 200 \n",
    "                if steps_done % POLICY_TARGET_UPDATE_INTERVAL == 0:\n",
    "                    hard_update(policy_target, policy)\n",
    "\n",
    "                #region testing\n",
    "                TEST_INTERVAL = 500\n",
    "                if steps_done % TEST_INTERVAL == 0:\n",
    "                    tests = list(zip(*[test() for _ in range(5)]))\n",
    "                    test_passes: List[bool] = tests[0] # type: ignore :P\n",
    "                    test_rewards: List[float] = tests[1] # type: ignore :P\n",
    "                    passed = all(test_passes)\n",
    "                    test_reward_mean = np.mean(test_rewards).item()\n",
    "                    test_reward_variance = max(max(test_rewards) - test_reward_mean, test_reward_mean - min(test_rewards))\n",
    "\n",
    "                    test_reward_history.append((steps_done, test_reward_mean, test_reward_variance))\n",
    "\n",
    "                    best = max(test_reward_history, key=lambda x: x[1] - x[2])\n",
    "\n",
    "                    test_reward = f\"{test_reward_mean:.3f} \\u00b1 {test_reward_variance:.3f}\"\n",
    "                    best_reward = f\"{best[1]:.3f} \\u00b1 {best[2]:.3f}\"\n",
    "                    print(f\"test reward: {test_reward} (best: {best_reward}, goal: {env.spec.reward_threshold})\")\n",
    "                    last_test_reward = test_reward\n",
    "                    if passed:\n",
    "                        print(\"solved!\")\n",
    "                        solved=True\n",
    "                        break\n",
    "                #endregion testing\n",
    "\n",
    "                pbar.set_description(f\"policy: {policy_loss.item():09.3f}, reward: {reward:+07.3f} (last episode: {last_episode_reward:.3f}), epsilon: {epsilon:.3f}, lr: {lr:.7f}, steps: {steps_done}\")\n",
    "            #endregion training\n",
    "\n",
    "            steps_done += 1\n",
    "\n",
    "            if done or trunc: break\n",
    "\n",
    "        last_episode_reward = episode_reward\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = memory.sample(10)\n",
    "# print(batch.actions)\n",
    "# print(batch.rewards)\n",
    "# print(batch.indices)\n",
    "print(batch.priorities)\n",
    "print(batch.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot rewards\n",
    "plt.figure()\n",
    "plt.plot([x[1]-x[2] for x in test_reward_history])\n",
    "plt.title(\"Test rewards\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot durations\n",
    "plt.figure()\n",
    "plt.plot(duration_history)\n",
    "plt.title(\"Episode durations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss\n",
    "fig, axs = plt.subplots(3)\n",
    "plt.subplots_adjust(\n",
    "    left=0.1,\n",
    "    bottom=0.1,\n",
    "    right=0.9,\n",
    "    top=0.9,\n",
    "    # wspace=0.4,\n",
    "    hspace=0.4,\n",
    ")\n",
    "fig.suptitle(\"Loss\")\n",
    "axs[0].plot(loss_history)\n",
    "axs[1].plot(range(len(loss_history))[-300:], loss_history[-300:])\n",
    "axs[2].hist(np.log10(loss_history))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate actions\n",
    "import ipywidgets as widgets\n",
    "plt.figure()\n",
    "# @widgets.interact(i=(0, len(action_history)-1), window_size=(1, len(action_history)-1))\n",
    "@widgets.interact(i=(0, len(action_history)-1), window_size=(1, 1000))\n",
    "def preview_actions(i=len(action_history)-100, window_size=72):\n",
    "    x = torch.zeros((len(action_history), env.action_space.n + 1))\n",
    "    x[range(len(action_history)), action_history] = 1\n",
    "    x[:, 2] = torch.as_tensor(terminal_history, dtype=torch.bool)\n",
    "    plt.imshow(x[i:i+window_size], aspect=\"auto\")\n",
    "    plt.show()\n",
    "    del x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview\n",
    "epsilon = 0\n",
    "for episode in range(5):\n",
    "    success, reward = test(render=True)\n",
    "    print(f\"success={success}, reward={reward}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- security metrics\n",
    "    - investigate paper: \"Deep Reinforcement Learning-Based Defense Strategy Selection\"\n",
    "- why DQN over other algorithms; does it reflect our real world scenario\n",
    "- plan: results early-mid january"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subgame",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09b31e4957d63d79e1e76c5537c345194b9f565583fac53cbbb105281d72baf4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
