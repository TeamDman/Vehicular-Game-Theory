{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.wrappers\n",
    "import tianshou as ts\n",
    "import torch\n",
    "import numpy as np\n",
    "import platoonenv\n",
    "from typing import Union\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TeamD\\.conda\\envs\\tianshou\\lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment Platoon-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# envName = \"CartPole-v0\"\n",
    "# envName = \"MountainCar-v0\"\n",
    "# envName = \"Acrobot-v1\"\n",
    "envName = \"Platoon-v0\"\n",
    "def make_env(render_mode: Union[str, None] = None):\n",
    "    env = gym.make(envName, render_mode=render_mode)\n",
    "    return env\n",
    "env = make_env(render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "obs_shape = env.observation_space.shape or env.observation_space.n\n",
    "action_shape = env.action_space.shape or env.action_space.n\n",
    "print(obs_shape)\n",
    "print(action_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done after 10 steps with -74 reward\n",
      "done after 10 steps with -62 reward\n",
      "done after 10 steps with -69 reward\n",
      "done after 10 steps with -68 reward\n",
      "done after 10 steps with -70 reward\n",
      "4 episodes done\n"
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "from time import sleep\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    reward = 0\n",
    "    for j in count():\n",
    "        env.render()\n",
    "        # sleep(1)\n",
    "        obs, rew, done, trunc, info = env.step(env.action_space.sample())\n",
    "        reward += rew\n",
    "        if trunc or done:\n",
    "            env.reset()\n",
    "            print(f\"done after {j} steps with {reward} reward\")\n",
    "            break\n",
    "print(i, \"episodes done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action 0 reward -9 obs [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action 1 reward -8 obs [1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action 2 reward -7 obs [1. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action 3 reward -6 obs [1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action 4 reward -5 obs [1. 1. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      "action 5 reward -4 obs [1. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      "action 6 reward -3 obs [1. 1. 1. 1. 1. 1. 1. 0. 0. 0.]\n",
      "action 7 reward -2 obs [1. 1. 1. 1. 1. 1. 1. 1. 0. 0.]\n",
      "action 8 reward -1 obs [1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
      "action 9 reward 0 obs [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "action 10 reward 0 obs [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Total reward was -45\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "reward = 0\n",
    "for i in range(env.action_space.n):\n",
    "    obs, rew, done, trunc, info = env.step(i)\n",
    "    reward += rew\n",
    "    print(f\"action {i} reward {rew} obs {obs}\")\n",
    "print(f\"Total reward was {reward}\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([9131., 8993., 9029., 8917., 9174., 9240., 9150., 9192., 8979.,\n",
       "        9152., 9043.]),\n",
       " array([ 0.        ,  0.90909091,  1.81818182,  2.72727273,  3.63636364,\n",
       "         4.54545455,  5.45454545,  6.36363636,  7.27272727,  8.18181818,\n",
       "         9.09090909, 10.        ]),\n",
       " <BarContainer object of 11 artists>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist([env.action_space.sample() for _ in range(100000)], bins=env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TeamD\\.conda\\envs\\tianshou\\lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment Platoon-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "train_envs = ts.env.DummyVectorEnv([lambda: make_env() for _ in range(10)])\n",
    "test_envs = ts.env.DummyVectorEnv([lambda: make_env() for _ in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=15\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "train_envs.seed(seed)\n",
    "test_envs.seed(seed)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.utils.net.common import Net\n",
    "from tianshou.utils.net.discrete import Actor, Critic\n",
    "# hidden_sizes=(128,128,128,128)\n",
    "hidden_sizes=(12,12,12)\n",
    "net = Net(obs_shape, hidden_sizes=hidden_sizes)\n",
    "actor = Actor(net, action_shape, softmax_output=True)\n",
    "actor_optim = torch.optim.Adam(actor.parameters(), lr=1e-4)\n",
    "net_c1 = Net(obs_shape, hidden_sizes=hidden_sizes)\n",
    "critic1 = Critic(net_c1, last_size=action_shape)\n",
    "critic1_optim = torch.optim.Adam(critic1.parameters(), lr=1e-3)\n",
    "net_c2 = Net(obs_shape, hidden_sizes=hidden_sizes)\n",
    "critic2 = Critic(net_c2, last_size=action_shape)\n",
    "critic2_optim = torch.optim.Adam(critic2.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy = ts.policy.DQNPolicy(net, optim, discount_factor=0.9, estimation_step=3, target_update_freq=320)\n",
    "alpha = 0.05\n",
    "alpha_lr = 3e-4\n",
    "# auto_alpha = True\n",
    "auto_alpha = False\n",
    "if auto_alpha:\n",
    "    target_entropy = 0.98 * np.log(np.prod(action_shape))\n",
    "    log_alpha = torch.zeros(1, requires_grad=True)\n",
    "    alpha_optim = torch.optim.Adam([log_alpha], lr=alpha_lr)\n",
    "    alpha = (target_entropy, log_alpha, alpha_optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.exploration import OUNoise\n",
    "policy = ts.policy.DiscreteSACPolicy(\n",
    "    actor, actor_optim,\n",
    "    critic1, critic1_optim,\n",
    "    critic2, critic2_optim,\n",
    "    0.005, # tau\n",
    "    0.95, # gamma\n",
    "    alpha,\n",
    "    estimation_step = 3,\n",
    "    reward_normalization = False,\n",
    "    # exploration_noise=OUNoise(0.0, 1.2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.data import VectorReplayBuffer\n",
    "train_collector = ts.data.Collector(policy, train_envs, VectorReplayBuffer(20000, 10), exploration_noise=True)\n",
    "test_collector = ts.data.Collector(policy, test_envs, exploration_noise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_collector.collect(n_step=5000, random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: -45\n"
     ]
    }
   ],
   "source": [
    "threshold = env.spec.reward_threshold or -45\n",
    "print(\"Threshold:\", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.utils import TensorboardLogger\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "log_path = os.path.join(\"log\", envName, 'tianplatoon.ipynb')\n",
    "writer = SummaryWriter(log_path)\n",
    "logger = TensorboardLogger(writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 10001it [00:13, 735.48it/s, env_step=10000, len=11, loss/actor=49.175, loss/critic1=23.803, loss/critic2=23.868, n/ep=0, n/st=10, rew=-77.00]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: -94.000000 ± 0.000000, best_reward: -94.000000 ± 0.000000 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 10001it [00:13, 737.26it/s, env_step=20000, len=11, loss/actor=76.886, loss/critic1=37.235, loss/critic2=37.238, n/ep=0, n/st=10, rew=-75.80]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: -104.000000 ± 0.000000, best_reward: -94.000000 ± 0.000000 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3:   4%|4         | 450/10000 [00:00<00:13, 701.51it/s, env_step=20440, len=11, loss/actor=77.654, loss/critic1=37.382, loss/critic2=37.454, n/ep=0, n/st=10, rew=-78.20] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11780/2134205474.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m result = ts.trainer.offpolicy_trainer(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtrain_collector\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtest_collector\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmax_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\tianshou\\lib\\site-packages\\tianshou\\trainer\\offpolicy.py\u001b[0m in \u001b[0;36moffpolicy_trainer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSee\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mtianshou\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather_info\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m     \"\"\"\n\u001b[1;32m--> 133\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mOffpolicyTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\tianshou\\lib\\site-packages\\tianshou\\trainer\\base.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    439\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_run\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m             \u001b[0mdeque\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# feed the entire iterator into a zero-length deque\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m             info = gather_info(\n\u001b[0;32m    443\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_collector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_collector\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\tianshou\\lib\\site-packages\\tianshou\\trainer\\base.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    297\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_update_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m                 \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\tianshou\\lib\\site-packages\\tianshou\\trainer\\offpolicy.py\u001b[0m in \u001b[0;36mpolicy_update_fn\u001b[1;34m(self, data, result)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_per_step\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"n/st\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient_step\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m             \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_collector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_update_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\tianshou\\lib\\site-packages\\tianshou\\policy\\base.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, sample_size, buffer, **kwargs)\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost_process_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\tianshou\\lib\\site-packages\\tianshou\\policy\\modelfree\\ddpg.py\u001b[0m in \u001b[0;36mprocess_fn\u001b[1;34m(self, batch, buffer, indices)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mBatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mReplayBuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     ) -> Batch:\n\u001b[1;32m--> 114\u001b[1;33m         batch = self.compute_nstep_return(\n\u001b[0m\u001b[0;32m    115\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_target_q\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rew_norm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\tianshou\\lib\\site-packages\\tianshou\\policy\\base.py\u001b[0m in \u001b[0;36mcompute_nstep_return\u001b[1;34m(batch, buffer, indice, target_q_fn, gamma, n_step, rew_norm)\u001b[0m\n\u001b[0;32m    391\u001b[0m             \u001b[0mtarget_q_torch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_q_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (bsz, ?)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m         \u001b[0mtarget_q\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_q_torch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 393\u001b[1;33m         \u001b[0mtarget_q\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_q\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mBasePolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    394\u001b[0m         \u001b[0mend_flag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m         \u001b[0mend_flag\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munfinished_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\tianshou\\lib\\site-packages\\tianshou\\policy\\base.py\u001b[0m in \u001b[0;36mvalue_mask\u001b[1;34m(buffer, indices)\u001b[0m\n\u001b[0;32m    300\u001b[0m             \u001b[1;34m\"obs_next\"\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m         \"\"\"\n\u001b[1;32m--> 302\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m~\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mterminated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = ts.trainer.offpolicy_trainer(\n",
    "    policy,\n",
    "    train_collector,\n",
    "    test_collector,\n",
    "    max_epoch=100,\n",
    "    step_per_epoch=10000,\n",
    "    step_per_collect=10,\n",
    "    # update_per_step=1,\n",
    "    update_per_step=0.1,\n",
    "    test_in_train=False,\n",
    "    episode_per_test=100,\n",
    "    batch_size=640,\n",
    "    stop_fn=lambda mean_rewards: mean_rewards >= threshold,\n",
    "    logger=logger\n",
    ")\n",
    "print(f\"Finished training! Use {result['duration']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'duration': '1425.46s',\n",
       " 'train_time/model': '1082.17s',\n",
       " 'test_step': 101000,\n",
       " 'test_episode': 10100,\n",
       " 'test_time': '22.15s',\n",
       " 'test_speed': '4559.27 step/s',\n",
       " 'best_reward': -63.0,\n",
       " 'best_result': '-63.00 ± 0.00',\n",
       " 'train_step': 1000000,\n",
       " 'train_episode': 100000,\n",
       " 'train_time/collector': '321.14s',\n",
       " 'train_speed': '712.60 step/s'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n/ep': 10,\n",
       " 'n/st': 100,\n",
       " 'rews': array([-63., -63., -63., -63., -63., -63., -63., -63., -63., -63.]),\n",
       " 'lens': array([10, 10, 10, 10, 10, 10, 10, 10, 10, 10]),\n",
       " 'idxs': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'rew': -63.0,\n",
       " 'len': 10.0,\n",
       " 'rew_std': 0.0,\n",
       " 'len_std': 0.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.eval()\n",
    "# policy.set_eps(0.05)\n",
    "collector = ts.data.Collector(policy, ts.env.DummyVectorEnv([lambda: make_env(\"human\")]), exploration_noise=False)\n",
    "collector.collect(n_episode=10, render=1 / 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tianshou",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71d9f79eb180b017341f284521d8f1f73000b55d56576dd42cfa51175f09c3a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
