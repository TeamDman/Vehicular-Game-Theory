{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.wrappers\n",
    "import tianshou as ts\n",
    "import torch\n",
    "import numpy as np\n",
    "import platoonenv\n",
    "from typing import Union\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# envName = \"CartPole-v0\"\n",
    "# envName = \"MountainCar-v0\"\n",
    "# envName = \"Acrobot-v1\"\n",
    "envName = \"Platoon-v0\"\n",
    "params=platoonenv.PlatoonEnvParams(\n",
    "    num_vehicles=10,\n",
    "    num_vulns=4,\n",
    "    num_attack=1,\n",
    "    attack_interval=600\n",
    ")\n",
    "def make_env(render_mode: Union[str, None] = None):\n",
    "    if envName == \"Platoon-v0\":\n",
    "        env = gym.make(envName, render_mode=render_mode, params=params)\n",
    "        # env = gym.wrappers.TimeLimit(env, max_episode_steps=100)\n",
    "        env = gym.wrappers.TimeLimit(env, max_episode_steps=10)\n",
    "    else:\n",
    "        env = gym.make(envName, render_mode=render_mode)\n",
    "    return env\n",
    "env = make_env(render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "obs_shape = env.observation_space.shape or env.observation_space.n\n",
    "action_shape = env.action_space.shape or env.action_space.n\n",
    "print(obs_shape)\n",
    "print(action_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done after 9 steps with -73 reward\n",
      "done after 9 steps with -75 reward\n",
      "done after 9 steps with -76 reward\n",
      "done after 9 steps with -66 reward\n",
      "done after 9 steps with -78 reward\n",
      "4 episodes done\n"
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "from time import sleep\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    reward = 0\n",
    "    for j in count():\n",
    "        env.render()\n",
    "        # sleep(1)\n",
    "        obs, rew, done, trunc, info = env.step(env.action_space.sample())\n",
    "        reward += rew\n",
    "        if trunc or done:\n",
    "            env.reset()\n",
    "            print(f\"done after {j} steps with {reward} reward\")\n",
    "            break\n",
    "print(i, \"episodes done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 1. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      "[1. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 0. 0. 0.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 0. 0.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for i in range(env.action_space.n):\n",
    "    print(env.step(i)[0])\n",
    "    \n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([923., 907., 959., 941., 873., 902., 847., 895., 901., 887., 965.]),\n",
       " array([ 0.        ,  0.90909091,  1.81818182,  2.72727273,  3.63636364,\n",
       "         4.54545455,  5.45454545,  6.36363636,  7.27272727,  8.18181818,\n",
       "         9.09090909, 10.        ]),\n",
       " <BarContainer object of 11 artists>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist([env.action_space.sample() for _ in range(10000)], bins=env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_envs = ts.env.DummyVectorEnv([lambda: make_env() for _ in range(10)])\n",
    "test_envs = ts.env.DummyVectorEnv([lambda: make_env() for _ in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=12\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "train_envs.seed(seed)\n",
    "test_envs.seed(seed)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.utils.net.common import Net\n",
    "from tianshou.utils.net.discrete import Actor, Critic\n",
    "hidden_sizes=(128,128,128,128)\n",
    "net = Net(obs_shape, hidden_sizes=hidden_sizes)\n",
    "actor = Actor(net, action_shape, softmax_output=True)\n",
    "actor_optim = torch.optim.Adam(actor.parameters(), lr=1e-4)\n",
    "net_c1 = Net(obs_shape, hidden_sizes=hidden_sizes)\n",
    "critic1 = Critic(net_c1, last_size=action_shape)\n",
    "critic1_optim = torch.optim.Adam(critic1.parameters(), lr=1e-3)\n",
    "net_c2 = Net(obs_shape, hidden_sizes=hidden_sizes)\n",
    "critic2 = Critic(net_c2, last_size=action_shape)\n",
    "critic2_optim = torch.optim.Adam(critic2.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy = ts.policy.DQNPolicy(net, optim, discount_factor=0.9, estimation_step=3, target_update_freq=320)\n",
    "alpha = 0.05\n",
    "alpha_lr = 3e-4\n",
    "auto_alpha = True\n",
    "if auto_alpha:\n",
    "    target_entropy = 0.98 * np.log(np.prod(action_shape))\n",
    "    log_alpha = torch.zeros(1, requires_grad=True)\n",
    "    alpha_optim = torch.optim.Adam([log_alpha], lr=alpha_lr)\n",
    "    alpha = (target_entropy, log_alpha, alpha_optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.exploration import OUNoise\n",
    "policy = ts.policy.DiscreteSACPolicy(\n",
    "    actor, actor_optim,\n",
    "    critic1, critic1_optim,\n",
    "    critic2, critic2_optim,\n",
    "    0.005, # tau\n",
    "    0.95, # gamma\n",
    "    alpha,\n",
    "    estimation_step = 3,\n",
    "    reward_normalization = False,\n",
    "    # exploration_noise=OUNoise(0.0, 1.2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.data import VectorReplayBuffer\n",
    "train_collector = ts.data.Collector(policy, train_envs, VectorReplayBuffer(20000, 10), exploration_noise=True)\n",
    "test_collector = ts.data.Collector(policy, test_envs, exploration_noise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_collector.collect(n_step=5000, random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 45\n"
     ]
    }
   ],
   "source": [
    "threshold = env.spec.reward_threshold or sum(range(params.num_vehicles))\n",
    "print(\"Threshold:\", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.utils import TensorboardLogger\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "log_path = os.path.join(\"log\", envName, 'tianplatoon.ipynb')\n",
    "writer = SummaryWriter(log_path)\n",
    "logger = TensorboardLogger(writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1:  49%|####8     | 4880/10000 [01:01<01:04, 79.86it/s, alpha=1.380, env_step=4870, len=10, loss/actor=86.219, loss/alpha=-0.003, loss/critic1=29.322, loss/critic2=29.411, n/ep=0, n/st=10, rew=-69.50] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5412/1280112869.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m result = ts.trainer.offpolicy_trainer(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtrain_collector\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtest_collector\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmax_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\subgame\\lib\\site-packages\\tianshou\\trainer\\offpolicy.py\u001b[0m in \u001b[0;36moffpolicy_trainer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSee\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mtianshou\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather_info\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m     \"\"\"\n\u001b[1;32m--> 133\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mOffpolicyTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\subgame\\lib\\site-packages\\tianshou\\trainer\\base.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    438\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_run\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m             \u001b[0mdeque\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# feed the entire iterator into a zero-length deque\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m             info = gather_info(\n\u001b[0;32m    442\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_collector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_collector\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\subgame\\lib\\site-packages\\tianshou\\trainer\\base.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    297\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_update_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m                 \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\subgame\\lib\\site-packages\\tianshou\\trainer\\offpolicy.py\u001b[0m in \u001b[0;36mpolicy_update_fn\u001b[1;34m(self, data, result)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_per_step\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"n/st\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient_step\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m             \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_collector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_update_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\subgame\\lib\\site-packages\\tianshou\\policy\\base.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, sample_size, buffer, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost_process_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\subgame\\lib\\site-packages\\tianshou\\policy\\modelfree\\discrete_sac.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, batch, **kwargs)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mcurrent_q1a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[0mcurrent_q2a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m             \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_q1a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_q2a\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0mactor_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_alpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mentropy\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobs\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\subgame\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\subgame\\lib\\site-packages\\tianshou\\utils\\net\\discrete.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, obs, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m     ) -> torch.Tensor:\n\u001b[0;32m    119\u001b[0m         \u001b[1;34m\"\"\"Mapping: s -> V(s).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"state\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\subgame\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\subgame\\lib\\site-packages\\tianshou\\utils\\net\\common.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, obs, state, info)\u001b[0m\n\u001b[0;32m    208\u001b[0m     ) -> Tuple[torch.Tensor, Any]:\n\u001b[0;32m    209\u001b[0m         \u001b[1;34m\"\"\"Mapping: obs -> flatten (inside MLP)-> logits.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m         \u001b[0mbsz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_dueling\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Dueling DQN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\subgame\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\subgame\\lib\\site-packages\\tianshou\\utils\\net\\common.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\subgame\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\subgame\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\subgame\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\subgame\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\subgame\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1455\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1456\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1457\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1458\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = ts.trainer.offpolicy_trainer(\n",
    "    policy,\n",
    "    train_collector,\n",
    "    test_collector,\n",
    "    max_epoch=100,\n",
    "    step_per_epoch=10000,\n",
    "    step_per_collect=10,\n",
    "    # update_per_step=1,\n",
    "    update_per_step=0.1,\n",
    "    episode_per_test=100,\n",
    "    batch_size=64,\n",
    "    stop_fn=lambda mean_rewards: mean_rewards >= threshold,\n",
    "    logger=logger\n",
    ")\n",
    "print(f\"Finished training! Use {result['duration']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'duration': '15.90s',\n",
       " 'train_time/model': '13.68s',\n",
       " 'test_step': 20734,\n",
       " 'test_episode': 200,\n",
       " 'test_time': '0.76s',\n",
       " 'test_speed': '27341.87 step/s',\n",
       " 'best_reward': 197.99,\n",
       " 'best_result': '197.99 Â± 5.92',\n",
       " 'train_step': 9730,\n",
       " 'train_episode': 293,\n",
       " 'train_time/collector': '1.47s',\n",
       " 'train_speed': '642.51 step/s'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n/ep': 10,\n",
       " 'n/st': 100,\n",
       " 'rews': array([-57., -57., -57., -57., -57., -57., -57., -57., -57., -57.]),\n",
       " 'lens': array([10, 10, 10, 10, 10, 10, 10, 10, 10, 10]),\n",
       " 'idxs': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'rew': -57.0,\n",
       " 'len': 10.0,\n",
       " 'rew_std': 0.0,\n",
       " 'len_std': 0.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.eval()\n",
    "# policy.set_eps(0.05)\n",
    "collector = ts.data.Collector(policy, ts.env.DummyVectorEnv([lambda: make_env(\"human\")]), exploration_noise=False)\n",
    "collector.collect(n_episode=10, render=1 / 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('subgame')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09b31e4957d63d79e1e76c5537c345194b9f565583fac53cbbb105281d72baf4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
